I0612 12:22:02.793528      17 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-652313554
I0612 12:22:02.793695      17 e2e.go:240] Starting e2e run "ae955d68-8d0c-11e9-b925-1ab852558ec8" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1560342121 - Will randomize all specs
Will run 204 of 3585 specs

Jun 12 12:22:02.926: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 12:22:02.928: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 12 12:22:02.944: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 12 12:22:02.972: INFO: 40 / 40 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 12 12:22:02.972: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
Jun 12 12:22:02.972: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 12 12:22:02.978: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cloud-controller-manager' (0 seconds elapsed)
Jun 12 12:22:02.978: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'flexvolume' (0 seconds elapsed)
Jun 12 12:22:02.978: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
Jun 12 12:22:02.978: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-master' (0 seconds elapsed)
Jun 12 12:22:02.978: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker' (0 seconds elapsed)
Jun 12 12:22:02.978: INFO: e2e test version: v1.14.3
Jun 12 12:22:02.979: INFO: kube-apiserver version: v1.14.3-aliyun.1
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:22:02.979: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename svcaccounts
Jun 12 12:22:03.093: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Jun 12 12:22:03.627: INFO: created pod pod-service-account-defaultsa
Jun 12 12:22:03.627: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 12 12:22:03.638: INFO: created pod pod-service-account-mountsa
Jun 12 12:22:03.638: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 12 12:22:03.648: INFO: created pod pod-service-account-nomountsa
Jun 12 12:22:03.648: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 12 12:22:03.655: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 12 12:22:03.655: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 12 12:22:03.667: INFO: created pod pod-service-account-mountsa-mountspec
Jun 12 12:22:03.667: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 12 12:22:03.672: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 12 12:22:03.672: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 12 12:22:03.678: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 12 12:22:03.679: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 12 12:22:03.687: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 12 12:22:03.687: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 12 12:22:03.696: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 12 12:22:03.696: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:22:03.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4929" for this suite.
Jun 12 12:22:25.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:22:25.809: INFO: namespace svcaccounts-4929 deletion completed in 22.109479529s

• [SLOW TEST:22.830 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:22:25.809: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Jun 12 12:22:25.839: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-652313554 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:22:25.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4888" for this suite.
Jun 12 12:22:31.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:22:31.981: INFO: namespace kubectl-4888 deletion completed in 6.082317843s

• [SLOW TEST:6.171 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:22:31.981: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 12 12:22:32.029: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c07f5b3a-8d0c-11e9-b925-1ab852558ec8" in namespace "downward-api-2863" to be "success or failure"
Jun 12 12:22:32.034: INFO: Pod "downwardapi-volume-c07f5b3a-8d0c-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.33778ms
Jun 12 12:22:34.038: INFO: Pod "downwardapi-volume-c07f5b3a-8d0c-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009298489s
Jun 12 12:22:36.042: INFO: Pod "downwardapi-volume-c07f5b3a-8d0c-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013264325s
STEP: Saw pod success
Jun 12 12:22:36.042: INFO: Pod "downwardapi-volume-c07f5b3a-8d0c-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:22:36.045: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod downwardapi-volume-c07f5b3a-8d0c-11e9-b925-1ab852558ec8 container client-container: <nil>
STEP: delete the pod
Jun 12 12:22:36.075: INFO: Waiting for pod downwardapi-volume-c07f5b3a-8d0c-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:22:36.077: INFO: Pod downwardapi-volume-c07f5b3a-8d0c-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:22:36.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2863" for this suite.
Jun 12 12:22:42.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:22:42.162: INFO: namespace downward-api-2863 deletion completed in 6.082217284s

• [SLOW TEST:10.181 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:22:42.162: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Jun 12 12:22:42.215: INFO: Waiting up to 5m0s for pod "var-expansion-c691aafe-8d0c-11e9-b925-1ab852558ec8" in namespace "var-expansion-1777" to be "success or failure"
Jun 12 12:22:42.219: INFO: Pod "var-expansion-c691aafe-8d0c-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.447638ms
Jun 12 12:22:44.222: INFO: Pod "var-expansion-c691aafe-8d0c-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007068914s
Jun 12 12:22:46.227: INFO: Pod "var-expansion-c691aafe-8d0c-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011329792s
Jun 12 12:22:48.230: INFO: Pod "var-expansion-c691aafe-8d0c-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014213963s
Jun 12 12:22:50.233: INFO: Pod "var-expansion-c691aafe-8d0c-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018091047s
STEP: Saw pod success
Jun 12 12:22:50.233: INFO: Pod "var-expansion-c691aafe-8d0c-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:22:50.236: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod var-expansion-c691aafe-8d0c-11e9-b925-1ab852558ec8 container dapi-container: <nil>
STEP: delete the pod
Jun 12 12:22:50.260: INFO: Waiting for pod var-expansion-c691aafe-8d0c-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:22:50.262: INFO: Pod var-expansion-c691aafe-8d0c-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:22:50.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1777" for this suite.
Jun 12 12:22:56.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:22:56.341: INFO: namespace var-expansion-1777 deletion completed in 6.076503289s

• [SLOW TEST:14.179 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:22:56.342: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 12 12:22:56.385: INFO: Waiting up to 5m0s for pod "pod-cf02c6a2-8d0c-11e9-b925-1ab852558ec8" in namespace "emptydir-3292" to be "success or failure"
Jun 12 12:22:56.387: INFO: Pod "pod-cf02c6a2-8d0c-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.20305ms
Jun 12 12:22:58.390: INFO: Pod "pod-cf02c6a2-8d0c-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005566118s
Jun 12 12:23:00.394: INFO: Pod "pod-cf02c6a2-8d0c-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009132076s
STEP: Saw pod success
Jun 12 12:23:00.394: INFO: Pod "pod-cf02c6a2-8d0c-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:23:00.397: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-cf02c6a2-8d0c-11e9-b925-1ab852558ec8 container test-container: <nil>
STEP: delete the pod
Jun 12 12:23:00.416: INFO: Waiting for pod pod-cf02c6a2-8d0c-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:23:00.419: INFO: Pod pod-cf02c6a2-8d0c-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:23:00.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3292" for this suite.
Jun 12 12:23:06.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:23:06.503: INFO: namespace emptydir-3292 deletion completed in 6.08073753s

• [SLOW TEST:10.162 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:23:06.504: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-d511ef03-8d0c-11e9-b925-1ab852558ec8
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-d511ef03-8d0c-11e9-b925-1ab852558ec8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:23:10.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4584" for this suite.
Jun 12 12:23:32.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:23:32.683: INFO: namespace projected-4584 deletion completed in 22.085423529s

• [SLOW TEST:26.180 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:23:32.683: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 12:23:32.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 version --client'
Jun 12 12:23:32.759: INFO: stderr: ""
Jun 12 12:23:32.759: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:44:30Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Jun 12 12:23:32.760: INFO: Not supported for server versions before "1.14.3"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:23:32.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9028" for this suite.
Jun 12 12:23:38.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:23:38.842: INFO: namespace kubectl-9028 deletion completed in 6.077879626s

S [SKIPPING] [6.159 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance] [It]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692

    Not supported for server versions before "1.14.3"

    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:922
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:23:38.842: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 12 12:23:39.136: INFO: Pod name wrapped-volume-race-e87e4dcc-8d0c-11e9-b925-1ab852558ec8: Found 0 pods out of 5
Jun 12 12:23:44.141: INFO: Pod name wrapped-volume-race-e87e4dcc-8d0c-11e9-b925-1ab852558ec8: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e87e4dcc-8d0c-11e9-b925-1ab852558ec8 in namespace emptydir-wrapper-153, will wait for the garbage collector to delete the pods
Jun 12 12:24:14.228: INFO: Deleting ReplicationController wrapped-volume-race-e87e4dcc-8d0c-11e9-b925-1ab852558ec8 took: 7.21509ms
Jun 12 12:24:14.628: INFO: Terminating ReplicationController wrapped-volume-race-e87e4dcc-8d0c-11e9-b925-1ab852558ec8 pods took: 400.191881ms
STEP: Creating RC which spawns configmap-volume pods
Jun 12 12:24:54.546: INFO: Pod name wrapped-volume-race-15705657-8d0d-11e9-b925-1ab852558ec8: Found 0 pods out of 5
Jun 12 12:24:59.551: INFO: Pod name wrapped-volume-race-15705657-8d0d-11e9-b925-1ab852558ec8: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-15705657-8d0d-11e9-b925-1ab852558ec8 in namespace emptydir-wrapper-153, will wait for the garbage collector to delete the pods
Jun 12 12:25:09.631: INFO: Deleting ReplicationController wrapped-volume-race-15705657-8d0d-11e9-b925-1ab852558ec8 took: 5.373616ms
Jun 12 12:25:10.031: INFO: Terminating ReplicationController wrapped-volume-race-15705657-8d0d-11e9-b925-1ab852558ec8 pods took: 400.210388ms
STEP: Creating RC which spawns configmap-volume pods
Jun 12 12:25:45.855: INFO: Pod name wrapped-volume-race-34049593-8d0d-11e9-b925-1ab852558ec8: Found 0 pods out of 5
Jun 12 12:25:50.860: INFO: Pod name wrapped-volume-race-34049593-8d0d-11e9-b925-1ab852558ec8: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-34049593-8d0d-11e9-b925-1ab852558ec8 in namespace emptydir-wrapper-153, will wait for the garbage collector to delete the pods
Jun 12 12:26:02.960: INFO: Deleting ReplicationController wrapped-volume-race-34049593-8d0d-11e9-b925-1ab852558ec8 took: 21.787531ms
Jun 12 12:26:03.360: INFO: Terminating ReplicationController wrapped-volume-race-34049593-8d0d-11e9-b925-1ab852558ec8 pods took: 400.19521ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:26:45.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-153" for this suite.
Jun 12 12:26:53.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:26:53.164: INFO: namespace emptydir-wrapper-153 deletion completed in 8.105774246s

• [SLOW TEST:194.323 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:26:53.165: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Jun 12 12:26:53.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 create -f - --namespace=kubectl-5195'
Jun 12 12:26:53.718: INFO: stderr: ""
Jun 12 12:26:53.718: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 12 12:26:53.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5195'
Jun 12 12:26:53.782: INFO: stderr: ""
Jun 12 12:26:53.782: INFO: stdout: "update-demo-nautilus-m7lxh update-demo-nautilus-p9swk "
Jun 12 12:26:53.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-m7lxh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5195'
Jun 12 12:26:53.841: INFO: stderr: ""
Jun 12 12:26:53.841: INFO: stdout: ""
Jun 12 12:26:53.841: INFO: update-demo-nautilus-m7lxh is created but not running
Jun 12 12:26:58.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5195'
Jun 12 12:26:58.907: INFO: stderr: ""
Jun 12 12:26:58.907: INFO: stdout: "update-demo-nautilus-m7lxh update-demo-nautilus-p9swk "
Jun 12 12:26:58.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-m7lxh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5195'
Jun 12 12:26:58.966: INFO: stderr: ""
Jun 12 12:26:58.966: INFO: stdout: "true"
Jun 12 12:26:58.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-m7lxh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5195'
Jun 12 12:26:59.024: INFO: stderr: ""
Jun 12 12:26:59.024: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 12 12:26:59.024: INFO: validating pod update-demo-nautilus-m7lxh
Jun 12 12:26:59.029: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 12 12:26:59.029: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 12 12:26:59.029: INFO: update-demo-nautilus-m7lxh is verified up and running
Jun 12 12:26:59.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-p9swk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5195'
Jun 12 12:26:59.089: INFO: stderr: ""
Jun 12 12:26:59.089: INFO: stdout: "true"
Jun 12 12:26:59.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-p9swk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5195'
Jun 12 12:26:59.148: INFO: stderr: ""
Jun 12 12:26:59.148: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 12 12:26:59.148: INFO: validating pod update-demo-nautilus-p9swk
Jun 12 12:26:59.153: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 12 12:26:59.153: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 12 12:26:59.153: INFO: update-demo-nautilus-p9swk is verified up and running
STEP: scaling down the replication controller
Jun 12 12:26:59.154: INFO: scanned /root for discovery docs: <nil>
Jun 12 12:26:59.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-5195'
Jun 12 12:27:00.237: INFO: stderr: ""
Jun 12 12:27:00.237: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 12 12:27:00.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5195'
Jun 12 12:27:00.301: INFO: stderr: ""
Jun 12 12:27:00.301: INFO: stdout: "update-demo-nautilus-m7lxh update-demo-nautilus-p9swk "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 12 12:27:05.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5195'
Jun 12 12:27:05.368: INFO: stderr: ""
Jun 12 12:27:05.368: INFO: stdout: "update-demo-nautilus-m7lxh "
Jun 12 12:27:05.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-m7lxh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5195'
Jun 12 12:27:05.427: INFO: stderr: ""
Jun 12 12:27:05.427: INFO: stdout: "true"
Jun 12 12:27:05.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-m7lxh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5195'
Jun 12 12:27:05.487: INFO: stderr: ""
Jun 12 12:27:05.487: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 12 12:27:05.487: INFO: validating pod update-demo-nautilus-m7lxh
Jun 12 12:27:05.490: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 12 12:27:05.490: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 12 12:27:05.490: INFO: update-demo-nautilus-m7lxh is verified up and running
STEP: scaling up the replication controller
Jun 12 12:27:05.491: INFO: scanned /root for discovery docs: <nil>
Jun 12 12:27:05.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-5195'
Jun 12 12:27:06.576: INFO: stderr: ""
Jun 12 12:27:06.576: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 12 12:27:06.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5195'
Jun 12 12:27:06.643: INFO: stderr: ""
Jun 12 12:27:06.643: INFO: stdout: "update-demo-nautilus-m7lxh update-demo-nautilus-mzz72 "
Jun 12 12:27:06.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-m7lxh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5195'
Jun 12 12:27:06.703: INFO: stderr: ""
Jun 12 12:27:06.703: INFO: stdout: "true"
Jun 12 12:27:06.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-m7lxh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5195'
Jun 12 12:27:06.763: INFO: stderr: ""
Jun 12 12:27:06.763: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 12 12:27:06.763: INFO: validating pod update-demo-nautilus-m7lxh
Jun 12 12:27:06.767: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 12 12:27:06.767: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 12 12:27:06.767: INFO: update-demo-nautilus-m7lxh is verified up and running
Jun 12 12:27:06.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-mzz72 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5195'
Jun 12 12:27:06.827: INFO: stderr: ""
Jun 12 12:27:06.827: INFO: stdout: ""
Jun 12 12:27:06.827: INFO: update-demo-nautilus-mzz72 is created but not running
Jun 12 12:27:11.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5195'
Jun 12 12:27:11.894: INFO: stderr: ""
Jun 12 12:27:11.894: INFO: stdout: "update-demo-nautilus-m7lxh update-demo-nautilus-mzz72 "
Jun 12 12:27:11.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-m7lxh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5195'
Jun 12 12:27:11.957: INFO: stderr: ""
Jun 12 12:27:11.957: INFO: stdout: "true"
Jun 12 12:27:11.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-m7lxh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5195'
Jun 12 12:27:12.015: INFO: stderr: ""
Jun 12 12:27:12.015: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 12 12:27:12.015: INFO: validating pod update-demo-nautilus-m7lxh
Jun 12 12:27:12.025: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 12 12:27:12.025: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 12 12:27:12.025: INFO: update-demo-nautilus-m7lxh is verified up and running
Jun 12 12:27:12.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-mzz72 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5195'
Jun 12 12:27:12.085: INFO: stderr: ""
Jun 12 12:27:12.085: INFO: stdout: "true"
Jun 12 12:27:12.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-mzz72 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5195'
Jun 12 12:27:12.143: INFO: stderr: ""
Jun 12 12:27:12.143: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 12 12:27:12.143: INFO: validating pod update-demo-nautilus-mzz72
Jun 12 12:27:12.147: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 12 12:27:12.147: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 12 12:27:12.147: INFO: update-demo-nautilus-mzz72 is verified up and running
STEP: using delete to clean up resources
Jun 12 12:27:12.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 delete --grace-period=0 --force -f - --namespace=kubectl-5195'
Jun 12 12:27:12.211: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 12 12:27:12.211: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 12 12:27:12.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5195'
Jun 12 12:27:12.276: INFO: stderr: "No resources found.\n"
Jun 12 12:27:12.276: INFO: stdout: ""
Jun 12 12:27:12.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -l name=update-demo --namespace=kubectl-5195 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 12 12:27:12.339: INFO: stderr: ""
Jun 12 12:27:12.339: INFO: stdout: "update-demo-nautilus-m7lxh\nupdate-demo-nautilus-mzz72\n"
Jun 12 12:27:12.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5195'
Jun 12 12:27:12.908: INFO: stderr: "No resources found.\n"
Jun 12 12:27:12.908: INFO: stdout: ""
Jun 12 12:27:12.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -l name=update-demo --namespace=kubectl-5195 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 12 12:27:12.969: INFO: stderr: ""
Jun 12 12:27:12.969: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:27:12.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5195" for this suite.
Jun 12 12:27:18.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:27:19.056: INFO: namespace kubectl-5195 deletion completed in 6.083566286s

• [SLOW TEST:25.891 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:27:19.056: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 12 12:27:19.094: INFO: Waiting up to 5m0s for pod "pod-6b9a0466-8d0d-11e9-b925-1ab852558ec8" in namespace "emptydir-993" to be "success or failure"
Jun 12 12:27:19.096: INFO: Pod "pod-6b9a0466-8d0d-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.107923ms
Jun 12 12:27:21.099: INFO: Pod "pod-6b9a0466-8d0d-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005732696s
Jun 12 12:27:23.103: INFO: Pod "pod-6b9a0466-8d0d-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00933691s
STEP: Saw pod success
Jun 12 12:27:23.103: INFO: Pod "pod-6b9a0466-8d0d-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:27:23.106: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-6b9a0466-8d0d-11e9-b925-1ab852558ec8 container test-container: <nil>
STEP: delete the pod
Jun 12 12:27:23.124: INFO: Waiting for pod pod-6b9a0466-8d0d-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:27:23.126: INFO: Pod pod-6b9a0466-8d0d-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:27:23.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-993" for this suite.
Jun 12 12:27:29.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:27:29.211: INFO: namespace emptydir-993 deletion completed in 6.08169939s

• [SLOW TEST:10.155 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:27:29.211: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 12 12:27:29.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7554'
Jun 12 12:27:29.325: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 12 12:27:29.325: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Jun 12 12:27:31.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 delete deployment e2e-test-nginx-deployment --namespace=kubectl-7554'
Jun 12 12:27:31.407: INFO: stderr: ""
Jun 12 12:27:31.407: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:27:31.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7554" for this suite.
Jun 12 12:27:37.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:27:37.498: INFO: namespace kubectl-7554 deletion completed in 6.082115068s

• [SLOW TEST:8.287 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:27:37.498: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 12 12:27:37.549: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8771,SelfLink:/api/v1/namespaces/watch-8771/configmaps/e2e-watch-test-watch-closed,UID:769a2b3f-8d0d-11e9-afb2-00163e04c212,ResourceVersion:12533,Generation:0,CreationTimestamp:2019-06-12 12:27:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 12 12:27:37.549: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8771,SelfLink:/api/v1/namespaces/watch-8771/configmaps/e2e-watch-test-watch-closed,UID:769a2b3f-8d0d-11e9-afb2-00163e04c212,ResourceVersion:12534,Generation:0,CreationTimestamp:2019-06-12 12:27:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 12 12:27:37.561: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8771,SelfLink:/api/v1/namespaces/watch-8771/configmaps/e2e-watch-test-watch-closed,UID:769a2b3f-8d0d-11e9-afb2-00163e04c212,ResourceVersion:12535,Generation:0,CreationTimestamp:2019-06-12 12:27:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 12 12:27:37.561: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8771,SelfLink:/api/v1/namespaces/watch-8771/configmaps/e2e-watch-test-watch-closed,UID:769a2b3f-8d0d-11e9-afb2-00163e04c212,ResourceVersion:12536,Generation:0,CreationTimestamp:2019-06-12 12:27:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:27:37.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8771" for this suite.
Jun 12 12:27:43.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:27:43.643: INFO: namespace watch-8771 deletion completed in 6.078755004s

• [SLOW TEST:6.145 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:27:43.643: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 12 12:27:43.686: INFO: Waiting up to 5m0s for pod "pod-7a42312f-8d0d-11e9-b925-1ab852558ec8" in namespace "emptydir-8451" to be "success or failure"
Jun 12 12:27:43.690: INFO: Pod "pod-7a42312f-8d0d-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.480722ms
Jun 12 12:27:45.693: INFO: Pod "pod-7a42312f-8d0d-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007822932s
STEP: Saw pod success
Jun 12 12:27:45.693: INFO: Pod "pod-7a42312f-8d0d-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:27:45.696: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-7a42312f-8d0d-11e9-b925-1ab852558ec8 container test-container: <nil>
STEP: delete the pod
Jun 12 12:27:45.714: INFO: Waiting for pod pod-7a42312f-8d0d-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:27:45.716: INFO: Pod pod-7a42312f-8d0d-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:27:45.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8451" for this suite.
Jun 12 12:27:51.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:27:51.798: INFO: namespace emptydir-8451 deletion completed in 6.079007016s

• [SLOW TEST:8.155 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:27:51.798: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Jun 12 12:27:51.837: INFO: Waiting up to 5m0s for pod "var-expansion-7f1e13d3-8d0d-11e9-b925-1ab852558ec8" in namespace "var-expansion-7770" to be "success or failure"
Jun 12 12:27:51.839: INFO: Pod "var-expansion-7f1e13d3-8d0d-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.155711ms
Jun 12 12:27:53.843: INFO: Pod "var-expansion-7f1e13d3-8d0d-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006097759s
STEP: Saw pod success
Jun 12 12:27:53.843: INFO: Pod "var-expansion-7f1e13d3-8d0d-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:27:53.845: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod var-expansion-7f1e13d3-8d0d-11e9-b925-1ab852558ec8 container dapi-container: <nil>
STEP: delete the pod
Jun 12 12:27:53.866: INFO: Waiting for pod var-expansion-7f1e13d3-8d0d-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:27:53.868: INFO: Pod var-expansion-7f1e13d3-8d0d-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:27:53.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7770" for this suite.
Jun 12 12:27:59.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:27:59.959: INFO: namespace var-expansion-7770 deletion completed in 6.079259725s

• [SLOW TEST:8.161 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:27:59.960: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Jun 12 12:27:59.996: INFO: Waiting up to 5m0s for pod "client-containers-83fb52cb-8d0d-11e9-b925-1ab852558ec8" in namespace "containers-7158" to be "success or failure"
Jun 12 12:27:59.998: INFO: Pod "client-containers-83fb52cb-8d0d-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.284643ms
Jun 12 12:28:02.002: INFO: Pod "client-containers-83fb52cb-8d0d-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005616674s
Jun 12 12:28:04.005: INFO: Pod "client-containers-83fb52cb-8d0d-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009199539s
STEP: Saw pod success
Jun 12 12:28:04.005: INFO: Pod "client-containers-83fb52cb-8d0d-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:28:04.008: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod client-containers-83fb52cb-8d0d-11e9-b925-1ab852558ec8 container test-container: <nil>
STEP: delete the pod
Jun 12 12:28:04.031: INFO: Waiting for pod client-containers-83fb52cb-8d0d-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:28:04.033: INFO: Pod client-containers-83fb52cb-8d0d-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:28:04.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7158" for this suite.
Jun 12 12:28:10.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:28:10.118: INFO: namespace containers-7158 deletion completed in 6.078958934s

• [SLOW TEST:10.159 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:28:10.119: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 12 12:28:10.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-5351'
Jun 12 12:28:10.230: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 12 12:28:10.230: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Jun 12 12:28:12.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 delete deployment e2e-test-nginx-deployment --namespace=kubectl-5351'
Jun 12 12:28:12.309: INFO: stderr: ""
Jun 12 12:28:12.309: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:28:12.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5351" for this suite.
Jun 12 12:28:34.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:28:34.392: INFO: namespace kubectl-5351 deletion completed in 22.079327828s

• [SLOW TEST:24.274 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:28:34.392: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-217
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-217
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-217
Jun 12 12:28:34.442: INFO: Found 0 stateful pods, waiting for 1
Jun 12 12:28:44.446: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 12 12:28:44.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-217 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 12 12:28:44.734: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 12 12:28:44.734: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 12 12:28:44.734: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 12 12:28:44.738: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 12 12:28:54.741: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 12 12:28:54.741: INFO: Waiting for statefulset status.replicas updated to 0
Jun 12 12:28:54.754: INFO: POD   NODE                       PHASE    GRACE  CONDITIONS
Jun 12 12:28:54.754: INFO: ss-0  cn-hongkong.192.168.0.218  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:34 +0000 UTC  }]
Jun 12 12:28:54.754: INFO: 
Jun 12 12:28:54.754: INFO: StatefulSet ss has not reached scale 3, at 1
Jun 12 12:28:55.757: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997237484s
Jun 12 12:28:56.761: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993674696s
Jun 12 12:28:57.765: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990051195s
Jun 12 12:28:58.768: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985999747s
Jun 12 12:28:59.772: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982841054s
Jun 12 12:29:00.776: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978952769s
Jun 12 12:29:01.780: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974853163s
Jun 12 12:29:02.783: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.971215408s
Jun 12 12:29:03.787: INFO: Verifying statefulset ss doesn't scale past 3 for another 967.893777ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-217
Jun 12 12:29:04.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-217 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 12 12:29:05.098: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 12 12:29:05.098: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 12 12:29:05.098: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 12 12:29:05.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-217 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 12 12:29:05.403: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 12 12:29:05.403: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 12 12:29:05.403: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 12 12:29:05.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-217 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 12 12:29:05.717: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 12 12:29:05.717: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 12 12:29:05.717: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 12 12:29:05.720: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 12 12:29:05.720: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 12 12:29:05.720: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 12 12:29:05.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-217 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 12 12:29:06.015: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 12 12:29:06.015: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 12 12:29:06.015: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 12 12:29:06.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-217 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 12 12:29:06.309: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 12 12:29:06.309: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 12 12:29:06.309: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 12 12:29:06.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-217 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 12 12:29:06.591: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 12 12:29:06.591: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 12 12:29:06.591: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 12 12:29:06.591: INFO: Waiting for statefulset status.replicas updated to 0
Jun 12 12:29:06.594: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 12 12:29:16.600: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 12 12:29:16.600: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 12 12:29:16.600: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 12 12:29:16.610: INFO: POD   NODE                       PHASE    GRACE  CONDITIONS
Jun 12 12:29:16.610: INFO: ss-0  cn-hongkong.192.168.0.218  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:34 +0000 UTC  }]
Jun 12 12:29:16.610: INFO: ss-1  cn-hongkong.192.168.0.217  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:54 +0000 UTC  }]
Jun 12 12:29:16.610: INFO: ss-2  cn-hongkong.192.168.0.219  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:54 +0000 UTC  }]
Jun 12 12:29:16.610: INFO: 
Jun 12 12:29:16.610: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 12 12:29:17.614: INFO: POD   NODE                       PHASE    GRACE  CONDITIONS
Jun 12 12:29:17.614: INFO: ss-0  cn-hongkong.192.168.0.218  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:34 +0000 UTC  }]
Jun 12 12:29:17.614: INFO: ss-1  cn-hongkong.192.168.0.217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:54 +0000 UTC  }]
Jun 12 12:29:17.614: INFO: ss-2  cn-hongkong.192.168.0.219  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:54 +0000 UTC  }]
Jun 12 12:29:17.614: INFO: 
Jun 12 12:29:17.614: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 12 12:29:18.618: INFO: POD   NODE                       PHASE    GRACE  CONDITIONS
Jun 12 12:29:18.618: INFO: ss-0  cn-hongkong.192.168.0.218  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:34 +0000 UTC  }]
Jun 12 12:29:18.618: INFO: ss-1  cn-hongkong.192.168.0.217  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:54 +0000 UTC  }]
Jun 12 12:29:18.618: INFO: ss-2  cn-hongkong.192.168.0.219  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:54 +0000 UTC  }]
Jun 12 12:29:18.618: INFO: 
Jun 12 12:29:18.618: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 12 12:29:19.622: INFO: POD   NODE                       PHASE    GRACE  CONDITIONS
Jun 12 12:29:19.622: INFO: ss-0  cn-hongkong.192.168.0.218  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:34 +0000 UTC  }]
Jun 12 12:29:19.622: INFO: 
Jun 12 12:29:19.622: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 12 12:29:20.625: INFO: POD   NODE                       PHASE    GRACE  CONDITIONS
Jun 12 12:29:20.625: INFO: ss-0  cn-hongkong.192.168.0.218  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:34 +0000 UTC  }]
Jun 12 12:29:20.625: INFO: 
Jun 12 12:29:20.625: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 12 12:29:21.629: INFO: POD   NODE                       PHASE    GRACE  CONDITIONS
Jun 12 12:29:21.629: INFO: ss-0  cn-hongkong.192.168.0.218  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:34 +0000 UTC  }]
Jun 12 12:29:21.629: INFO: 
Jun 12 12:29:21.629: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 12 12:29:22.632: INFO: POD   NODE                       PHASE    GRACE  CONDITIONS
Jun 12 12:29:22.632: INFO: ss-0  cn-hongkong.192.168.0.218  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:29:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:28:34 +0000 UTC  }]
Jun 12 12:29:22.632: INFO: 
Jun 12 12:29:22.633: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 12 12:29:23.636: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.974818709s
Jun 12 12:29:24.640: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.970953591s
Jun 12 12:29:25.643: INFO: Verifying statefulset ss doesn't scale past 0 for another 967.615287ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-217
Jun 12 12:29:26.646: INFO: Scaling statefulset ss to 0
Jun 12 12:29:26.654: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jun 12 12:29:26.656: INFO: Deleting all statefulset in ns statefulset-217
Jun 12 12:29:26.658: INFO: Scaling statefulset ss to 0
Jun 12 12:29:26.664: INFO: Waiting for statefulset status.replicas updated to 0
Jun 12 12:29:26.666: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:29:26.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-217" for this suite.
Jun 12 12:29:32.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:29:32.763: INFO: namespace statefulset-217 deletion completed in 6.079920116s

• [SLOW TEST:58.371 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:29:32.764: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-bb4c11c6-8d0d-11e9-b925-1ab852558ec8
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:29:32.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8681" for this suite.
Jun 12 12:29:38.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:29:38.875: INFO: namespace configmap-8681 deletion completed in 6.078635527s

• [SLOW TEST:6.112 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:29:38.875: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 12 12:29:38.912: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bef08dad-8d0d-11e9-b925-1ab852558ec8" in namespace "projected-6050" to be "success or failure"
Jun 12 12:29:38.914: INFO: Pod "downwardapi-volume-bef08dad-8d0d-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.479304ms
Jun 12 12:29:40.918: INFO: Pod "downwardapi-volume-bef08dad-8d0d-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00583146s
STEP: Saw pod success
Jun 12 12:29:40.918: INFO: Pod "downwardapi-volume-bef08dad-8d0d-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:29:40.920: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod downwardapi-volume-bef08dad-8d0d-11e9-b925-1ab852558ec8 container client-container: <nil>
STEP: delete the pod
Jun 12 12:29:40.945: INFO: Waiting for pod downwardapi-volume-bef08dad-8d0d-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:29:40.950: INFO: Pod downwardapi-volume-bef08dad-8d0d-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:29:40.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6050" for this suite.
Jun 12 12:29:46.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:29:47.038: INFO: namespace projected-6050 deletion completed in 6.085104877s

• [SLOW TEST:8.163 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:29:47.038: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2292.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2292.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2292.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2292.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2292.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2292.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 12 12:30:07.131: INFO: DNS probes using dns-2292/dns-test-c3cedbd5-8d0d-11e9-b925-1ab852558ec8 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:30:07.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2292" for this suite.
Jun 12 12:30:13.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:30:13.234: INFO: namespace dns-2292 deletion completed in 6.088338818s

• [SLOW TEST:26.195 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:30:13.234: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Jun 12 12:30:13.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 api-versions'
Jun 12 12:30:13.345: INFO: stderr: ""
Jun 12 12:30:13.345: INFO: stdout: "admissionregistration.k8s.io/v1beta1\nalicloud.com/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:30:13.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7684" for this suite.
Jun 12 12:30:19.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:30:19.432: INFO: namespace kubectl-7684 deletion completed in 6.082208439s

• [SLOW TEST:6.198 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:30:19.432: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jun 12 12:30:19.465: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 12 12:30:19.472: INFO: Waiting for terminating namespaces to be deleted...
Jun 12 12:30:19.474: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.217 before test
Jun 12 12:30:19.479: INFO: sonobuoy-systemd-logs-daemon-set-abb5e6914f274695-4skg8 from heptio-sonobuoy started at 2019-06-12 12:21:07 +0000 UTC (2 container statuses recorded)
Jun 12 12:30:19.479: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 12 12:30:19.479: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 12 12:30:19.479: INFO: flexvolume-qzj4v from kube-system started at 2019-06-12 12:20:56 +0000 UTC (1 container statuses recorded)
Jun 12 12:30:19.479: INFO: 	Container acs-flexvolume ready: true, restart count 0
Jun 12 12:30:19.479: INFO: kube-flannel-ds-x7nmf from kube-system started at 2019-06-12 11:16:31 +0000 UTC (2 container statuses recorded)
Jun 12 12:30:19.479: INFO: 	Container install-cni ready: true, restart count 0
Jun 12 12:30:19.479: INFO: 	Container kube-flannel ready: true, restart count 0
Jun 12 12:30:19.479: INFO: aliyun-acr-credential-helper-6bb54dfdfd-5gk8t from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 12:30:19.479: INFO: 	Container aliyun-acr-credential-helper ready: true, restart count 0
Jun 12 12:30:19.479: INFO: nginx-ingress-controller-8584849666-qrnjk from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 12:30:19.479: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jun 12 12:30:19.479: INFO: kube-proxy-worker-p54dg from kube-system started at 2019-06-12 11:16:30 +0000 UTC (1 container statuses recorded)
Jun 12 12:30:19.479: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Jun 12 12:30:19.479: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.218 before test
Jun 12 12:30:19.484: INFO: kube-proxy-worker-2mnhq from kube-system started at 2019-06-12 11:16:30 +0000 UTC (1 container statuses recorded)
Jun 12 12:30:19.484: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Jun 12 12:30:19.484: INFO: kube-flannel-ds-9h6cz from kube-system started at 2019-06-12 11:16:31 +0000 UTC (2 container statuses recorded)
Jun 12 12:30:19.484: INFO: 	Container install-cni ready: true, restart count 0
Jun 12 12:30:19.484: INFO: 	Container kube-flannel ready: true, restart count 0
Jun 12 12:30:19.484: INFO: flexvolume-5mrxl from kube-system started at 2019-06-12 12:20:55 +0000 UTC (1 container statuses recorded)
Jun 12 12:30:19.484: INFO: 	Container acs-flexvolume ready: true, restart count 0
Jun 12 12:30:19.484: INFO: metrics-server-645cb87c9d-ddkrk from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 12:30:19.484: INFO: 	Container metrics-server ready: true, restart count 0
Jun 12 12:30:19.484: INFO: coredns-7594b4cbdd-9ht8c from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 12:30:19.484: INFO: 	Container coredns ready: true, restart count 0
Jun 12 12:30:19.484: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 12:30:19.484: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 12 12:30:19.484: INFO: sonobuoy-systemd-logs-daemon-set-abb5e6914f274695-27hnn from heptio-sonobuoy started at 2019-06-12 12:21:07 +0000 UTC (2 container statuses recorded)
Jun 12 12:30:19.484: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 12 12:30:19.484: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 12 12:30:19.484: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.219 before test
Jun 12 12:30:19.496: INFO: coredns-7594b4cbdd-76756 from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 12:30:19.496: INFO: 	Container coredns ready: true, restart count 0
Jun 12 12:30:19.496: INFO: tiller-deploy-7f49bdd789-tp29r from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 12:30:19.496: INFO: 	Container tiller ready: true, restart count 0
Jun 12 12:30:19.496: INFO: sonobuoy-systemd-logs-daemon-set-abb5e6914f274695-6v8ls from heptio-sonobuoy started at 2019-06-12 12:21:07 +0000 UTC (2 container statuses recorded)
Jun 12 12:30:19.497: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 12 12:30:19.497: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 12 12:30:19.497: INFO: kube-flannel-ds-2xjfg from kube-system started at 2019-06-12 11:16:31 +0000 UTC (2 container statuses recorded)
Jun 12 12:30:19.497: INFO: 	Container install-cni ready: true, restart count 0
Jun 12 12:30:19.497: INFO: 	Container kube-flannel ready: true, restart count 0
Jun 12 12:30:19.497: INFO: flexvolume-mwfdp from kube-system started at 2019-06-12 12:20:54 +0000 UTC (1 container statuses recorded)
Jun 12 12:30:19.497: INFO: 	Container acs-flexvolume ready: true, restart count 0
Jun 12 12:30:19.497: INFO: nginx-ingress-controller-8584849666-ft4jg from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 12:30:19.497: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jun 12 12:30:19.497: INFO: sonobuoy-e2e-job-1d838d5dfce648c8 from heptio-sonobuoy started at 2019-06-12 12:21:07 +0000 UTC (2 container statuses recorded)
Jun 12 12:30:19.497: INFO: 	Container e2e ready: true, restart count 0
Jun 12 12:30:19.497: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 12 12:30:19.497: INFO: kube-proxy-worker-94r4b from kube-system started at 2019-06-12 11:16:30 +0000 UTC (1 container statuses recorded)
Jun 12 12:30:19.497: INFO: 	Container kube-proxy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15a773586b5f0df4], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:30:20.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5192" for this suite.
Jun 12 12:30:26.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:30:26.602: INFO: namespace sched-pred-5192 deletion completed in 6.08087151s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.170 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:30:26.602: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 12 12:30:26.658: INFO: Waiting up to 5m0s for pod "pod-db654a18-8d0d-11e9-b925-1ab852558ec8" in namespace "emptydir-8322" to be "success or failure"
Jun 12 12:30:26.660: INFO: Pod "pod-db654a18-8d0d-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.275885ms
Jun 12 12:30:28.663: INFO: Pod "pod-db654a18-8d0d-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005257066s
STEP: Saw pod success
Jun 12 12:30:28.663: INFO: Pod "pod-db654a18-8d0d-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:30:28.665: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-db654a18-8d0d-11e9-b925-1ab852558ec8 container test-container: <nil>
STEP: delete the pod
Jun 12 12:30:28.682: INFO: Waiting for pod pod-db654a18-8d0d-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:30:28.684: INFO: Pod pod-db654a18-8d0d-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:30:28.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8322" for this suite.
Jun 12 12:30:34.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:30:34.768: INFO: namespace emptydir-8322 deletion completed in 6.081050908s

• [SLOW TEST:8.166 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:30:34.768: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-2g54x in namespace proxy-9435
I0612 12:30:34.821053      17 runners.go:184] Created replication controller with name: proxy-service-2g54x, namespace: proxy-9435, replica count: 1
I0612 12:30:35.871384      17 runners.go:184] proxy-service-2g54x Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0612 12:30:36.871543      17 runners.go:184] proxy-service-2g54x Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0612 12:30:37.871711      17 runners.go:184] proxy-service-2g54x Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0612 12:30:38.871874      17 runners.go:184] proxy-service-2g54x Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0612 12:30:39.872051      17 runners.go:184] proxy-service-2g54x Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0612 12:30:40.872234      17 runners.go:184] proxy-service-2g54x Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 12 12:30:40.875: INFO: setup took 6.080747531s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 12 12:30:40.881: INFO: (0) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 5.932718ms)
Jun 12 12:30:40.883: INFO: (0) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 6.977026ms)
Jun 12 12:30:40.883: INFO: (0) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 7.70927ms)
Jun 12 12:30:40.883: INFO: (0) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 7.517397ms)
Jun 12 12:30:40.883: INFO: (0) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 7.809382ms)
Jun 12 12:30:40.883: INFO: (0) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 7.615798ms)
Jun 12 12:30:40.889: INFO: (0) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 13.911982ms)
Jun 12 12:30:40.889: INFO: (0) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 13.997132ms)
Jun 12 12:30:40.890: INFO: (0) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 14.098665ms)
Jun 12 12:30:40.890: INFO: (0) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 14.155969ms)
Jun 12 12:30:40.890: INFO: (0) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 14.248942ms)
Jun 12 12:30:40.891: INFO: (0) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 15.255966ms)
Jun 12 12:30:40.891: INFO: (0) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 15.322923ms)
Jun 12 12:30:40.891: INFO: (0) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 15.256184ms)
Jun 12 12:30:40.891: INFO: (0) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 15.474889ms)
Jun 12 12:30:40.894: INFO: (0) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 18.357858ms)
Jun 12 12:30:40.897: INFO: (1) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 3.591234ms)
Jun 12 12:30:40.898: INFO: (1) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 3.946875ms)
Jun 12 12:30:40.898: INFO: (1) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 4.247942ms)
Jun 12 12:30:40.899: INFO: (1) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 4.701914ms)
Jun 12 12:30:40.899: INFO: (1) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 4.867489ms)
Jun 12 12:30:40.899: INFO: (1) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 4.682694ms)
Jun 12 12:30:40.899: INFO: (1) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 5.287097ms)
Jun 12 12:30:40.899: INFO: (1) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 5.212409ms)
Jun 12 12:30:40.900: INFO: (1) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 5.642259ms)
Jun 12 12:30:40.900: INFO: (1) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 5.86172ms)
Jun 12 12:30:40.900: INFO: (1) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 6.358025ms)
Jun 12 12:30:40.900: INFO: (1) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 6.238802ms)
Jun 12 12:30:40.901: INFO: (1) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 7.294233ms)
Jun 12 12:30:40.902: INFO: (1) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 7.897219ms)
Jun 12 12:30:40.902: INFO: (1) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 7.798951ms)
Jun 12 12:30:40.902: INFO: (1) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 7.938262ms)
Jun 12 12:30:40.905: INFO: (2) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 3.124381ms)
Jun 12 12:30:40.906: INFO: (2) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 3.995357ms)
Jun 12 12:30:40.906: INFO: (2) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 3.945045ms)
Jun 12 12:30:40.906: INFO: (2) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 4.172235ms)
Jun 12 12:30:40.907: INFO: (2) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 4.246583ms)
Jun 12 12:30:40.907: INFO: (2) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 4.314274ms)
Jun 12 12:30:40.907: INFO: (2) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 4.140193ms)
Jun 12 12:30:40.908: INFO: (2) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 5.757344ms)
Jun 12 12:30:40.908: INFO: (2) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 5.458633ms)
Jun 12 12:30:40.909: INFO: (2) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 6.305919ms)
Jun 12 12:30:40.909: INFO: (2) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 6.157461ms)
Jun 12 12:30:40.909: INFO: (2) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 6.899201ms)
Jun 12 12:30:40.909: INFO: (2) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 6.885213ms)
Jun 12 12:30:40.909: INFO: (2) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 6.731815ms)
Jun 12 12:30:40.909: INFO: (2) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 6.718087ms)
Jun 12 12:30:40.910: INFO: (2) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 7.204425ms)
Jun 12 12:30:40.917: INFO: (3) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 6.670499ms)
Jun 12 12:30:40.917: INFO: (3) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 6.944359ms)
Jun 12 12:30:40.917: INFO: (3) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 7.126992ms)
Jun 12 12:30:40.917: INFO: (3) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 7.127338ms)
Jun 12 12:30:40.917: INFO: (3) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 7.484577ms)
Jun 12 12:30:40.918: INFO: (3) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 7.570364ms)
Jun 12 12:30:40.918: INFO: (3) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 7.935229ms)
Jun 12 12:30:40.918: INFO: (3) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 8.055412ms)
Jun 12 12:30:40.919: INFO: (3) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 8.548012ms)
Jun 12 12:30:40.919: INFO: (3) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 8.407079ms)
Jun 12 12:30:40.920: INFO: (3) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 9.771775ms)
Jun 12 12:30:40.920: INFO: (3) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 10.009712ms)
Jun 12 12:30:40.920: INFO: (3) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 10.144115ms)
Jun 12 12:30:40.920: INFO: (3) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 10.089055ms)
Jun 12 12:30:40.920: INFO: (3) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 10.070386ms)
Jun 12 12:30:40.920: INFO: (3) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 10.290366ms)
Jun 12 12:30:40.925: INFO: (4) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 4.850021ms)
Jun 12 12:30:40.926: INFO: (4) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 5.470971ms)
Jun 12 12:30:40.926: INFO: (4) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 5.967245ms)
Jun 12 12:30:40.927: INFO: (4) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 6.091293ms)
Jun 12 12:30:40.927: INFO: (4) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 5.938959ms)
Jun 12 12:30:40.927: INFO: (4) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 6.433356ms)
Jun 12 12:30:40.927: INFO: (4) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 6.472263ms)
Jun 12 12:30:40.927: INFO: (4) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 6.950102ms)
Jun 12 12:30:40.928: INFO: (4) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 7.091782ms)
Jun 12 12:30:40.928: INFO: (4) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 7.017222ms)
Jun 12 12:30:40.928: INFO: (4) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 7.087583ms)
Jun 12 12:30:40.928: INFO: (4) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 7.276073ms)
Jun 12 12:30:40.928: INFO: (4) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 7.508885ms)
Jun 12 12:30:40.928: INFO: (4) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 7.728692ms)
Jun 12 12:30:40.929: INFO: (4) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 7.826416ms)
Jun 12 12:30:40.929: INFO: (4) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 7.996108ms)
Jun 12 12:30:40.933: INFO: (5) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 3.634664ms)
Jun 12 12:30:40.934: INFO: (5) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 4.476325ms)
Jun 12 12:30:40.934: INFO: (5) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 5.134325ms)
Jun 12 12:30:40.934: INFO: (5) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 5.565187ms)
Jun 12 12:30:40.934: INFO: (5) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 5.421739ms)
Jun 12 12:30:40.935: INFO: (5) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 5.479644ms)
Jun 12 12:30:40.935: INFO: (5) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 5.76367ms)
Jun 12 12:30:40.935: INFO: (5) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 5.746557ms)
Jun 12 12:30:40.935: INFO: (5) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 5.956518ms)
Jun 12 12:30:40.935: INFO: (5) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 6.262814ms)
Jun 12 12:30:40.936: INFO: (5) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 6.468562ms)
Jun 12 12:30:40.936: INFO: (5) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 6.936769ms)
Jun 12 12:30:40.936: INFO: (5) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 6.974653ms)
Jun 12 12:30:40.936: INFO: (5) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 7.138713ms)
Jun 12 12:30:40.936: INFO: (5) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 7.325506ms)
Jun 12 12:30:40.937: INFO: (5) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 7.644221ms)
Jun 12 12:30:40.942: INFO: (6) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 4.854891ms)
Jun 12 12:30:40.942: INFO: (6) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 5.10648ms)
Jun 12 12:30:40.942: INFO: (6) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 5.438563ms)
Jun 12 12:30:40.942: INFO: (6) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 5.442666ms)
Jun 12 12:30:40.942: INFO: (6) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 5.818303ms)
Jun 12 12:30:40.944: INFO: (6) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 6.89071ms)
Jun 12 12:30:40.944: INFO: (6) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 6.985478ms)
Jun 12 12:30:40.944: INFO: (6) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 6.859542ms)
Jun 12 12:30:40.944: INFO: (6) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 7.004315ms)
Jun 12 12:30:40.944: INFO: (6) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 7.364102ms)
Jun 12 12:30:40.944: INFO: (6) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 7.18612ms)
Jun 12 12:30:40.944: INFO: (6) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 7.50105ms)
Jun 12 12:30:40.944: INFO: (6) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 7.688929ms)
Jun 12 12:30:40.944: INFO: (6) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 7.849214ms)
Jun 12 12:30:40.944: INFO: (6) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 7.439746ms)
Jun 12 12:30:40.946: INFO: (6) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 8.480905ms)
Jun 12 12:30:40.950: INFO: (7) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 4.349895ms)
Jun 12 12:30:40.952: INFO: (7) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 6.065782ms)
Jun 12 12:30:40.952: INFO: (7) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 5.976185ms)
Jun 12 12:30:40.952: INFO: (7) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 6.010888ms)
Jun 12 12:30:40.952: INFO: (7) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 6.210244ms)
Jun 12 12:30:40.952: INFO: (7) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 6.309409ms)
Jun 12 12:30:40.952: INFO: (7) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 6.511134ms)
Jun 12 12:30:40.953: INFO: (7) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 6.857824ms)
Jun 12 12:30:40.953: INFO: (7) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 6.766267ms)
Jun 12 12:30:40.953: INFO: (7) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 6.901573ms)
Jun 12 12:30:40.953: INFO: (7) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 7.257003ms)
Jun 12 12:30:40.953: INFO: (7) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 7.346905ms)
Jun 12 12:30:40.953: INFO: (7) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 7.268167ms)
Jun 12 12:30:40.953: INFO: (7) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 7.219261ms)
Jun 12 12:30:40.953: INFO: (7) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 7.320766ms)
Jun 12 12:30:40.953: INFO: (7) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 7.313593ms)
Jun 12 12:30:40.957: INFO: (8) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 3.536833ms)
Jun 12 12:30:40.958: INFO: (8) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 4.796689ms)
Jun 12 12:30:40.958: INFO: (8) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 4.306741ms)
Jun 12 12:30:40.958: INFO: (8) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 4.344273ms)
Jun 12 12:30:40.958: INFO: (8) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 4.683296ms)
Jun 12 12:30:40.959: INFO: (8) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 5.129592ms)
Jun 12 12:30:40.959: INFO: (8) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 4.918382ms)
Jun 12 12:30:40.959: INFO: (8) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 5.387163ms)
Jun 12 12:30:40.959: INFO: (8) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 5.401788ms)
Jun 12 12:30:40.959: INFO: (8) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 5.454159ms)
Jun 12 12:30:40.960: INFO: (8) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 6.21441ms)
Jun 12 12:30:40.960: INFO: (8) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 6.074976ms)
Jun 12 12:30:40.960: INFO: (8) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 6.136767ms)
Jun 12 12:30:40.960: INFO: (8) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 6.284105ms)
Jun 12 12:30:40.960: INFO: (8) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 6.084823ms)
Jun 12 12:30:40.960: INFO: (8) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 6.679287ms)
Jun 12 12:30:40.964: INFO: (9) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 4.072246ms)
Jun 12 12:30:40.966: INFO: (9) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 5.398513ms)
Jun 12 12:30:40.966: INFO: (9) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 5.881147ms)
Jun 12 12:30:40.967: INFO: (9) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 5.896373ms)
Jun 12 12:30:40.967: INFO: (9) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 6.320936ms)
Jun 12 12:30:40.967: INFO: (9) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 6.479054ms)
Jun 12 12:30:40.967: INFO: (9) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 6.805803ms)
Jun 12 12:30:40.967: INFO: (9) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 6.876295ms)
Jun 12 12:30:40.967: INFO: (9) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 6.524868ms)
Jun 12 12:30:40.967: INFO: (9) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 6.726215ms)
Jun 12 12:30:40.967: INFO: (9) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 7.112749ms)
Jun 12 12:30:40.968: INFO: (9) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 7.500968ms)
Jun 12 12:30:40.968: INFO: (9) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 7.05054ms)
Jun 12 12:30:40.968: INFO: (9) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 7.556061ms)
Jun 12 12:30:40.968: INFO: (9) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 7.288422ms)
Jun 12 12:30:40.969: INFO: (9) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 8.581824ms)
Jun 12 12:30:40.972: INFO: (10) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 3.194954ms)
Jun 12 12:30:40.973: INFO: (10) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 3.758908ms)
Jun 12 12:30:40.973: INFO: (10) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 4.05471ms)
Jun 12 12:30:40.973: INFO: (10) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 3.945613ms)
Jun 12 12:30:40.973: INFO: (10) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 3.977629ms)
Jun 12 12:30:40.973: INFO: (10) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 4.226356ms)
Jun 12 12:30:40.974: INFO: (10) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 5.156619ms)
Jun 12 12:30:40.974: INFO: (10) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 5.196284ms)
Jun 12 12:30:40.976: INFO: (10) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 6.397092ms)
Jun 12 12:30:40.976: INFO: (10) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 6.424906ms)
Jun 12 12:30:40.976: INFO: (10) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 6.753301ms)
Jun 12 12:30:40.976: INFO: (10) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 6.624275ms)
Jun 12 12:30:40.976: INFO: (10) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 6.719382ms)
Jun 12 12:30:40.976: INFO: (10) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 6.929352ms)
Jun 12 12:30:40.976: INFO: (10) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 6.946111ms)
Jun 12 12:30:40.976: INFO: (10) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 6.892298ms)
Jun 12 12:30:40.981: INFO: (11) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 5.061283ms)
Jun 12 12:30:40.983: INFO: (11) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 6.505341ms)
Jun 12 12:30:40.983: INFO: (11) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 6.76133ms)
Jun 12 12:30:40.983: INFO: (11) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 6.847639ms)
Jun 12 12:30:40.983: INFO: (11) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 7.278744ms)
Jun 12 12:30:40.984: INFO: (11) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 7.26254ms)
Jun 12 12:30:40.984: INFO: (11) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 7.359316ms)
Jun 12 12:30:40.984: INFO: (11) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 7.195968ms)
Jun 12 12:30:40.984: INFO: (11) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 7.248592ms)
Jun 12 12:30:40.984: INFO: (11) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 7.332407ms)
Jun 12 12:30:40.984: INFO: (11) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 7.53499ms)
Jun 12 12:30:40.984: INFO: (11) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 7.658095ms)
Jun 12 12:30:40.985: INFO: (11) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 8.58504ms)
Jun 12 12:30:40.985: INFO: (11) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 8.530514ms)
Jun 12 12:30:40.985: INFO: (11) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 8.585857ms)
Jun 12 12:30:40.985: INFO: (11) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 8.814354ms)
Jun 12 12:30:40.990: INFO: (12) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 4.600263ms)
Jun 12 12:30:40.990: INFO: (12) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 4.450661ms)
Jun 12 12:30:40.992: INFO: (12) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 6.341726ms)
Jun 12 12:30:40.992: INFO: (12) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 5.856644ms)
Jun 12 12:30:40.992: INFO: (12) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 5.724432ms)
Jun 12 12:30:40.992: INFO: (12) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 6.451972ms)
Jun 12 12:30:40.992: INFO: (12) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 6.039186ms)
Jun 12 12:30:40.992: INFO: (12) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 6.866621ms)
Jun 12 12:30:40.992: INFO: (12) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 6.494542ms)
Jun 12 12:30:40.992: INFO: (12) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 6.759603ms)
Jun 12 12:30:40.992: INFO: (12) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 6.777629ms)
Jun 12 12:30:40.993: INFO: (12) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 7.00769ms)
Jun 12 12:30:40.993: INFO: (12) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 6.863759ms)
Jun 12 12:30:40.993: INFO: (12) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 6.703343ms)
Jun 12 12:30:40.993: INFO: (12) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 6.663933ms)
Jun 12 12:30:40.993: INFO: (12) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 7.41232ms)
Jun 12 12:30:40.997: INFO: (13) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 3.880553ms)
Jun 12 12:30:40.998: INFO: (13) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 4.611823ms)
Jun 12 12:30:40.998: INFO: (13) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 4.816559ms)
Jun 12 12:30:40.998: INFO: (13) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 4.820605ms)
Jun 12 12:30:40.998: INFO: (13) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 4.991508ms)
Jun 12 12:30:40.998: INFO: (13) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 5.137951ms)
Jun 12 12:30:40.998: INFO: (13) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 5.196249ms)
Jun 12 12:30:40.998: INFO: (13) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 5.609554ms)
Jun 12 12:30:40.999: INFO: (13) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 6.044341ms)
Jun 12 12:30:40.999: INFO: (13) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 5.950868ms)
Jun 12 12:30:40.999: INFO: (13) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 6.186816ms)
Jun 12 12:30:41.000: INFO: (13) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 6.727698ms)
Jun 12 12:30:41.000: INFO: (13) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 7.475946ms)
Jun 12 12:30:41.000: INFO: (13) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 7.617432ms)
Jun 12 12:30:41.002: INFO: (13) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 9.370845ms)
Jun 12 12:30:41.002: INFO: (13) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 9.471587ms)
Jun 12 12:30:41.015: INFO: (14) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 12.776561ms)
Jun 12 12:30:41.017: INFO: (14) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 14.667536ms)
Jun 12 12:30:41.018: INFO: (14) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 15.287622ms)
Jun 12 12:30:41.019: INFO: (14) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 16.006127ms)
Jun 12 12:30:41.019: INFO: (14) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 16.643792ms)
Jun 12 12:30:41.019: INFO: (14) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 16.835274ms)
Jun 12 12:30:41.021: INFO: (14) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 17.883135ms)
Jun 12 12:30:41.021: INFO: (14) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 18.152523ms)
Jun 12 12:30:41.026: INFO: (14) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 23.465072ms)
Jun 12 12:30:41.038: INFO: (14) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 35.400407ms)
Jun 12 12:30:41.038: INFO: (14) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 35.586399ms)
Jun 12 12:30:41.039: INFO: (14) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 35.898596ms)
Jun 12 12:30:41.039: INFO: (14) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 35.883767ms)
Jun 12 12:30:41.039: INFO: (14) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 36.125765ms)
Jun 12 12:30:41.039: INFO: (14) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 36.27422ms)
Jun 12 12:30:41.039: INFO: (14) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 36.209468ms)
Jun 12 12:30:41.049: INFO: (15) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 10.287389ms)
Jun 12 12:30:41.051: INFO: (15) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 12.139375ms)
Jun 12 12:30:41.052: INFO: (15) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 12.643504ms)
Jun 12 12:30:41.052: INFO: (15) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 12.599654ms)
Jun 12 12:30:41.052: INFO: (15) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 12.908643ms)
Jun 12 12:30:41.053: INFO: (15) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 13.328694ms)
Jun 12 12:30:41.053: INFO: (15) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 13.810993ms)
Jun 12 12:30:41.053: INFO: (15) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 13.680848ms)
Jun 12 12:30:41.053: INFO: (15) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 13.725626ms)
Jun 12 12:30:41.053: INFO: (15) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 13.594124ms)
Jun 12 12:30:41.054: INFO: (15) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 15.091262ms)
Jun 12 12:30:41.054: INFO: (15) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 15.138871ms)
Jun 12 12:30:41.055: INFO: (15) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 15.20854ms)
Jun 12 12:30:41.055: INFO: (15) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 15.239436ms)
Jun 12 12:30:41.055: INFO: (15) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 15.395514ms)
Jun 12 12:30:41.055: INFO: (15) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 15.558758ms)
Jun 12 12:30:41.064: INFO: (16) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 8.696907ms)
Jun 12 12:30:41.071: INFO: (16) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 15.813675ms)
Jun 12 12:30:41.071: INFO: (16) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 16.077978ms)
Jun 12 12:30:41.071: INFO: (16) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 16.225021ms)
Jun 12 12:30:41.071: INFO: (16) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 16.010951ms)
Jun 12 12:30:41.071: INFO: (16) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 15.972496ms)
Jun 12 12:30:41.072: INFO: (16) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 16.177969ms)
Jun 12 12:30:41.072: INFO: (16) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 16.365418ms)
Jun 12 12:30:41.072: INFO: (16) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 16.030493ms)
Jun 12 12:30:41.072: INFO: (16) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 16.824404ms)
Jun 12 12:30:41.072: INFO: (16) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 16.439754ms)
Jun 12 12:30:41.072: INFO: (16) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 16.333184ms)
Jun 12 12:30:41.072: INFO: (16) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 16.566956ms)
Jun 12 12:30:41.072: INFO: (16) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 16.671135ms)
Jun 12 12:30:41.072: INFO: (16) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 17.242711ms)
Jun 12 12:30:41.072: INFO: (16) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 16.81714ms)
Jun 12 12:30:41.077: INFO: (17) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 4.322131ms)
Jun 12 12:30:41.078: INFO: (17) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 5.013201ms)
Jun 12 12:30:41.078: INFO: (17) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 5.053196ms)
Jun 12 12:30:41.078: INFO: (17) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 5.365588ms)
Jun 12 12:30:41.078: INFO: (17) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 5.563068ms)
Jun 12 12:30:41.078: INFO: (17) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 5.910482ms)
Jun 12 12:30:41.078: INFO: (17) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 5.864097ms)
Jun 12 12:30:41.079: INFO: (17) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 5.926821ms)
Jun 12 12:30:41.079: INFO: (17) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 5.99342ms)
Jun 12 12:30:41.079: INFO: (17) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 6.717862ms)
Jun 12 12:30:41.079: INFO: (17) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 6.773608ms)
Jun 12 12:30:41.080: INFO: (17) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 7.095649ms)
Jun 12 12:30:41.080: INFO: (17) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 7.338152ms)
Jun 12 12:30:41.080: INFO: (17) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 7.4477ms)
Jun 12 12:30:41.080: INFO: (17) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 7.387339ms)
Jun 12 12:30:41.080: INFO: (17) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 7.353509ms)
Jun 12 12:30:41.084: INFO: (18) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 3.920445ms)
Jun 12 12:30:41.086: INFO: (18) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 5.333221ms)
Jun 12 12:30:41.086: INFO: (18) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 5.446838ms)
Jun 12 12:30:41.086: INFO: (18) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 5.625883ms)
Jun 12 12:30:41.086: INFO: (18) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 5.93917ms)
Jun 12 12:30:41.086: INFO: (18) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 5.933408ms)
Jun 12 12:30:41.086: INFO: (18) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 5.83438ms)
Jun 12 12:30:41.086: INFO: (18) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 6.090823ms)
Jun 12 12:30:41.086: INFO: (18) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 6.019793ms)
Jun 12 12:30:41.087: INFO: (18) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 6.549486ms)
Jun 12 12:30:41.087: INFO: (18) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 6.636249ms)
Jun 12 12:30:41.087: INFO: (18) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 6.5125ms)
Jun 12 12:30:41.087: INFO: (18) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 6.618887ms)
Jun 12 12:30:41.087: INFO: (18) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 6.77445ms)
Jun 12 12:30:41.088: INFO: (18) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 7.288015ms)
Jun 12 12:30:41.088: INFO: (18) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 7.794401ms)
Jun 12 12:30:41.091: INFO: (19) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph/proxy/rewriteme">test</a> (200; 2.829826ms)
Jun 12 12:30:41.092: INFO: (19) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:1080/proxy/rewriteme">test<... (200; 3.633595ms)
Jun 12 12:30:41.092: INFO: (19) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:160/proxy/: foo (200; 3.892701ms)
Jun 12 12:30:41.093: INFO: (19) /api/v1/namespaces/proxy-9435/pods/proxy-service-2g54x-x5zph:162/proxy/: bar (200; 4.423723ms)
Jun 12 12:30:41.093: INFO: (19) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:162/proxy/: bar (200; 4.806705ms)
Jun 12 12:30:41.093: INFO: (19) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:1080/proxy/rewriteme">... (200; 4.776427ms)
Jun 12 12:30:41.093: INFO: (19) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:460/proxy/: tls baz (200; 4.983492ms)
Jun 12 12:30:41.093: INFO: (19) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname1/proxy/: foo (200; 5.163868ms)
Jun 12 12:30:41.093: INFO: (19) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/: <a href="/api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:443/proxy/tlsrewritem... (200; 4.978315ms)
Jun 12 12:30:41.093: INFO: (19) /api/v1/namespaces/proxy-9435/services/proxy-service-2g54x:portname2/proxy/: bar (200; 4.965942ms)
Jun 12 12:30:41.093: INFO: (19) /api/v1/namespaces/proxy-9435/pods/https:proxy-service-2g54x-x5zph:462/proxy/: tls qux (200; 4.876133ms)
Jun 12 12:30:41.094: INFO: (19) /api/v1/namespaces/proxy-9435/pods/http:proxy-service-2g54x-x5zph:160/proxy/: foo (200; 5.165879ms)
Jun 12 12:30:41.094: INFO: (19) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname2/proxy/: bar (200; 5.611466ms)
Jun 12 12:30:41.094: INFO: (19) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname2/proxy/: tls qux (200; 5.929935ms)
Jun 12 12:30:41.095: INFO: (19) /api/v1/namespaces/proxy-9435/services/https:proxy-service-2g54x:tlsportname1/proxy/: tls baz (200; 6.065014ms)
Jun 12 12:30:41.095: INFO: (19) /api/v1/namespaces/proxy-9435/services/http:proxy-service-2g54x:portname1/proxy/: foo (200; 6.29457ms)
STEP: deleting ReplicationController proxy-service-2g54x in namespace proxy-9435, will wait for the garbage collector to delete the pods
Jun 12 12:30:41.153: INFO: Deleting ReplicationController proxy-service-2g54x took: 5.958208ms
Jun 12 12:30:41.553: INFO: Terminating ReplicationController proxy-service-2g54x pods took: 400.141218ms
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:30:43.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9435" for this suite.
Jun 12 12:30:49.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:30:49.640: INFO: namespace proxy-9435 deletion completed in 6.081940186s

• [SLOW TEST:14.872 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:30:49.640: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0612 12:31:20.199547      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 12 12:31:20.199: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:31:20.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3097" for this suite.
Jun 12 12:31:26.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:31:26.280: INFO: namespace gc-3097 deletion completed in 6.077846227s

• [SLOW TEST:36.640 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:31:26.280: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jun 12 12:31:26.320: INFO: Waiting up to 5m0s for pod "downward-api-fef5c7ce-8d0d-11e9-b925-1ab852558ec8" in namespace "downward-api-4110" to be "success or failure"
Jun 12 12:31:26.322: INFO: Pod "downward-api-fef5c7ce-8d0d-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.281616ms
Jun 12 12:31:28.326: INFO: Pod "downward-api-fef5c7ce-8d0d-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005721541s
Jun 12 12:31:30.329: INFO: Pod "downward-api-fef5c7ce-8d0d-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009182264s
STEP: Saw pod success
Jun 12 12:31:30.329: INFO: Pod "downward-api-fef5c7ce-8d0d-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:31:30.332: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod downward-api-fef5c7ce-8d0d-11e9-b925-1ab852558ec8 container dapi-container: <nil>
STEP: delete the pod
Jun 12 12:31:30.350: INFO: Waiting for pod downward-api-fef5c7ce-8d0d-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:31:30.352: INFO: Pod downward-api-fef5c7ce-8d0d-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:31:30.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4110" for this suite.
Jun 12 12:31:36.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:31:36.436: INFO: namespace downward-api-4110 deletion completed in 6.078868836s

• [SLOW TEST:10.156 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:31:36.436: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 12:31:36.483: INFO: Create a RollingUpdate DaemonSet
Jun 12 12:31:36.489: INFO: Check that daemon pods launch on every node of the cluster
Jun 12 12:31:36.494: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:36.494: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:36.494: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:36.496: INFO: Number of nodes with available pods: 0
Jun 12 12:31:36.496: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 12:31:37.500: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:37.500: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:37.500: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:37.503: INFO: Number of nodes with available pods: 0
Jun 12 12:31:37.503: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 12:31:38.500: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:38.500: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:38.500: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:38.503: INFO: Number of nodes with available pods: 1
Jun 12 12:31:38.503: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 12:31:39.500: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:39.500: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:39.500: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:39.503: INFO: Number of nodes with available pods: 3
Jun 12 12:31:39.503: INFO: Number of running nodes: 3, number of available pods: 3
Jun 12 12:31:39.504: INFO: Update the DaemonSet to trigger a rollout
Jun 12 12:31:39.511: INFO: Updating DaemonSet daemon-set
Jun 12 12:31:43.520: INFO: Roll back the DaemonSet before rollout is complete
Jun 12 12:31:43.527: INFO: Updating DaemonSet daemon-set
Jun 12 12:31:43.527: INFO: Make sure DaemonSet rollback is complete
Jun 12 12:31:43.530: INFO: Wrong image for pod: daemon-set-ngrjd. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 12 12:31:43.530: INFO: Pod daemon-set-ngrjd is not available
Jun 12 12:31:43.541: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:43.541: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:43.541: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:44.545: INFO: Wrong image for pod: daemon-set-ngrjd. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 12 12:31:44.545: INFO: Pod daemon-set-ngrjd is not available
Jun 12 12:31:44.548: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:44.548: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:44.548: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:45.545: INFO: Pod daemon-set-xqsv9 is not available
Jun 12 12:31:45.548: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:45.548: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 12:31:45.548: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8759, will wait for the garbage collector to delete the pods
Jun 12 12:31:45.614: INFO: Deleting DaemonSet.extensions daemon-set took: 7.351312ms
Jun 12 12:31:45.714: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.165995ms
Jun 12 12:31:54.417: INFO: Number of nodes with available pods: 0
Jun 12 12:31:54.417: INFO: Number of running nodes: 0, number of available pods: 0
Jun 12 12:31:54.421: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8759/daemonsets","resourceVersion":"13864"},"items":null}

Jun 12 12:31:54.423: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8759/pods","resourceVersion":"13864"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:31:54.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8759" for this suite.
Jun 12 12:32:00.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:32:00.518: INFO: namespace daemonsets-8759 deletion completed in 6.081774792s

• [SLOW TEST:24.082 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:32:00.518: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 12 12:32:00.557: INFO: Waiting up to 5m0s for pod "downwardapi-volume-135d9a8a-8d0e-11e9-b925-1ab852558ec8" in namespace "downward-api-1974" to be "success or failure"
Jun 12 12:32:00.561: INFO: Pod "downwardapi-volume-135d9a8a-8d0e-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.564958ms
Jun 12 12:32:02.564: INFO: Pod "downwardapi-volume-135d9a8a-8d0e-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007245501s
STEP: Saw pod success
Jun 12 12:32:02.564: INFO: Pod "downwardapi-volume-135d9a8a-8d0e-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:32:02.567: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod downwardapi-volume-135d9a8a-8d0e-11e9-b925-1ab852558ec8 container client-container: <nil>
STEP: delete the pod
Jun 12 12:32:02.585: INFO: Waiting for pod downwardapi-volume-135d9a8a-8d0e-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:32:02.587: INFO: Pod downwardapi-volume-135d9a8a-8d0e-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:32:02.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1974" for this suite.
Jun 12 12:32:08.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:32:08.673: INFO: namespace downward-api-1974 deletion completed in 6.083045811s

• [SLOW TEST:8.154 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:32:08.673: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-qkvf
STEP: Creating a pod to test atomic-volume-subpath
Jun 12 12:32:08.741: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-qkvf" in namespace "subpath-7420" to be "success or failure"
Jun 12 12:32:08.744: INFO: Pod "pod-subpath-test-downwardapi-qkvf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.325154ms
Jun 12 12:32:10.747: INFO: Pod "pod-subpath-test-downwardapi-qkvf": Phase="Running", Reason="", readiness=true. Elapsed: 2.005900507s
Jun 12 12:32:12.751: INFO: Pod "pod-subpath-test-downwardapi-qkvf": Phase="Running", Reason="", readiness=true. Elapsed: 4.009375264s
Jun 12 12:32:14.754: INFO: Pod "pod-subpath-test-downwardapi-qkvf": Phase="Running", Reason="", readiness=true. Elapsed: 6.013012512s
Jun 12 12:32:16.758: INFO: Pod "pod-subpath-test-downwardapi-qkvf": Phase="Running", Reason="", readiness=true. Elapsed: 8.016641907s
Jun 12 12:32:18.761: INFO: Pod "pod-subpath-test-downwardapi-qkvf": Phase="Running", Reason="", readiness=true. Elapsed: 10.020089503s
Jun 12 12:32:20.765: INFO: Pod "pod-subpath-test-downwardapi-qkvf": Phase="Running", Reason="", readiness=true. Elapsed: 12.023582585s
Jun 12 12:32:22.769: INFO: Pod "pod-subpath-test-downwardapi-qkvf": Phase="Running", Reason="", readiness=true. Elapsed: 14.0272311s
Jun 12 12:32:24.772: INFO: Pod "pod-subpath-test-downwardapi-qkvf": Phase="Running", Reason="", readiness=true. Elapsed: 16.030943958s
Jun 12 12:32:26.776: INFO: Pod "pod-subpath-test-downwardapi-qkvf": Phase="Running", Reason="", readiness=true. Elapsed: 18.034719332s
Jun 12 12:32:28.779: INFO: Pod "pod-subpath-test-downwardapi-qkvf": Phase="Running", Reason="", readiness=true. Elapsed: 20.037967832s
Jun 12 12:32:30.783: INFO: Pod "pod-subpath-test-downwardapi-qkvf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.041665912s
STEP: Saw pod success
Jun 12 12:32:30.783: INFO: Pod "pod-subpath-test-downwardapi-qkvf" satisfied condition "success or failure"
Jun 12 12:32:30.786: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-subpath-test-downwardapi-qkvf container test-container-subpath-downwardapi-qkvf: <nil>
STEP: delete the pod
Jun 12 12:32:30.805: INFO: Waiting for pod pod-subpath-test-downwardapi-qkvf to disappear
Jun 12 12:32:30.808: INFO: Pod pod-subpath-test-downwardapi-qkvf no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-qkvf
Jun 12 12:32:30.808: INFO: Deleting pod "pod-subpath-test-downwardapi-qkvf" in namespace "subpath-7420"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:32:30.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7420" for this suite.
Jun 12 12:32:36.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:32:36.893: INFO: namespace subpath-7420 deletion completed in 6.080364419s

• [SLOW TEST:28.220 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:32:36.894: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-2841
Jun 12 12:32:38.947: INFO: Started pod liveness-exec in namespace container-probe-2841
STEP: checking the pod's current state and verifying that restartCount is present
Jun 12 12:32:38.949: INFO: Initial restart count of pod liveness-exec is 0
Jun 12 12:33:29.043: INFO: Restart count of pod container-probe-2841/liveness-exec is now 1 (50.094492884s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:33:29.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2841" for this suite.
Jun 12 12:33:35.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:33:35.166: INFO: namespace container-probe-2841 deletion completed in 6.106764939s

• [SLOW TEST:58.272 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:33:35.166: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 12 12:33:35.202: INFO: Waiting up to 5m0s for pod "pod-4bc7ae1c-8d0e-11e9-b925-1ab852558ec8" in namespace "emptydir-5111" to be "success or failure"
Jun 12 12:33:35.204: INFO: Pod "pod-4bc7ae1c-8d0e-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.259498ms
Jun 12 12:33:37.208: INFO: Pod "pod-4bc7ae1c-8d0e-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005865738s
Jun 12 12:33:39.211: INFO: Pod "pod-4bc7ae1c-8d0e-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009370686s
STEP: Saw pod success
Jun 12 12:33:39.211: INFO: Pod "pod-4bc7ae1c-8d0e-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:33:39.216: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-4bc7ae1c-8d0e-11e9-b925-1ab852558ec8 container test-container: <nil>
STEP: delete the pod
Jun 12 12:33:39.233: INFO: Waiting for pod pod-4bc7ae1c-8d0e-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:33:39.235: INFO: Pod pod-4bc7ae1c-8d0e-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:33:39.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5111" for this suite.
Jun 12 12:33:45.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:33:45.317: INFO: namespace emptydir-5111 deletion completed in 6.078782477s

• [SLOW TEST:10.151 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:33:45.317: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-wv67
STEP: Creating a pod to test atomic-volume-subpath
Jun 12 12:33:45.402: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wv67" in namespace "subpath-9221" to be "success or failure"
Jun 12 12:33:45.405: INFO: Pod "pod-subpath-test-configmap-wv67": Phase="Pending", Reason="", readiness=false. Elapsed: 3.537498ms
Jun 12 12:33:47.409: INFO: Pod "pod-subpath-test-configmap-wv67": Phase="Running", Reason="", readiness=true. Elapsed: 2.007023029s
Jun 12 12:33:49.412: INFO: Pod "pod-subpath-test-configmap-wv67": Phase="Running", Reason="", readiness=true. Elapsed: 4.010554586s
Jun 12 12:33:51.416: INFO: Pod "pod-subpath-test-configmap-wv67": Phase="Running", Reason="", readiness=true. Elapsed: 6.014119413s
Jun 12 12:33:53.419: INFO: Pod "pod-subpath-test-configmap-wv67": Phase="Running", Reason="", readiness=true. Elapsed: 8.017539741s
Jun 12 12:33:55.423: INFO: Pod "pod-subpath-test-configmap-wv67": Phase="Running", Reason="", readiness=true. Elapsed: 10.021502229s
Jun 12 12:33:57.427: INFO: Pod "pod-subpath-test-configmap-wv67": Phase="Running", Reason="", readiness=true. Elapsed: 12.025342202s
Jun 12 12:33:59.432: INFO: Pod "pod-subpath-test-configmap-wv67": Phase="Running", Reason="", readiness=true. Elapsed: 14.030191062s
Jun 12 12:34:01.435: INFO: Pod "pod-subpath-test-configmap-wv67": Phase="Running", Reason="", readiness=true. Elapsed: 16.033810068s
Jun 12 12:34:03.439: INFO: Pod "pod-subpath-test-configmap-wv67": Phase="Running", Reason="", readiness=true. Elapsed: 18.037296882s
Jun 12 12:34:05.443: INFO: Pod "pod-subpath-test-configmap-wv67": Phase="Running", Reason="", readiness=true. Elapsed: 20.040902392s
Jun 12 12:34:07.446: INFO: Pod "pod-subpath-test-configmap-wv67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.04422974s
STEP: Saw pod success
Jun 12 12:34:07.446: INFO: Pod "pod-subpath-test-configmap-wv67" satisfied condition "success or failure"
Jun 12 12:34:07.448: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-subpath-test-configmap-wv67 container test-container-subpath-configmap-wv67: <nil>
STEP: delete the pod
Jun 12 12:34:07.473: INFO: Waiting for pod pod-subpath-test-configmap-wv67 to disappear
Jun 12 12:34:07.475: INFO: Pod pod-subpath-test-configmap-wv67 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-wv67
Jun 12 12:34:07.475: INFO: Deleting pod "pod-subpath-test-configmap-wv67" in namespace "subpath-9221"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:34:07.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9221" for this suite.
Jun 12 12:34:13.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:34:13.558: INFO: namespace subpath-9221 deletion completed in 6.077186305s

• [SLOW TEST:28.240 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:34:13.558: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-62ad96f2-8d0e-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume configMaps
Jun 12 12:34:13.629: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-62aeaaa2-8d0e-11e9-b925-1ab852558ec8" in namespace "projected-6074" to be "success or failure"
Jun 12 12:34:13.632: INFO: Pod "pod-projected-configmaps-62aeaaa2-8d0e-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.584793ms
Jun 12 12:34:15.636: INFO: Pod "pod-projected-configmaps-62aeaaa2-8d0e-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00692494s
STEP: Saw pod success
Jun 12 12:34:15.636: INFO: Pod "pod-projected-configmaps-62aeaaa2-8d0e-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:34:15.638: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-projected-configmaps-62aeaaa2-8d0e-11e9-b925-1ab852558ec8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 12 12:34:15.661: INFO: Waiting for pod pod-projected-configmaps-62aeaaa2-8d0e-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:34:15.663: INFO: Pod pod-projected-configmaps-62aeaaa2-8d0e-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:34:15.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6074" for this suite.
Jun 12 12:34:21.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:34:21.747: INFO: namespace projected-6074 deletion completed in 6.080208835s

• [SLOW TEST:8.189 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:34:21.747: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 12 12:34:21.801: INFO: Waiting up to 5m0s for pod "downwardapi-volume-678da4e5-8d0e-11e9-b925-1ab852558ec8" in namespace "projected-8544" to be "success or failure"
Jun 12 12:34:21.803: INFO: Pod "downwardapi-volume-678da4e5-8d0e-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.141343ms
Jun 12 12:34:23.806: INFO: Pod "downwardapi-volume-678da4e5-8d0e-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005303485s
STEP: Saw pod success
Jun 12 12:34:23.806: INFO: Pod "downwardapi-volume-678da4e5-8d0e-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:34:23.809: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod downwardapi-volume-678da4e5-8d0e-11e9-b925-1ab852558ec8 container client-container: <nil>
STEP: delete the pod
Jun 12 12:34:23.826: INFO: Waiting for pod downwardapi-volume-678da4e5-8d0e-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:34:23.828: INFO: Pod downwardapi-volume-678da4e5-8d0e-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:34:23.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8544" for this suite.
Jun 12 12:34:29.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:34:29.920: INFO: namespace projected-8544 deletion completed in 6.083368465s

• [SLOW TEST:8.173 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:34:29.920: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 12:34:29.955: INFO: Creating ReplicaSet my-hostname-basic-6c6b9380-8d0e-11e9-b925-1ab852558ec8
Jun 12 12:34:29.964: INFO: Pod name my-hostname-basic-6c6b9380-8d0e-11e9-b925-1ab852558ec8: Found 0 pods out of 1
Jun 12 12:34:34.968: INFO: Pod name my-hostname-basic-6c6b9380-8d0e-11e9-b925-1ab852558ec8: Found 1 pods out of 1
Jun 12 12:34:34.968: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-6c6b9380-8d0e-11e9-b925-1ab852558ec8" is running
Jun 12 12:34:34.971: INFO: Pod "my-hostname-basic-6c6b9380-8d0e-11e9-b925-1ab852558ec8-w6wpg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-12 12:34:29 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-12 12:34:33 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-12 12:34:33 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-12 12:34:29 +0000 UTC Reason: Message:}])
Jun 12 12:34:34.971: INFO: Trying to dial the pod
Jun 12 12:34:39.980: INFO: Controller my-hostname-basic-6c6b9380-8d0e-11e9-b925-1ab852558ec8: Got expected result from replica 1 [my-hostname-basic-6c6b9380-8d0e-11e9-b925-1ab852558ec8-w6wpg]: "my-hostname-basic-6c6b9380-8d0e-11e9-b925-1ab852558ec8-w6wpg", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:34:39.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1838" for this suite.
Jun 12 12:34:45.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:34:46.064: INFO: namespace replicaset-1838 deletion completed in 6.080564319s

• [SLOW TEST:16.144 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:34:46.064: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-760b8cf0-8d0e-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume configMaps
Jun 12 12:34:46.116: INFO: Waiting up to 5m0s for pod "pod-configmaps-760c7f75-8d0e-11e9-b925-1ab852558ec8" in namespace "configmap-7983" to be "success or failure"
Jun 12 12:34:46.118: INFO: Pod "pod-configmaps-760c7f75-8d0e-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.115086ms
Jun 12 12:34:48.121: INFO: Pod "pod-configmaps-760c7f75-8d0e-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005630023s
STEP: Saw pod success
Jun 12 12:34:48.121: INFO: Pod "pod-configmaps-760c7f75-8d0e-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:34:48.124: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-configmaps-760c7f75-8d0e-11e9-b925-1ab852558ec8 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 12 12:34:48.143: INFO: Waiting for pod pod-configmaps-760c7f75-8d0e-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:34:48.145: INFO: Pod pod-configmaps-760c7f75-8d0e-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:34:48.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7983" for this suite.
Jun 12 12:34:54.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:34:54.234: INFO: namespace configmap-7983 deletion completed in 6.084764845s

• [SLOW TEST:8.170 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:34:54.235: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Jun 12 12:34:54.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 create -f - --namespace=kubectl-8175'
Jun 12 12:34:54.465: INFO: stderr: ""
Jun 12 12:34:54.465: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 12 12:34:54.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8175'
Jun 12 12:34:54.533: INFO: stderr: ""
Jun 12 12:34:54.533: INFO: stdout: "update-demo-nautilus-2ppwz update-demo-nautilus-d2v4t "
Jun 12 12:34:54.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-2ppwz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8175'
Jun 12 12:34:54.595: INFO: stderr: ""
Jun 12 12:34:54.595: INFO: stdout: ""
Jun 12 12:34:54.595: INFO: update-demo-nautilus-2ppwz is created but not running
Jun 12 12:34:59.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8175'
Jun 12 12:34:59.664: INFO: stderr: ""
Jun 12 12:34:59.664: INFO: stdout: "update-demo-nautilus-2ppwz update-demo-nautilus-d2v4t "
Jun 12 12:34:59.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-2ppwz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8175'
Jun 12 12:34:59.728: INFO: stderr: ""
Jun 12 12:34:59.728: INFO: stdout: "true"
Jun 12 12:34:59.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-2ppwz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8175'
Jun 12 12:34:59.790: INFO: stderr: ""
Jun 12 12:34:59.790: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 12 12:34:59.791: INFO: validating pod update-demo-nautilus-2ppwz
Jun 12 12:34:59.796: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 12 12:34:59.796: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 12 12:34:59.796: INFO: update-demo-nautilus-2ppwz is verified up and running
Jun 12 12:34:59.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-d2v4t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8175'
Jun 12 12:34:59.857: INFO: stderr: ""
Jun 12 12:34:59.857: INFO: stdout: "true"
Jun 12 12:34:59.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-d2v4t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8175'
Jun 12 12:34:59.919: INFO: stderr: ""
Jun 12 12:34:59.919: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 12 12:34:59.919: INFO: validating pod update-demo-nautilus-d2v4t
Jun 12 12:34:59.923: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 12 12:34:59.923: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 12 12:34:59.923: INFO: update-demo-nautilus-d2v4t is verified up and running
STEP: rolling-update to new replication controller
Jun 12 12:34:59.924: INFO: scanned /root for discovery docs: <nil>
Jun 12 12:34:59.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-8175'
Jun 12 12:35:23.276: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 12 12:35:23.276: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 12 12:35:23.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8175'
Jun 12 12:35:23.345: INFO: stderr: ""
Jun 12 12:35:23.345: INFO: stdout: "update-demo-kitten-5wzgz update-demo-kitten-cqx6l "
Jun 12 12:35:23.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-kitten-5wzgz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8175'
Jun 12 12:35:23.404: INFO: stderr: ""
Jun 12 12:35:23.404: INFO: stdout: "true"
Jun 12 12:35:23.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-kitten-5wzgz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8175'
Jun 12 12:35:23.464: INFO: stderr: ""
Jun 12 12:35:23.464: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 12 12:35:23.464: INFO: validating pod update-demo-kitten-5wzgz
Jun 12 12:35:23.469: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 12 12:35:23.469: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 12 12:35:23.469: INFO: update-demo-kitten-5wzgz is verified up and running
Jun 12 12:35:23.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-kitten-cqx6l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8175'
Jun 12 12:35:23.531: INFO: stderr: ""
Jun 12 12:35:23.531: INFO: stdout: "true"
Jun 12 12:35:23.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-kitten-cqx6l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8175'
Jun 12 12:35:23.591: INFO: stderr: ""
Jun 12 12:35:23.591: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 12 12:35:23.591: INFO: validating pod update-demo-kitten-cqx6l
Jun 12 12:35:23.595: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 12 12:35:23.595: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 12 12:35:23.595: INFO: update-demo-kitten-cqx6l is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:35:23.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8175" for this suite.
Jun 12 12:35:45.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:35:45.679: INFO: namespace kubectl-8175 deletion completed in 22.080592668s

• [SLOW TEST:51.445 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:35:45.680: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2018.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2018.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 12 12:36:05.753: INFO: DNS probes using dns-2018/dns-test-99926612-8d0e-11e9-b925-1ab852558ec8 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:36:05.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2018" for this suite.
Jun 12 12:36:11.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:36:11.846: INFO: namespace dns-2018 deletion completed in 6.078835955s

• [SLOW TEST:26.167 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:36:11.847: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 12 12:36:11.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-1889'
Jun 12 12:36:11.947: INFO: stderr: ""
Jun 12 12:36:11.947: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jun 12 12:36:16.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pod e2e-test-nginx-pod --namespace=kubectl-1889 -o json'
Jun 12 12:36:17.061: INFO: stderr: ""
Jun 12 12:36:17.061: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-06-12T12:36:11Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-1889\",\n        \"resourceVersion\": \"15033\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-1889/pods/e2e-test-nginx-pod\",\n        \"uid\": \"a93524c3-8d0e-11e9-afb2-00163e04c212\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-sh468\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cn-hongkong.192.168.0.217\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-sh468\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-sh468\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-12T12:36:11Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-12T12:36:13Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-12T12:36:13Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-12T12:36:11Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://a47d87cfcdc2b19a96300b336ba7ae477cfc4af0868467776fc056f0cd7957af\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-06-12T12:36:13Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.0.217\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.20.2.169\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-06-12T12:36:11Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 12 12:36:17.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 replace -f - --namespace=kubectl-1889'
Jun 12 12:36:17.304: INFO: stderr: ""
Jun 12 12:36:17.304: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Jun 12 12:36:17.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 delete pods e2e-test-nginx-pod --namespace=kubectl-1889'
Jun 12 12:36:24.362: INFO: stderr: ""
Jun 12 12:36:24.362: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:36:24.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1889" for this suite.
Jun 12 12:36:30.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:36:30.448: INFO: namespace kubectl-1889 deletion completed in 6.081367135s

• [SLOW TEST:18.601 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:36:30.448: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 12 12:36:38.518: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 12 12:36:38.520: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 12 12:36:40.520: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 12 12:36:40.524: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 12 12:36:42.520: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 12 12:36:42.524: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 12 12:36:44.520: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 12 12:36:44.524: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 12 12:36:46.520: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 12 12:36:46.524: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 12 12:36:48.520: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 12 12:36:48.524: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 12 12:36:50.520: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 12 12:36:50.524: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 12 12:36:52.520: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 12 12:36:52.530: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 12 12:36:54.520: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 12 12:36:54.524: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 12 12:36:56.520: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 12 12:36:56.523: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 12 12:36:58.520: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 12 12:36:58.523: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:36:58.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5966" for this suite.
Jun 12 12:37:20.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:37:20.614: INFO: namespace container-lifecycle-hook-5966 deletion completed in 22.079476594s

• [SLOW TEST:50.165 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:37:20.614: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6468
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6468
STEP: Creating statefulset with conflicting port in namespace statefulset-6468
STEP: Waiting until pod test-pod will start running in namespace statefulset-6468
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6468
Jun 12 12:37:24.673: INFO: Observed stateful pod in namespace: statefulset-6468, name: ss-0, uid: d46b6c42-8d0e-11e9-80ba-00163e04d588, status phase: Pending. Waiting for statefulset controller to delete.
Jun 12 12:37:34.351: INFO: Observed stateful pod in namespace: statefulset-6468, name: ss-0, uid: d46b6c42-8d0e-11e9-80ba-00163e04d588, status phase: Failed. Waiting for statefulset controller to delete.
Jun 12 12:37:34.358: INFO: Observed stateful pod in namespace: statefulset-6468, name: ss-0, uid: d46b6c42-8d0e-11e9-80ba-00163e04d588, status phase: Failed. Waiting for statefulset controller to delete.
Jun 12 12:37:34.363: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6468
STEP: Removing pod with conflicting port in namespace statefulset-6468
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6468 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jun 12 12:37:36.381: INFO: Deleting all statefulset in ns statefulset-6468
Jun 12 12:37:36.383: INFO: Scaling statefulset ss to 0
Jun 12 12:37:46.396: INFO: Waiting for statefulset status.replicas updated to 0
Jun 12 12:37:46.398: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:37:46.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6468" for this suite.
Jun 12 12:37:52.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:37:52.504: INFO: namespace statefulset-6468 deletion completed in 6.086455121s

• [SLOW TEST:31.890 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:37:52.505: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-7408/configmap-test-e52ad351-8d0e-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume configMaps
Jun 12 12:37:52.551: INFO: Waiting up to 5m0s for pod "pod-configmaps-e52ba743-8d0e-11e9-b925-1ab852558ec8" in namespace "configmap-7408" to be "success or failure"
Jun 12 12:37:52.553: INFO: Pod "pod-configmaps-e52ba743-8d0e-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073202ms
Jun 12 12:37:54.556: INFO: Pod "pod-configmaps-e52ba743-8d0e-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005382407s
STEP: Saw pod success
Jun 12 12:37:54.557: INFO: Pod "pod-configmaps-e52ba743-8d0e-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:37:54.559: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-configmaps-e52ba743-8d0e-11e9-b925-1ab852558ec8 container env-test: <nil>
STEP: delete the pod
Jun 12 12:37:54.587: INFO: Waiting for pod pod-configmaps-e52ba743-8d0e-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:37:54.589: INFO: Pod pod-configmaps-e52ba743-8d0e-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:37:54.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7408" for this suite.
Jun 12 12:38:00.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:38:00.671: INFO: namespace configmap-7408 deletion completed in 6.078953498s

• [SLOW TEST:8.167 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:38:00.672: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5959
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 12 12:38:00.703: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 12 12:38:26.788: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.1.143:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5959 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 12:38:26.788: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 12:38:26.997: INFO: Found all expected endpoints: [netserver-0]
Jun 12 12:38:27.001: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.2.26:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5959 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 12:38:27.001: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 12:38:27.304: INFO: Found all expected endpoints: [netserver-1]
Jun 12 12:38:27.307: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.2.174:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5959 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 12:38:27.307: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 12:38:27.571: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:38:27.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5959" for this suite.
Jun 12 12:38:49.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:38:49.655: INFO: namespace pod-network-test-5959 deletion completed in 22.078952647s

• [SLOW TEST:48.983 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:38:49.655: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 12 12:38:57.735: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 12 12:38:57.737: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 12 12:38:59.738: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 12 12:38:59.741: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 12 12:39:01.738: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 12 12:39:01.741: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 12 12:39:03.738: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 12 12:39:03.741: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 12 12:39:05.738: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 12 12:39:05.741: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 12 12:39:07.738: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 12 12:39:07.741: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 12 12:39:09.738: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 12 12:39:09.741: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 12 12:39:11.738: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 12 12:39:11.741: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 12 12:39:13.738: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 12 12:39:13.741: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 12 12:39:15.738: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 12 12:39:15.741: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 12 12:39:17.738: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 12 12:39:17.741: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:39:17.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1553" for this suite.
Jun 12 12:39:39.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:39:39.827: INFO: namespace container-lifecycle-hook-1553 deletion completed in 22.082772168s

• [SLOW TEST:50.173 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:39:39.828: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-252275a1-8d0f-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume configMaps
Jun 12 12:39:39.868: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-25234f87-8d0f-11e9-b925-1ab852558ec8" in namespace "projected-7868" to be "success or failure"
Jun 12 12:39:39.871: INFO: Pod "pod-projected-configmaps-25234f87-8d0f-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.060323ms
Jun 12 12:39:41.875: INFO: Pod "pod-projected-configmaps-25234f87-8d0f-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006620358s
STEP: Saw pod success
Jun 12 12:39:41.875: INFO: Pod "pod-projected-configmaps-25234f87-8d0f-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:39:41.877: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-projected-configmaps-25234f87-8d0f-11e9-b925-1ab852558ec8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 12 12:39:41.897: INFO: Waiting for pod pod-projected-configmaps-25234f87-8d0f-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:39:41.899: INFO: Pod pod-projected-configmaps-25234f87-8d0f-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:39:41.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7868" for this suite.
Jun 12 12:39:47.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:39:47.987: INFO: namespace projected-7868 deletion completed in 6.084830355s

• [SLOW TEST:8.159 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:39:47.987: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Jun 12 12:39:48.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 cluster-info'
Jun 12 12:39:48.331: INFO: stderr: ""
Jun 12 12:39:48.331: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\x1b[0;32mmetrics-server\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/heapster/proxy\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:39:48.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5347" for this suite.
Jun 12 12:39:54.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:39:54.422: INFO: namespace kubectl-5347 deletion completed in 6.087437955s

• [SLOW TEST:6.436 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:39:54.423: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 12 12:39:54.462: INFO: Waiting up to 5m0s for pod "pod-2dd5cf7f-8d0f-11e9-b925-1ab852558ec8" in namespace "emptydir-5577" to be "success or failure"
Jun 12 12:39:54.464: INFO: Pod "pod-2dd5cf7f-8d0f-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.167304ms
Jun 12 12:39:56.467: INFO: Pod "pod-2dd5cf7f-8d0f-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005813032s
STEP: Saw pod success
Jun 12 12:39:56.468: INFO: Pod "pod-2dd5cf7f-8d0f-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:39:56.470: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-2dd5cf7f-8d0f-11e9-b925-1ab852558ec8 container test-container: <nil>
STEP: delete the pod
Jun 12 12:39:56.489: INFO: Waiting for pod pod-2dd5cf7f-8d0f-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:39:56.491: INFO: Pod pod-2dd5cf7f-8d0f-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:39:56.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5577" for this suite.
Jun 12 12:40:02.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:40:02.575: INFO: namespace emptydir-5577 deletion completed in 6.08144268s

• [SLOW TEST:8.152 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:40:02.576: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 12:40:02.626: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 12 12:40:02.637: INFO: Number of nodes with available pods: 0
Jun 12 12:40:02.637: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun 12 12:40:02.652: INFO: Number of nodes with available pods: 0
Jun 12 12:40:02.652: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 12:40:03.655: INFO: Number of nodes with available pods: 0
Jun 12 12:40:03.655: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 12:40:04.655: INFO: Number of nodes with available pods: 1
Jun 12 12:40:04.655: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 12 12:40:04.673: INFO: Number of nodes with available pods: 1
Jun 12 12:40:04.673: INFO: Number of running nodes: 0, number of available pods: 1
Jun 12 12:40:05.676: INFO: Number of nodes with available pods: 0
Jun 12 12:40:05.676: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 12 12:40:05.687: INFO: Number of nodes with available pods: 0
Jun 12 12:40:05.687: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 12:40:06.690: INFO: Number of nodes with available pods: 0
Jun 12 12:40:06.690: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 12:40:07.690: INFO: Number of nodes with available pods: 0
Jun 12 12:40:07.690: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 12:40:08.690: INFO: Number of nodes with available pods: 0
Jun 12 12:40:08.690: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 12:40:09.690: INFO: Number of nodes with available pods: 0
Jun 12 12:40:09.690: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 12:40:10.691: INFO: Number of nodes with available pods: 0
Jun 12 12:40:10.691: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 12:40:11.691: INFO: Number of nodes with available pods: 1
Jun 12 12:40:11.691: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1136, will wait for the garbage collector to delete the pods
Jun 12 12:40:11.755: INFO: Deleting DaemonSet.extensions daemon-set took: 6.671692ms
Jun 12 12:40:12.155: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.156245ms
Jun 12 12:40:14.957: INFO: Number of nodes with available pods: 0
Jun 12 12:40:14.957: INFO: Number of running nodes: 0, number of available pods: 0
Jun 12 12:40:14.960: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1136/daemonsets","resourceVersion":"16135"},"items":null}

Jun 12 12:40:14.962: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1136/pods","resourceVersion":"16135"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:40:14.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1136" for this suite.
Jun 12 12:40:20.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:40:21.066: INFO: namespace daemonsets-1136 deletion completed in 6.079866955s

• [SLOW TEST:18.490 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:40:21.066: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 12 12:40:21.117: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-8715,SelfLink:/api/v1/namespaces/watch-8715/configmaps/e2e-watch-test-resource-version,UID:3db85e7a-8d0f-11e9-afb2-00163e04c212,ResourceVersion:16183,Generation:0,CreationTimestamp:2019-06-12 12:40:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 12 12:40:21.117: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-8715,SelfLink:/api/v1/namespaces/watch-8715/configmaps/e2e-watch-test-resource-version,UID:3db85e7a-8d0f-11e9-afb2-00163e04c212,ResourceVersion:16184,Generation:0,CreationTimestamp:2019-06-12 12:40:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:40:21.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8715" for this suite.
Jun 12 12:40:27.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:40:27.199: INFO: namespace watch-8715 deletion completed in 6.079014849s

• [SLOW TEST:6.133 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:40:27.199: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 12:40:27.226: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 12 12:40:27.234: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 12 12:40:32.237: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 12 12:40:32.237: INFO: Creating deployment "test-rolling-update-deployment"
Jun 12 12:40:32.246: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 12 12:40:32.250: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 12 12:40:34.256: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 12 12:40:34.258: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940032, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940032, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940032, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940032, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 12 12:40:36.261: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jun 12 12:40:36.267: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-3992,SelfLink:/apis/apps/v1/namespaces/deployment-3992/deployments/test-rolling-update-deployment,UID:445caec0-8d0f-11e9-afb2-00163e04c212,ResourceVersion:16275,Generation:1,CreationTimestamp:2019-06-12 12:40:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-12 12:40:32 +0000 UTC 2019-06-12 12:40:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-12 12:40:36 +0000 UTC 2019-06-12 12:40:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 12 12:40:36.270: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-3992,SelfLink:/apis/apps/v1/namespaces/deployment-3992/replicasets/test-rolling-update-deployment-67599b4d9,UID:445ff5ae-8d0f-11e9-80ba-00163e04d588,ResourceVersion:16264,Generation:1,CreationTimestamp:2019-06-12 12:40:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 445caec0-8d0f-11e9-afb2-00163e04c212 0xc0025cbf10 0xc0025cbf11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 12 12:40:36.270: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 12 12:40:36.270: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-3992,SelfLink:/apis/apps/v1/namespaces/deployment-3992/replicasets/test-rolling-update-controller,UID:415ff9fc-8d0f-11e9-afb2-00163e04c212,ResourceVersion:16274,Generation:2,CreationTimestamp:2019-06-12 12:40:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 445caec0-8d0f-11e9-afb2-00163e04c212 0xc0025cbe47 0xc0025cbe48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 12 12:40:36.273: INFO: Pod "test-rolling-update-deployment-67599b4d9-p9cxz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-p9cxz,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-3992,SelfLink:/api/v1/namespaces/deployment-3992/pods/test-rolling-update-deployment-67599b4d9-p9cxz,UID:4460ce2d-8d0f-11e9-80ba-00163e04d588,ResourceVersion:16263,Generation:0,CreationTimestamp:2019-06-12 12:40:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 445ff5ae-8d0f-11e9-80ba-00163e04d588 0xc0027d07a0 0xc0027d07a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-pvc8l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-pvc8l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-pvc8l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027d0800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027d0820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:40:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:40:36 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:40:36 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:40:32 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.217,PodIP:172.20.2.179,StartTime:2019-06-12 12:40:32 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-12 12:40:35 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://3a8399b68377cf94dcc3e22274d2c5c567537011815310bae0e531182b642786}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:40:36.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3992" for this suite.
Jun 12 12:40:42.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:40:42.368: INFO: namespace deployment-3992 deletion completed in 6.092409559s

• [SLOW TEST:15.169 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:40:42.369: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-53
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-53 to expose endpoints map[]
Jun 12 12:40:42.409: INFO: Get endpoints failed (2.324202ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jun 12 12:40:43.412: INFO: successfully validated that service endpoint-test2 in namespace services-53 exposes endpoints map[] (1.005619461s elapsed)
STEP: Creating pod pod1 in namespace services-53
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-53 to expose endpoints map[pod1:[80]]
Jun 12 12:40:46.447: INFO: successfully validated that service endpoint-test2 in namespace services-53 exposes endpoints map[pod1:[80]] (3.027914466s elapsed)
STEP: Creating pod pod2 in namespace services-53
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-53 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 12 12:40:49.503: INFO: successfully validated that service endpoint-test2 in namespace services-53 exposes endpoints map[pod1:[80] pod2:[80]] (3.051049871s elapsed)
STEP: Deleting pod pod1 in namespace services-53
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-53 to expose endpoints map[pod2:[80]]
Jun 12 12:40:50.524: INFO: successfully validated that service endpoint-test2 in namespace services-53 exposes endpoints map[pod2:[80]] (1.015858174s elapsed)
STEP: Deleting pod pod2 in namespace services-53
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-53 to expose endpoints map[]
Jun 12 12:40:51.545: INFO: successfully validated that service endpoint-test2 in namespace services-53 exposes endpoints map[] (1.013178852s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:40:51.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-53" for this suite.
Jun 12 12:41:09.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:41:09.658: INFO: namespace services-53 deletion completed in 18.084727309s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:27.290 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:41:09.658: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-5aae0728-8d0f-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume secrets
Jun 12 12:41:09.703: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5aaee832-8d0f-11e9-b925-1ab852558ec8" in namespace "projected-7602" to be "success or failure"
Jun 12 12:41:09.707: INFO: Pod "pod-projected-secrets-5aaee832-8d0f-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.139671ms
Jun 12 12:41:11.710: INFO: Pod "pod-projected-secrets-5aaee832-8d0f-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007611541s
STEP: Saw pod success
Jun 12 12:41:11.710: INFO: Pod "pod-projected-secrets-5aaee832-8d0f-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:41:11.713: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-projected-secrets-5aaee832-8d0f-11e9-b925-1ab852558ec8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 12 12:41:11.739: INFO: Waiting for pod pod-projected-secrets-5aaee832-8d0f-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:41:11.741: INFO: Pod pod-projected-secrets-5aaee832-8d0f-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:41:11.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7602" for this suite.
Jun 12 12:41:17.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:41:17.826: INFO: namespace projected-7602 deletion completed in 6.082009997s

• [SLOW TEST:8.168 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:41:17.826: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1850
I0612 12:41:17.874941      17 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1850, replica count: 1
I0612 12:41:18.925251      17 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0612 12:41:19.925413      17 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 12 12:41:20.041: INFO: Created: latency-svc-5cns4
Jun 12 12:41:20.047: INFO: Got endpoints: latency-svc-5cns4 [21.677715ms]
Jun 12 12:41:20.059: INFO: Created: latency-svc-np8n6
Jun 12 12:41:20.060: INFO: Created: latency-svc-dckdd
Jun 12 12:41:20.064: INFO: Got endpoints: latency-svc-np8n6 [17.453069ms]
Jun 12 12:41:20.067: INFO: Got endpoints: latency-svc-dckdd [20.254308ms]
Jun 12 12:41:20.068: INFO: Created: latency-svc-5kk5j
Jun 12 12:41:20.073: INFO: Got endpoints: latency-svc-5kk5j [25.672432ms]
Jun 12 12:41:20.074: INFO: Created: latency-svc-xrqgq
Jun 12 12:41:20.080: INFO: Created: latency-svc-dc9gc
Jun 12 12:41:20.086: INFO: Got endpoints: latency-svc-xrqgq [38.760289ms]
Jun 12 12:41:20.087: INFO: Created: latency-svc-g5792
Jun 12 12:41:20.088: INFO: Got endpoints: latency-svc-dc9gc [40.136323ms]
Jun 12 12:41:20.095: INFO: Got endpoints: latency-svc-g5792 [47.162627ms]
Jun 12 12:41:20.095: INFO: Created: latency-svc-g6br2
Jun 12 12:41:20.102: INFO: Got endpoints: latency-svc-g6br2 [54.185984ms]
Jun 12 12:41:20.102: INFO: Created: latency-svc-x4jl6
Jun 12 12:41:20.103: INFO: Created: latency-svc-bfdw4
Jun 12 12:41:20.121: INFO: Got endpoints: latency-svc-bfdw4 [73.374864ms]
Jun 12 12:41:20.121: INFO: Got endpoints: latency-svc-x4jl6 [73.584903ms]
Jun 12 12:41:20.124: INFO: Created: latency-svc-jj7pw
Jun 12 12:41:20.124: INFO: Created: latency-svc-jthqx
Jun 12 12:41:20.124: INFO: Created: latency-svc-f9jbl
Jun 12 12:41:20.126: INFO: Created: latency-svc-z5r88
Jun 12 12:41:20.133: INFO: Got endpoints: latency-svc-f9jbl [85.640975ms]
Jun 12 12:41:20.133: INFO: Got endpoints: latency-svc-jj7pw [85.851854ms]
Jun 12 12:41:20.134: INFO: Got endpoints: latency-svc-jthqx [85.980815ms]
Jun 12 12:41:20.134: INFO: Got endpoints: latency-svc-z5r88 [86.240037ms]
Jun 12 12:41:20.134: INFO: Created: latency-svc-rp4n8
Jun 12 12:41:20.141: INFO: Created: latency-svc-pppbs
Jun 12 12:41:20.142: INFO: Got endpoints: latency-svc-rp4n8 [94.516669ms]
Jun 12 12:41:20.148: INFO: Created: latency-svc-ggrjs
Jun 12 12:41:20.150: INFO: Got endpoints: latency-svc-pppbs [102.093451ms]
Jun 12 12:41:20.153: INFO: Created: latency-svc-nxzcj
Jun 12 12:41:20.156: INFO: Got endpoints: latency-svc-ggrjs [91.352146ms]
Jun 12 12:41:20.157: INFO: Created: latency-svc-n2zbf
Jun 12 12:41:20.161: INFO: Got endpoints: latency-svc-nxzcj [93.215971ms]
Jun 12 12:41:20.172: INFO: Got endpoints: latency-svc-n2zbf [98.750127ms]
Jun 12 12:41:20.173: INFO: Created: latency-svc-s77lq
Jun 12 12:41:20.179: INFO: Created: latency-svc-kxqkj
Jun 12 12:41:20.182: INFO: Got endpoints: latency-svc-s77lq [96.318387ms]
Jun 12 12:41:20.183: INFO: Created: latency-svc-c6nqj
Jun 12 12:41:20.187: INFO: Got endpoints: latency-svc-kxqkj [99.046646ms]
Jun 12 12:41:20.187: INFO: Created: latency-svc-7m286
Jun 12 12:41:20.193: INFO: Got endpoints: latency-svc-7m286 [91.630812ms]
Jun 12 12:41:20.193: INFO: Created: latency-svc-kbrtm
Jun 12 12:41:20.193: INFO: Got endpoints: latency-svc-c6nqj [98.758354ms]
Jun 12 12:41:20.201: INFO: Got endpoints: latency-svc-kbrtm [80.110652ms]
Jun 12 12:41:20.202: INFO: Created: latency-svc-t48nq
Jun 12 12:41:20.203: INFO: Created: latency-svc-jm7jn
Jun 12 12:41:20.208: INFO: Got endpoints: latency-svc-t48nq [86.393632ms]
Jun 12 12:41:20.208: INFO: Created: latency-svc-wccjf
Jun 12 12:41:20.209: INFO: Got endpoints: latency-svc-jm7jn [75.349599ms]
Jun 12 12:41:20.214: INFO: Got endpoints: latency-svc-wccjf [80.852441ms]
Jun 12 12:41:20.215: INFO: Created: latency-svc-zt6f9
Jun 12 12:41:20.219: INFO: Got endpoints: latency-svc-zt6f9 [85.660951ms]
Jun 12 12:41:20.225: INFO: Created: latency-svc-qnncs
Jun 12 12:41:20.228: INFO: Created: latency-svc-llsfn
Jun 12 12:41:20.228: INFO: Got endpoints: latency-svc-qnncs [94.48784ms]
Jun 12 12:41:20.233: INFO: Created: latency-svc-8bx8b
Jun 12 12:41:20.234: INFO: Got endpoints: latency-svc-llsfn [91.320365ms]
Jun 12 12:41:20.241: INFO: Created: latency-svc-hwvbt
Jun 12 12:41:20.241: INFO: Got endpoints: latency-svc-8bx8b [91.150895ms]
Jun 12 12:41:20.245: INFO: Created: latency-svc-4zsv8
Jun 12 12:41:20.250: INFO: Got endpoints: latency-svc-hwvbt [93.868755ms]
Jun 12 12:41:20.255: INFO: Got endpoints: latency-svc-4zsv8 [94.562201ms]
Jun 12 12:41:20.256: INFO: Created: latency-svc-5bzf7
Jun 12 12:41:20.260: INFO: Created: latency-svc-wsf9x
Jun 12 12:41:20.262: INFO: Got endpoints: latency-svc-5bzf7 [90.453963ms]
Jun 12 12:41:20.263: INFO: Created: latency-svc-6r9mt
Jun 12 12:41:20.266: INFO: Created: latency-svc-46qpz
Jun 12 12:41:20.270: INFO: Created: latency-svc-kr695
Jun 12 12:41:20.276: INFO: Created: latency-svc-mbtwq
Jun 12 12:41:20.287: INFO: Created: latency-svc-nxwqj
Jun 12 12:41:20.287: INFO: Created: latency-svc-7qlz8
Jun 12 12:41:20.290: INFO: Created: latency-svc-9pmrc
Jun 12 12:41:20.300: INFO: Got endpoints: latency-svc-wsf9x [117.666867ms]
Jun 12 12:41:20.301: INFO: Created: latency-svc-kt69n
Jun 12 12:41:20.301: INFO: Created: latency-svc-znpdq
Jun 12 12:41:20.303: INFO: Created: latency-svc-s9vwr
Jun 12 12:41:20.312: INFO: Created: latency-svc-xhtpk
Jun 12 12:41:20.312: INFO: Created: latency-svc-nsrfk
Jun 12 12:41:20.320: INFO: Created: latency-svc-tq28k
Jun 12 12:41:20.326: INFO: Created: latency-svc-mzx8k
Jun 12 12:41:20.331: INFO: Created: latency-svc-r9blr
Jun 12 12:41:20.346: INFO: Got endpoints: latency-svc-6r9mt [159.61008ms]
Jun 12 12:41:20.358: INFO: Created: latency-svc-t9ddk
Jun 12 12:41:20.398: INFO: Got endpoints: latency-svc-46qpz [142.903206ms]
Jun 12 12:41:20.407: INFO: Created: latency-svc-2jxz6
Jun 12 12:41:20.453: INFO: Got endpoints: latency-svc-kr695 [259.7507ms]
Jun 12 12:41:20.463: INFO: Created: latency-svc-6khd2
Jun 12 12:41:20.496: INFO: Got endpoints: latency-svc-mbtwq [302.250736ms]
Jun 12 12:41:20.507: INFO: Created: latency-svc-swjbd
Jun 12 12:41:20.548: INFO: Got endpoints: latency-svc-nxwqj [346.763076ms]
Jun 12 12:41:20.560: INFO: Created: latency-svc-9zjd7
Jun 12 12:41:20.595: INFO: Got endpoints: latency-svc-7qlz8 [387.824568ms]
Jun 12 12:41:20.604: INFO: Created: latency-svc-6jvst
Jun 12 12:41:20.645: INFO: Got endpoints: latency-svc-9pmrc [436.087709ms]
Jun 12 12:41:20.653: INFO: Created: latency-svc-sckcz
Jun 12 12:41:20.695: INFO: Got endpoints: latency-svc-znpdq [480.820922ms]
Jun 12 12:41:20.707: INFO: Created: latency-svc-c4dtq
Jun 12 12:41:20.746: INFO: Got endpoints: latency-svc-kt69n [526.703326ms]
Jun 12 12:41:20.756: INFO: Created: latency-svc-7b8jm
Jun 12 12:41:20.795: INFO: Got endpoints: latency-svc-s9vwr [566.711083ms]
Jun 12 12:41:20.804: INFO: Created: latency-svc-g5npv
Jun 12 12:41:20.849: INFO: Got endpoints: latency-svc-xhtpk [615.240958ms]
Jun 12 12:41:20.858: INFO: Created: latency-svc-z825g
Jun 12 12:41:20.896: INFO: Got endpoints: latency-svc-nsrfk [654.896829ms]
Jun 12 12:41:20.905: INFO: Created: latency-svc-h6p2n
Jun 12 12:41:20.951: INFO: Got endpoints: latency-svc-tq28k [701.138906ms]
Jun 12 12:41:20.959: INFO: Created: latency-svc-b44g7
Jun 12 12:41:20.997: INFO: Got endpoints: latency-svc-mzx8k [734.38847ms]
Jun 12 12:41:21.006: INFO: Created: latency-svc-nvsqs
Jun 12 12:41:21.047: INFO: Got endpoints: latency-svc-r9blr [746.863233ms]
Jun 12 12:41:21.060: INFO: Created: latency-svc-gw4rr
Jun 12 12:41:21.097: INFO: Got endpoints: latency-svc-t9ddk [750.271104ms]
Jun 12 12:41:21.106: INFO: Created: latency-svc-9bzwc
Jun 12 12:41:21.146: INFO: Got endpoints: latency-svc-2jxz6 [747.760389ms]
Jun 12 12:41:21.165: INFO: Created: latency-svc-4d8cg
Jun 12 12:41:21.196: INFO: Got endpoints: latency-svc-6khd2 [742.356774ms]
Jun 12 12:41:21.206: INFO: Created: latency-svc-6zk4w
Jun 12 12:41:21.256: INFO: Got endpoints: latency-svc-swjbd [760.239361ms]
Jun 12 12:41:21.268: INFO: Created: latency-svc-6sqnd
Jun 12 12:41:21.296: INFO: Got endpoints: latency-svc-9zjd7 [748.152379ms]
Jun 12 12:41:21.304: INFO: Created: latency-svc-m5shz
Jun 12 12:41:21.346: INFO: Got endpoints: latency-svc-6jvst [751.012613ms]
Jun 12 12:41:21.354: INFO: Created: latency-svc-zhvdt
Jun 12 12:41:21.396: INFO: Got endpoints: latency-svc-sckcz [750.701033ms]
Jun 12 12:41:21.403: INFO: Created: latency-svc-62rqd
Jun 12 12:41:21.447: INFO: Got endpoints: latency-svc-c4dtq [751.997873ms]
Jun 12 12:41:21.457: INFO: Created: latency-svc-gkh8p
Jun 12 12:41:21.496: INFO: Got endpoints: latency-svc-7b8jm [749.92945ms]
Jun 12 12:41:21.504: INFO: Created: latency-svc-q8p5k
Jun 12 12:41:21.548: INFO: Got endpoints: latency-svc-g5npv [752.788717ms]
Jun 12 12:41:21.556: INFO: Created: latency-svc-4c8dg
Jun 12 12:41:21.596: INFO: Got endpoints: latency-svc-z825g [747.071373ms]
Jun 12 12:41:21.604: INFO: Created: latency-svc-2j9bg
Jun 12 12:41:21.645: INFO: Got endpoints: latency-svc-h6p2n [749.466927ms]
Jun 12 12:41:21.654: INFO: Created: latency-svc-mmpzt
Jun 12 12:41:21.695: INFO: Got endpoints: latency-svc-b44g7 [744.513485ms]
Jun 12 12:41:21.703: INFO: Created: latency-svc-msqzb
Jun 12 12:41:21.745: INFO: Got endpoints: latency-svc-nvsqs [748.619066ms]
Jun 12 12:41:21.755: INFO: Created: latency-svc-5ptdt
Jun 12 12:41:21.795: INFO: Got endpoints: latency-svc-gw4rr [748.404773ms]
Jun 12 12:41:21.804: INFO: Created: latency-svc-5cbzd
Jun 12 12:41:21.846: INFO: Got endpoints: latency-svc-9bzwc [749.880208ms]
Jun 12 12:41:21.856: INFO: Created: latency-svc-662wh
Jun 12 12:41:21.896: INFO: Got endpoints: latency-svc-4d8cg [749.5671ms]
Jun 12 12:41:21.904: INFO: Created: latency-svc-g5tv6
Jun 12 12:41:21.949: INFO: Got endpoints: latency-svc-6zk4w [753.025258ms]
Jun 12 12:41:21.959: INFO: Created: latency-svc-sz9gz
Jun 12 12:41:21.997: INFO: Got endpoints: latency-svc-6sqnd [740.727857ms]
Jun 12 12:41:22.005: INFO: Created: latency-svc-9zjck
Jun 12 12:41:22.046: INFO: Got endpoints: latency-svc-m5shz [750.078371ms]
Jun 12 12:41:22.056: INFO: Created: latency-svc-pz22j
Jun 12 12:41:22.100: INFO: Got endpoints: latency-svc-zhvdt [753.722493ms]
Jun 12 12:41:22.109: INFO: Created: latency-svc-dkq8n
Jun 12 12:41:22.146: INFO: Got endpoints: latency-svc-62rqd [750.122535ms]
Jun 12 12:41:22.157: INFO: Created: latency-svc-6zncd
Jun 12 12:41:22.198: INFO: Got endpoints: latency-svc-gkh8p [750.697145ms]
Jun 12 12:41:22.208: INFO: Created: latency-svc-72znj
Jun 12 12:41:22.248: INFO: Got endpoints: latency-svc-q8p5k [752.708448ms]
Jun 12 12:41:22.260: INFO: Created: latency-svc-gbp9f
Jun 12 12:41:22.296: INFO: Got endpoints: latency-svc-4c8dg [747.679303ms]
Jun 12 12:41:22.305: INFO: Created: latency-svc-4455g
Jun 12 12:41:22.348: INFO: Got endpoints: latency-svc-2j9bg [752.425863ms]
Jun 12 12:41:22.574: INFO: Got endpoints: latency-svc-mmpzt [928.715868ms]
Jun 12 12:41:22.575: INFO: Got endpoints: latency-svc-5ptdt [829.256134ms]
Jun 12 12:41:22.575: INFO: Got endpoints: latency-svc-msqzb [879.378207ms]
Jun 12 12:41:22.575: INFO: Got endpoints: latency-svc-5cbzd [779.633012ms]
Jun 12 12:41:22.579: INFO: Created: latency-svc-sp96m
Jun 12 12:41:22.585: INFO: Created: latency-svc-m2p84
Jun 12 12:41:22.591: INFO: Created: latency-svc-phncd
Jun 12 12:41:22.599: INFO: Got endpoints: latency-svc-662wh [752.54045ms]
Jun 12 12:41:22.600: INFO: Created: latency-svc-dt66b
Jun 12 12:41:22.602: INFO: Created: latency-svc-dzr65
Jun 12 12:41:22.607: INFO: Created: latency-svc-v7fs2
Jun 12 12:41:22.647: INFO: Got endpoints: latency-svc-g5tv6 [751.808671ms]
Jun 12 12:41:22.657: INFO: Created: latency-svc-4gwst
Jun 12 12:41:22.701: INFO: Got endpoints: latency-svc-sz9gz [752.731063ms]
Jun 12 12:41:22.710: INFO: Created: latency-svc-4bp4r
Jun 12 12:41:22.747: INFO: Got endpoints: latency-svc-9zjck [750.637545ms]
Jun 12 12:41:22.759: INFO: Created: latency-svc-cvrzf
Jun 12 12:41:22.798: INFO: Got endpoints: latency-svc-pz22j [751.992243ms]
Jun 12 12:41:22.809: INFO: Created: latency-svc-jxf8h
Jun 12 12:41:22.848: INFO: Got endpoints: latency-svc-dkq8n [747.475672ms]
Jun 12 12:41:22.860: INFO: Created: latency-svc-d2pwc
Jun 12 12:41:22.896: INFO: Got endpoints: latency-svc-6zncd [750.663037ms]
Jun 12 12:41:22.905: INFO: Created: latency-svc-gvx6m
Jun 12 12:41:22.952: INFO: Got endpoints: latency-svc-72znj [753.944932ms]
Jun 12 12:41:22.970: INFO: Created: latency-svc-qcfdr
Jun 12 12:41:22.996: INFO: Got endpoints: latency-svc-gbp9f [747.788303ms]
Jun 12 12:41:23.005: INFO: Created: latency-svc-wxpjz
Jun 12 12:41:23.056: INFO: Got endpoints: latency-svc-4455g [760.010401ms]
Jun 12 12:41:23.069: INFO: Created: latency-svc-gbtht
Jun 12 12:41:23.099: INFO: Got endpoints: latency-svc-sp96m [750.206261ms]
Jun 12 12:41:23.107: INFO: Created: latency-svc-cgjlk
Jun 12 12:41:23.155: INFO: Got endpoints: latency-svc-m2p84 [581.226838ms]
Jun 12 12:41:23.179: INFO: Created: latency-svc-z9g5g
Jun 12 12:41:23.207: INFO: Got endpoints: latency-svc-phncd [632.802186ms]
Jun 12 12:41:23.218: INFO: Created: latency-svc-qz7lp
Jun 12 12:41:23.250: INFO: Got endpoints: latency-svc-dt66b [675.318777ms]
Jun 12 12:41:23.323: INFO: Got endpoints: latency-svc-dzr65 [748.272425ms]
Jun 12 12:41:23.324: INFO: Created: latency-svc-kgbdt
Jun 12 12:41:23.333: INFO: Created: latency-svc-tdhnw
Jun 12 12:41:23.382: INFO: Got endpoints: latency-svc-v7fs2 [783.014836ms]
Jun 12 12:41:23.426: INFO: Got endpoints: latency-svc-4gwst [778.802344ms]
Jun 12 12:41:23.427: INFO: Created: latency-svc-c5cqb
Jun 12 12:41:23.452: INFO: Got endpoints: latency-svc-4bp4r [750.470787ms]
Jun 12 12:41:23.453: INFO: Created: latency-svc-2nrrh
Jun 12 12:41:23.462: INFO: Created: latency-svc-s2scj
Jun 12 12:41:23.502: INFO: Got endpoints: latency-svc-cvrzf [754.369412ms]
Jun 12 12:41:23.510: INFO: Created: latency-svc-w54jh
Jun 12 12:41:23.548: INFO: Got endpoints: latency-svc-jxf8h [749.87373ms]
Jun 12 12:41:23.558: INFO: Created: latency-svc-q9zrs
Jun 12 12:41:23.597: INFO: Got endpoints: latency-svc-d2pwc [749.094794ms]
Jun 12 12:41:23.606: INFO: Created: latency-svc-cszth
Jun 12 12:41:23.645: INFO: Got endpoints: latency-svc-gvx6m [748.918127ms]
Jun 12 12:41:23.657: INFO: Created: latency-svc-wtwzr
Jun 12 12:41:23.696: INFO: Got endpoints: latency-svc-qcfdr [743.690202ms]
Jun 12 12:41:23.704: INFO: Created: latency-svc-xt7pt
Jun 12 12:41:23.745: INFO: Got endpoints: latency-svc-wxpjz [749.254652ms]
Jun 12 12:41:23.753: INFO: Created: latency-svc-mwm86
Jun 12 12:41:23.810: INFO: Got endpoints: latency-svc-gbtht [753.961324ms]
Jun 12 12:41:23.818: INFO: Created: latency-svc-kpxdw
Jun 12 12:41:23.846: INFO: Got endpoints: latency-svc-cgjlk [747.297734ms]
Jun 12 12:41:23.854: INFO: Created: latency-svc-5wcqc
Jun 12 12:41:23.899: INFO: Got endpoints: latency-svc-z9g5g [743.535883ms]
Jun 12 12:41:23.909: INFO: Created: latency-svc-rhf92
Jun 12 12:41:23.952: INFO: Got endpoints: latency-svc-qz7lp [744.52086ms]
Jun 12 12:41:23.962: INFO: Created: latency-svc-2bkdr
Jun 12 12:41:23.997: INFO: Got endpoints: latency-svc-kgbdt [746.30597ms]
Jun 12 12:41:24.007: INFO: Created: latency-svc-98mrz
Jun 12 12:41:24.048: INFO: Got endpoints: latency-svc-tdhnw [724.52547ms]
Jun 12 12:41:24.060: INFO: Created: latency-svc-6mxs5
Jun 12 12:41:24.098: INFO: Got endpoints: latency-svc-c5cqb [715.816802ms]
Jun 12 12:41:24.109: INFO: Created: latency-svc-z59tq
Jun 12 12:41:24.146: INFO: Got endpoints: latency-svc-2nrrh [719.537465ms]
Jun 12 12:41:24.159: INFO: Created: latency-svc-x5g8l
Jun 12 12:41:24.198: INFO: Got endpoints: latency-svc-s2scj [745.986579ms]
Jun 12 12:41:24.211: INFO: Created: latency-svc-5wblx
Jun 12 12:41:24.258: INFO: Got endpoints: latency-svc-w54jh [756.480497ms]
Jun 12 12:41:24.267: INFO: Created: latency-svc-tc6sf
Jun 12 12:41:24.296: INFO: Got endpoints: latency-svc-q9zrs [748.422588ms]
Jun 12 12:41:24.310: INFO: Created: latency-svc-fjj88
Jun 12 12:41:24.347: INFO: Got endpoints: latency-svc-cszth [750.40029ms]
Jun 12 12:41:24.357: INFO: Created: latency-svc-cw68r
Jun 12 12:41:24.397: INFO: Got endpoints: latency-svc-wtwzr [751.383721ms]
Jun 12 12:41:24.406: INFO: Created: latency-svc-b9wxn
Jun 12 12:41:24.451: INFO: Got endpoints: latency-svc-xt7pt [755.233179ms]
Jun 12 12:41:24.462: INFO: Created: latency-svc-4dtws
Jun 12 12:41:24.497: INFO: Got endpoints: latency-svc-mwm86 [751.593399ms]
Jun 12 12:41:24.508: INFO: Created: latency-svc-ftkdf
Jun 12 12:41:24.546: INFO: Got endpoints: latency-svc-kpxdw [735.778263ms]
Jun 12 12:41:24.562: INFO: Created: latency-svc-rc67w
Jun 12 12:41:24.597: INFO: Got endpoints: latency-svc-5wcqc [750.807992ms]
Jun 12 12:41:24.608: INFO: Created: latency-svc-xht4k
Jun 12 12:41:24.647: INFO: Got endpoints: latency-svc-rhf92 [748.019978ms]
Jun 12 12:41:24.654: INFO: Created: latency-svc-dlt9t
Jun 12 12:41:24.696: INFO: Got endpoints: latency-svc-2bkdr [743.743761ms]
Jun 12 12:41:24.704: INFO: Created: latency-svc-vf2pg
Jun 12 12:41:24.746: INFO: Got endpoints: latency-svc-98mrz [749.5556ms]
Jun 12 12:41:24.760: INFO: Created: latency-svc-xts94
Jun 12 12:41:24.796: INFO: Got endpoints: latency-svc-6mxs5 [748.198471ms]
Jun 12 12:41:24.805: INFO: Created: latency-svc-47zmq
Jun 12 12:41:24.846: INFO: Got endpoints: latency-svc-z59tq [747.726254ms]
Jun 12 12:41:24.854: INFO: Created: latency-svc-74sdw
Jun 12 12:41:24.895: INFO: Got endpoints: latency-svc-x5g8l [749.316326ms]
Jun 12 12:41:24.903: INFO: Created: latency-svc-r7v5s
Jun 12 12:41:24.946: INFO: Got endpoints: latency-svc-5wblx [747.64842ms]
Jun 12 12:41:24.955: INFO: Created: latency-svc-zxqqr
Jun 12 12:41:24.995: INFO: Got endpoints: latency-svc-tc6sf [736.587787ms]
Jun 12 12:41:25.003: INFO: Created: latency-svc-lfjf2
Jun 12 12:41:25.045: INFO: Got endpoints: latency-svc-fjj88 [748.430529ms]
Jun 12 12:41:25.053: INFO: Created: latency-svc-q6xp9
Jun 12 12:41:25.097: INFO: Got endpoints: latency-svc-cw68r [749.397146ms]
Jun 12 12:41:25.105: INFO: Created: latency-svc-b2gs5
Jun 12 12:41:25.145: INFO: Got endpoints: latency-svc-b9wxn [748.499226ms]
Jun 12 12:41:25.155: INFO: Created: latency-svc-f6v8d
Jun 12 12:41:25.196: INFO: Got endpoints: latency-svc-4dtws [745.145912ms]
Jun 12 12:41:25.204: INFO: Created: latency-svc-t694b
Jun 12 12:41:25.245: INFO: Got endpoints: latency-svc-ftkdf [748.38147ms]
Jun 12 12:41:25.258: INFO: Created: latency-svc-4f22h
Jun 12 12:41:25.297: INFO: Got endpoints: latency-svc-rc67w [751.001031ms]
Jun 12 12:41:25.305: INFO: Created: latency-svc-kcwsb
Jun 12 12:41:25.345: INFO: Got endpoints: latency-svc-xht4k [747.932276ms]
Jun 12 12:41:25.356: INFO: Created: latency-svc-2gdgv
Jun 12 12:41:25.395: INFO: Got endpoints: latency-svc-dlt9t [748.085651ms]
Jun 12 12:41:25.404: INFO: Created: latency-svc-2jsm8
Jun 12 12:41:25.446: INFO: Got endpoints: latency-svc-vf2pg [749.871992ms]
Jun 12 12:41:25.458: INFO: Created: latency-svc-5gfm5
Jun 12 12:41:25.496: INFO: Got endpoints: latency-svc-xts94 [749.771335ms]
Jun 12 12:41:25.504: INFO: Created: latency-svc-8pncz
Jun 12 12:41:25.572: INFO: Got endpoints: latency-svc-47zmq [775.394879ms]
Jun 12 12:41:25.581: INFO: Created: latency-svc-g5wnm
Jun 12 12:41:25.601: INFO: Got endpoints: latency-svc-74sdw [755.18969ms]
Jun 12 12:41:25.611: INFO: Created: latency-svc-m6fqz
Jun 12 12:41:25.656: INFO: Got endpoints: latency-svc-r7v5s [760.41485ms]
Jun 12 12:41:25.665: INFO: Created: latency-svc-crd8t
Jun 12 12:41:25.696: INFO: Got endpoints: latency-svc-zxqqr [750.020836ms]
Jun 12 12:41:25.704: INFO: Created: latency-svc-q2nt8
Jun 12 12:41:25.747: INFO: Got endpoints: latency-svc-lfjf2 [751.93612ms]
Jun 12 12:41:25.758: INFO: Created: latency-svc-fqv29
Jun 12 12:41:25.796: INFO: Got endpoints: latency-svc-q6xp9 [751.168693ms]
Jun 12 12:41:25.804: INFO: Created: latency-svc-bwnbh
Jun 12 12:41:25.846: INFO: Got endpoints: latency-svc-b2gs5 [749.148123ms]
Jun 12 12:41:25.855: INFO: Created: latency-svc-q7b65
Jun 12 12:41:25.896: INFO: Got endpoints: latency-svc-f6v8d [750.308617ms]
Jun 12 12:41:25.904: INFO: Created: latency-svc-sf7lv
Jun 12 12:41:25.946: INFO: Got endpoints: latency-svc-t694b [749.683977ms]
Jun 12 12:41:25.956: INFO: Created: latency-svc-mlmwq
Jun 12 12:41:25.996: INFO: Got endpoints: latency-svc-4f22h [750.14828ms]
Jun 12 12:41:26.004: INFO: Created: latency-svc-8nnz6
Jun 12 12:41:26.046: INFO: Got endpoints: latency-svc-kcwsb [749.427123ms]
Jun 12 12:41:26.057: INFO: Created: latency-svc-48c98
Jun 12 12:41:26.096: INFO: Got endpoints: latency-svc-2gdgv [751.551766ms]
Jun 12 12:41:26.105: INFO: Created: latency-svc-jnwml
Jun 12 12:41:26.146: INFO: Got endpoints: latency-svc-2jsm8 [750.635429ms]
Jun 12 12:41:26.154: INFO: Created: latency-svc-8g6n9
Jun 12 12:41:26.198: INFO: Got endpoints: latency-svc-5gfm5 [752.667893ms]
Jun 12 12:41:26.207: INFO: Created: latency-svc-8sthf
Jun 12 12:41:26.246: INFO: Got endpoints: latency-svc-8pncz [750.126862ms]
Jun 12 12:41:26.258: INFO: Created: latency-svc-knj4g
Jun 12 12:41:26.297: INFO: Got endpoints: latency-svc-g5wnm [725.176767ms]
Jun 12 12:41:26.305: INFO: Created: latency-svc-wsfkx
Jun 12 12:41:26.347: INFO: Got endpoints: latency-svc-m6fqz [746.106ms]
Jun 12 12:41:26.356: INFO: Created: latency-svc-fhwtn
Jun 12 12:41:26.397: INFO: Got endpoints: latency-svc-crd8t [741.820431ms]
Jun 12 12:41:26.409: INFO: Created: latency-svc-cpn9l
Jun 12 12:41:26.446: INFO: Got endpoints: latency-svc-q2nt8 [750.207784ms]
Jun 12 12:41:26.456: INFO: Created: latency-svc-cqbtl
Jun 12 12:41:26.496: INFO: Got endpoints: latency-svc-fqv29 [748.965156ms]
Jun 12 12:41:26.507: INFO: Created: latency-svc-4lwg7
Jun 12 12:41:26.546: INFO: Got endpoints: latency-svc-bwnbh [749.473816ms]
Jun 12 12:41:26.557: INFO: Created: latency-svc-d2str
Jun 12 12:41:26.595: INFO: Got endpoints: latency-svc-q7b65 [749.448727ms]
Jun 12 12:41:26.609: INFO: Created: latency-svc-72v75
Jun 12 12:41:26.646: INFO: Got endpoints: latency-svc-sf7lv [750.764733ms]
Jun 12 12:41:26.657: INFO: Created: latency-svc-tmhgj
Jun 12 12:41:26.697: INFO: Got endpoints: latency-svc-mlmwq [750.682387ms]
Jun 12 12:41:26.707: INFO: Created: latency-svc-5sjvx
Jun 12 12:41:26.745: INFO: Got endpoints: latency-svc-8nnz6 [749.599982ms]
Jun 12 12:41:26.752: INFO: Created: latency-svc-8gp76
Jun 12 12:41:26.797: INFO: Got endpoints: latency-svc-48c98 [750.647444ms]
Jun 12 12:41:26.810: INFO: Created: latency-svc-7khq2
Jun 12 12:41:26.848: INFO: Got endpoints: latency-svc-jnwml [751.934351ms]
Jun 12 12:41:26.856: INFO: Created: latency-svc-bcwlq
Jun 12 12:41:26.899: INFO: Got endpoints: latency-svc-8g6n9 [753.545099ms]
Jun 12 12:41:26.910: INFO: Created: latency-svc-qlw42
Jun 12 12:41:26.949: INFO: Got endpoints: latency-svc-8sthf [750.235299ms]
Jun 12 12:41:26.958: INFO: Created: latency-svc-gnxfq
Jun 12 12:41:26.996: INFO: Got endpoints: latency-svc-knj4g [750.039469ms]
Jun 12 12:41:27.004: INFO: Created: latency-svc-mh8hw
Jun 12 12:41:27.054: INFO: Got endpoints: latency-svc-wsfkx [756.977686ms]
Jun 12 12:41:27.064: INFO: Created: latency-svc-vqh2k
Jun 12 12:41:27.097: INFO: Got endpoints: latency-svc-fhwtn [750.329089ms]
Jun 12 12:41:27.107: INFO: Created: latency-svc-l9dp9
Jun 12 12:41:27.146: INFO: Got endpoints: latency-svc-cpn9l [748.994898ms]
Jun 12 12:41:27.158: INFO: Created: latency-svc-hzn2x
Jun 12 12:41:27.196: INFO: Got endpoints: latency-svc-cqbtl [750.592562ms]
Jun 12 12:41:27.206: INFO: Created: latency-svc-vs29j
Jun 12 12:41:27.247: INFO: Got endpoints: latency-svc-4lwg7 [750.910519ms]
Jun 12 12:41:27.259: INFO: Created: latency-svc-99s9w
Jun 12 12:41:27.295: INFO: Got endpoints: latency-svc-d2str [749.402963ms]
Jun 12 12:41:27.304: INFO: Created: latency-svc-thzbp
Jun 12 12:41:27.345: INFO: Got endpoints: latency-svc-72v75 [750.157863ms]
Jun 12 12:41:27.359: INFO: Created: latency-svc-qmdk5
Jun 12 12:41:27.395: INFO: Got endpoints: latency-svc-tmhgj [748.862472ms]
Jun 12 12:41:27.403: INFO: Created: latency-svc-kt96z
Jun 12 12:41:27.445: INFO: Got endpoints: latency-svc-5sjvx [748.83675ms]
Jun 12 12:41:27.456: INFO: Created: latency-svc-ssp2h
Jun 12 12:41:27.495: INFO: Got endpoints: latency-svc-8gp76 [750.157818ms]
Jun 12 12:41:27.504: INFO: Created: latency-svc-x4c8n
Jun 12 12:41:27.565: INFO: Got endpoints: latency-svc-7khq2 [768.455287ms]
Jun 12 12:41:27.574: INFO: Created: latency-svc-r6z26
Jun 12 12:41:27.596: INFO: Got endpoints: latency-svc-bcwlq [747.490813ms]
Jun 12 12:41:27.604: INFO: Created: latency-svc-mrpxv
Jun 12 12:41:27.648: INFO: Got endpoints: latency-svc-qlw42 [748.444853ms]
Jun 12 12:41:27.658: INFO: Created: latency-svc-fg5xr
Jun 12 12:41:27.700: INFO: Got endpoints: latency-svc-gnxfq [751.093863ms]
Jun 12 12:41:27.709: INFO: Created: latency-svc-9jhlh
Jun 12 12:41:27.745: INFO: Got endpoints: latency-svc-mh8hw [748.848545ms]
Jun 12 12:41:27.756: INFO: Created: latency-svc-4qgv2
Jun 12 12:41:27.795: INFO: Got endpoints: latency-svc-vqh2k [741.578471ms]
Jun 12 12:41:27.804: INFO: Created: latency-svc-92lb8
Jun 12 12:41:27.846: INFO: Got endpoints: latency-svc-l9dp9 [748.232126ms]
Jun 12 12:41:27.855: INFO: Created: latency-svc-4vh9j
Jun 12 12:41:27.897: INFO: Got endpoints: latency-svc-hzn2x [750.062497ms]
Jun 12 12:41:27.947: INFO: Got endpoints: latency-svc-vs29j [750.251373ms]
Jun 12 12:41:27.997: INFO: Got endpoints: latency-svc-99s9w [749.91183ms]
Jun 12 12:41:28.047: INFO: Got endpoints: latency-svc-thzbp [751.491085ms]
Jun 12 12:41:28.100: INFO: Got endpoints: latency-svc-qmdk5 [754.312002ms]
Jun 12 12:41:28.163: INFO: Got endpoints: latency-svc-kt96z [767.212845ms]
Jun 12 12:41:28.196: INFO: Got endpoints: latency-svc-ssp2h [750.49512ms]
Jun 12 12:41:28.248: INFO: Got endpoints: latency-svc-x4c8n [752.23695ms]
Jun 12 12:41:28.296: INFO: Got endpoints: latency-svc-r6z26 [730.861446ms]
Jun 12 12:41:28.372: INFO: Got endpoints: latency-svc-mrpxv [775.9139ms]
Jun 12 12:41:28.418: INFO: Got endpoints: latency-svc-fg5xr [770.431874ms]
Jun 12 12:41:28.446: INFO: Got endpoints: latency-svc-9jhlh [746.682734ms]
Jun 12 12:41:28.516: INFO: Got endpoints: latency-svc-4qgv2 [771.25594ms]
Jun 12 12:41:28.550: INFO: Got endpoints: latency-svc-92lb8 [754.796585ms]
Jun 12 12:41:28.605: INFO: Got endpoints: latency-svc-4vh9j [759.652091ms]
Jun 12 12:41:28.605: INFO: Latencies: [17.453069ms 20.254308ms 25.672432ms 38.760289ms 40.136323ms 47.162627ms 54.185984ms 73.374864ms 73.584903ms 75.349599ms 80.110652ms 80.852441ms 85.640975ms 85.660951ms 85.851854ms 85.980815ms 86.240037ms 86.393632ms 90.453963ms 91.150895ms 91.320365ms 91.352146ms 91.630812ms 93.215971ms 93.868755ms 94.48784ms 94.516669ms 94.562201ms 96.318387ms 98.750127ms 98.758354ms 99.046646ms 102.093451ms 117.666867ms 142.903206ms 159.61008ms 259.7507ms 302.250736ms 346.763076ms 387.824568ms 436.087709ms 480.820922ms 526.703326ms 566.711083ms 581.226838ms 615.240958ms 632.802186ms 654.896829ms 675.318777ms 701.138906ms 715.816802ms 719.537465ms 724.52547ms 725.176767ms 730.861446ms 734.38847ms 735.778263ms 736.587787ms 740.727857ms 741.578471ms 741.820431ms 742.356774ms 743.535883ms 743.690202ms 743.743761ms 744.513485ms 744.52086ms 745.145912ms 745.986579ms 746.106ms 746.30597ms 746.682734ms 746.863233ms 747.071373ms 747.297734ms 747.475672ms 747.490813ms 747.64842ms 747.679303ms 747.726254ms 747.760389ms 747.788303ms 747.932276ms 748.019978ms 748.085651ms 748.152379ms 748.198471ms 748.232126ms 748.272425ms 748.38147ms 748.404773ms 748.422588ms 748.430529ms 748.444853ms 748.499226ms 748.619066ms 748.83675ms 748.848545ms 748.862472ms 748.918127ms 748.965156ms 748.994898ms 749.094794ms 749.148123ms 749.254652ms 749.316326ms 749.397146ms 749.402963ms 749.427123ms 749.448727ms 749.466927ms 749.473816ms 749.5556ms 749.5671ms 749.599982ms 749.683977ms 749.771335ms 749.871992ms 749.87373ms 749.880208ms 749.91183ms 749.92945ms 750.020836ms 750.039469ms 750.062497ms 750.078371ms 750.122535ms 750.126862ms 750.14828ms 750.157818ms 750.157863ms 750.206261ms 750.207784ms 750.235299ms 750.251373ms 750.271104ms 750.308617ms 750.329089ms 750.40029ms 750.470787ms 750.49512ms 750.592562ms 750.635429ms 750.637545ms 750.647444ms 750.663037ms 750.682387ms 750.697145ms 750.701033ms 750.764733ms 750.807992ms 750.910519ms 751.001031ms 751.012613ms 751.093863ms 751.168693ms 751.383721ms 751.491085ms 751.551766ms 751.593399ms 751.808671ms 751.934351ms 751.93612ms 751.992243ms 751.997873ms 752.23695ms 752.425863ms 752.54045ms 752.667893ms 752.708448ms 752.731063ms 752.788717ms 753.025258ms 753.545099ms 753.722493ms 753.944932ms 753.961324ms 754.312002ms 754.369412ms 754.796585ms 755.18969ms 755.233179ms 756.480497ms 756.977686ms 759.652091ms 760.010401ms 760.239361ms 760.41485ms 767.212845ms 768.455287ms 770.431874ms 771.25594ms 775.394879ms 775.9139ms 778.802344ms 779.633012ms 783.014836ms 829.256134ms 879.378207ms 928.715868ms]
Jun 12 12:41:28.605: INFO: 50 %ile: 748.965156ms
Jun 12 12:41:28.605: INFO: 90 %ile: 755.18969ms
Jun 12 12:41:28.605: INFO: 99 %ile: 879.378207ms
Jun 12 12:41:28.605: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:41:28.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1850" for this suite.
Jun 12 12:41:50.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:41:50.689: INFO: namespace svc-latency-1850 deletion completed in 22.079410127s

• [SLOW TEST:32.863 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:41:50.689: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-7323c820-8d0f-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume configMaps
Jun 12 12:41:50.737: INFO: Waiting up to 5m0s for pod "pod-configmaps-73249f9e-8d0f-11e9-b925-1ab852558ec8" in namespace "configmap-5325" to be "success or failure"
Jun 12 12:41:50.739: INFO: Pod "pod-configmaps-73249f9e-8d0f-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.210626ms
Jun 12 12:41:52.743: INFO: Pod "pod-configmaps-73249f9e-8d0f-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006275517s
Jun 12 12:41:54.747: INFO: Pod "pod-configmaps-73249f9e-8d0f-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009939354s
STEP: Saw pod success
Jun 12 12:41:54.747: INFO: Pod "pod-configmaps-73249f9e-8d0f-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:41:54.750: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-configmaps-73249f9e-8d0f-11e9-b925-1ab852558ec8 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 12 12:41:54.768: INFO: Waiting for pod pod-configmaps-73249f9e-8d0f-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:41:54.770: INFO: Pod pod-configmaps-73249f9e-8d0f-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:41:54.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5325" for this suite.
Jun 12 12:42:00.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:42:00.854: INFO: namespace configmap-5325 deletion completed in 6.080871652s

• [SLOW TEST:10.165 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:42:00.854: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 12:42:00.884: INFO: Creating deployment "test-recreate-deployment"
Jun 12 12:42:00.890: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 12 12:42:00.894: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jun 12 12:42:02.900: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 12 12:42:02.902: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940120, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940120, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940120, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940120, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 12 12:42:04.905: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 12 12:42:04.912: INFO: Updating deployment test-recreate-deployment
Jun 12 12:42:04.912: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jun 12 12:42:04.982: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-3096,SelfLink:/apis/apps/v1/namespaces/deployment-3096/deployments/test-recreate-deployment,UID:79333248-8d0f-11e9-afb2-00163e04c212,ResourceVersion:17933,Generation:2,CreationTimestamp:2019-06-12 12:42:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-06-12 12:42:04 +0000 UTC 2019-06-12 12:42:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-06-12 12:42:04 +0000 UTC 2019-06-12 12:42:00 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jun 12 12:42:04.985: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-3096,SelfLink:/apis/apps/v1/namespaces/deployment-3096/replicasets/test-recreate-deployment-c9cbd8684,UID:7b9c4a1f-8d0f-11e9-80ba-00163e04d588,ResourceVersion:17932,Generation:1,CreationTimestamp:2019-06-12 12:42:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 79333248-8d0f-11e9-afb2-00163e04c212 0xc002c32810 0xc002c32811}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 12 12:42:04.985: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 12 12:42:04.985: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-3096,SelfLink:/apis/apps/v1/namespaces/deployment-3096/replicasets/test-recreate-deployment-7d57d5ff7c,UID:7930c3ae-8d0f-11e9-80ba-00163e04d588,ResourceVersion:17923,Generation:2,CreationTimestamp:2019-06-12 12:42:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 79333248-8d0f-11e9-afb2-00163e04c212 0xc002c32747 0xc002c32748}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 12 12:42:04.987: INFO: Pod "test-recreate-deployment-c9cbd8684-xl7p6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-xl7p6,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-3096,SelfLink:/api/v1/namespaces/deployment-3096/pods/test-recreate-deployment-c9cbd8684-xl7p6,UID:7b9cdc5b-8d0f-11e9-80ba-00163e04d588,ResourceVersion:17930,Generation:0,CreationTimestamp:2019-06-12 12:42:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 7b9c4a1f-8d0f-11e9-80ba-00163e04d588 0xc002c33050 0xc002c33051}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-6xwlb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6xwlb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6xwlb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.218,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c330b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c330d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:42:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:42:04.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3096" for this suite.
Jun 12 12:42:11.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:42:11.077: INFO: namespace deployment-3096 deletion completed in 6.081957633s

• [SLOW TEST:10.223 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:42:11.077: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jun 12 12:42:11.107: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 12 12:42:11.116: INFO: Waiting for terminating namespaces to be deleted...
Jun 12 12:42:11.118: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.217 before test
Jun 12 12:42:11.124: INFO: kube-proxy-worker-p54dg from kube-system started at 2019-06-12 11:16:30 +0000 UTC (1 container statuses recorded)
Jun 12 12:42:11.124: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Jun 12 12:42:11.124: INFO: kube-flannel-ds-x7nmf from kube-system started at 2019-06-12 11:16:31 +0000 UTC (2 container statuses recorded)
Jun 12 12:42:11.124: INFO: 	Container install-cni ready: true, restart count 0
Jun 12 12:42:11.124: INFO: 	Container kube-flannel ready: true, restart count 0
Jun 12 12:42:11.124: INFO: aliyun-acr-credential-helper-6bb54dfdfd-5gk8t from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 12:42:11.124: INFO: 	Container aliyun-acr-credential-helper ready: true, restart count 0
Jun 12 12:42:11.124: INFO: nginx-ingress-controller-8584849666-qrnjk from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 12:42:11.124: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jun 12 12:42:11.124: INFO: flexvolume-qzj4v from kube-system started at 2019-06-12 12:20:56 +0000 UTC (1 container statuses recorded)
Jun 12 12:42:11.124: INFO: 	Container acs-flexvolume ready: true, restart count 0
Jun 12 12:42:11.124: INFO: sonobuoy-systemd-logs-daemon-set-abb5e6914f274695-4skg8 from heptio-sonobuoy started at 2019-06-12 12:21:07 +0000 UTC (2 container statuses recorded)
Jun 12 12:42:11.124: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 12 12:42:11.124: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 12 12:42:11.124: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.218 before test
Jun 12 12:42:11.129: INFO: coredns-7594b4cbdd-9ht8c from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 12:42:11.129: INFO: 	Container coredns ready: true, restart count 0
Jun 12 12:42:11.129: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 12:42:11.129: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 12 12:42:11.129: INFO: sonobuoy-systemd-logs-daemon-set-abb5e6914f274695-27hnn from heptio-sonobuoy started at 2019-06-12 12:21:07 +0000 UTC (2 container statuses recorded)
Jun 12 12:42:11.129: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 12 12:42:11.129: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 12 12:42:11.129: INFO: kube-proxy-worker-2mnhq from kube-system started at 2019-06-12 11:16:30 +0000 UTC (1 container statuses recorded)
Jun 12 12:42:11.129: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Jun 12 12:42:11.129: INFO: kube-flannel-ds-9h6cz from kube-system started at 2019-06-12 11:16:31 +0000 UTC (2 container statuses recorded)
Jun 12 12:42:11.129: INFO: 	Container install-cni ready: true, restart count 0
Jun 12 12:42:11.129: INFO: 	Container kube-flannel ready: true, restart count 0
Jun 12 12:42:11.129: INFO: flexvolume-5mrxl from kube-system started at 2019-06-12 12:20:55 +0000 UTC (1 container statuses recorded)
Jun 12 12:42:11.129: INFO: 	Container acs-flexvolume ready: true, restart count 0
Jun 12 12:42:11.129: INFO: metrics-server-645cb87c9d-ddkrk from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 12:42:11.129: INFO: 	Container metrics-server ready: true, restart count 0
Jun 12 12:42:11.129: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.219 before test
Jun 12 12:42:11.136: INFO: coredns-7594b4cbdd-76756 from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 12:42:11.136: INFO: 	Container coredns ready: true, restart count 0
Jun 12 12:42:11.136: INFO: tiller-deploy-7f49bdd789-tp29r from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 12:42:11.136: INFO: 	Container tiller ready: true, restart count 0
Jun 12 12:42:11.136: INFO: sonobuoy-systemd-logs-daemon-set-abb5e6914f274695-6v8ls from heptio-sonobuoy started at 2019-06-12 12:21:07 +0000 UTC (2 container statuses recorded)
Jun 12 12:42:11.136: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 12 12:42:11.136: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 12 12:42:11.136: INFO: kube-flannel-ds-2xjfg from kube-system started at 2019-06-12 11:16:31 +0000 UTC (2 container statuses recorded)
Jun 12 12:42:11.136: INFO: 	Container install-cni ready: true, restart count 0
Jun 12 12:42:11.136: INFO: 	Container kube-flannel ready: true, restart count 0
Jun 12 12:42:11.136: INFO: flexvolume-mwfdp from kube-system started at 2019-06-12 12:20:54 +0000 UTC (1 container statuses recorded)
Jun 12 12:42:11.136: INFO: 	Container acs-flexvolume ready: true, restart count 0
Jun 12 12:42:11.136: INFO: nginx-ingress-controller-8584849666-ft4jg from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 12:42:11.136: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jun 12 12:42:11.136: INFO: sonobuoy-e2e-job-1d838d5dfce648c8 from heptio-sonobuoy started at 2019-06-12 12:21:07 +0000 UTC (2 container statuses recorded)
Jun 12 12:42:11.136: INFO: 	Container e2e ready: true, restart count 0
Jun 12 12:42:11.136: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 12 12:42:11.136: INFO: kube-proxy-worker-94r4b from kube-system started at 2019-06-12 11:16:30 +0000 UTC (1 container statuses recorded)
Jun 12 12:42:11.136: INFO: 	Container kube-proxy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node cn-hongkong.192.168.0.217
STEP: verifying the node has the label node cn-hongkong.192.168.0.218
STEP: verifying the node has the label node cn-hongkong.192.168.0.219
Jun 12 12:42:11.212: INFO: Pod sonobuoy requesting resource cpu=0m on Node cn-hongkong.192.168.0.218
Jun 12 12:42:11.212: INFO: Pod sonobuoy-e2e-job-1d838d5dfce648c8 requesting resource cpu=0m on Node cn-hongkong.192.168.0.219
Jun 12 12:42:11.212: INFO: Pod sonobuoy-systemd-logs-daemon-set-abb5e6914f274695-27hnn requesting resource cpu=0m on Node cn-hongkong.192.168.0.218
Jun 12 12:42:11.212: INFO: Pod sonobuoy-systemd-logs-daemon-set-abb5e6914f274695-4skg8 requesting resource cpu=0m on Node cn-hongkong.192.168.0.217
Jun 12 12:42:11.212: INFO: Pod sonobuoy-systemd-logs-daemon-set-abb5e6914f274695-6v8ls requesting resource cpu=0m on Node cn-hongkong.192.168.0.219
Jun 12 12:42:11.212: INFO: Pod aliyun-acr-credential-helper-6bb54dfdfd-5gk8t requesting resource cpu=0m on Node cn-hongkong.192.168.0.217
Jun 12 12:42:11.212: INFO: Pod coredns-7594b4cbdd-76756 requesting resource cpu=100m on Node cn-hongkong.192.168.0.219
Jun 12 12:42:11.212: INFO: Pod coredns-7594b4cbdd-9ht8c requesting resource cpu=100m on Node cn-hongkong.192.168.0.218
Jun 12 12:42:11.212: INFO: Pod flexvolume-5mrxl requesting resource cpu=100m on Node cn-hongkong.192.168.0.218
Jun 12 12:42:11.212: INFO: Pod flexvolume-mwfdp requesting resource cpu=100m on Node cn-hongkong.192.168.0.219
Jun 12 12:42:11.212: INFO: Pod flexvolume-qzj4v requesting resource cpu=100m on Node cn-hongkong.192.168.0.217
Jun 12 12:42:11.212: INFO: Pod kube-flannel-ds-2xjfg requesting resource cpu=0m on Node cn-hongkong.192.168.0.219
Jun 12 12:42:11.212: INFO: Pod kube-flannel-ds-9h6cz requesting resource cpu=0m on Node cn-hongkong.192.168.0.218
Jun 12 12:42:11.212: INFO: Pod kube-flannel-ds-x7nmf requesting resource cpu=0m on Node cn-hongkong.192.168.0.217
Jun 12 12:42:11.212: INFO: Pod kube-proxy-worker-2mnhq requesting resource cpu=0m on Node cn-hongkong.192.168.0.218
Jun 12 12:42:11.212: INFO: Pod kube-proxy-worker-94r4b requesting resource cpu=0m on Node cn-hongkong.192.168.0.219
Jun 12 12:42:11.212: INFO: Pod kube-proxy-worker-p54dg requesting resource cpu=0m on Node cn-hongkong.192.168.0.217
Jun 12 12:42:11.212: INFO: Pod metrics-server-645cb87c9d-ddkrk requesting resource cpu=0m on Node cn-hongkong.192.168.0.218
Jun 12 12:42:11.212: INFO: Pod nginx-ingress-controller-8584849666-ft4jg requesting resource cpu=0m on Node cn-hongkong.192.168.0.219
Jun 12 12:42:11.212: INFO: Pod nginx-ingress-controller-8584849666-qrnjk requesting resource cpu=0m on Node cn-hongkong.192.168.0.217
Jun 12 12:42:11.212: INFO: Pod tiller-deploy-7f49bdd789-tp29r requesting resource cpu=0m on Node cn-hongkong.192.168.0.219
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f59bac1-8d0f-11e9-b925-1ab852558ec8.15a773fe20819563], Reason = [Scheduled], Message = [Successfully assigned sched-pred-930/filler-pod-7f59bac1-8d0f-11e9-b925-1ab852558ec8 to cn-hongkong.192.168.0.217]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f59bac1-8d0f-11e9-b925-1ab852558ec8.15a773fe4d7aa529], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f59bac1-8d0f-11e9-b925-1ab852558ec8.15a773fe4fee2b5c], Reason = [Created], Message = [Created container filler-pod-7f59bac1-8d0f-11e9-b925-1ab852558ec8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f59bac1-8d0f-11e9-b925-1ab852558ec8.15a773fe62b7d5a7], Reason = [Started], Message = [Started container filler-pod-7f59bac1-8d0f-11e9-b925-1ab852558ec8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f5b358f-8d0f-11e9-b925-1ab852558ec8.15a773fe2107cb61], Reason = [Scheduled], Message = [Successfully assigned sched-pred-930/filler-pod-7f5b358f-8d0f-11e9-b925-1ab852558ec8 to cn-hongkong.192.168.0.218]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f5b358f-8d0f-11e9-b925-1ab852558ec8.15a773fe54027aae], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f5b358f-8d0f-11e9-b925-1ab852558ec8.15a773fe58b5db4b], Reason = [Created], Message = [Created container filler-pod-7f5b358f-8d0f-11e9-b925-1ab852558ec8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f5b358f-8d0f-11e9-b925-1ab852558ec8.15a773fe6bc0ccbe], Reason = [Started], Message = [Started container filler-pod-7f5b358f-8d0f-11e9-b925-1ab852558ec8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f5c7a2d-8d0f-11e9-b925-1ab852558ec8.15a773fe21b1782b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-930/filler-pod-7f5c7a2d-8d0f-11e9-b925-1ab852558ec8 to cn-hongkong.192.168.0.219]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f5c7a2d-8d0f-11e9-b925-1ab852558ec8.15a773fe52772009], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f5c7a2d-8d0f-11e9-b925-1ab852558ec8.15a773fe9af6dd9b], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f5c7a2d-8d0f-11e9-b925-1ab852558ec8.15a773fe9e496653], Reason = [Created], Message = [Created container filler-pod-7f5c7a2d-8d0f-11e9-b925-1ab852558ec8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f5c7a2d-8d0f-11e9-b925-1ab852558ec8.15a773feb0199ed4], Reason = [Started], Message = [Started container filler-pod-7f5c7a2d-8d0f-11e9-b925-1ab852558ec8]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15a773ff10f12cb8], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node cn-hongkong.192.168.0.217
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node cn-hongkong.192.168.0.218
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node cn-hongkong.192.168.0.219
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:42:16.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-930" for this suite.
Jun 12 12:42:22.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:42:22.396: INFO: namespace sched-pred-930 deletion completed in 6.08566743s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:11.319 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:42:22.396: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Jun 12 12:42:22.864: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Jun 12 12:42:24.911: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 12 12:42:26.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 12 12:42:28.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 12 12:42:30.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 12 12:42:32.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695940142, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 12 12:42:38.657: INFO: Waited 3.722955596s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:42:39.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-692" for this suite.
Jun 12 12:42:45.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:42:45.386: INFO: namespace aggregator-692 deletion completed in 6.172720643s

• [SLOW TEST:22.990 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:42:45.386: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-779.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-779.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-779.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-779.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-779.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-779.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-779.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-779.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-779.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-779.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-779.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-779.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-779.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 215.14.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.14.215_udp@PTR;check="$$(dig +tcp +noall +answer +search 215.14.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.14.215_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-779.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-779.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-779.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-779.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-779.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-779.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-779.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-779.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-779.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-779.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-779.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-779.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-779.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 215.14.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.14.215_udp@PTR;check="$$(dig +tcp +noall +answer +search 215.14.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.14.215_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 12 12:43:03.466: INFO: Unable to read wheezy_udp@dns-test-service.dns-779.svc.cluster.local from pod dns-779/dns-test-93bfa500-8d0f-11e9-b925-1ab852558ec8: the server could not find the requested resource (get pods dns-test-93bfa500-8d0f-11e9-b925-1ab852558ec8)
Jun 12 12:43:03.469: INFO: Unable to read wheezy_tcp@dns-test-service.dns-779.svc.cluster.local from pod dns-779/dns-test-93bfa500-8d0f-11e9-b925-1ab852558ec8: the server could not find the requested resource (get pods dns-test-93bfa500-8d0f-11e9-b925-1ab852558ec8)
Jun 12 12:43:03.472: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-779.svc.cluster.local from pod dns-779/dns-test-93bfa500-8d0f-11e9-b925-1ab852558ec8: the server could not find the requested resource (get pods dns-test-93bfa500-8d0f-11e9-b925-1ab852558ec8)
Jun 12 12:43:03.475: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-779.svc.cluster.local from pod dns-779/dns-test-93bfa500-8d0f-11e9-b925-1ab852558ec8: the server could not find the requested resource (get pods dns-test-93bfa500-8d0f-11e9-b925-1ab852558ec8)
Jun 12 12:43:03.493: INFO: Unable to read jessie_udp@dns-test-service.dns-779.svc.cluster.local from pod dns-779/dns-test-93bfa500-8d0f-11e9-b925-1ab852558ec8: the server could not find the requested resource (get pods dns-test-93bfa500-8d0f-11e9-b925-1ab852558ec8)
Jun 12 12:43:03.496: INFO: Unable to read jessie_tcp@dns-test-service.dns-779.svc.cluster.local from pod dns-779/dns-test-93bfa500-8d0f-11e9-b925-1ab852558ec8: the server could not find the requested resource (get pods dns-test-93bfa500-8d0f-11e9-b925-1ab852558ec8)
Jun 12 12:43:03.498: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-779.svc.cluster.local from pod dns-779/dns-test-93bfa500-8d0f-11e9-b925-1ab852558ec8: the server could not find the requested resource (get pods dns-test-93bfa500-8d0f-11e9-b925-1ab852558ec8)
Jun 12 12:43:03.501: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-779.svc.cluster.local from pod dns-779/dns-test-93bfa500-8d0f-11e9-b925-1ab852558ec8: the server could not find the requested resource (get pods dns-test-93bfa500-8d0f-11e9-b925-1ab852558ec8)
Jun 12 12:43:03.516: INFO: Lookups using dns-779/dns-test-93bfa500-8d0f-11e9-b925-1ab852558ec8 failed for: [wheezy_udp@dns-test-service.dns-779.svc.cluster.local wheezy_tcp@dns-test-service.dns-779.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-779.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-779.svc.cluster.local jessie_udp@dns-test-service.dns-779.svc.cluster.local jessie_tcp@dns-test-service.dns-779.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-779.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-779.svc.cluster.local]

Jun 12 12:43:08.572: INFO: DNS probes using dns-779/dns-test-93bfa500-8d0f-11e9-b925-1ab852558ec8 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:43:08.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-779" for this suite.
Jun 12 12:43:14.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:43:14.722: INFO: namespace dns-779 deletion completed in 6.078801349s

• [SLOW TEST:29.336 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:43:14.722: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jun 12 12:43:14.764: INFO: Waiting up to 5m0s for pod "downward-api-a539601f-8d0f-11e9-b925-1ab852558ec8" in namespace "downward-api-6139" to be "success or failure"
Jun 12 12:43:14.767: INFO: Pod "downward-api-a539601f-8d0f-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.599787ms
Jun 12 12:43:16.771: INFO: Pod "downward-api-a539601f-8d0f-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006971203s
Jun 12 12:43:18.774: INFO: Pod "downward-api-a539601f-8d0f-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010178403s
STEP: Saw pod success
Jun 12 12:43:18.774: INFO: Pod "downward-api-a539601f-8d0f-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:43:18.776: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod downward-api-a539601f-8d0f-11e9-b925-1ab852558ec8 container dapi-container: <nil>
STEP: delete the pod
Jun 12 12:43:18.794: INFO: Waiting for pod downward-api-a539601f-8d0f-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:43:18.797: INFO: Pod downward-api-a539601f-8d0f-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:43:18.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6139" for this suite.
Jun 12 12:43:24.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:43:24.878: INFO: namespace downward-api-6139 deletion completed in 6.078288069s

• [SLOW TEST:10.156 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:43:24.879: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Jun 12 12:43:24.916: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-652313554 proxy --unix-socket=/tmp/kubectl-proxy-unix920854605/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:43:24.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5554" for this suite.
Jun 12 12:43:30.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:43:31.049: INFO: namespace kubectl-5554 deletion completed in 6.081123989s

• [SLOW TEST:6.170 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:43:31.049: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-aef91700-8d0f-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume secrets
Jun 12 12:43:31.122: INFO: Waiting up to 5m0s for pod "pod-secrets-aef9e312-8d0f-11e9-b925-1ab852558ec8" in namespace "secrets-1863" to be "success or failure"
Jun 12 12:43:31.124: INFO: Pod "pod-secrets-aef9e312-8d0f-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.234813ms
Jun 12 12:43:33.132: INFO: Pod "pod-secrets-aef9e312-8d0f-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010103705s
Jun 12 12:43:35.135: INFO: Pod "pod-secrets-aef9e312-8d0f-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013640304s
STEP: Saw pod success
Jun 12 12:43:35.135: INFO: Pod "pod-secrets-aef9e312-8d0f-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:43:35.138: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-secrets-aef9e312-8d0f-11e9-b925-1ab852558ec8 container secret-volume-test: <nil>
STEP: delete the pod
Jun 12 12:43:35.157: INFO: Waiting for pod pod-secrets-aef9e312-8d0f-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:43:35.159: INFO: Pod pod-secrets-aef9e312-8d0f-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:43:35.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1863" for this suite.
Jun 12 12:43:41.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:43:41.257: INFO: namespace secrets-1863 deletion completed in 6.088390224s

• [SLOW TEST:10.208 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:43:41.257: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-b50a0591-8d0f-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume configMaps
Jun 12 12:43:41.300: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b50ad096-8d0f-11e9-b925-1ab852558ec8" in namespace "projected-1057" to be "success or failure"
Jun 12 12:43:41.304: INFO: Pod "pod-projected-configmaps-b50ad096-8d0f-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.323448ms
Jun 12 12:43:43.308: INFO: Pod "pod-projected-configmaps-b50ad096-8d0f-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007913156s
STEP: Saw pod success
Jun 12 12:43:43.308: INFO: Pod "pod-projected-configmaps-b50ad096-8d0f-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:43:43.310: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-projected-configmaps-b50ad096-8d0f-11e9-b925-1ab852558ec8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 12 12:43:43.331: INFO: Waiting for pod pod-projected-configmaps-b50ad096-8d0f-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:43:43.333: INFO: Pod pod-projected-configmaps-b50ad096-8d0f-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:43:43.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1057" for this suite.
Jun 12 12:43:49.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:43:49.421: INFO: namespace projected-1057 deletion completed in 6.084681753s

• [SLOW TEST:8.165 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:43:49.422: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Jun 12 12:43:49.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 create -f - --namespace=kubectl-5399'
Jun 12 12:43:49.695: INFO: stderr: ""
Jun 12 12:43:49.695: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 12 12:43:49.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5399'
Jun 12 12:43:49.759: INFO: stderr: ""
Jun 12 12:43:49.759: INFO: stdout: "update-demo-nautilus-7sp7c update-demo-nautilus-rfsz6 "
Jun 12 12:43:49.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-7sp7c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5399'
Jun 12 12:43:49.818: INFO: stderr: ""
Jun 12 12:43:49.818: INFO: stdout: ""
Jun 12 12:43:49.818: INFO: update-demo-nautilus-7sp7c is created but not running
Jun 12 12:43:54.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5399'
Jun 12 12:43:54.885: INFO: stderr: ""
Jun 12 12:43:54.885: INFO: stdout: "update-demo-nautilus-7sp7c update-demo-nautilus-rfsz6 "
Jun 12 12:43:54.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-7sp7c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5399'
Jun 12 12:43:54.947: INFO: stderr: ""
Jun 12 12:43:54.947: INFO: stdout: "true"
Jun 12 12:43:54.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-7sp7c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5399'
Jun 12 12:43:55.007: INFO: stderr: ""
Jun 12 12:43:55.007: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 12 12:43:55.007: INFO: validating pod update-demo-nautilus-7sp7c
Jun 12 12:43:55.012: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 12 12:43:55.012: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 12 12:43:55.012: INFO: update-demo-nautilus-7sp7c is verified up and running
Jun 12 12:43:55.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-rfsz6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5399'
Jun 12 12:43:55.072: INFO: stderr: ""
Jun 12 12:43:55.072: INFO: stdout: "true"
Jun 12 12:43:55.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods update-demo-nautilus-rfsz6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5399'
Jun 12 12:43:55.134: INFO: stderr: ""
Jun 12 12:43:55.134: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 12 12:43:55.134: INFO: validating pod update-demo-nautilus-rfsz6
Jun 12 12:43:55.138: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 12 12:43:55.138: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 12 12:43:55.138: INFO: update-demo-nautilus-rfsz6 is verified up and running
STEP: using delete to clean up resources
Jun 12 12:43:55.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 delete --grace-period=0 --force -f - --namespace=kubectl-5399'
Jun 12 12:43:55.204: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 12 12:43:55.204: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 12 12:43:55.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5399'
Jun 12 12:43:55.272: INFO: stderr: "No resources found.\n"
Jun 12 12:43:55.272: INFO: stdout: ""
Jun 12 12:43:55.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -l name=update-demo --namespace=kubectl-5399 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 12 12:43:55.339: INFO: stderr: ""
Jun 12 12:43:55.339: INFO: stdout: "update-demo-nautilus-7sp7c\nupdate-demo-nautilus-rfsz6\n"
Jun 12 12:43:55.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5399'
Jun 12 12:43:55.907: INFO: stderr: "No resources found.\n"
Jun 12 12:43:55.907: INFO: stdout: ""
Jun 12 12:43:55.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -l name=update-demo --namespace=kubectl-5399 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 12 12:43:55.971: INFO: stderr: ""
Jun 12 12:43:55.971: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:43:55.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5399" for this suite.
Jun 12 12:44:17.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:44:18.055: INFO: namespace kubectl-5399 deletion completed in 22.080178765s

• [SLOW TEST:28.634 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:44:18.055: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-7855
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7855
STEP: Deleting pre-stop pod
Jun 12 12:44:31.132: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:44:31.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7855" for this suite.
Jun 12 12:45:09.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:45:09.225: INFO: namespace prestop-7855 deletion completed in 38.082040993s

• [SLOW TEST:51.170 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:45:09.225: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 12 12:45:09.273: INFO: Waiting up to 5m0s for pod "pod-e97a7e29-8d0f-11e9-b925-1ab852558ec8" in namespace "emptydir-9251" to be "success or failure"
Jun 12 12:45:09.276: INFO: Pod "pod-e97a7e29-8d0f-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185119ms
Jun 12 12:45:11.279: INFO: Pod "pod-e97a7e29-8d0f-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00574035s
Jun 12 12:45:13.283: INFO: Pod "pod-e97a7e29-8d0f-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009523273s
STEP: Saw pod success
Jun 12 12:45:13.283: INFO: Pod "pod-e97a7e29-8d0f-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:45:13.286: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-e97a7e29-8d0f-11e9-b925-1ab852558ec8 container test-container: <nil>
STEP: delete the pod
Jun 12 12:45:13.304: INFO: Waiting for pod pod-e97a7e29-8d0f-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:45:13.306: INFO: Pod pod-e97a7e29-8d0f-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:45:13.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9251" for this suite.
Jun 12 12:45:19.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:45:19.406: INFO: namespace emptydir-9251 deletion completed in 6.097319707s

• [SLOW TEST:10.182 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:45:19.407: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 12:45:19.443: INFO: (0) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 4.713946ms)
Jun 12 12:45:19.446: INFO: (1) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.930147ms)
Jun 12 12:45:19.449: INFO: (2) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.621382ms)
Jun 12 12:45:19.452: INFO: (3) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 3.44551ms)
Jun 12 12:45:19.455: INFO: (4) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.850968ms)
Jun 12 12:45:19.458: INFO: (5) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.682552ms)
Jun 12 12:45:19.461: INFO: (6) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.729151ms)
Jun 12 12:45:19.464: INFO: (7) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.787697ms)
Jun 12 12:45:19.466: INFO: (8) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.766759ms)
Jun 12 12:45:19.469: INFO: (9) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.78626ms)
Jun 12 12:45:19.472: INFO: (10) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.743186ms)
Jun 12 12:45:19.475: INFO: (11) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.862038ms)
Jun 12 12:45:19.477: INFO: (12) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.702611ms)
Jun 12 12:45:19.481: INFO: (13) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 3.722459ms)
Jun 12 12:45:19.488: INFO: (14) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 6.590106ms)
Jun 12 12:45:19.491: INFO: (15) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.709392ms)
Jun 12 12:45:19.494: INFO: (16) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.936704ms)
Jun 12 12:45:19.496: INFO: (17) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.80085ms)
Jun 12 12:45:19.499: INFO: (18) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.692046ms)
Jun 12 12:45:19.502: INFO: (19) /api/v1/nodes/cn-hongkong.192.168.0.217:10250/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.797254ms)
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:45:19.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6885" for this suite.
Jun 12 12:45:25.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:45:25.586: INFO: namespace proxy-6885 deletion completed in 6.081397694s

• [SLOW TEST:6.179 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:45:25.586: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-f33a25d3-8d0f-11e9-b925-1ab852558ec8
STEP: Creating configMap with name cm-test-opt-upd-f33a2602-8d0f-11e9-b925-1ab852558ec8
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-f33a25d3-8d0f-11e9-b925-1ab852558ec8
STEP: Updating configmap cm-test-opt-upd-f33a2602-8d0f-11e9-b925-1ab852558ec8
STEP: Creating configMap with name cm-test-opt-create-f33a2637-8d0f-11e9-b925-1ab852558ec8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:45:31.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8307" for this suite.
Jun 12 12:45:53.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:45:53.834: INFO: namespace configmap-8307 deletion completed in 22.083547827s

• [SLOW TEST:28.248 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:45:53.834: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-2440
Jun 12 12:45:59.893: INFO: Started pod liveness-http in namespace container-probe-2440
STEP: checking the pod's current state and verifying that restartCount is present
Jun 12 12:45:59.895: INFO: Initial restart count of pod liveness-http is 0
Jun 12 12:46:17.936: INFO: Restart count of pod container-probe-2440/liveness-http is now 1 (18.040415641s elapsed)
Jun 12 12:46:37.971: INFO: Restart count of pod container-probe-2440/liveness-http is now 2 (38.07544626s elapsed)
Jun 12 12:46:58.006: INFO: Restart count of pod container-probe-2440/liveness-http is now 3 (58.110901618s elapsed)
Jun 12 12:47:18.040: INFO: Restart count of pod container-probe-2440/liveness-http is now 4 (1m18.144323026s elapsed)
Jun 12 12:48:32.175: INFO: Restart count of pod container-probe-2440/liveness-http is now 5 (2m32.279300557s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:48:32.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2440" for this suite.
Jun 12 12:48:38.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:48:38.271: INFO: namespace container-probe-2440 deletion completed in 6.080622493s

• [SLOW TEST:164.436 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:48:38.271: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 12 12:48:38.322: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66146a79-8d10-11e9-b925-1ab852558ec8" in namespace "projected-1211" to be "success or failure"
Jun 12 12:48:38.325: INFO: Pod "downwardapi-volume-66146a79-8d10-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.434907ms
Jun 12 12:48:40.329: INFO: Pod "downwardapi-volume-66146a79-8d10-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006149398s
Jun 12 12:48:42.333: INFO: Pod "downwardapi-volume-66146a79-8d10-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010465608s
STEP: Saw pod success
Jun 12 12:48:42.333: INFO: Pod "downwardapi-volume-66146a79-8d10-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:48:42.336: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod downwardapi-volume-66146a79-8d10-11e9-b925-1ab852558ec8 container client-container: <nil>
STEP: delete the pod
Jun 12 12:48:42.373: INFO: Waiting for pod downwardapi-volume-66146a79-8d10-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:48:42.376: INFO: Pod downwardapi-volume-66146a79-8d10-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:48:42.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1211" for this suite.
Jun 12 12:48:48.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:48:48.457: INFO: namespace projected-1211 deletion completed in 6.077810487s

• [SLOW TEST:10.186 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:48:48.457: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-6c252228-8d10-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume secrets
Jun 12 12:48:48.502: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6c26029e-8d10-11e9-b925-1ab852558ec8" in namespace "projected-4676" to be "success or failure"
Jun 12 12:48:48.504: INFO: Pod "pod-projected-secrets-6c26029e-8d10-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185156ms
Jun 12 12:48:50.508: INFO: Pod "pod-projected-secrets-6c26029e-8d10-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005691581s
Jun 12 12:48:52.511: INFO: Pod "pod-projected-secrets-6c26029e-8d10-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009225578s
STEP: Saw pod success
Jun 12 12:48:52.512: INFO: Pod "pod-projected-secrets-6c26029e-8d10-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:48:52.514: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-projected-secrets-6c26029e-8d10-11e9-b925-1ab852558ec8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 12 12:48:52.531: INFO: Waiting for pod pod-projected-secrets-6c26029e-8d10-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:48:52.534: INFO: Pod pod-projected-secrets-6c26029e-8d10-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:48:52.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4676" for this suite.
Jun 12 12:48:58.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:48:58.619: INFO: namespace projected-4676 deletion completed in 6.082648844s

• [SLOW TEST:10.163 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:48:58.620: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-7235908e-8d10-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume configMaps
Jun 12 12:48:58.673: INFO: Waiting up to 5m0s for pod "pod-configmaps-72366200-8d10-11e9-b925-1ab852558ec8" in namespace "configmap-1536" to be "success or failure"
Jun 12 12:48:58.676: INFO: Pod "pod-configmaps-72366200-8d10-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.825502ms
Jun 12 12:49:00.680: INFO: Pod "pod-configmaps-72366200-8d10-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007161094s
STEP: Saw pod success
Jun 12 12:49:00.680: INFO: Pod "pod-configmaps-72366200-8d10-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:49:00.682: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-configmaps-72366200-8d10-11e9-b925-1ab852558ec8 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 12 12:49:00.704: INFO: Waiting for pod pod-configmaps-72366200-8d10-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:49:00.706: INFO: Pod pod-configmaps-72366200-8d10-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:49:00.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1536" for this suite.
Jun 12 12:49:06.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:49:06.791: INFO: namespace configmap-1536 deletion completed in 6.0808296s

• [SLOW TEST:8.171 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:49:06.791: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 12:49:06.857: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"7716992a-8d10-11e9-afb2-00163e04c212", Controller:(*bool)(0xc00293f1de), BlockOwnerDeletion:(*bool)(0xc00293f1df)}}
Jun 12 12:49:06.862: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"7714acf8-8d10-11e9-afb2-00163e04c212", Controller:(*bool)(0xc00283b90e), BlockOwnerDeletion:(*bool)(0xc00283b90f)}}
Jun 12 12:49:06.868: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"77159b4c-8d10-11e9-afb2-00163e04c212", Controller:(*bool)(0xc003419bbe), BlockOwnerDeletion:(*bool)(0xc003419bbf)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:49:11.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6643" for this suite.
Jun 12 12:49:17.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:49:17.965: INFO: namespace gc-6643 deletion completed in 6.081344054s

• [SLOW TEST:11.174 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:49:17.965: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-7dbb904b-8d10-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume configMaps
Jun 12 12:49:18.006: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7dbc7b9a-8d10-11e9-b925-1ab852558ec8" in namespace "projected-8464" to be "success or failure"
Jun 12 12:49:18.010: INFO: Pod "pod-projected-configmaps-7dbc7b9a-8d10-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.351098ms
Jun 12 12:49:20.013: INFO: Pod "pod-projected-configmaps-7dbc7b9a-8d10-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006334112s
STEP: Saw pod success
Jun 12 12:49:20.013: INFO: Pod "pod-projected-configmaps-7dbc7b9a-8d10-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:49:20.015: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-projected-configmaps-7dbc7b9a-8d10-11e9-b925-1ab852558ec8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 12 12:49:20.036: INFO: Waiting for pod pod-projected-configmaps-7dbc7b9a-8d10-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:49:20.038: INFO: Pod pod-projected-configmaps-7dbc7b9a-8d10-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:49:20.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8464" for this suite.
Jun 12 12:49:26.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:49:26.127: INFO: namespace projected-8464 deletion completed in 6.085194171s

• [SLOW TEST:8.162 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:49:26.127: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 12 12:49:28.694: INFO: Successfully updated pod "pod-update-829a6e34-8d10-11e9-b925-1ab852558ec8"
STEP: verifying the updated pod is in kubernetes
Jun 12 12:49:28.699: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:49:28.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3941" for this suite.
Jun 12 12:49:50.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:49:50.786: INFO: namespace pods-3941 deletion completed in 22.084610923s

• [SLOW TEST:24.660 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:49:50.787: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 12 12:49:50.825: INFO: Waiting up to 5m0s for pod "pod-914bcf0d-8d10-11e9-b925-1ab852558ec8" in namespace "emptydir-3249" to be "success or failure"
Jun 12 12:49:50.828: INFO: Pod "pod-914bcf0d-8d10-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.136478ms
Jun 12 12:49:52.831: INFO: Pod "pod-914bcf0d-8d10-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005657038s
STEP: Saw pod success
Jun 12 12:49:52.831: INFO: Pod "pod-914bcf0d-8d10-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:49:52.834: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-914bcf0d-8d10-11e9-b925-1ab852558ec8 container test-container: <nil>
STEP: delete the pod
Jun 12 12:49:52.857: INFO: Waiting for pod pod-914bcf0d-8d10-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:49:52.859: INFO: Pod pod-914bcf0d-8d10-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:49:52.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3249" for this suite.
Jun 12 12:49:58.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:49:58.942: INFO: namespace emptydir-3249 deletion completed in 6.079611113s

• [SLOW TEST:8.155 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:49:58.942: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-9628b785-8d10-11e9-b925-1ab852558ec8
Jun 12 12:49:58.984: INFO: Pod name my-hostname-basic-9628b785-8d10-11e9-b925-1ab852558ec8: Found 0 pods out of 1
Jun 12 12:50:03.987: INFO: Pod name my-hostname-basic-9628b785-8d10-11e9-b925-1ab852558ec8: Found 1 pods out of 1
Jun 12 12:50:03.987: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9628b785-8d10-11e9-b925-1ab852558ec8" are running
Jun 12 12:50:03.990: INFO: Pod "my-hostname-basic-9628b785-8d10-11e9-b925-1ab852558ec8-2v4hp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-12 12:49:58 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-12 12:50:00 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-12 12:50:00 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-12 12:49:58 +0000 UTC Reason: Message:}])
Jun 12 12:50:03.990: INFO: Trying to dial the pod
Jun 12 12:50:08.999: INFO: Controller my-hostname-basic-9628b785-8d10-11e9-b925-1ab852558ec8: Got expected result from replica 1 [my-hostname-basic-9628b785-8d10-11e9-b925-1ab852558ec8-2v4hp]: "my-hostname-basic-9628b785-8d10-11e9-b925-1ab852558ec8-2v4hp", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:50:08.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8660" for this suite.
Jun 12 12:50:15.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:50:15.082: INFO: namespace replication-controller-8660 deletion completed in 6.079605988s

• [SLOW TEST:16.140 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:50:15.083: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Jun 12 12:50:15.112: INFO: namespace kubectl-6057
Jun 12 12:50:15.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 create -f - --namespace=kubectl-6057'
Jun 12 12:50:15.579: INFO: stderr: ""
Jun 12 12:50:15.579: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 12 12:50:16.583: INFO: Selector matched 1 pods for map[app:redis]
Jun 12 12:50:16.583: INFO: Found 0 / 1
Jun 12 12:50:17.582: INFO: Selector matched 1 pods for map[app:redis]
Jun 12 12:50:17.582: INFO: Found 0 / 1
Jun 12 12:50:18.582: INFO: Selector matched 1 pods for map[app:redis]
Jun 12 12:50:18.583: INFO: Found 0 / 1
Jun 12 12:50:19.582: INFO: Selector matched 1 pods for map[app:redis]
Jun 12 12:50:19.582: INFO: Found 1 / 1
Jun 12 12:50:19.582: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 12 12:50:19.585: INFO: Selector matched 1 pods for map[app:redis]
Jun 12 12:50:19.585: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 12 12:50:19.585: INFO: wait on redis-master startup in kubectl-6057 
Jun 12 12:50:19.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 logs redis-master-5zm5c redis-master --namespace=kubectl-6057'
Jun 12 12:50:19.667: INFO: stderr: ""
Jun 12 12:50:19.667: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 12 Jun 12:50:18.835 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 12 Jun 12:50:18.835 # Server started, Redis version 3.2.12\n1:M 12 Jun 12:50:18.835 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 12 Jun 12:50:18.836 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jun 12 12:50:19.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-6057'
Jun 12 12:50:19.745: INFO: stderr: ""
Jun 12 12:50:19.745: INFO: stdout: "service/rm2 exposed\n"
Jun 12 12:50:19.749: INFO: Service rm2 in namespace kubectl-6057 found.
STEP: exposing service
Jun 12 12:50:21.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-6057'
Jun 12 12:50:21.835: INFO: stderr: ""
Jun 12 12:50:21.835: INFO: stdout: "service/rm3 exposed\n"
Jun 12 12:50:21.841: INFO: Service rm3 in namespace kubectl-6057 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:50:23.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6057" for this suite.
Jun 12 12:50:45.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:50:45.933: INFO: namespace kubectl-6057 deletion completed in 22.082933097s

• [SLOW TEST:30.850 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:50:45.933: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:50:47.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-725" for this suite.
Jun 12 12:51:26.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:51:26.078: INFO: namespace kubelet-test-725 deletion completed in 38.082295282s

• [SLOW TEST:40.144 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:51:26.078: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-ca17dc29-8d10-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume secrets
Jun 12 12:51:26.118: INFO: Waiting up to 5m0s for pod "pod-secrets-ca18a8bb-8d10-11e9-b925-1ab852558ec8" in namespace "secrets-9465" to be "success or failure"
Jun 12 12:51:26.124: INFO: Pod "pod-secrets-ca18a8bb-8d10-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.449813ms
Jun 12 12:51:28.127: INFO: Pod "pod-secrets-ca18a8bb-8d10-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009330712s
STEP: Saw pod success
Jun 12 12:51:28.127: INFO: Pod "pod-secrets-ca18a8bb-8d10-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:51:28.129: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-secrets-ca18a8bb-8d10-11e9-b925-1ab852558ec8 container secret-volume-test: <nil>
STEP: delete the pod
Jun 12 12:51:28.148: INFO: Waiting for pod pod-secrets-ca18a8bb-8d10-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:51:28.150: INFO: Pod pod-secrets-ca18a8bb-8d10-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:51:28.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9465" for this suite.
Jun 12 12:51:34.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:51:34.231: INFO: namespace secrets-9465 deletion completed in 6.077939026s

• [SLOW TEST:8.153 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:51:34.231: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 12:51:34.260: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:51:40.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1052" for this suite.
Jun 12 12:51:46.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:51:46.407: INFO: namespace custom-resource-definition-1052 deletion completed in 6.083730151s

• [SLOW TEST:12.176 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:51:46.407: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:52:46.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2202" for this suite.
Jun 12 12:53:08.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:53:08.536: INFO: namespace container-probe-2202 deletion completed in 22.084530215s

• [SLOW TEST:82.129 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:53:08.536: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-072c4d8d-8d11-11e9-b925-1ab852558ec8
STEP: Creating secret with name s-test-opt-upd-072c4dc4-8d11-11e9-b925-1ab852558ec8
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-072c4d8d-8d11-11e9-b925-1ab852558ec8
STEP: Updating secret s-test-opt-upd-072c4dc4-8d11-11e9-b925-1ab852558ec8
STEP: Creating secret with name s-test-opt-create-072c4e04-8d11-11e9-b925-1ab852558ec8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:54:25.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4400" for this suite.
Jun 12 12:54:47.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:54:47.218: INFO: namespace projected-4400 deletion completed in 22.08638026s

• [SLOW TEST:98.681 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:54:47.218: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
Jun 12 12:54:47.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 create -f - --namespace=kubectl-8543'
Jun 12 12:54:47.499: INFO: stderr: ""
Jun 12 12:54:47.499: INFO: stdout: "pod/pause created\n"
Jun 12 12:54:47.499: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 12 12:54:47.499: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8543" to be "running and ready"
Jun 12 12:54:47.504: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.823149ms
Jun 12 12:54:49.507: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008027264s
Jun 12 12:54:51.510: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.011622277s
Jun 12 12:54:51.511: INFO: Pod "pause" satisfied condition "running and ready"
Jun 12 12:54:51.511: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 12 12:54:51.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 label pods pause testing-label=testing-label-value --namespace=kubectl-8543'
Jun 12 12:54:51.580: INFO: stderr: ""
Jun 12 12:54:51.580: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 12 12:54:51.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pod pause -L testing-label --namespace=kubectl-8543'
Jun 12 12:54:51.643: INFO: stderr: ""
Jun 12 12:54:51.643: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 12 12:54:51.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 label pods pause testing-label- --namespace=kubectl-8543'
Jun 12 12:54:51.742: INFO: stderr: ""
Jun 12 12:54:51.742: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 12 12:54:51.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pod pause -L testing-label --namespace=kubectl-8543'
Jun 12 12:54:51.805: INFO: stderr: ""
Jun 12 12:54:51.805: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
Jun 12 12:54:51.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 delete --grace-period=0 --force -f - --namespace=kubectl-8543'
Jun 12 12:54:51.880: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 12 12:54:51.880: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 12 12:54:51.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get rc,svc -l name=pause --no-headers --namespace=kubectl-8543'
Jun 12 12:54:51.949: INFO: stderr: "No resources found.\n"
Jun 12 12:54:51.949: INFO: stdout: ""
Jun 12 12:54:51.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -l name=pause --namespace=kubectl-8543 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 12 12:54:52.009: INFO: stderr: ""
Jun 12 12:54:52.009: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:54:52.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8543" for this suite.
Jun 12 12:54:58.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:54:58.097: INFO: namespace kubectl-8543 deletion completed in 6.084445399s

• [SLOW TEST:10.880 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:54:58.098: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 12 12:54:58.152: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4879e01d-8d11-11e9-b925-1ab852558ec8" in namespace "projected-775" to be "success or failure"
Jun 12 12:54:58.154: INFO: Pod "downwardapi-volume-4879e01d-8d11-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.274081ms
Jun 12 12:55:00.158: INFO: Pod "downwardapi-volume-4879e01d-8d11-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005817036s
STEP: Saw pod success
Jun 12 12:55:00.158: INFO: Pod "downwardapi-volume-4879e01d-8d11-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:55:00.160: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod downwardapi-volume-4879e01d-8d11-11e9-b925-1ab852558ec8 container client-container: <nil>
STEP: delete the pod
Jun 12 12:55:00.181: INFO: Waiting for pod downwardapi-volume-4879e01d-8d11-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:55:00.183: INFO: Pod downwardapi-volume-4879e01d-8d11-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:55:00.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-775" for this suite.
Jun 12 12:55:06.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:55:06.270: INFO: namespace projected-775 deletion completed in 6.083837058s

• [SLOW TEST:8.172 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:55:06.270: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 12 12:55:10.841: INFO: Successfully updated pod "pod-update-activedeadlineseconds-4d56cd9e-8d11-11e9-b925-1ab852558ec8"
Jun 12 12:55:10.841: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4d56cd9e-8d11-11e9-b925-1ab852558ec8" in namespace "pods-7125" to be "terminated due to deadline exceeded"
Jun 12 12:55:10.843: INFO: Pod "pod-update-activedeadlineseconds-4d56cd9e-8d11-11e9-b925-1ab852558ec8": Phase="Running", Reason="", readiness=true. Elapsed: 2.258881ms
Jun 12 12:55:12.847: INFO: Pod "pod-update-activedeadlineseconds-4d56cd9e-8d11-11e9-b925-1ab852558ec8": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.00553542s
Jun 12 12:55:12.847: INFO: Pod "pod-update-activedeadlineseconds-4d56cd9e-8d11-11e9-b925-1ab852558ec8" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:55:12.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7125" for this suite.
Jun 12 12:55:18.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:55:18.939: INFO: namespace pods-7125 deletion completed in 6.088457528s

• [SLOW TEST:12.668 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:55:18.939: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 12 12:55:22.010: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:55:23.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6518" for this suite.
Jun 12 12:55:45.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:55:45.113: INFO: namespace replicaset-6518 deletion completed in 22.08099052s

• [SLOW TEST:26.174 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:55:45.113: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jun 12 12:55:46.171: INFO: Waiting up to 5m0s for pod "downward-api-64802337-8d11-11e9-b925-1ab852558ec8" in namespace "downward-api-1030" to be "success or failure"
Jun 12 12:55:46.175: INFO: Pod "downward-api-64802337-8d11-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.450972ms
Jun 12 12:55:48.179: INFO: Pod "downward-api-64802337-8d11-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008098009s
STEP: Saw pod success
Jun 12 12:55:48.179: INFO: Pod "downward-api-64802337-8d11-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:55:48.182: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod downward-api-64802337-8d11-11e9-b925-1ab852558ec8 container dapi-container: <nil>
STEP: delete the pod
Jun 12 12:55:48.206: INFO: Waiting for pod downward-api-64802337-8d11-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:55:48.208: INFO: Pod downward-api-64802337-8d11-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:55:48.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1030" for this suite.
Jun 12 12:55:54.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:55:54.295: INFO: namespace downward-api-1030 deletion completed in 6.078150904s

• [SLOW TEST:9.182 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:55:54.295: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-69f835df-8d11-11e9-b925-1ab852558ec8
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-69f835df-8d11-11e9-b925-1ab852558ec8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:55:58.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8213" for this suite.
Jun 12 12:56:20.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:56:20.478: INFO: namespace configmap-8213 deletion completed in 22.089361496s

• [SLOW TEST:26.183 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:56:20.479: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun 12 12:56:23.034: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4105 pod-service-account-79e0520c-8d11-11e9-b925-1ab852558ec8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 12 12:56:23.326: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4105 pod-service-account-79e0520c-8d11-11e9-b925-1ab852558ec8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 12 12:56:23.667: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4105 pod-service-account-79e0520c-8d11-11e9-b925-1ab852558ec8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:56:24.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4105" for this suite.
Jun 12 12:56:30.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:56:30.111: INFO: namespace svcaccounts-4105 deletion completed in 6.081337988s

• [SLOW TEST:9.632 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:56:30.111: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Jun 12 12:56:32.171: INFO: Pod pod-hostip-7f509c59-8d11-11e9-b925-1ab852558ec8 has hostIP: 192.168.0.217
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:56:32.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1429" for this suite.
Jun 12 12:56:54.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:56:54.254: INFO: namespace pods-1429 deletion completed in 22.079329089s

• [SLOW TEST:24.143 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:56:54.255: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 12:56:54.292: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 12 12:56:59.295: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 12 12:56:59.295: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 12 12:57:01.298: INFO: Creating deployment "test-rollover-deployment"
Jun 12 12:57:01.306: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 12 12:57:03.311: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 12 12:57:03.315: INFO: Ensure that both replica sets have 1 created replica
Jun 12 12:57:03.320: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 12 12:57:03.326: INFO: Updating deployment test-rollover-deployment
Jun 12 12:57:03.326: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 12 12:57:05.332: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 12 12:57:05.336: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 12 12:57:05.341: INFO: all replica sets need to contain the pod-template-hash label
Jun 12 12:57:05.341: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941021, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941021, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941023, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941021, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 12 12:57:07.346: INFO: all replica sets need to contain the pod-template-hash label
Jun 12 12:57:07.346: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941021, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941021, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941025, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941021, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 12 12:57:09.346: INFO: all replica sets need to contain the pod-template-hash label
Jun 12 12:57:09.346: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941021, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941021, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941025, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941021, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 12 12:57:11.346: INFO: all replica sets need to contain the pod-template-hash label
Jun 12 12:57:11.346: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941021, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941021, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941025, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941021, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 12 12:57:13.346: INFO: all replica sets need to contain the pod-template-hash label
Jun 12 12:57:13.346: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941021, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941021, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941025, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941021, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 12 12:57:15.346: INFO: all replica sets need to contain the pod-template-hash label
Jun 12 12:57:15.346: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941021, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941021, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941025, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941021, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 12 12:57:17.346: INFO: 
Jun 12 12:57:17.346: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jun 12 12:57:17.353: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-994,SelfLink:/apis/apps/v1/namespaces/deployment-994/deployments/test-rollover-deployment,UID:91e3d29c-8d11-11e9-afb2-00163e04c212,ResourceVersion:21688,Generation:2,CreationTimestamp:2019-06-12 12:57:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-12 12:57:01 +0000 UTC 2019-06-12 12:57:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-12 12:57:15 +0000 UTC 2019-06-12 12:57:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 12 12:57:17.355: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-994,SelfLink:/apis/apps/v1/namespaces/deployment-994/replicasets/test-rollover-deployment-766b4d6c9d,UID:9314bfa7-8d11-11e9-80ba-00163e04d588,ResourceVersion:21677,Generation:2,CreationTimestamp:2019-06-12 12:57:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 91e3d29c-8d11-11e9-afb2-00163e04c212 0xc002deb017 0xc002deb018}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 12 12:57:17.355: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 12 12:57:17.355: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-994,SelfLink:/apis/apps/v1/namespaces/deployment-994/replicasets/test-rollover-controller,UID:8db515b9-8d11-11e9-afb2-00163e04c212,ResourceVersion:21687,Generation:2,CreationTimestamp:2019-06-12 12:56:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 91e3d29c-8d11-11e9-afb2-00163e04c212 0xc002deae67 0xc002deae68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 12 12:57:17.355: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-994,SelfLink:/apis/apps/v1/namespaces/deployment-994/replicasets/test-rollover-deployment-6455657675,UID:91e1718a-8d11-11e9-80ba-00163e04d588,ResourceVersion:21629,Generation:2,CreationTimestamp:2019-06-12 12:57:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 91e3d29c-8d11-11e9-afb2-00163e04c212 0xc002deaf37 0xc002deaf38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 12 12:57:17.358: INFO: Pod "test-rollover-deployment-766b4d6c9d-9jkfc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-9jkfc,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-994,SelfLink:/api/v1/namespaces/deployment-994/pods/test-rollover-deployment-766b4d6c9d-9jkfc,UID:931c92f8-8d11-11e9-80ba-00163e04d588,ResourceVersion:21642,Generation:0,CreationTimestamp:2019-06-12 12:57:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d 9314bfa7-8d11-11e9-80ba-00163e04d588 0xc002debb67 0xc002debb68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tlwkd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tlwkd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-tlwkd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.218,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002debbd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002debbf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:57:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:57:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:57:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:57:03 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.218,PodIP:172.20.2.48,StartTime:2019-06-12 12:57:03 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-12 12:57:04 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://6a13e831526e4117f03e29ec454db47fda96d2e1cc7f01c7d26ed27e9d34c818}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:57:17.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-994" for this suite.
Jun 12 12:57:23.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:57:23.443: INFO: namespace deployment-994 deletion completed in 6.081528135s

• [SLOW TEST:29.188 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:57:23.443: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-g9k5
STEP: Creating a pod to test atomic-volume-subpath
Jun 12 12:57:23.488: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-g9k5" in namespace "subpath-7100" to be "success or failure"
Jun 12 12:57:23.497: INFO: Pod "pod-subpath-test-projected-g9k5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.058151ms
Jun 12 12:57:25.500: INFO: Pod "pod-subpath-test-projected-g9k5": Phase="Running", Reason="", readiness=true. Elapsed: 2.011928737s
Jun 12 12:57:27.504: INFO: Pod "pod-subpath-test-projected-g9k5": Phase="Running", Reason="", readiness=true. Elapsed: 4.01550945s
Jun 12 12:57:29.511: INFO: Pod "pod-subpath-test-projected-g9k5": Phase="Running", Reason="", readiness=true. Elapsed: 6.022564078s
Jun 12 12:57:31.515: INFO: Pod "pod-subpath-test-projected-g9k5": Phase="Running", Reason="", readiness=true. Elapsed: 8.02656787s
Jun 12 12:57:33.518: INFO: Pod "pod-subpath-test-projected-g9k5": Phase="Running", Reason="", readiness=true. Elapsed: 10.030060022s
Jun 12 12:57:35.522: INFO: Pod "pod-subpath-test-projected-g9k5": Phase="Running", Reason="", readiness=true. Elapsed: 12.033468886s
Jun 12 12:57:37.525: INFO: Pod "pod-subpath-test-projected-g9k5": Phase="Running", Reason="", readiness=true. Elapsed: 14.03678761s
Jun 12 12:57:39.529: INFO: Pod "pod-subpath-test-projected-g9k5": Phase="Running", Reason="", readiness=true. Elapsed: 16.040984906s
Jun 12 12:57:41.533: INFO: Pod "pod-subpath-test-projected-g9k5": Phase="Running", Reason="", readiness=true. Elapsed: 18.044882523s
Jun 12 12:57:43.536: INFO: Pod "pod-subpath-test-projected-g9k5": Phase="Running", Reason="", readiness=true. Elapsed: 20.048373322s
Jun 12 12:57:45.540: INFO: Pod "pod-subpath-test-projected-g9k5": Phase="Running", Reason="", readiness=true. Elapsed: 22.051975091s
Jun 12 12:57:47.544: INFO: Pod "pod-subpath-test-projected-g9k5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.055592462s
STEP: Saw pod success
Jun 12 12:57:47.544: INFO: Pod "pod-subpath-test-projected-g9k5" satisfied condition "success or failure"
Jun 12 12:57:47.546: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-subpath-test-projected-g9k5 container test-container-subpath-projected-g9k5: <nil>
STEP: delete the pod
Jun 12 12:57:47.565: INFO: Waiting for pod pod-subpath-test-projected-g9k5 to disappear
Jun 12 12:57:47.567: INFO: Pod pod-subpath-test-projected-g9k5 no longer exists
STEP: Deleting pod pod-subpath-test-projected-g9k5
Jun 12 12:57:47.567: INFO: Deleting pod "pod-subpath-test-projected-g9k5" in namespace "subpath-7100"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:57:47.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7100" for this suite.
Jun 12 12:57:53.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:57:53.656: INFO: namespace subpath-7100 deletion completed in 6.080411918s

• [SLOW TEST:30.213 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:57:53.656: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-b11bf8ac-8d11-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume secrets
Jun 12 12:57:53.701: INFO: Waiting up to 5m0s for pod "pod-secrets-b11d16bf-8d11-11e9-b925-1ab852558ec8" in namespace "secrets-4855" to be "success or failure"
Jun 12 12:57:53.707: INFO: Pod "pod-secrets-b11d16bf-8d11-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.851081ms
Jun 12 12:57:55.710: INFO: Pod "pod-secrets-b11d16bf-8d11-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009019381s
STEP: Saw pod success
Jun 12 12:57:55.710: INFO: Pod "pod-secrets-b11d16bf-8d11-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:57:55.713: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-secrets-b11d16bf-8d11-11e9-b925-1ab852558ec8 container secret-volume-test: <nil>
STEP: delete the pod
Jun 12 12:57:55.741: INFO: Waiting for pod pod-secrets-b11d16bf-8d11-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:57:55.744: INFO: Pod pod-secrets-b11d16bf-8d11-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:57:55.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4855" for this suite.
Jun 12 12:58:01.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:58:01.826: INFO: namespace secrets-4855 deletion completed in 6.078856563s

• [SLOW TEST:8.169 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:58:01.826: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-110
Jun 12 12:58:03.873: INFO: Started pod liveness-http in namespace container-probe-110
STEP: checking the pod's current state and verifying that restartCount is present
Jun 12 12:58:03.876: INFO: Initial restart count of pod liveness-http is 0
Jun 12 12:58:29.927: INFO: Restart count of pod container-probe-110/liveness-http is now 1 (26.051456842s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:58:29.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-110" for this suite.
Jun 12 12:58:35.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:58:36.045: INFO: namespace container-probe-110 deletion completed in 6.095374401s

• [SLOW TEST:34.219 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:58:36.046: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:58:40.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9021" for this suite.
Jun 12 12:59:30.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:59:30.193: INFO: namespace kubelet-test-9021 deletion completed in 50.085588498s

• [SLOW TEST:54.147 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:59:30.193: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 12 12:59:30.238: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eaa72fc4-8d11-11e9-b925-1ab852558ec8" in namespace "downward-api-8877" to be "success or failure"
Jun 12 12:59:30.241: INFO: Pod "downwardapi-volume-eaa72fc4-8d11-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.250041ms
Jun 12 12:59:32.245: INFO: Pod "downwardapi-volume-eaa72fc4-8d11-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00638399s
STEP: Saw pod success
Jun 12 12:59:32.245: INFO: Pod "downwardapi-volume-eaa72fc4-8d11-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:59:32.248: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod downwardapi-volume-eaa72fc4-8d11-11e9-b925-1ab852558ec8 container client-container: <nil>
STEP: delete the pod
Jun 12 12:59:32.267: INFO: Waiting for pod downwardapi-volume-eaa72fc4-8d11-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:59:32.269: INFO: Pod downwardapi-volume-eaa72fc4-8d11-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:59:32.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8877" for this suite.
Jun 12 12:59:38.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:59:38.352: INFO: namespace downward-api-8877 deletion completed in 6.080184734s

• [SLOW TEST:8.159 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:59:38.353: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 12 12:59:38.391: INFO: Waiting up to 5m0s for pod "pod-ef837063-8d11-11e9-b925-1ab852558ec8" in namespace "emptydir-313" to be "success or failure"
Jun 12 12:59:38.400: INFO: Pod "pod-ef837063-8d11-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.932606ms
Jun 12 12:59:40.404: INFO: Pod "pod-ef837063-8d11-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012912089s
Jun 12 12:59:42.407: INFO: Pod "pod-ef837063-8d11-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016253701s
STEP: Saw pod success
Jun 12 12:59:42.408: INFO: Pod "pod-ef837063-8d11-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 12:59:42.410: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-ef837063-8d11-11e9-b925-1ab852558ec8 container test-container: <nil>
STEP: delete the pod
Jun 12 12:59:42.427: INFO: Waiting for pod pod-ef837063-8d11-11e9-b925-1ab852558ec8 to disappear
Jun 12 12:59:42.429: INFO: Pod pod-ef837063-8d11-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:59:42.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-313" for this suite.
Jun 12 12:59:48.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 12:59:48.518: INFO: namespace emptydir-313 deletion completed in 6.085419572s

• [SLOW TEST:10.165 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 12:59:48.518: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun 12 12:59:50.573: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-f59287fe-8d11-11e9-b925-1ab852558ec8,GenerateName:,Namespace:events-6338,SelfLink:/api/v1/namespaces/events-6338/pods/send-events-f59287fe-8d11-11e9-b925-1ab852558ec8,UID:f59488b5-8d11-11e9-afb2-00163e04c212,ResourceVersion:22306,Generation:0,CreationTimestamp:2019-06-12 12:59:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 549227608,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vzj4n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vzj4n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-vzj4n true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d1d960} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d1d980}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:59:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:59:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:59:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 12:59:48 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.217,PodIP:172.20.2.211,StartTime:2019-06-12 12:59:48 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-06-12 12:59:49 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://e50061beb65a6d7838d5f0c38bf4e392ac688086d5b14dc76cd757dbc12e2e48}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jun 12 12:59:52.576: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun 12 12:59:54.586: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 12:59:54.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6338" for this suite.
Jun 12 13:00:32.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:00:32.685: INFO: namespace events-6338 deletion completed in 38.086541894s

• [SLOW TEST:44.167 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:00:32.685: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:00:59.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1177" for this suite.
Jun 12 13:01:05.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:01:05.893: INFO: namespace namespaces-1177 deletion completed in 6.078531955s
STEP: Destroying namespace "nsdeletetest-1349" for this suite.
Jun 12 13:01:05.895: INFO: Namespace nsdeletetest-1349 was already deleted
STEP: Destroying namespace "nsdeletetest-1959" for this suite.
Jun 12 13:01:11.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:01:11.974: INFO: namespace nsdeletetest-1959 deletion completed in 6.079333387s

• [SLOW TEST:39.289 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:01:11.974: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 12 13:01:12.016: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2751534c-8d12-11e9-b925-1ab852558ec8" in namespace "downward-api-6071" to be "success or failure"
Jun 12 13:01:12.018: INFO: Pod "downwardapi-volume-2751534c-8d12-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18632ms
Jun 12 13:01:14.021: INFO: Pod "downwardapi-volume-2751534c-8d12-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00568844s
Jun 12 13:01:16.025: INFO: Pod "downwardapi-volume-2751534c-8d12-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009181908s
STEP: Saw pod success
Jun 12 13:01:16.025: INFO: Pod "downwardapi-volume-2751534c-8d12-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:01:16.028: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod downwardapi-volume-2751534c-8d12-11e9-b925-1ab852558ec8 container client-container: <nil>
STEP: delete the pod
Jun 12 13:01:16.046: INFO: Waiting for pod downwardapi-volume-2751534c-8d12-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:01:16.049: INFO: Pod downwardapi-volume-2751534c-8d12-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:01:16.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6071" for this suite.
Jun 12 13:01:22.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:01:22.152: INFO: namespace downward-api-6071 deletion completed in 6.080983553s

• [SLOW TEST:10.178 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:01:22.152: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 12 13:01:23.221: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d65f0d5-8d12-11e9-b925-1ab852558ec8" in namespace "downward-api-2703" to be "success or failure"
Jun 12 13:01:23.233: INFO: Pod "downwardapi-volume-2d65f0d5-8d12-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.919886ms
Jun 12 13:01:25.237: INFO: Pod "downwardapi-volume-2d65f0d5-8d12-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015769124s
Jun 12 13:01:27.241: INFO: Pod "downwardapi-volume-2d65f0d5-8d12-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019326383s
STEP: Saw pod success
Jun 12 13:01:27.241: INFO: Pod "downwardapi-volume-2d65f0d5-8d12-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:01:27.244: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod downwardapi-volume-2d65f0d5-8d12-11e9-b925-1ab852558ec8 container client-container: <nil>
STEP: delete the pod
Jun 12 13:01:27.262: INFO: Waiting for pod downwardapi-volume-2d65f0d5-8d12-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:01:27.264: INFO: Pod downwardapi-volume-2d65f0d5-8d12-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:01:27.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2703" for this suite.
Jun 12 13:01:33.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:01:33.350: INFO: namespace downward-api-2703 deletion completed in 6.082474476s

• [SLOW TEST:11.198 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:01:33.350: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-340e8f44-8d12-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume configMaps
Jun 12 13:01:33.410: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-34122c90-8d12-11e9-b925-1ab852558ec8" in namespace "projected-3988" to be "success or failure"
Jun 12 13:01:33.413: INFO: Pod "pod-projected-configmaps-34122c90-8d12-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.277167ms
Jun 12 13:01:35.416: INFO: Pod "pod-projected-configmaps-34122c90-8d12-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005915088s
Jun 12 13:01:37.420: INFO: Pod "pod-projected-configmaps-34122c90-8d12-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009612089s
STEP: Saw pod success
Jun 12 13:01:37.420: INFO: Pod "pod-projected-configmaps-34122c90-8d12-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:01:37.423: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-projected-configmaps-34122c90-8d12-11e9-b925-1ab852558ec8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 12 13:01:37.442: INFO: Waiting for pod pod-projected-configmaps-34122c90-8d12-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:01:37.444: INFO: Pod pod-projected-configmaps-34122c90-8d12-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:01:37.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3988" for this suite.
Jun 12 13:01:43.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:01:43.544: INFO: namespace projected-3988 deletion completed in 6.08136532s

• [SLOW TEST:10.193 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:01:43.544: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jun 12 13:01:43.594: INFO: Waiting up to 5m0s for pod "downward-api-3a23cadf-8d12-11e9-b925-1ab852558ec8" in namespace "downward-api-4419" to be "success or failure"
Jun 12 13:01:43.596: INFO: Pod "downward-api-3a23cadf-8d12-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.281625ms
Jun 12 13:01:45.600: INFO: Pod "downward-api-3a23cadf-8d12-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005930082s
STEP: Saw pod success
Jun 12 13:01:45.600: INFO: Pod "downward-api-3a23cadf-8d12-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:01:45.603: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod downward-api-3a23cadf-8d12-11e9-b925-1ab852558ec8 container dapi-container: <nil>
STEP: delete the pod
Jun 12 13:01:45.622: INFO: Waiting for pod downward-api-3a23cadf-8d12-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:01:45.624: INFO: Pod downward-api-3a23cadf-8d12-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:01:45.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4419" for this suite.
Jun 12 13:01:51.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:01:51.705: INFO: namespace downward-api-4419 deletion completed in 6.077892836s

• [SLOW TEST:8.162 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:01:51.706: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:01:53.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6592" for this suite.
Jun 12 13:02:31.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:02:31.856: INFO: namespace kubelet-test-6592 deletion completed in 38.081207745s

• [SLOW TEST:40.150 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:02:31.856: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jun 12 13:02:31.892: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:02:36.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3995" for this suite.
Jun 12 13:02:58.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:02:58.497: INFO: namespace init-container-3995 deletion completed in 22.081187629s

• [SLOW TEST:26.641 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:02:58.497: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 13:02:58.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 version'
Jun 12 13:02:58.602: INFO: stderr: ""
Jun 12 13:02:58.602: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:44:30Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"11+\", GitVersion:\"v1.14.3-aliyun.1\", GitCommit:\"a55423c\", GitTreeState:\"\", BuildDate:\"2019-06-12T02:46:29Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:02:58.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4804" for this suite.
Jun 12 13:03:04.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:03:04.687: INFO: namespace kubectl-4804 deletion completed in 6.081534718s

• [SLOW TEST:6.191 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:03:04.688: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0612 13:03:14.794897      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 12 13:03:14.794: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:03:14.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7681" for this suite.
Jun 12 13:03:20.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:03:20.892: INFO: namespace gc-7681 deletion completed in 6.094670798s

• [SLOW TEST:16.205 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:03:20.892: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Jun 12 13:03:20.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 --namespace=kubectl-1237 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jun 12 13:03:23.496: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jun 12 13:03:23.496: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:03:25.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1237" for this suite.
Jun 12 13:03:31.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:03:31.588: INFO: namespace kubectl-1237 deletion completed in 6.083398888s

• [SLOW TEST:10.696 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:03:31.588: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:03:31.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-790" for this suite.
Jun 12 13:03:37.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:03:37.714: INFO: namespace services-790 deletion completed in 6.082290419s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.125 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:03:37.714: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jun 12 13:03:40.306: INFO: Successfully updated pod "labelsupdate7e327143-8d12-11e9-b925-1ab852558ec8"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:03:42.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7390" for this suite.
Jun 12 13:04:04.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:04:04.430: INFO: namespace projected-7390 deletion completed in 22.106031047s

• [SLOW TEST:26.716 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:04:04.430: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 12 13:04:04.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1388'
Jun 12 13:04:04.536: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 12 13:04:04.536: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Jun 12 13:04:04.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 delete jobs e2e-test-nginx-job --namespace=kubectl-1388'
Jun 12 13:04:04.620: INFO: stderr: ""
Jun 12 13:04:04.620: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:04:04.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1388" for this suite.
Jun 12 13:04:10.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:04:10.709: INFO: namespace kubectl-1388 deletion completed in 6.085419958s

• [SLOW TEST:6.279 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:04:10.709: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-91d97078-8d12-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume secrets
Jun 12 13:04:10.781: INFO: Waiting up to 5m0s for pod "pod-secrets-91def122-8d12-11e9-b925-1ab852558ec8" in namespace "secrets-6472" to be "success or failure"
Jun 12 13:04:10.783: INFO: Pod "pod-secrets-91def122-8d12-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.232218ms
Jun 12 13:04:12.787: INFO: Pod "pod-secrets-91def122-8d12-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005890671s
Jun 12 13:04:14.790: INFO: Pod "pod-secrets-91def122-8d12-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009745537s
STEP: Saw pod success
Jun 12 13:04:14.790: INFO: Pod "pod-secrets-91def122-8d12-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:04:14.793: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-secrets-91def122-8d12-11e9-b925-1ab852558ec8 container secret-volume-test: <nil>
STEP: delete the pod
Jun 12 13:04:14.812: INFO: Waiting for pod pod-secrets-91def122-8d12-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:04:14.814: INFO: Pod pod-secrets-91def122-8d12-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:04:14.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6472" for this suite.
Jun 12 13:04:20.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:04:20.899: INFO: namespace secrets-6472 deletion completed in 6.081300806s
STEP: Destroying namespace "secret-namespace-2860" for this suite.
Jun 12 13:04:26.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:04:26.982: INFO: namespace secret-namespace-2860 deletion completed in 6.083276805s

• [SLOW TEST:16.273 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:04:26.982: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jun 12 13:04:27.013: INFO: PodSpec: initContainers in spec.initContainers
Jun 12 13:05:15.365: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9b8cdf77-8d12-11e9-b925-1ab852558ec8", GenerateName:"", Namespace:"init-container-1480", SelfLink:"/api/v1/namespaces/init-container-1480/pods/pod-init-9b8cdf77-8d12-11e9-b925-1ab852558ec8", UID:"9c27c7b7-8d12-11e9-afb2-00163e04c212", ResourceVersion:"23786", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63695941468, loc:(*time.Location)(0x8a1a0e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"13930627"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-65pzm", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002515280), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-65pzm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-65pzm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-65pzm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0004ab228), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cn-hongkong.192.168.0.217", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0023849c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0004ab2b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0004ab2d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0004ab2d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0004ab2dc)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941467, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941467, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941467, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63695941468, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.0.217", PodIP:"172.20.2.219", StartTime:(*v1.Time)(0xc002277900), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002712930)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0027129a0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://bdf8a2e2f15b41e6097009a238fb9b4182c3a5dc37ec61bbdd9893ce1af09f5d"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002277940), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002277920), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:05:15.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1480" for this suite.
Jun 12 13:05:37.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:05:37.456: INFO: namespace init-container-1480 deletion completed in 22.08721165s

• [SLOW TEST:70.474 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:05:37.456: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 12 13:05:37.507: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c58fd3a5-8d12-11e9-b925-1ab852558ec8" in namespace "downward-api-1366" to be "success or failure"
Jun 12 13:05:37.510: INFO: Pod "downwardapi-volume-c58fd3a5-8d12-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.486905ms
Jun 12 13:05:39.514: INFO: Pod "downwardapi-volume-c58fd3a5-8d12-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00720741s
Jun 12 13:05:41.518: INFO: Pod "downwardapi-volume-c58fd3a5-8d12-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010996643s
STEP: Saw pod success
Jun 12 13:05:41.518: INFO: Pod "downwardapi-volume-c58fd3a5-8d12-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:05:41.520: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod downwardapi-volume-c58fd3a5-8d12-11e9-b925-1ab852558ec8 container client-container: <nil>
STEP: delete the pod
Jun 12 13:05:41.550: INFO: Waiting for pod downwardapi-volume-c58fd3a5-8d12-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:05:41.553: INFO: Pod downwardapi-volume-c58fd3a5-8d12-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:05:41.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1366" for this suite.
Jun 12 13:05:47.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:05:47.637: INFO: namespace downward-api-1366 deletion completed in 6.080182845s

• [SLOW TEST:10.180 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:05:47.637: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Jun 12 13:05:47.667: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jun 12 13:05:47.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 create -f - --namespace=kubectl-446'
Jun 12 13:05:47.915: INFO: stderr: ""
Jun 12 13:05:47.915: INFO: stdout: "service/redis-slave created\n"
Jun 12 13:05:47.915: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jun 12 13:05:47.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 create -f - --namespace=kubectl-446'
Jun 12 13:05:48.167: INFO: stderr: ""
Jun 12 13:05:48.167: INFO: stdout: "service/redis-master created\n"
Jun 12 13:05:48.168: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 12 13:05:48.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 create -f - --namespace=kubectl-446'
Jun 12 13:05:48.399: INFO: stderr: ""
Jun 12 13:05:48.399: INFO: stdout: "service/frontend created\n"
Jun 12 13:05:48.399: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jun 12 13:05:48.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 create -f - --namespace=kubectl-446'
Jun 12 13:05:48.618: INFO: stderr: ""
Jun 12 13:05:48.618: INFO: stdout: "deployment.apps/frontend created\n"
Jun 12 13:05:48.618: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 12 13:05:48.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 create -f - --namespace=kubectl-446'
Jun 12 13:05:48.814: INFO: stderr: ""
Jun 12 13:05:48.814: INFO: stdout: "deployment.apps/redis-master created\n"
Jun 12 13:05:48.814: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jun 12 13:05:48.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 create -f - --namespace=kubectl-446'
Jun 12 13:05:49.030: INFO: stderr: ""
Jun 12 13:05:49.030: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jun 12 13:05:49.030: INFO: Waiting for all frontend pods to be Running.
Jun 12 13:06:14.081: INFO: Waiting for frontend to serve content.
Jun 12 13:06:15.097: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jun 12 13:06:20.110: INFO: Trying to add a new entry to the guestbook.
Jun 12 13:06:20.124: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jun 12 13:06:20.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 delete --grace-period=0 --force -f - --namespace=kubectl-446'
Jun 12 13:06:20.233: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 12 13:06:20.233: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jun 12 13:06:20.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 delete --grace-period=0 --force -f - --namespace=kubectl-446'
Jun 12 13:06:20.314: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 12 13:06:20.314: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 12 13:06:20.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 delete --grace-period=0 --force -f - --namespace=kubectl-446'
Jun 12 13:06:20.400: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 12 13:06:20.400: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 12 13:06:20.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 delete --grace-period=0 --force -f - --namespace=kubectl-446'
Jun 12 13:06:20.469: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 12 13:06:20.469: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 12 13:06:20.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 delete --grace-period=0 --force -f - --namespace=kubectl-446'
Jun 12 13:06:20.537: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 12 13:06:20.537: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 12 13:06:20.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 delete --grace-period=0 --force -f - --namespace=kubectl-446'
Jun 12 13:06:20.605: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 12 13:06:20.605: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:06:20.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-446" for this suite.
Jun 12 13:06:58.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:06:58.721: INFO: namespace kubectl-446 deletion completed in 38.112538918s

• [SLOW TEST:71.084 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:06:58.721: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jun 12 13:06:58.761: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:07:03.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2081" for this suite.
Jun 12 13:07:09.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:07:09.216: INFO: namespace init-container-2081 deletion completed in 6.081088286s

• [SLOW TEST:10.495 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:07:09.217: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 12 13:07:17.284: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9196 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:07:17.284: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:07:17.506: INFO: Exec stderr: ""
Jun 12 13:07:17.506: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9196 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:07:17.506: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:07:17.771: INFO: Exec stderr: ""
Jun 12 13:07:17.771: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9196 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:07:17.771: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:07:18.024: INFO: Exec stderr: ""
Jun 12 13:07:18.024: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9196 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:07:18.024: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:07:18.307: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 12 13:07:18.307: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9196 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:07:18.307: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:07:18.568: INFO: Exec stderr: ""
Jun 12 13:07:18.568: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9196 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:07:18.569: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:07:18.851: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 12 13:07:18.851: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9196 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:07:18.851: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:07:19.076: INFO: Exec stderr: ""
Jun 12 13:07:19.076: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9196 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:07:19.076: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:07:19.388: INFO: Exec stderr: ""
Jun 12 13:07:19.388: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9196 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:07:19.388: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:07:19.649: INFO: Exec stderr: ""
Jun 12 13:07:19.649: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9196 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:07:19.649: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:07:19.970: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:07:19.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9196" for this suite.
Jun 12 13:08:09.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:08:10.067: INFO: namespace e2e-kubelet-etc-hosts-9196 deletion completed in 50.092740742s

• [SLOW TEST:60.850 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:08:10.067: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 13:08:10.119: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 12 13:08:10.138: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:10.138: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:10.138: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:10.144: INFO: Number of nodes with available pods: 0
Jun 12 13:08:10.144: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 13:08:11.148: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:11.148: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:11.148: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:11.151: INFO: Number of nodes with available pods: 0
Jun 12 13:08:11.151: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 13:08:12.148: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:12.148: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:12.148: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:12.151: INFO: Number of nodes with available pods: 3
Jun 12 13:08:12.151: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 12 13:08:12.172: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:12.172: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:12.172: INFO: Wrong image for pod: daemon-set-v6s6f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:12.175: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:12.175: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:12.175: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:13.178: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:13.178: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:13.178: INFO: Wrong image for pod: daemon-set-v6s6f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:13.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:13.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:13.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:14.179: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:14.179: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:14.179: INFO: Wrong image for pod: daemon-set-v6s6f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:14.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:14.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:14.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:15.178: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:15.178: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:15.178: INFO: Wrong image for pod: daemon-set-v6s6f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:15.178: INFO: Pod daemon-set-v6s6f is not available
Jun 12 13:08:15.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:15.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:15.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:16.179: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:16.179: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:16.179: INFO: Wrong image for pod: daemon-set-v6s6f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:16.179: INFO: Pod daemon-set-v6s6f is not available
Jun 12 13:08:16.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:16.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:16.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:17.179: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:17.179: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:17.179: INFO: Wrong image for pod: daemon-set-v6s6f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:17.179: INFO: Pod daemon-set-v6s6f is not available
Jun 12 13:08:17.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:17.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:17.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:18.179: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:18.179: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:18.179: INFO: Wrong image for pod: daemon-set-v6s6f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:18.179: INFO: Pod daemon-set-v6s6f is not available
Jun 12 13:08:18.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:18.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:18.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:19.179: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:19.179: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:19.179: INFO: Wrong image for pod: daemon-set-v6s6f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:19.179: INFO: Pod daemon-set-v6s6f is not available
Jun 12 13:08:19.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:19.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:19.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:20.179: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:20.179: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:20.179: INFO: Wrong image for pod: daemon-set-v6s6f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:20.179: INFO: Pod daemon-set-v6s6f is not available
Jun 12 13:08:20.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:20.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:20.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:21.178: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:21.178: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:21.178: INFO: Wrong image for pod: daemon-set-v6s6f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:21.178: INFO: Pod daemon-set-v6s6f is not available
Jun 12 13:08:21.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:21.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:21.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:22.178: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:22.178: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:22.178: INFO: Wrong image for pod: daemon-set-v6s6f. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:22.178: INFO: Pod daemon-set-v6s6f is not available
Jun 12 13:08:22.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:22.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:22.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:23.179: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:23.179: INFO: Pod daemon-set-8fsbt is not available
Jun 12 13:08:23.179: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:23.183: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:23.183: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:23.183: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:24.179: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:24.179: INFO: Pod daemon-set-8fsbt is not available
Jun 12 13:08:24.179: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:24.183: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:24.183: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:24.183: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:25.179: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:25.179: INFO: Pod daemon-set-8fsbt is not available
Jun 12 13:08:25.179: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:25.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:25.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:25.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:26.179: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:26.179: INFO: Pod daemon-set-8fsbt is not available
Jun 12 13:08:26.179: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:26.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:26.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:26.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:27.179: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:27.179: INFO: Pod daemon-set-576dj is not available
Jun 12 13:08:27.179: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:27.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:27.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:27.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:28.178: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:28.178: INFO: Pod daemon-set-576dj is not available
Jun 12 13:08:28.178: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:28.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:28.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:28.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:29.178: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:29.178: INFO: Pod daemon-set-576dj is not available
Jun 12 13:08:29.178: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:29.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:29.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:29.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:30.178: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:30.178: INFO: Pod daemon-set-576dj is not available
Jun 12 13:08:30.178: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:30.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:30.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:30.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:31.178: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:31.178: INFO: Pod daemon-set-576dj is not available
Jun 12 13:08:31.178: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:31.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:31.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:31.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:32.178: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:32.178: INFO: Pod daemon-set-576dj is not available
Jun 12 13:08:32.178: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:32.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:32.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:32.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:33.179: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:33.179: INFO: Pod daemon-set-576dj is not available
Jun 12 13:08:33.179: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:33.183: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:33.183: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:33.183: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:34.179: INFO: Wrong image for pod: daemon-set-576dj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:34.179: INFO: Pod daemon-set-576dj is not available
Jun 12 13:08:34.179: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:34.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:34.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:34.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:35.179: INFO: Pod daemon-set-769l9 is not available
Jun 12 13:08:35.179: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:35.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:35.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:35.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:36.178: INFO: Pod daemon-set-769l9 is not available
Jun 12 13:08:36.178: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:36.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:36.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:36.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:37.179: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:37.183: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:37.183: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:37.183: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:38.178: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:38.178: INFO: Pod daemon-set-v4ls8 is not available
Jun 12 13:08:38.185: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:38.185: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:38.185: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:39.179: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:39.179: INFO: Pod daemon-set-v4ls8 is not available
Jun 12 13:08:39.183: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:39.183: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:39.183: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:40.178: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:40.178: INFO: Pod daemon-set-v4ls8 is not available
Jun 12 13:08:40.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:40.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:40.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:41.179: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:41.180: INFO: Pod daemon-set-v4ls8 is not available
Jun 12 13:08:41.183: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:41.183: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:41.183: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:42.178: INFO: Wrong image for pod: daemon-set-v4ls8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 12 13:08:42.178: INFO: Pod daemon-set-v4ls8 is not available
Jun 12 13:08:42.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:42.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:42.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:43.178: INFO: Pod daemon-set-fkw9g is not available
Jun 12 13:08:43.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:43.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:43.182: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 12 13:08:43.186: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:43.186: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:43.186: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:43.188: INFO: Number of nodes with available pods: 2
Jun 12 13:08:43.188: INFO: Node cn-hongkong.192.168.0.218 is running more than one daemon pod
Jun 12 13:08:44.192: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:44.192: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:44.192: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:44.195: INFO: Number of nodes with available pods: 2
Jun 12 13:08:44.195: INFO: Node cn-hongkong.192.168.0.218 is running more than one daemon pod
Jun 12 13:08:45.193: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:45.193: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:45.193: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:08:45.196: INFO: Number of nodes with available pods: 3
Jun 12 13:08:45.196: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5483, will wait for the garbage collector to delete the pods
Jun 12 13:08:45.267: INFO: Deleting DaemonSet.extensions daemon-set took: 6.712406ms
Jun 12 13:08:45.667: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.157815ms
Jun 12 13:08:54.470: INFO: Number of nodes with available pods: 0
Jun 12 13:08:54.470: INFO: Number of running nodes: 0, number of available pods: 0
Jun 12 13:08:54.472: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5483/daemonsets","resourceVersion":"24816"},"items":null}

Jun 12 13:08:54.474: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5483/pods","resourceVersion":"24816"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:08:54.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5483" for this suite.
Jun 12 13:09:00.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:09:00.581: INFO: namespace daemonsets-5483 deletion completed in 6.092170214s

• [SLOW TEST:50.514 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:09:00.581: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 13:09:04.642: INFO: Waiting up to 5m0s for pod "client-envvars-4106ef25-8d13-11e9-b925-1ab852558ec8" in namespace "pods-276" to be "success or failure"
Jun 12 13:09:04.651: INFO: Pod "client-envvars-4106ef25-8d13-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.159524ms
Jun 12 13:09:06.655: INFO: Pod "client-envvars-4106ef25-8d13-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012629539s
STEP: Saw pod success
Jun 12 13:09:06.655: INFO: Pod "client-envvars-4106ef25-8d13-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:09:06.657: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod client-envvars-4106ef25-8d13-11e9-b925-1ab852558ec8 container env3cont: <nil>
STEP: delete the pod
Jun 12 13:09:06.678: INFO: Waiting for pod client-envvars-4106ef25-8d13-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:09:06.680: INFO: Pod client-envvars-4106ef25-8d13-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:09:06.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-276" for this suite.
Jun 12 13:09:56.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:09:56.761: INFO: namespace pods-276 deletion completed in 50.0771556s

• [SLOW TEST:56.179 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:09:56.761: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 12 13:09:56.823: INFO: Waiting up to 5m0s for pod "downwardapi-volume-60205b20-8d13-11e9-b925-1ab852558ec8" in namespace "projected-6883" to be "success or failure"
Jun 12 13:09:56.828: INFO: Pod "downwardapi-volume-60205b20-8d13-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.227177ms
Jun 12 13:09:58.837: INFO: Pod "downwardapi-volume-60205b20-8d13-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013568524s
STEP: Saw pod success
Jun 12 13:09:58.837: INFO: Pod "downwardapi-volume-60205b20-8d13-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:09:58.840: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod downwardapi-volume-60205b20-8d13-11e9-b925-1ab852558ec8 container client-container: <nil>
STEP: delete the pod
Jun 12 13:09:58.860: INFO: Waiting for pod downwardapi-volume-60205b20-8d13-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:09:58.863: INFO: Pod downwardapi-volume-60205b20-8d13-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:09:58.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6883" for this suite.
Jun 12 13:10:04.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:10:04.961: INFO: namespace projected-6883 deletion completed in 6.085308062s

• [SLOW TEST:8.200 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:10:04.961: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 13:10:04.997: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:10:08.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-556" for this suite.
Jun 12 13:10:58.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:10:58.129: INFO: namespace pods-556 deletion completed in 50.084634399s

• [SLOW TEST:53.168 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:10:58.129: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jun 12 13:10:58.160: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 12 13:10:58.171: INFO: Waiting for terminating namespaces to be deleted...
Jun 12 13:10:58.173: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.217 before test
Jun 12 13:10:58.177: INFO: kube-proxy-worker-p54dg from kube-system started at 2019-06-12 11:16:30 +0000 UTC (1 container statuses recorded)
Jun 12 13:10:58.177: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Jun 12 13:10:58.177: INFO: kube-flannel-ds-x7nmf from kube-system started at 2019-06-12 11:16:31 +0000 UTC (2 container statuses recorded)
Jun 12 13:10:58.177: INFO: 	Container install-cni ready: true, restart count 0
Jun 12 13:10:58.177: INFO: 	Container kube-flannel ready: true, restart count 0
Jun 12 13:10:58.177: INFO: aliyun-acr-credential-helper-6bb54dfdfd-5gk8t from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 13:10:58.177: INFO: 	Container aliyun-acr-credential-helper ready: true, restart count 0
Jun 12 13:10:58.177: INFO: nginx-ingress-controller-8584849666-qrnjk from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 13:10:58.177: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jun 12 13:10:58.177: INFO: flexvolume-qzj4v from kube-system started at 2019-06-12 12:20:56 +0000 UTC (1 container statuses recorded)
Jun 12 13:10:58.177: INFO: 	Container acs-flexvolume ready: true, restart count 0
Jun 12 13:10:58.177: INFO: sonobuoy-systemd-logs-daemon-set-abb5e6914f274695-4skg8 from heptio-sonobuoy started at 2019-06-12 12:21:07 +0000 UTC (2 container statuses recorded)
Jun 12 13:10:58.177: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 12 13:10:58.177: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 12 13:10:58.177: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.218 before test
Jun 12 13:10:58.182: INFO: kube-proxy-worker-2mnhq from kube-system started at 2019-06-12 11:16:30 +0000 UTC (1 container statuses recorded)
Jun 12 13:10:58.182: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Jun 12 13:10:58.182: INFO: kube-flannel-ds-9h6cz from kube-system started at 2019-06-12 11:16:31 +0000 UTC (2 container statuses recorded)
Jun 12 13:10:58.182: INFO: 	Container install-cni ready: true, restart count 0
Jun 12 13:10:58.182: INFO: 	Container kube-flannel ready: true, restart count 0
Jun 12 13:10:58.182: INFO: flexvolume-5mrxl from kube-system started at 2019-06-12 12:20:55 +0000 UTC (1 container statuses recorded)
Jun 12 13:10:58.182: INFO: 	Container acs-flexvolume ready: true, restart count 0
Jun 12 13:10:58.182: INFO: metrics-server-645cb87c9d-ddkrk from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 13:10:58.182: INFO: 	Container metrics-server ready: true, restart count 0
Jun 12 13:10:58.182: INFO: coredns-7594b4cbdd-9ht8c from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 13:10:58.182: INFO: 	Container coredns ready: true, restart count 0
Jun 12 13:10:58.182: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 13:10:58.182: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 12 13:10:58.182: INFO: sonobuoy-systemd-logs-daemon-set-abb5e6914f274695-27hnn from heptio-sonobuoy started at 2019-06-12 12:21:07 +0000 UTC (2 container statuses recorded)
Jun 12 13:10:58.182: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 12 13:10:58.182: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 12 13:10:58.182: INFO: 
Logging pods the kubelet thinks is on node cn-hongkong.192.168.0.219 before test
Jun 12 13:10:58.188: INFO: sonobuoy-e2e-job-1d838d5dfce648c8 from heptio-sonobuoy started at 2019-06-12 12:21:07 +0000 UTC (2 container statuses recorded)
Jun 12 13:10:58.188: INFO: 	Container e2e ready: true, restart count 0
Jun 12 13:10:58.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 12 13:10:58.188: INFO: kube-proxy-worker-94r4b from kube-system started at 2019-06-12 11:16:30 +0000 UTC (1 container statuses recorded)
Jun 12 13:10:58.188: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Jun 12 13:10:58.188: INFO: flexvolume-mwfdp from kube-system started at 2019-06-12 12:20:54 +0000 UTC (1 container statuses recorded)
Jun 12 13:10:58.188: INFO: 	Container acs-flexvolume ready: true, restart count 0
Jun 12 13:10:58.188: INFO: nginx-ingress-controller-8584849666-ft4jg from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 13:10:58.188: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jun 12 13:10:58.188: INFO: sonobuoy-systemd-logs-daemon-set-abb5e6914f274695-6v8ls from heptio-sonobuoy started at 2019-06-12 12:21:07 +0000 UTC (2 container statuses recorded)
Jun 12 13:10:58.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 12 13:10:58.188: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 12 13:10:58.188: INFO: kube-flannel-ds-2xjfg from kube-system started at 2019-06-12 11:16:31 +0000 UTC (2 container statuses recorded)
Jun 12 13:10:58.188: INFO: 	Container install-cni ready: true, restart count 0
Jun 12 13:10:58.188: INFO: 	Container kube-flannel ready: true, restart count 0
Jun 12 13:10:58.188: INFO: coredns-7594b4cbdd-76756 from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 13:10:58.188: INFO: 	Container coredns ready: true, restart count 0
Jun 12 13:10:58.188: INFO: tiller-deploy-7f49bdd789-tp29r from kube-system started at 2019-06-12 12:20:58 +0000 UTC (1 container statuses recorded)
Jun 12 13:10:58.188: INFO: 	Container tiller ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-85ecf9d5-8d13-11e9-b925-1ab852558ec8 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-85ecf9d5-8d13-11e9-b925-1ab852558ec8 off the node cn-hongkong.192.168.0.218
STEP: verifying the node doesn't have the label kubernetes.io/e2e-85ecf9d5-8d13-11e9-b925-1ab852558ec8
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:11:02.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9872" for this suite.
Jun 12 13:11:10.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:11:10.359: INFO: namespace sched-pred-9872 deletion completed in 8.080578298s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:12.230 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:11:10.359: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:11:14.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8315" for this suite.
Jun 12 13:11:20.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:11:20.494: INFO: namespace kubelet-test-8315 deletion completed in 6.080773734s

• [SLOW TEST:10.135 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:11:20.495: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 12 13:11:20.534: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9205b91c-8d13-11e9-b925-1ab852558ec8" in namespace "downward-api-154" to be "success or failure"
Jun 12 13:11:20.536: INFO: Pod "downwardapi-volume-9205b91c-8d13-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.105802ms
Jun 12 13:11:22.539: INFO: Pod "downwardapi-volume-9205b91c-8d13-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005620222s
Jun 12 13:11:24.543: INFO: Pod "downwardapi-volume-9205b91c-8d13-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009005476s
STEP: Saw pod success
Jun 12 13:11:24.543: INFO: Pod "downwardapi-volume-9205b91c-8d13-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:11:24.545: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod downwardapi-volume-9205b91c-8d13-11e9-b925-1ab852558ec8 container client-container: <nil>
STEP: delete the pod
Jun 12 13:11:24.563: INFO: Waiting for pod downwardapi-volume-9205b91c-8d13-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:11:24.565: INFO: Pod downwardapi-volume-9205b91c-8d13-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:11:24.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-154" for this suite.
Jun 12 13:11:30.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:11:30.658: INFO: namespace downward-api-154 deletion completed in 6.090311281s

• [SLOW TEST:10.164 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:11:30.659: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-flt6
STEP: Creating a pod to test atomic-volume-subpath
Jun 12 13:11:30.707: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-flt6" in namespace "subpath-1388" to be "success or failure"
Jun 12 13:11:30.716: INFO: Pod "pod-subpath-test-secret-flt6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.80144ms
Jun 12 13:11:32.720: INFO: Pod "pod-subpath-test-secret-flt6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012114926s
Jun 12 13:11:34.723: INFO: Pod "pod-subpath-test-secret-flt6": Phase="Running", Reason="", readiness=true. Elapsed: 4.016017518s
Jun 12 13:11:36.727: INFO: Pod "pod-subpath-test-secret-flt6": Phase="Running", Reason="", readiness=true. Elapsed: 6.019446871s
Jun 12 13:11:38.730: INFO: Pod "pod-subpath-test-secret-flt6": Phase="Running", Reason="", readiness=true. Elapsed: 8.022729749s
Jun 12 13:11:40.734: INFO: Pod "pod-subpath-test-secret-flt6": Phase="Running", Reason="", readiness=true. Elapsed: 10.026181202s
Jun 12 13:11:42.737: INFO: Pod "pod-subpath-test-secret-flt6": Phase="Running", Reason="", readiness=true. Elapsed: 12.030068648s
Jun 12 13:11:44.741: INFO: Pod "pod-subpath-test-secret-flt6": Phase="Running", Reason="", readiness=true. Elapsed: 14.033580945s
Jun 12 13:11:46.745: INFO: Pod "pod-subpath-test-secret-flt6": Phase="Running", Reason="", readiness=true. Elapsed: 16.037203119s
Jun 12 13:11:48.747: INFO: Pod "pod-subpath-test-secret-flt6": Phase="Running", Reason="", readiness=true. Elapsed: 18.040082277s
Jun 12 13:11:50.751: INFO: Pod "pod-subpath-test-secret-flt6": Phase="Running", Reason="", readiness=true. Elapsed: 20.043728045s
Jun 12 13:11:52.755: INFO: Pod "pod-subpath-test-secret-flt6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.047453853s
STEP: Saw pod success
Jun 12 13:11:52.755: INFO: Pod "pod-subpath-test-secret-flt6" satisfied condition "success or failure"
Jun 12 13:11:52.758: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-subpath-test-secret-flt6 container test-container-subpath-secret-flt6: <nil>
STEP: delete the pod
Jun 12 13:11:52.775: INFO: Waiting for pod pod-subpath-test-secret-flt6 to disappear
Jun 12 13:11:52.777: INFO: Pod pod-subpath-test-secret-flt6 no longer exists
STEP: Deleting pod pod-subpath-test-secret-flt6
Jun 12 13:11:52.777: INFO: Deleting pod "pod-subpath-test-secret-flt6" in namespace "subpath-1388"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:11:52.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1388" for this suite.
Jun 12 13:11:58.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:11:58.878: INFO: namespace subpath-1388 deletion completed in 6.083986189s

• [SLOW TEST:28.219 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:11:58.878: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 12 13:11:58.917: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8e6b07b-8d13-11e9-b925-1ab852558ec8" in namespace "downward-api-5325" to be "success or failure"
Jun 12 13:11:58.919: INFO: Pod "downwardapi-volume-a8e6b07b-8d13-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.286685ms
Jun 12 13:12:00.923: INFO: Pod "downwardapi-volume-a8e6b07b-8d13-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005730944s
STEP: Saw pod success
Jun 12 13:12:00.923: INFO: Pod "downwardapi-volume-a8e6b07b-8d13-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:12:00.926: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod downwardapi-volume-a8e6b07b-8d13-11e9-b925-1ab852558ec8 container client-container: <nil>
STEP: delete the pod
Jun 12 13:12:00.948: INFO: Waiting for pod downwardapi-volume-a8e6b07b-8d13-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:12:00.950: INFO: Pod downwardapi-volume-a8e6b07b-8d13-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:12:00.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5325" for this suite.
Jun 12 13:12:06.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:12:07.037: INFO: namespace downward-api-5325 deletion completed in 6.083197252s

• [SLOW TEST:8.159 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:12:07.037: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 12 13:12:07.078: INFO: Waiting up to 5m0s for pod "downwardapi-volume-adc37f10-8d13-11e9-b925-1ab852558ec8" in namespace "projected-1632" to be "success or failure"
Jun 12 13:12:07.082: INFO: Pod "downwardapi-volume-adc37f10-8d13-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.86611ms
Jun 12 13:12:09.086: INFO: Pod "downwardapi-volume-adc37f10-8d13-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007569643s
STEP: Saw pod success
Jun 12 13:12:09.086: INFO: Pod "downwardapi-volume-adc37f10-8d13-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:12:09.089: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod downwardapi-volume-adc37f10-8d13-11e9-b925-1ab852558ec8 container client-container: <nil>
STEP: delete the pod
Jun 12 13:12:09.107: INFO: Waiting for pod downwardapi-volume-adc37f10-8d13-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:12:09.110: INFO: Pod downwardapi-volume-adc37f10-8d13-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:12:09.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1632" for this suite.
Jun 12 13:12:15.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:12:15.214: INFO: namespace projected-1632 deletion completed in 6.100951958s

• [SLOW TEST:8.177 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:12:15.214: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jun 12 13:12:20.790: INFO: Successfully updated pod "labelsupdateb2a3249b-8d13-11e9-b925-1ab852558ec8"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:12:22.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5642" for this suite.
Jun 12 13:12:44.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:12:44.901: INFO: namespace downward-api-5642 deletion completed in 22.09065827s

• [SLOW TEST:29.687 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:12:44.901: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-c454cdae-8d13-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume configMaps
Jun 12 13:12:44.943: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c455d624-8d13-11e9-b925-1ab852558ec8" in namespace "projected-5179" to be "success or failure"
Jun 12 13:12:44.945: INFO: Pod "pod-projected-configmaps-c455d624-8d13-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.201399ms
Jun 12 13:12:46.948: INFO: Pod "pod-projected-configmaps-c455d624-8d13-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005614865s
Jun 12 13:12:48.952: INFO: Pod "pod-projected-configmaps-c455d624-8d13-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009203781s
STEP: Saw pod success
Jun 12 13:12:48.952: INFO: Pod "pod-projected-configmaps-c455d624-8d13-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:12:48.954: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-projected-configmaps-c455d624-8d13-11e9-b925-1ab852558ec8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 12 13:12:48.975: INFO: Waiting for pod pod-projected-configmaps-c455d624-8d13-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:12:48.977: INFO: Pod pod-projected-configmaps-c455d624-8d13-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:12:48.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5179" for this suite.
Jun 12 13:12:54.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:12:55.067: INFO: namespace projected-5179 deletion completed in 6.085331623s

• [SLOW TEST:10.166 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:12:55.067: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 12 13:12:55.110: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-942,SelfLink:/api/v1/namespaces/watch-942/configmaps/e2e-watch-test-configmap-a,UID:ca6753e2-8d13-11e9-afb2-00163e04c212,ResourceVersion:25840,Generation:0,CreationTimestamp:2019-06-12 13:12:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 12 13:12:55.110: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-942,SelfLink:/api/v1/namespaces/watch-942/configmaps/e2e-watch-test-configmap-a,UID:ca6753e2-8d13-11e9-afb2-00163e04c212,ResourceVersion:25840,Generation:0,CreationTimestamp:2019-06-12 13:12:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 12 13:13:05.122: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-942,SelfLink:/api/v1/namespaces/watch-942/configmaps/e2e-watch-test-configmap-a,UID:ca6753e2-8d13-11e9-afb2-00163e04c212,ResourceVersion:25869,Generation:0,CreationTimestamp:2019-06-12 13:12:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 12 13:13:05.123: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-942,SelfLink:/api/v1/namespaces/watch-942/configmaps/e2e-watch-test-configmap-a,UID:ca6753e2-8d13-11e9-afb2-00163e04c212,ResourceVersion:25869,Generation:0,CreationTimestamp:2019-06-12 13:12:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 12 13:13:15.157: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-942,SelfLink:/api/v1/namespaces/watch-942/configmaps/e2e-watch-test-configmap-a,UID:ca6753e2-8d13-11e9-afb2-00163e04c212,ResourceVersion:25896,Generation:0,CreationTimestamp:2019-06-12 13:12:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 12 13:13:15.157: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-942,SelfLink:/api/v1/namespaces/watch-942/configmaps/e2e-watch-test-configmap-a,UID:ca6753e2-8d13-11e9-afb2-00163e04c212,ResourceVersion:25896,Generation:0,CreationTimestamp:2019-06-12 13:12:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 12 13:13:25.167: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-942,SelfLink:/api/v1/namespaces/watch-942/configmaps/e2e-watch-test-configmap-a,UID:ca6753e2-8d13-11e9-afb2-00163e04c212,ResourceVersion:25922,Generation:0,CreationTimestamp:2019-06-12 13:12:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 12 13:13:25.167: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-942,SelfLink:/api/v1/namespaces/watch-942/configmaps/e2e-watch-test-configmap-a,UID:ca6753e2-8d13-11e9-afb2-00163e04c212,ResourceVersion:25922,Generation:0,CreationTimestamp:2019-06-12 13:12:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 12 13:13:35.174: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-942,SelfLink:/api/v1/namespaces/watch-942/configmaps/e2e-watch-test-configmap-b,UID:e24897ca-8d13-11e9-afb2-00163e04c212,ResourceVersion:25949,Generation:0,CreationTimestamp:2019-06-12 13:13:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 12 13:13:35.174: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-942,SelfLink:/api/v1/namespaces/watch-942/configmaps/e2e-watch-test-configmap-b,UID:e24897ca-8d13-11e9-afb2-00163e04c212,ResourceVersion:25949,Generation:0,CreationTimestamp:2019-06-12 13:13:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 12 13:13:45.201: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-942,SelfLink:/api/v1/namespaces/watch-942/configmaps/e2e-watch-test-configmap-b,UID:e24897ca-8d13-11e9-afb2-00163e04c212,ResourceVersion:25977,Generation:0,CreationTimestamp:2019-06-12 13:13:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 12 13:13:45.201: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-942,SelfLink:/api/v1/namespaces/watch-942/configmaps/e2e-watch-test-configmap-b,UID:e24897ca-8d13-11e9-afb2-00163e04c212,ResourceVersion:25977,Generation:0,CreationTimestamp:2019-06-12 13:13:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:13:55.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-942" for this suite.
Jun 12 13:14:01.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:14:01.293: INFO: namespace watch-942 deletion completed in 6.088724131s

• [SLOW TEST:66.226 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:14:01.294: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-4475
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4475 to expose endpoints map[]
Jun 12 13:14:01.331: INFO: Get endpoints failed (3.616822ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jun 12 13:14:02.334: INFO: successfully validated that service multi-endpoint-test in namespace services-4475 exposes endpoints map[] (1.006787147s elapsed)
STEP: Creating pod pod1 in namespace services-4475
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4475 to expose endpoints map[pod1:[100]]
Jun 12 13:14:04.373: INFO: successfully validated that service multi-endpoint-test in namespace services-4475 exposes endpoints map[pod1:[100]] (2.022607568s elapsed)
STEP: Creating pod pod2 in namespace services-4475
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4475 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 12 13:14:06.426: INFO: successfully validated that service multi-endpoint-test in namespace services-4475 exposes endpoints map[pod1:[100] pod2:[101]] (2.048466915s elapsed)
STEP: Deleting pod pod1 in namespace services-4475
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4475 to expose endpoints map[pod2:[101]]
Jun 12 13:14:07.478: INFO: successfully validated that service multi-endpoint-test in namespace services-4475 exposes endpoints map[pod2:[101]] (1.045628606s elapsed)
STEP: Deleting pod pod2 in namespace services-4475
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4475 to expose endpoints map[]
Jun 12 13:14:08.491: INFO: successfully validated that service multi-endpoint-test in namespace services-4475 exposes endpoints map[] (1.005426357s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:14:08.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4475" for this suite.
Jun 12 13:14:30.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:14:30.599: INFO: namespace services-4475 deletion completed in 22.08163926s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:29.306 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:14:30.599: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-03558826-8d14-11e9-b925-1ab852558ec8
STEP: Creating secret with name secret-projected-all-test-volume-03558810-8d14-11e9-b925-1ab852558ec8
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 12 13:14:30.646: INFO: Waiting up to 5m0s for pod "projected-volume-035587de-8d14-11e9-b925-1ab852558ec8" in namespace "projected-7732" to be "success or failure"
Jun 12 13:14:30.649: INFO: Pod "projected-volume-035587de-8d14-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.25236ms
Jun 12 13:14:32.652: INFO: Pod "projected-volume-035587de-8d14-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005957411s
STEP: Saw pod success
Jun 12 13:14:32.652: INFO: Pod "projected-volume-035587de-8d14-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:14:32.655: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod projected-volume-035587de-8d14-11e9-b925-1ab852558ec8 container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 12 13:14:32.687: INFO: Waiting for pod projected-volume-035587de-8d14-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:14:32.689: INFO: Pod projected-volume-035587de-8d14-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:14:32.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7732" for this suite.
Jun 12 13:14:38.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:14:38.779: INFO: namespace projected-7732 deletion completed in 6.086848151s

• [SLOW TEST:8.180 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:14:38.779: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
Jun 12 13:14:38.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 create -f - --namespace=kubectl-147'
Jun 12 13:14:39.222: INFO: stderr: ""
Jun 12 13:14:39.222: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Jun 12 13:14:40.226: INFO: Selector matched 1 pods for map[app:redis]
Jun 12 13:14:40.226: INFO: Found 0 / 1
Jun 12 13:14:41.226: INFO: Selector matched 1 pods for map[app:redis]
Jun 12 13:14:41.226: INFO: Found 1 / 1
Jun 12 13:14:41.226: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 12 13:14:41.228: INFO: Selector matched 1 pods for map[app:redis]
Jun 12 13:14:41.228: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jun 12 13:14:41.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 logs redis-master-5676w redis-master --namespace=kubectl-147'
Jun 12 13:14:41.304: INFO: stderr: ""
Jun 12 13:14:41.304: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 12 Jun 13:14:40.295 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 12 Jun 13:14:40.295 # Server started, Redis version 3.2.12\n1:M 12 Jun 13:14:40.295 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 12 Jun 13:14:40.295 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jun 12 13:14:41.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 log redis-master-5676w redis-master --namespace=kubectl-147 --tail=1'
Jun 12 13:14:41.379: INFO: stderr: ""
Jun 12 13:14:41.379: INFO: stdout: "1:M 12 Jun 13:14:40.295 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jun 12 13:14:41.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 log redis-master-5676w redis-master --namespace=kubectl-147 --limit-bytes=1'
Jun 12 13:14:41.451: INFO: stderr: ""
Jun 12 13:14:41.452: INFO: stdout: " "
STEP: exposing timestamps
Jun 12 13:14:41.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 log redis-master-5676w redis-master --namespace=kubectl-147 --tail=1 --timestamps'
Jun 12 13:14:41.521: INFO: stderr: ""
Jun 12 13:14:41.521: INFO: stdout: "2019-06-12T13:14:40.296069015Z 1:M 12 Jun 13:14:40.295 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jun 12 13:14:44.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 log redis-master-5676w redis-master --namespace=kubectl-147 --since=1s'
Jun 12 13:14:44.096: INFO: stderr: ""
Jun 12 13:14:44.096: INFO: stdout: ""
Jun 12 13:14:44.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 log redis-master-5676w redis-master --namespace=kubectl-147 --since=24h'
Jun 12 13:14:44.166: INFO: stderr: ""
Jun 12 13:14:44.166: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 12 Jun 13:14:40.295 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 12 Jun 13:14:40.295 # Server started, Redis version 3.2.12\n1:M 12 Jun 13:14:40.295 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 12 Jun 13:14:40.295 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
Jun 12 13:14:44.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 delete --grace-period=0 --force -f - --namespace=kubectl-147'
Jun 12 13:14:44.232: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 12 13:14:44.232: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jun 12 13:14:44.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get rc,svc -l name=nginx --no-headers --namespace=kubectl-147'
Jun 12 13:14:44.296: INFO: stderr: "No resources found.\n"
Jun 12 13:14:44.296: INFO: stdout: ""
Jun 12 13:14:44.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -l name=nginx --namespace=kubectl-147 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 12 13:14:44.354: INFO: stderr: ""
Jun 12 13:14:44.355: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:14:44.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-147" for this suite.
Jun 12 13:14:50.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:14:50.445: INFO: namespace kubectl-147 deletion completed in 6.086448535s

• [SLOW TEST:11.666 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:14:50.445: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0612 13:14:51.509502      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 12 13:14:51.509: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:14:51.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8696" for this suite.
Jun 12 13:14:57.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:14:57.592: INFO: namespace gc-8696 deletion completed in 6.079625058s

• [SLOW TEST:7.147 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:14:57.592: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-136c4604-8d14-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume secrets
Jun 12 13:14:57.645: INFO: Waiting up to 5m0s for pod "pod-secrets-136ea733-8d14-11e9-b925-1ab852558ec8" in namespace "secrets-2067" to be "success or failure"
Jun 12 13:14:57.654: INFO: Pod "pod-secrets-136ea733-8d14-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.979988ms
Jun 12 13:14:59.657: INFO: Pod "pod-secrets-136ea733-8d14-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012356062s
Jun 12 13:15:01.661: INFO: Pod "pod-secrets-136ea733-8d14-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016014548s
STEP: Saw pod success
Jun 12 13:15:01.661: INFO: Pod "pod-secrets-136ea733-8d14-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:15:01.663: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-secrets-136ea733-8d14-11e9-b925-1ab852558ec8 container secret-volume-test: <nil>
STEP: delete the pod
Jun 12 13:15:01.683: INFO: Waiting for pod pod-secrets-136ea733-8d14-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:15:01.685: INFO: Pod pod-secrets-136ea733-8d14-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:15:01.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2067" for this suite.
Jun 12 13:15:07.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:15:07.774: INFO: namespace secrets-2067 deletion completed in 6.080468605s

• [SLOW TEST:10.182 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:15:07.774: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 12 13:15:07.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9977'
Jun 12 13:15:07.877: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 12 13:15:07.877: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jun 12 13:15:07.882: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-rzjcp]
Jun 12 13:15:07.882: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-rzjcp" in namespace "kubectl-9977" to be "running and ready"
Jun 12 13:15:07.887: INFO: Pod "e2e-test-nginx-rc-rzjcp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.875152ms
Jun 12 13:15:09.891: INFO: Pod "e2e-test-nginx-rc-rzjcp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008354138s
Jun 12 13:15:11.894: INFO: Pod "e2e-test-nginx-rc-rzjcp": Phase="Running", Reason="", readiness=true. Elapsed: 4.012023511s
Jun 12 13:15:11.894: INFO: Pod "e2e-test-nginx-rc-rzjcp" satisfied condition "running and ready"
Jun 12 13:15:11.894: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-rzjcp]
Jun 12 13:15:11.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 logs rc/e2e-test-nginx-rc --namespace=kubectl-9977'
Jun 12 13:15:11.975: INFO: stderr: ""
Jun 12 13:15:11.975: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Jun 12 13:15:11.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 delete rc e2e-test-nginx-rc --namespace=kubectl-9977'
Jun 12 13:15:12.040: INFO: stderr: ""
Jun 12 13:15:12.040: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:15:12.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9977" for this suite.
Jun 12 13:15:34.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:15:34.131: INFO: namespace kubectl-9977 deletion completed in 22.086717033s

• [SLOW TEST:26.356 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:15:34.131: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:15:40.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9348" for this suite.
Jun 12 13:15:46.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:15:46.328: INFO: namespace namespaces-9348 deletion completed in 6.081752008s
STEP: Destroying namespace "nsdeletetest-7979" for this suite.
Jun 12 13:15:46.330: INFO: Namespace nsdeletetest-7979 was already deleted
STEP: Destroying namespace "nsdeletetest-9372" for this suite.
Jun 12 13:15:52.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:15:52.427: INFO: namespace nsdeletetest-9372 deletion completed in 6.096058467s

• [SLOW TEST:18.296 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:15:52.427: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 12 13:15:58.522: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 12 13:15:58.524: INFO: Pod pod-with-poststart-http-hook still exists
Jun 12 13:16:00.524: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 12 13:16:00.528: INFO: Pod pod-with-poststart-http-hook still exists
Jun 12 13:16:02.524: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 12 13:16:02.527: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:16:02.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8299" for this suite.
Jun 12 13:16:24.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:16:24.612: INFO: namespace container-lifecycle-hook-8299 deletion completed in 22.081008522s

• [SLOW TEST:32.185 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:16:24.612: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:16:24.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2937" for this suite.
Jun 12 13:16:46.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:16:46.774: INFO: namespace pods-2937 deletion completed in 22.089620466s

• [SLOW TEST:22.161 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:16:46.774: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-5483d95e-8d14-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume configMaps
Jun 12 13:16:46.847: INFO: Waiting up to 5m0s for pod "pod-configmaps-54858ef8-8d14-11e9-b925-1ab852558ec8" in namespace "configmap-3355" to be "success or failure"
Jun 12 13:16:46.849: INFO: Pod "pod-configmaps-54858ef8-8d14-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.294588ms
Jun 12 13:16:48.852: INFO: Pod "pod-configmaps-54858ef8-8d14-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005633226s
STEP: Saw pod success
Jun 12 13:16:48.852: INFO: Pod "pod-configmaps-54858ef8-8d14-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:16:48.855: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-configmaps-54858ef8-8d14-11e9-b925-1ab852558ec8 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 12 13:16:48.874: INFO: Waiting for pod pod-configmaps-54858ef8-8d14-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:16:48.876: INFO: Pod pod-configmaps-54858ef8-8d14-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:16:48.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3355" for this suite.
Jun 12 13:16:54.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:16:54.970: INFO: namespace configmap-3355 deletion completed in 6.090333356s

• [SLOW TEST:8.196 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:16:54.970: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jun 12 13:16:55.006: INFO: Waiting up to 5m0s for pod "downward-api-596249a5-8d14-11e9-b925-1ab852558ec8" in namespace "downward-api-4250" to be "success or failure"
Jun 12 13:16:55.008: INFO: Pod "downward-api-596249a5-8d14-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.191213ms
Jun 12 13:16:57.012: INFO: Pod "downward-api-596249a5-8d14-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005649316s
Jun 12 13:16:59.015: INFO: Pod "downward-api-596249a5-8d14-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008812453s
STEP: Saw pod success
Jun 12 13:16:59.015: INFO: Pod "downward-api-596249a5-8d14-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:16:59.017: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod downward-api-596249a5-8d14-11e9-b925-1ab852558ec8 container dapi-container: <nil>
STEP: delete the pod
Jun 12 13:16:59.033: INFO: Waiting for pod downward-api-596249a5-8d14-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:16:59.035: INFO: Pod downward-api-596249a5-8d14-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:16:59.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4250" for this suite.
Jun 12 13:17:05.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:17:05.125: INFO: namespace downward-api-4250 deletion completed in 6.086587759s

• [SLOW TEST:10.155 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:17:05.125: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-5f70df60-8d14-11e9-b925-1ab852558ec8
STEP: Creating configMap with name cm-test-opt-upd-5f70df93-8d14-11e9-b925-1ab852558ec8
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-5f70df60-8d14-11e9-b925-1ab852558ec8
STEP: Updating configmap cm-test-opt-upd-5f70df93-8d14-11e9-b925-1ab852558ec8
STEP: Creating configMap with name cm-test-opt-create-5f70dfa6-8d14-11e9-b925-1ab852558ec8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:18:27.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6019" for this suite.
Jun 12 13:18:49.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:18:49.681: INFO: namespace projected-6019 deletion completed in 22.0926964s

• [SLOW TEST:104.555 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:18:49.681: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 12 13:18:50.726: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9dc23b8f-8d14-11e9-b925-1ab852558ec8" in namespace "projected-4104" to be "success or failure"
Jun 12 13:18:50.728: INFO: Pod "downwardapi-volume-9dc23b8f-8d14-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.513543ms
Jun 12 13:18:52.732: INFO: Pod "downwardapi-volume-9dc23b8f-8d14-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006284755s
STEP: Saw pod success
Jun 12 13:18:52.732: INFO: Pod "downwardapi-volume-9dc23b8f-8d14-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:18:52.735: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod downwardapi-volume-9dc23b8f-8d14-11e9-b925-1ab852558ec8 container client-container: <nil>
STEP: delete the pod
Jun 12 13:18:52.754: INFO: Waiting for pod downwardapi-volume-9dc23b8f-8d14-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:18:52.756: INFO: Pod downwardapi-volume-9dc23b8f-8d14-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:18:52.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4104" for this suite.
Jun 12 13:18:58.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:18:58.837: INFO: namespace projected-4104 deletion completed in 6.078120022s

• [SLOW TEST:9.156 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:18:58.837: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5854
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5854
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5854
Jun 12 13:18:58.883: INFO: Found 0 stateful pods, waiting for 1
Jun 12 13:19:08.886: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 12 13:19:08.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-5854 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 12 13:19:09.181: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 12 13:19:09.181: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 12 13:19:09.181: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 12 13:19:09.185: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 12 13:19:19.188: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 12 13:19:19.188: INFO: Waiting for statefulset status.replicas updated to 0
Jun 12 13:19:19.202: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999701s
Jun 12 13:19:20.205: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997309854s
Jun 12 13:19:21.209: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993680721s
Jun 12 13:19:22.213: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989806302s
Jun 12 13:19:23.218: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.986155165s
Jun 12 13:19:24.222: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.981198205s
Jun 12 13:19:25.225: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.977324183s
Jun 12 13:19:26.229: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.973913424s
Jun 12 13:19:27.232: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.970507944s
Jun 12 13:19:28.236: INFO: Verifying statefulset ss doesn't scale past 1 for another 966.941759ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5854
Jun 12 13:19:29.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-5854 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 12 13:19:29.524: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 12 13:19:29.524: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 12 13:19:29.524: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 12 13:19:29.528: INFO: Found 1 stateful pods, waiting for 3
Jun 12 13:19:39.531: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 12 13:19:39.531: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 12 13:19:39.531: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 12 13:19:39.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-5854 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 12 13:19:39.824: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 12 13:19:39.824: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 12 13:19:39.824: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 12 13:19:39.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-5854 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 12 13:19:40.103: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 12 13:19:40.103: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 12 13:19:40.103: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 12 13:19:40.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-5854 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 12 13:19:40.398: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 12 13:19:40.399: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 12 13:19:40.399: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 12 13:19:40.399: INFO: Waiting for statefulset status.replicas updated to 0
Jun 12 13:19:40.402: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 12 13:19:50.408: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 12 13:19:50.408: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 12 13:19:50.408: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 12 13:19:50.420: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999765s
Jun 12 13:19:51.423: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99559103s
Jun 12 13:19:52.427: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992036046s
Jun 12 13:19:53.431: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988082352s
Jun 12 13:19:54.435: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984284111s
Jun 12 13:19:55.439: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980378285s
Jun 12 13:19:56.442: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976562074s
Jun 12 13:19:57.447: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972699649s
Jun 12 13:19:58.451: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.968601336s
Jun 12 13:19:59.454: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.699751ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5854
Jun 12 13:20:00.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-5854 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 12 13:20:00.754: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 12 13:20:00.754: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 12 13:20:00.754: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 12 13:20:00.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-5854 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 12 13:20:01.046: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 12 13:20:01.046: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 12 13:20:01.046: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 12 13:20:01.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-5854 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 12 13:20:01.329: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 12 13:20:01.329: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 12 13:20:01.329: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 12 13:20:01.329: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jun 12 13:20:31.343: INFO: Deleting all statefulset in ns statefulset-5854
Jun 12 13:20:31.345: INFO: Scaling statefulset ss to 0
Jun 12 13:20:31.353: INFO: Waiting for statefulset status.replicas updated to 0
Jun 12 13:20:31.355: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:20:31.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5854" for this suite.
Jun 12 13:20:37.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:20:37.453: INFO: namespace statefulset-5854 deletion completed in 6.083112647s

• [SLOW TEST:98.615 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:20:37.453: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-ddfeeef2-8d14-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume configMaps
Jun 12 13:20:37.497: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ddffc32e-8d14-11e9-b925-1ab852558ec8" in namespace "projected-4789" to be "success or failure"
Jun 12 13:20:37.503: INFO: Pod "pod-projected-configmaps-ddffc32e-8d14-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.385343ms
Jun 12 13:20:39.507: INFO: Pod "pod-projected-configmaps-ddffc32e-8d14-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009937546s
STEP: Saw pod success
Jun 12 13:20:39.507: INFO: Pod "pod-projected-configmaps-ddffc32e-8d14-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:20:39.510: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-projected-configmaps-ddffc32e-8d14-11e9-b925-1ab852558ec8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 12 13:20:39.529: INFO: Waiting for pod pod-projected-configmaps-ddffc32e-8d14-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:20:39.532: INFO: Pod pod-projected-configmaps-ddffc32e-8d14-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:20:39.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4789" for this suite.
Jun 12 13:20:45.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:20:45.616: INFO: namespace projected-4789 deletion completed in 6.077509395s

• [SLOW TEST:8.163 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:20:45.616: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:20:50.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1689" for this suite.
Jun 12 13:21:08.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:21:08.759: INFO: namespace replication-controller-1689 deletion completed in 18.078895747s

• [SLOW TEST:23.143 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:21:08.759: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:21:08.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1235" for this suite.
Jun 12 13:21:14.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:21:14.893: INFO: namespace kubelet-test-1235 deletion completed in 6.07882761s

• [SLOW TEST:6.134 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:21:14.894: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 12 13:21:14.926: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:21:24.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8038" for this suite.
Jun 12 13:21:30.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:21:30.482: INFO: namespace pods-8038 deletion completed in 6.080118681s

• [SLOW TEST:15.588 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:21:30.482: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 12 13:21:36.567: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 12 13:21:36.569: INFO: Pod pod-with-prestop-http-hook still exists
Jun 12 13:21:38.569: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 12 13:21:38.573: INFO: Pod pod-with-prestop-http-hook still exists
Jun 12 13:21:40.569: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 12 13:21:40.573: INFO: Pod pod-with-prestop-http-hook still exists
Jun 12 13:21:42.569: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 12 13:21:42.573: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:21:42.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1439" for this suite.
Jun 12 13:22:04.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:22:04.669: INFO: namespace container-lifecycle-hook-1439 deletion completed in 22.085162717s

• [SLOW TEST:34.187 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:22:04.669: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-11fe37e4-8d15-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume secrets
Jun 12 13:22:04.734: INFO: Waiting up to 5m0s for pod "pod-secrets-11ff2b43-8d15-11e9-b925-1ab852558ec8" in namespace "secrets-9632" to be "success or failure"
Jun 12 13:22:04.736: INFO: Pod "pod-secrets-11ff2b43-8d15-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.240507ms
Jun 12 13:22:06.740: INFO: Pod "pod-secrets-11ff2b43-8d15-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005704134s
STEP: Saw pod success
Jun 12 13:22:06.740: INFO: Pod "pod-secrets-11ff2b43-8d15-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:22:06.742: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-secrets-11ff2b43-8d15-11e9-b925-1ab852558ec8 container secret-env-test: <nil>
STEP: delete the pod
Jun 12 13:22:06.760: INFO: Waiting for pod pod-secrets-11ff2b43-8d15-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:22:06.762: INFO: Pod pod-secrets-11ff2b43-8d15-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:22:06.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9632" for this suite.
Jun 12 13:22:12.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:22:12.845: INFO: namespace secrets-9632 deletion completed in 6.079547696s

• [SLOW TEST:8.176 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:22:12.845: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-16daf43e-8d15-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume secrets
Jun 12 13:22:12.891: INFO: Waiting up to 5m0s for pod "pod-secrets-16dbda76-8d15-11e9-b925-1ab852558ec8" in namespace "secrets-7403" to be "success or failure"
Jun 12 13:22:12.893: INFO: Pod "pod-secrets-16dbda76-8d15-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.142941ms
Jun 12 13:22:14.896: INFO: Pod "pod-secrets-16dbda76-8d15-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005260801s
STEP: Saw pod success
Jun 12 13:22:14.896: INFO: Pod "pod-secrets-16dbda76-8d15-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:22:14.898: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-secrets-16dbda76-8d15-11e9-b925-1ab852558ec8 container secret-volume-test: <nil>
STEP: delete the pod
Jun 12 13:22:14.916: INFO: Waiting for pod pod-secrets-16dbda76-8d15-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:22:14.922: INFO: Pod pod-secrets-16dbda76-8d15-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:22:14.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7403" for this suite.
Jun 12 13:22:20.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:22:21.020: INFO: namespace secrets-7403 deletion completed in 6.083717621s

• [SLOW TEST:8.174 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:22:21.020: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-1bb973e3-8d15-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume configMaps
Jun 12 13:22:21.060: INFO: Waiting up to 5m0s for pod "pod-configmaps-1bba5a8d-8d15-11e9-b925-1ab852558ec8" in namespace "configmap-4394" to be "success or failure"
Jun 12 13:22:21.063: INFO: Pod "pod-configmaps-1bba5a8d-8d15-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.646095ms
Jun 12 13:22:23.066: INFO: Pod "pod-configmaps-1bba5a8d-8d15-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006894621s
Jun 12 13:22:25.070: INFO: Pod "pod-configmaps-1bba5a8d-8d15-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010248501s
STEP: Saw pod success
Jun 12 13:22:25.070: INFO: Pod "pod-configmaps-1bba5a8d-8d15-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:22:25.072: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-configmaps-1bba5a8d-8d15-11e9-b925-1ab852558ec8 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 12 13:22:25.090: INFO: Waiting for pod pod-configmaps-1bba5a8d-8d15-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:22:25.092: INFO: Pod pod-configmaps-1bba5a8d-8d15-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:22:25.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4394" for this suite.
Jun 12 13:22:31.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:22:31.182: INFO: namespace configmap-4394 deletion completed in 6.087646535s

• [SLOW TEST:10.163 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:22:31.183: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 12 13:22:31.248: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:31.248: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:31.248: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:31.251: INFO: Number of nodes with available pods: 0
Jun 12 13:22:31.251: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 13:22:32.255: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:32.255: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:32.255: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:32.258: INFO: Number of nodes with available pods: 0
Jun 12 13:22:32.258: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 13:22:33.255: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:33.255: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:33.255: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:33.258: INFO: Number of nodes with available pods: 2
Jun 12 13:22:33.258: INFO: Node cn-hongkong.192.168.0.218 is running more than one daemon pod
Jun 12 13:22:34.255: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:34.255: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:34.255: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:34.258: INFO: Number of nodes with available pods: 3
Jun 12 13:22:34.258: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 12 13:22:34.274: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:34.274: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:34.274: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:34.276: INFO: Number of nodes with available pods: 2
Jun 12 13:22:34.276: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 13:22:35.281: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:35.281: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:35.281: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:35.284: INFO: Number of nodes with available pods: 2
Jun 12 13:22:35.284: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 13:22:36.281: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:36.281: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:36.281: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:36.284: INFO: Number of nodes with available pods: 2
Jun 12 13:22:36.284: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 13:22:37.281: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:37.281: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:37.281: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:37.284: INFO: Number of nodes with available pods: 2
Jun 12 13:22:37.284: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 13:22:38.281: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:38.281: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:38.281: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:38.284: INFO: Number of nodes with available pods: 2
Jun 12 13:22:38.284: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 13:22:39.280: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:39.281: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:39.281: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:39.283: INFO: Number of nodes with available pods: 2
Jun 12 13:22:39.283: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 13:22:40.281: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:40.281: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:40.281: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:22:40.284: INFO: Number of nodes with available pods: 3
Jun 12 13:22:40.284: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6026, will wait for the garbage collector to delete the pods
Jun 12 13:22:40.346: INFO: Deleting DaemonSet.extensions daemon-set took: 7.507799ms
Jun 12 13:22:40.746: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.197266ms
Jun 12 13:22:54.449: INFO: Number of nodes with available pods: 0
Jun 12 13:22:54.449: INFO: Number of running nodes: 0, number of available pods: 0
Jun 12 13:22:54.452: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6026/daemonsets","resourceVersion":"28458"},"items":null}

Jun 12 13:22:54.454: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6026/pods","resourceVersion":"28458"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:22:54.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6026" for this suite.
Jun 12 13:23:00.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:23:00.551: INFO: namespace daemonsets-6026 deletion completed in 6.08267899s

• [SLOW TEST:29.369 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:23:00.551: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 12 13:23:00.596: INFO: Waiting up to 5m0s for pod "downwardapi-volume-334abf10-8d15-11e9-b925-1ab852558ec8" in namespace "downward-api-3125" to be "success or failure"
Jun 12 13:23:00.598: INFO: Pod "downwardapi-volume-334abf10-8d15-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.253591ms
Jun 12 13:23:02.601: INFO: Pod "downwardapi-volume-334abf10-8d15-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005549046s
Jun 12 13:23:04.605: INFO: Pod "downwardapi-volume-334abf10-8d15-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009287763s
STEP: Saw pod success
Jun 12 13:23:04.605: INFO: Pod "downwardapi-volume-334abf10-8d15-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:23:04.608: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod downwardapi-volume-334abf10-8d15-11e9-b925-1ab852558ec8 container client-container: <nil>
STEP: delete the pod
Jun 12 13:23:04.627: INFO: Waiting for pod downwardapi-volume-334abf10-8d15-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:23:04.629: INFO: Pod downwardapi-volume-334abf10-8d15-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:23:04.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3125" for this suite.
Jun 12 13:23:10.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:23:10.717: INFO: namespace downward-api-3125 deletion completed in 6.081514504s

• [SLOW TEST:10.166 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:23:10.718: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Jun 12 13:23:10.764: INFO: Waiting up to 5m0s for pod "client-containers-395a2ded-8d15-11e9-b925-1ab852558ec8" in namespace "containers-5798" to be "success or failure"
Jun 12 13:23:10.766: INFO: Pod "client-containers-395a2ded-8d15-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.157668ms
Jun 12 13:23:12.770: INFO: Pod "client-containers-395a2ded-8d15-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005823966s
Jun 12 13:23:14.773: INFO: Pod "client-containers-395a2ded-8d15-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009276462s
STEP: Saw pod success
Jun 12 13:23:14.773: INFO: Pod "client-containers-395a2ded-8d15-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:23:14.776: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod client-containers-395a2ded-8d15-11e9-b925-1ab852558ec8 container test-container: <nil>
STEP: delete the pod
Jun 12 13:23:14.800: INFO: Waiting for pod client-containers-395a2ded-8d15-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:23:14.802: INFO: Pod client-containers-395a2ded-8d15-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:23:14.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5798" for this suite.
Jun 12 13:23:20.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:23:20.887: INFO: namespace containers-5798 deletion completed in 6.081871032s

• [SLOW TEST:10.170 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:23:20.888: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-6627
Jun 12 13:23:24.937: INFO: Started pod liveness-exec in namespace container-probe-6627
STEP: checking the pod's current state and verifying that restartCount is present
Jun 12 13:23:24.939: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:27:25.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6627" for this suite.
Jun 12 13:27:31.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:27:31.512: INFO: namespace container-probe-6627 deletion completed in 6.095669956s

• [SLOW TEST:250.624 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:27:31.512: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 13:27:31.553: INFO: (0) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 4.659962ms)
Jun 12 13:27:31.556: INFO: (1) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.943172ms)
Jun 12 13:27:31.559: INFO: (2) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.874759ms)
Jun 12 13:27:31.562: INFO: (3) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.765704ms)
Jun 12 13:27:31.566: INFO: (4) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 4.250286ms)
Jun 12 13:27:31.569: INFO: (5) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 3.103159ms)
Jun 12 13:27:31.572: INFO: (6) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 3.099423ms)
Jun 12 13:27:31.575: INFO: (7) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.81214ms)
Jun 12 13:27:31.578: INFO: (8) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.981258ms)
Jun 12 13:27:31.581: INFO: (9) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.942143ms)
Jun 12 13:27:31.584: INFO: (10) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.773139ms)
Jun 12 13:27:31.587: INFO: (11) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 3.707069ms)
Jun 12 13:27:31.590: INFO: (12) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.664019ms)
Jun 12 13:27:31.593: INFO: (13) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.689468ms)
Jun 12 13:27:31.596: INFO: (14) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.829195ms)
Jun 12 13:27:31.598: INFO: (15) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.683685ms)
Jun 12 13:27:31.601: INFO: (16) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.636869ms)
Jun 12 13:27:31.604: INFO: (17) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.857136ms)
Jun 12 13:27:31.607: INFO: (18) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.77918ms)
Jun 12 13:27:31.609: INFO: (19) /api/v1/nodes/cn-hongkong.192.168.0.217/proxy/logs/: <pre>
<a href="alicloud/">alicloud/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/... (200; 2.666231ms)
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:27:31.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3339" for this suite.
Jun 12 13:27:37.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:27:37.702: INFO: namespace proxy-3339 deletion completed in 6.08001454s

• [SLOW TEST:6.190 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:27:37.702: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 12 13:27:37.740: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d87b9a06-8d15-11e9-b925-1ab852558ec8" in namespace "projected-3393" to be "success or failure"
Jun 12 13:27:37.745: INFO: Pod "downwardapi-volume-d87b9a06-8d15-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.301208ms
Jun 12 13:27:39.748: INFO: Pod "downwardapi-volume-d87b9a06-8d15-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007782511s
Jun 12 13:27:41.752: INFO: Pod "downwardapi-volume-d87b9a06-8d15-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011554878s
STEP: Saw pod success
Jun 12 13:27:41.752: INFO: Pod "downwardapi-volume-d87b9a06-8d15-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:27:41.755: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod downwardapi-volume-d87b9a06-8d15-11e9-b925-1ab852558ec8 container client-container: <nil>
STEP: delete the pod
Jun 12 13:27:41.776: INFO: Waiting for pod downwardapi-volume-d87b9a06-8d15-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:27:41.779: INFO: Pod downwardapi-volume-d87b9a06-8d15-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:27:41.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3393" for this suite.
Jun 12 13:27:47.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:27:47.862: INFO: namespace projected-3393 deletion completed in 6.077776386s

• [SLOW TEST:10.160 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:27:47.862: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0612 13:27:53.928326      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 12 13:27:53.928: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:27:53.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9547" for this suite.
Jun 12 13:27:59.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:28:00.012: INFO: namespace gc-9547 deletion completed in 6.081346806s

• [SLOW TEST:12.150 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:28:00.013: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 12 13:28:01.063: INFO: Waiting up to 5m0s for pod "pod-e5c8450c-8d15-11e9-b925-1ab852558ec8" in namespace "emptydir-8954" to be "success or failure"
Jun 12 13:28:01.066: INFO: Pod "pod-e5c8450c-8d15-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.092713ms
Jun 12 13:28:03.069: INFO: Pod "pod-e5c8450c-8d15-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006545892s
STEP: Saw pod success
Jun 12 13:28:03.069: INFO: Pod "pod-e5c8450c-8d15-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:28:03.072: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-e5c8450c-8d15-11e9-b925-1ab852558ec8 container test-container: <nil>
STEP: delete the pod
Jun 12 13:28:03.092: INFO: Waiting for pod pod-e5c8450c-8d15-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:28:03.094: INFO: Pod pod-e5c8450c-8d15-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:28:03.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8954" for this suite.
Jun 12 13:28:09.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:28:09.193: INFO: namespace emptydir-8954 deletion completed in 6.085453165s

• [SLOW TEST:9.180 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:28:09.194: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-eb42dd8d-8d15-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume secrets
Jun 12 13:28:09.258: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eb448ba8-8d15-11e9-b925-1ab852558ec8" in namespace "projected-35" to be "success or failure"
Jun 12 13:28:09.261: INFO: Pod "pod-projected-secrets-eb448ba8-8d15-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.431556ms
Jun 12 13:28:11.265: INFO: Pod "pod-projected-secrets-eb448ba8-8d15-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007124639s
STEP: Saw pod success
Jun 12 13:28:11.265: INFO: Pod "pod-projected-secrets-eb448ba8-8d15-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:28:11.268: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-projected-secrets-eb448ba8-8d15-11e9-b925-1ab852558ec8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 12 13:28:11.288: INFO: Waiting for pod pod-projected-secrets-eb448ba8-8d15-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:28:11.290: INFO: Pod pod-projected-secrets-eb448ba8-8d15-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:28:11.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-35" for this suite.
Jun 12 13:28:17.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:28:17.395: INFO: namespace projected-35 deletion completed in 6.080561561s

• [SLOW TEST:8.201 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:28:17.395: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Jun 12 13:28:17.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 create -f - --namespace=kubectl-4399'
Jun 12 13:28:17.921: INFO: stderr: ""
Jun 12 13:28:17.921: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 12 13:28:18.924: INFO: Selector matched 1 pods for map[app:redis]
Jun 12 13:28:18.924: INFO: Found 0 / 1
Jun 12 13:28:19.925: INFO: Selector matched 1 pods for map[app:redis]
Jun 12 13:28:19.925: INFO: Found 0 / 1
Jun 12 13:28:20.924: INFO: Selector matched 1 pods for map[app:redis]
Jun 12 13:28:20.924: INFO: Found 1 / 1
Jun 12 13:28:20.924: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 12 13:28:20.927: INFO: Selector matched 1 pods for map[app:redis]
Jun 12 13:28:20.927: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 12 13:28:20.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 patch pod redis-master-hlzqv --namespace=kubectl-4399 -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 12 13:28:20.999: INFO: stderr: ""
Jun 12 13:28:20.999: INFO: stdout: "pod/redis-master-hlzqv patched\n"
STEP: checking annotations
Jun 12 13:28:21.006: INFO: Selector matched 1 pods for map[app:redis]
Jun 12 13:28:21.006: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:28:21.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4399" for this suite.
Jun 12 13:28:43.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:28:43.089: INFO: namespace kubectl-4399 deletion completed in 22.079854024s

• [SLOW TEST:25.694 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:28:43.089: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 12 13:28:43.130: INFO: Waiting up to 5m0s for pod "pod-ff74c114-8d15-11e9-b925-1ab852558ec8" in namespace "emptydir-2739" to be "success or failure"
Jun 12 13:28:43.133: INFO: Pod "pod-ff74c114-8d15-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.143366ms
Jun 12 13:28:45.136: INFO: Pod "pod-ff74c114-8d15-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005726488s
STEP: Saw pod success
Jun 12 13:28:45.136: INFO: Pod "pod-ff74c114-8d15-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:28:45.139: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-ff74c114-8d15-11e9-b925-1ab852558ec8 container test-container: <nil>
STEP: delete the pod
Jun 12 13:28:45.159: INFO: Waiting for pod pod-ff74c114-8d15-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:28:45.161: INFO: Pod pod-ff74c114-8d15-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:28:45.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2739" for this suite.
Jun 12 13:28:51.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:28:51.244: INFO: namespace emptydir-2739 deletion completed in 6.079279533s

• [SLOW TEST:8.154 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:28:51.244: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 13:29:09.294: INFO: Container started at 2019-06-12 13:28:52 +0000 UTC, pod became ready at 2019-06-12 13:29:07 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:29:09.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4995" for this suite.
Jun 12 13:29:31.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:29:31.382: INFO: namespace container-probe-4995 deletion completed in 22.084481707s

• [SLOW TEST:40.138 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:29:31.382: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 13:29:31.433: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 12 13:29:36.437: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 12 13:29:36.437: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jun 12 13:29:36.454: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-3075,SelfLink:/apis/apps/v1/namespaces/deployment-3075/deployments/test-cleanup-deployment,UID:1f3f775e-8d16-11e9-afb2-00163e04c212,ResourceVersion:30037,Generation:1,CreationTimestamp:2019-06-12 13:29:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Jun 12 13:29:36.456: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jun 12 13:29:36.456: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jun 12 13:29:36.456: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-3075,SelfLink:/apis/apps/v1/namespaces/deployment-3075/replicasets/test-cleanup-controller,UID:1c40a812-8d16-11e9-afb2-00163e04c212,ResourceVersion:30038,Generation:1,CreationTimestamp:2019-06-12 13:29:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 1f3f775e-8d16-11e9-afb2-00163e04c212 0xc0031e0937 0xc0031e0938}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 12 13:29:36.459: INFO: Pod "test-cleanup-controller-btb9f" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-btb9f,GenerateName:test-cleanup-controller-,Namespace:deployment-3075,SelfLink:/api/v1/namespaces/deployment-3075/pods/test-cleanup-controller-btb9f,UID:1c3d0fe9-8d16-11e9-80ba-00163e04d588,ResourceVersion:30031,Generation:0,CreationTimestamp:2019-06-12 13:29:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 1c40a812-8d16-11e9-afb2-00163e04c212 0xc0031e0ec7 0xc0031e0ec8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vcg56 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vcg56,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vcg56 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.218,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031e0f30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031e0f60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:29:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:29:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:29:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:29:31 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.218,PodIP:172.20.2.101,StartTime:2019-06-12 13:29:31 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-12 13:29:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://cb1e46db0c83f282e5e6b4daa7cc4d15ebeaa50cfdc8a77c15cc4b468abca25f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:29:36.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3075" for this suite.
Jun 12 13:29:42.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:29:42.556: INFO: namespace deployment-3075 deletion completed in 6.086905467s

• [SLOW TEST:11.174 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:29:42.556: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 13:29:42.588: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:29:46.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7773" for this suite.
Jun 12 13:30:36.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:30:36.915: INFO: namespace pods-7773 deletion completed in 50.082352631s

• [SLOW TEST:54.358 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:30:36.915: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-6891/secret-test-434db2ea-8d16-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume secrets
Jun 12 13:30:36.959: INFO: Waiting up to 5m0s for pod "pod-configmaps-434e8f70-8d16-11e9-b925-1ab852558ec8" in namespace "secrets-6891" to be "success or failure"
Jun 12 13:30:36.962: INFO: Pod "pod-configmaps-434e8f70-8d16-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.211922ms
Jun 12 13:30:38.965: INFO: Pod "pod-configmaps-434e8f70-8d16-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005486009s
Jun 12 13:30:40.968: INFO: Pod "pod-configmaps-434e8f70-8d16-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00857208s
STEP: Saw pod success
Jun 12 13:30:40.968: INFO: Pod "pod-configmaps-434e8f70-8d16-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:30:40.970: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-configmaps-434e8f70-8d16-11e9-b925-1ab852558ec8 container env-test: <nil>
STEP: delete the pod
Jun 12 13:30:40.996: INFO: Waiting for pod pod-configmaps-434e8f70-8d16-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:30:40.998: INFO: Pod pod-configmaps-434e8f70-8d16-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:30:40.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6891" for this suite.
Jun 12 13:30:47.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:30:47.081: INFO: namespace secrets-6891 deletion completed in 6.079097693s

• [SLOW TEST:10.166 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:30:47.081: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Jun 12 13:30:47.139: INFO: Waiting up to 5m0s for pod "var-expansion-495ce1ca-8d16-11e9-b925-1ab852558ec8" in namespace "var-expansion-2939" to be "success or failure"
Jun 12 13:30:47.142: INFO: Pod "var-expansion-495ce1ca-8d16-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.238947ms
Jun 12 13:30:49.145: INFO: Pod "var-expansion-495ce1ca-8d16-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005813259s
Jun 12 13:30:51.150: INFO: Pod "var-expansion-495ce1ca-8d16-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010272469s
STEP: Saw pod success
Jun 12 13:30:51.150: INFO: Pod "var-expansion-495ce1ca-8d16-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:30:51.152: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod var-expansion-495ce1ca-8d16-11e9-b925-1ab852558ec8 container dapi-container: <nil>
STEP: delete the pod
Jun 12 13:30:51.171: INFO: Waiting for pod var-expansion-495ce1ca-8d16-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:30:51.173: INFO: Pod var-expansion-495ce1ca-8d16-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:30:51.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2939" for this suite.
Jun 12 13:30:57.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:30:57.316: INFO: namespace var-expansion-2939 deletion completed in 6.134330774s

• [SLOW TEST:10.235 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:30:57.316: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 12 13:30:57.357: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 12 13:31:02.360: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:31:03.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4671" for this suite.
Jun 12 13:31:09.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:31:09.472: INFO: namespace replication-controller-4671 deletion completed in 6.088980309s

• [SLOW TEST:12.156 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:31:09.472: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-56b5a858-8d16-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume secrets
Jun 12 13:31:09.519: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-56b6b748-8d16-11e9-b925-1ab852558ec8" in namespace "projected-3237" to be "success or failure"
Jun 12 13:31:09.524: INFO: Pod "pod-projected-secrets-56b6b748-8d16-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.796963ms
Jun 12 13:31:11.527: INFO: Pod "pod-projected-secrets-56b6b748-8d16-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008082502s
Jun 12 13:31:13.531: INFO: Pod "pod-projected-secrets-56b6b748-8d16-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012267333s
STEP: Saw pod success
Jun 12 13:31:13.531: INFO: Pod "pod-projected-secrets-56b6b748-8d16-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:31:13.534: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-projected-secrets-56b6b748-8d16-11e9-b925-1ab852558ec8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 12 13:31:13.551: INFO: Waiting for pod pod-projected-secrets-56b6b748-8d16-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:31:13.554: INFO: Pod pod-projected-secrets-56b6b748-8d16-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:31:13.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3237" for this suite.
Jun 12 13:31:19.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:31:19.639: INFO: namespace projected-3237 deletion completed in 6.082245634s

• [SLOW TEST:10.167 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:31:19.640: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jun 12 13:31:24.212: INFO: Successfully updated pod "annotationupdate5cc44d25-8d16-11e9-b925-1ab852558ec8"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:31:26.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6665" for this suite.
Jun 12 13:31:48.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:31:48.336: INFO: namespace projected-6665 deletion completed in 22.096689244s

• [SLOW TEST:28.696 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:31:48.336: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 12 13:31:48.376: INFO: Waiting up to 5m0s for pod "pod-6ddfaedb-8d16-11e9-b925-1ab852558ec8" in namespace "emptydir-9287" to be "success or failure"
Jun 12 13:31:48.378: INFO: Pod "pod-6ddfaedb-8d16-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091351ms
Jun 12 13:31:50.383: INFO: Pod "pod-6ddfaedb-8d16-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007669141s
STEP: Saw pod success
Jun 12 13:31:50.383: INFO: Pod "pod-6ddfaedb-8d16-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:31:50.386: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-6ddfaedb-8d16-11e9-b925-1ab852558ec8 container test-container: <nil>
STEP: delete the pod
Jun 12 13:31:50.403: INFO: Waiting for pod pod-6ddfaedb-8d16-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:31:50.405: INFO: Pod pod-6ddfaedb-8d16-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:31:50.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9287" for this suite.
Jun 12 13:31:56.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:31:56.503: INFO: namespace emptydir-9287 deletion completed in 6.094934322s

• [SLOW TEST:8.167 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:31:56.503: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Jun 12 13:31:56.546: INFO: Waiting up to 5m0s for pod "client-containers-72bd99c4-8d16-11e9-b925-1ab852558ec8" in namespace "containers-2893" to be "success or failure"
Jun 12 13:31:56.549: INFO: Pod "client-containers-72bd99c4-8d16-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.088974ms
Jun 12 13:31:58.552: INFO: Pod "client-containers-72bd99c4-8d16-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006535682s
STEP: Saw pod success
Jun 12 13:31:58.552: INFO: Pod "client-containers-72bd99c4-8d16-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:31:58.555: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod client-containers-72bd99c4-8d16-11e9-b925-1ab852558ec8 container test-container: <nil>
STEP: delete the pod
Jun 12 13:31:58.572: INFO: Waiting for pod client-containers-72bd99c4-8d16-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:31:58.574: INFO: Pod client-containers-72bd99c4-8d16-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:31:58.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2893" for this suite.
Jun 12 13:32:04.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:32:04.661: INFO: namespace containers-2893 deletion completed in 6.083180134s

• [SLOW TEST:8.157 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:32:04.661: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-779a0606-8d16-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume secrets
Jun 12 13:32:04.711: INFO: Waiting up to 5m0s for pod "pod-secrets-779c9499-8d16-11e9-b925-1ab852558ec8" in namespace "secrets-7876" to be "success or failure"
Jun 12 13:32:04.713: INFO: Pod "pod-secrets-779c9499-8d16-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075948ms
Jun 12 13:32:06.717: INFO: Pod "pod-secrets-779c9499-8d16-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005546781s
STEP: Saw pod success
Jun 12 13:32:06.717: INFO: Pod "pod-secrets-779c9499-8d16-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:32:06.719: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-secrets-779c9499-8d16-11e9-b925-1ab852558ec8 container secret-volume-test: <nil>
STEP: delete the pod
Jun 12 13:32:06.740: INFO: Waiting for pod pod-secrets-779c9499-8d16-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:32:06.742: INFO: Pod pod-secrets-779c9499-8d16-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:32:06.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7876" for this suite.
Jun 12 13:32:12.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:32:12.825: INFO: namespace secrets-7876 deletion completed in 6.079407953s

• [SLOW TEST:8.164 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:32:12.825: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0612 13:32:22.882767      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 12 13:32:22.882: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:32:22.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6533" for this suite.
Jun 12 13:32:28.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:32:28.967: INFO: namespace gc-6533 deletion completed in 6.081494729s

• [SLOW TEST:16.143 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:32:28.968: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5975
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 12 13:32:29.018: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 12 13:32:53.090: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.137:8080/dial?request=hostName&protocol=http&host=172.20.2.108&port=8080&tries=1'] Namespace:pod-network-test-5975 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:32:53.090: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:32:53.318: INFO: Waiting for endpoints: map[]
Jun 12 13:32:53.322: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.137:8080/dial?request=hostName&protocol=http&host=172.20.1.157&port=8080&tries=1'] Namespace:pod-network-test-5975 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:32:53.322: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:32:53.592: INFO: Waiting for endpoints: map[]
Jun 12 13:32:53.595: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.137:8080/dial?request=hostName&protocol=http&host=172.20.2.136&port=8080&tries=1'] Namespace:pod-network-test-5975 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:32:53.595: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:32:53.879: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:32:53.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5975" for this suite.
Jun 12 13:33:15.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:33:15.974: INFO: namespace pod-network-test-5975 deletion completed in 22.091351429s

• [SLOW TEST:47.007 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:33:15.975: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-a21c256e-8d16-11e9-b925-1ab852558ec8
STEP: Creating secret with name s-test-opt-upd-a21c25a2-8d16-11e9-b925-1ab852558ec8
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a21c256e-8d16-11e9-b925-1ab852558ec8
STEP: Updating secret s-test-opt-upd-a21c25a2-8d16-11e9-b925-1ab852558ec8
STEP: Creating secret with name s-test-opt-create-a21c25b6-8d16-11e9-b925-1ab852558ec8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:33:22.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3701" for this suite.
Jun 12 13:33:44.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:33:44.203: INFO: namespace secrets-3701 deletion completed in 22.10439071s

• [SLOW TEST:28.228 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:33:44.203: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-8380/configmap-test-b2ef964e-8d16-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume configMaps
Jun 12 13:33:44.256: INFO: Waiting up to 5m0s for pod "pod-configmaps-b2f069cd-8d16-11e9-b925-1ab852558ec8" in namespace "configmap-8380" to be "success or failure"
Jun 12 13:33:44.260: INFO: Pod "pod-configmaps-b2f069cd-8d16-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.247804ms
Jun 12 13:33:46.264: INFO: Pod "pod-configmaps-b2f069cd-8d16-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007568466s
Jun 12 13:33:48.267: INFO: Pod "pod-configmaps-b2f069cd-8d16-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011034129s
STEP: Saw pod success
Jun 12 13:33:48.267: INFO: Pod "pod-configmaps-b2f069cd-8d16-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:33:48.270: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-configmaps-b2f069cd-8d16-11e9-b925-1ab852558ec8 container env-test: <nil>
STEP: delete the pod
Jun 12 13:33:48.301: INFO: Waiting for pod pod-configmaps-b2f069cd-8d16-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:33:48.303: INFO: Pod pod-configmaps-b2f069cd-8d16-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:33:48.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8380" for this suite.
Jun 12 13:33:54.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:33:54.385: INFO: namespace configmap-8380 deletion completed in 6.078717067s

• [SLOW TEST:10.182 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:33:54.385: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-7235
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 12 13:33:54.413: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 12 13:34:14.489: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.140:8080/dial?request=hostName&protocol=udp&host=172.20.1.159&port=8081&tries=1'] Namespace:pod-network-test-7235 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:34:14.489: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:34:14.707: INFO: Waiting for endpoints: map[]
Jun 12 13:34:14.711: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.140:8080/dial?request=hostName&protocol=udp&host=172.20.2.139&port=8081&tries=1'] Namespace:pod-network-test-7235 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:34:14.711: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:34:14.967: INFO: Waiting for endpoints: map[]
Jun 12 13:34:14.970: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.2.140:8080/dial?request=hostName&protocol=udp&host=172.20.2.109&port=8081&tries=1'] Namespace:pod-network-test-7235 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:34:14.970: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:34:15.233: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:34:15.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7235" for this suite.
Jun 12 13:34:37.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:34:37.323: INFO: namespace pod-network-test-7235 deletion completed in 22.085658533s

• [SLOW TEST:42.938 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:34:37.324: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-xg8w
STEP: Creating a pod to test atomic-volume-subpath
Jun 12 13:34:37.369: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xg8w" in namespace "subpath-6800" to be "success or failure"
Jun 12 13:34:37.374: INFO: Pod "pod-subpath-test-configmap-xg8w": Phase="Pending", Reason="", readiness=false. Elapsed: 5.644616ms
Jun 12 13:34:39.379: INFO: Pod "pod-subpath-test-configmap-xg8w": Phase="Running", Reason="", readiness=true. Elapsed: 2.009845749s
Jun 12 13:34:41.382: INFO: Pod "pod-subpath-test-configmap-xg8w": Phase="Running", Reason="", readiness=true. Elapsed: 4.013398682s
Jun 12 13:34:43.386: INFO: Pod "pod-subpath-test-configmap-xg8w": Phase="Running", Reason="", readiness=true. Elapsed: 6.016949788s
Jun 12 13:34:45.389: INFO: Pod "pod-subpath-test-configmap-xg8w": Phase="Running", Reason="", readiness=true. Elapsed: 8.020720006s
Jun 12 13:34:47.393: INFO: Pod "pod-subpath-test-configmap-xg8w": Phase="Running", Reason="", readiness=true. Elapsed: 10.024249234s
Jun 12 13:34:49.396: INFO: Pod "pod-subpath-test-configmap-xg8w": Phase="Running", Reason="", readiness=true. Elapsed: 12.027674115s
Jun 12 13:34:51.400: INFO: Pod "pod-subpath-test-configmap-xg8w": Phase="Running", Reason="", readiness=true. Elapsed: 14.031313502s
Jun 12 13:34:53.404: INFO: Pod "pod-subpath-test-configmap-xg8w": Phase="Running", Reason="", readiness=true. Elapsed: 16.035063159s
Jun 12 13:34:55.408: INFO: Pod "pod-subpath-test-configmap-xg8w": Phase="Running", Reason="", readiness=true. Elapsed: 18.038768505s
Jun 12 13:34:57.411: INFO: Pod "pod-subpath-test-configmap-xg8w": Phase="Running", Reason="", readiness=true. Elapsed: 20.041958506s
Jun 12 13:34:59.414: INFO: Pod "pod-subpath-test-configmap-xg8w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.045201241s
STEP: Saw pod success
Jun 12 13:34:59.414: INFO: Pod "pod-subpath-test-configmap-xg8w" satisfied condition "success or failure"
Jun 12 13:34:59.417: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-subpath-test-configmap-xg8w container test-container-subpath-configmap-xg8w: <nil>
STEP: delete the pod
Jun 12 13:34:59.435: INFO: Waiting for pod pod-subpath-test-configmap-xg8w to disappear
Jun 12 13:34:59.442: INFO: Pod pod-subpath-test-configmap-xg8w no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xg8w
Jun 12 13:34:59.442: INFO: Deleting pod "pod-subpath-test-configmap-xg8w" in namespace "subpath-6800"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:34:59.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6800" for this suite.
Jun 12 13:35:05.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:35:05.526: INFO: namespace subpath-6800 deletion completed in 6.078187702s

• [SLOW TEST:28.202 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:35:05.526: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:35:07.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6852" for this suite.
Jun 12 13:35:13.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:35:13.681: INFO: namespace emptydir-wrapper-6852 deletion completed in 6.078138416s

• [SLOW TEST:8.154 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:35:13.681: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Jun 12 13:35:13.716: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-3384" to be "success or failure"
Jun 12 13:35:13.718: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.229799ms
Jun 12 13:35:15.722: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005527885s
STEP: Saw pod success
Jun 12 13:35:15.722: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jun 12 13:35:15.724: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jun 12 13:35:15.743: INFO: Waiting for pod pod-host-path-test to disappear
Jun 12 13:35:15.745: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:35:15.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-3384" for this suite.
Jun 12 13:35:21.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:35:21.831: INFO: namespace hostpath-3384 deletion completed in 6.082449193s

• [SLOW TEST:8.150 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:35:21.831: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 12 13:35:21.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-894'
Jun 12 13:35:21.940: INFO: stderr: ""
Jun 12 13:35:21.940: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Jun 12 13:35:21.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 delete pods e2e-test-nginx-pod --namespace=kubectl-894'
Jun 12 13:35:34.354: INFO: stderr: ""
Jun 12 13:35:34.354: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:35:34.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-894" for this suite.
Jun 12 13:35:40.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:35:40.447: INFO: namespace kubectl-894 deletion completed in 6.089158561s

• [SLOW TEST:18.616 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:35:40.448: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-f838ffbc-8d16-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume configMaps
Jun 12 13:35:40.491: INFO: Waiting up to 5m0s for pod "pod-configmaps-f839f6cc-8d16-11e9-b925-1ab852558ec8" in namespace "configmap-1945" to be "success or failure"
Jun 12 13:35:40.494: INFO: Pod "pod-configmaps-f839f6cc-8d16-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.439853ms
Jun 12 13:35:42.498: INFO: Pod "pod-configmaps-f839f6cc-8d16-11e9-b925-1ab852558ec8": Phase="Running", Reason="", readiness=true. Elapsed: 2.006276102s
Jun 12 13:35:44.501: INFO: Pod "pod-configmaps-f839f6cc-8d16-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009600035s
STEP: Saw pod success
Jun 12 13:35:44.501: INFO: Pod "pod-configmaps-f839f6cc-8d16-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:35:44.504: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-configmaps-f839f6cc-8d16-11e9-b925-1ab852558ec8 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 12 13:35:44.522: INFO: Waiting for pod pod-configmaps-f839f6cc-8d16-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:35:44.524: INFO: Pod pod-configmaps-f839f6cc-8d16-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:35:44.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1945" for this suite.
Jun 12 13:35:50.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:35:50.612: INFO: namespace configmap-1945 deletion completed in 6.084549834s

• [SLOW TEST:10.164 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:35:50.612: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 12 13:35:50.652: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fe47e0bf-8d16-11e9-b925-1ab852558ec8" in namespace "projected-3204" to be "success or failure"
Jun 12 13:35:50.654: INFO: Pod "downwardapi-volume-fe47e0bf-8d16-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.204291ms
Jun 12 13:35:52.657: INFO: Pod "downwardapi-volume-fe47e0bf-8d16-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00516924s
STEP: Saw pod success
Jun 12 13:35:52.657: INFO: Pod "downwardapi-volume-fe47e0bf-8d16-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:35:52.659: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod downwardapi-volume-fe47e0bf-8d16-11e9-b925-1ab852558ec8 container client-container: <nil>
STEP: delete the pod
Jun 12 13:35:52.677: INFO: Waiting for pod downwardapi-volume-fe47e0bf-8d16-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:35:52.679: INFO: Pod downwardapi-volume-fe47e0bf-8d16-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:35:52.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3204" for this suite.
Jun 12 13:35:58.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:35:58.757: INFO: namespace projected-3204 deletion completed in 6.075599271s

• [SLOW TEST:8.145 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:35:58.758: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Jun 12 13:35:58.798: INFO: Waiting up to 5m0s for pod "client-containers-0322fa3a-8d17-11e9-b925-1ab852558ec8" in namespace "containers-540" to be "success or failure"
Jun 12 13:35:58.800: INFO: Pod "client-containers-0322fa3a-8d17-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.158262ms
Jun 12 13:36:00.804: INFO: Pod "client-containers-0322fa3a-8d17-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005657958s
Jun 12 13:36:02.808: INFO: Pod "client-containers-0322fa3a-8d17-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009378914s
STEP: Saw pod success
Jun 12 13:36:02.808: INFO: Pod "client-containers-0322fa3a-8d17-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:36:02.810: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod client-containers-0322fa3a-8d17-11e9-b925-1ab852558ec8 container test-container: <nil>
STEP: delete the pod
Jun 12 13:36:02.833: INFO: Waiting for pod client-containers-0322fa3a-8d17-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:36:02.835: INFO: Pod client-containers-0322fa3a-8d17-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:36:02.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-540" for this suite.
Jun 12 13:36:08.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:36:08.926: INFO: namespace containers-540 deletion completed in 6.087103128s

• [SLOW TEST:10.168 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:36:08.926: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jun 12 13:36:11.514: INFO: Successfully updated pod "annotationupdate0934fbac-8d17-11e9-b925-1ab852558ec8"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:36:13.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6229" for this suite.
Jun 12 13:36:35.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:36:35.623: INFO: namespace downward-api-6229 deletion completed in 22.083482337s

• [SLOW TEST:26.697 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:36:35.623: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0612 13:37:15.686136      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 12 13:37:15.686: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:37:15.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8533" for this suite.
Jun 12 13:37:21.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:37:21.774: INFO: namespace gc-8533 deletion completed in 6.084764186s

• [SLOW TEST:46.151 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:37:21.774: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-349df1db-8d17-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume configMaps
Jun 12 13:37:21.823: INFO: Waiting up to 5m0s for pod "pod-configmaps-349fd785-8d17-11e9-b925-1ab852558ec8" in namespace "configmap-186" to be "success or failure"
Jun 12 13:37:21.829: INFO: Pod "pod-configmaps-349fd785-8d17-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.948615ms
Jun 12 13:37:23.833: INFO: Pod "pod-configmaps-349fd785-8d17-11e9-b925-1ab852558ec8": Phase="Running", Reason="", readiness=true. Elapsed: 2.009356778s
Jun 12 13:37:25.836: INFO: Pod "pod-configmaps-349fd785-8d17-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012943187s
STEP: Saw pod success
Jun 12 13:37:25.836: INFO: Pod "pod-configmaps-349fd785-8d17-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:37:25.839: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-configmaps-349fd785-8d17-11e9-b925-1ab852558ec8 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 12 13:37:25.860: INFO: Waiting for pod pod-configmaps-349fd785-8d17-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:37:25.863: INFO: Pod pod-configmaps-349fd785-8d17-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:37:25.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-186" for this suite.
Jun 12 13:37:31.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:37:31.947: INFO: namespace configmap-186 deletion completed in 6.081680604s

• [SLOW TEST:10.174 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:37:31.948: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-3938
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 12 13:37:31.978: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 12 13:37:52.067: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.2.117 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3938 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:37:52.067: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:37:53.287: INFO: Found all expected endpoints: [netserver-0]
Jun 12 13:37:53.290: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.2.150 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3938 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:37:53.290: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:37:54.555: INFO: Found all expected endpoints: [netserver-1]
Jun 12 13:37:54.558: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.1.163 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3938 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 12 13:37:54.558: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
Jun 12 13:37:55.853: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:37:55.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3938" for this suite.
Jun 12 13:38:17.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:38:17.937: INFO: namespace pod-network-test-3938 deletion completed in 22.080105861s

• [SLOW TEST:45.989 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:38:17.938: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-3749
Jun 12 13:38:19.987: INFO: Started pod liveness-http in namespace container-probe-3749
STEP: checking the pod's current state and verifying that restartCount is present
Jun 12 13:38:19.989: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:42:20.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3749" for this suite.
Jun 12 13:42:26.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:42:26.515: INFO: namespace container-probe-3749 deletion completed in 6.083915934s

• [SLOW TEST:248.577 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:42:26.515: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 12 13:42:26.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6947'
Jun 12 13:42:26.908: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 12 13:42:26.908: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jun 12 13:42:26.913: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Jun 12 13:42:26.914: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jun 12 13:42:26.919: INFO: scanned /root for discovery docs: <nil>
Jun 12 13:42:26.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-6947'
Jun 12 13:42:42.668: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 12 13:42:42.668: INFO: stdout: "Created e2e-test-nginx-rc-d64b42a65280ec563166a36ee63a147a\nScaling up e2e-test-nginx-rc-d64b42a65280ec563166a36ee63a147a from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-d64b42a65280ec563166a36ee63a147a up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-d64b42a65280ec563166a36ee63a147a to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jun 12 13:42:42.668: INFO: stdout: "Created e2e-test-nginx-rc-d64b42a65280ec563166a36ee63a147a\nScaling up e2e-test-nginx-rc-d64b42a65280ec563166a36ee63a147a from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-d64b42a65280ec563166a36ee63a147a up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-d64b42a65280ec563166a36ee63a147a to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jun 12 13:42:42.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-6947'
Jun 12 13:42:42.736: INFO: stderr: ""
Jun 12 13:42:42.736: INFO: stdout: "e2e-test-nginx-rc-72dff e2e-test-nginx-rc-d64b42a65280ec563166a36ee63a147a-xm9hr "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Jun 12 13:42:47.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-6947'
Jun 12 13:42:47.802: INFO: stderr: ""
Jun 12 13:42:47.803: INFO: stdout: "e2e-test-nginx-rc-d64b42a65280ec563166a36ee63a147a-xm9hr "
Jun 12 13:42:47.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods e2e-test-nginx-rc-d64b42a65280ec563166a36ee63a147a-xm9hr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6947'
Jun 12 13:42:47.863: INFO: stderr: ""
Jun 12 13:42:47.863: INFO: stdout: "true"
Jun 12 13:42:47.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 get pods e2e-test-nginx-rc-d64b42a65280ec563166a36ee63a147a-xm9hr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6947'
Jun 12 13:42:47.924: INFO: stderr: ""
Jun 12 13:42:47.924: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jun 12 13:42:47.924: INFO: e2e-test-nginx-rc-d64b42a65280ec563166a36ee63a147a-xm9hr is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Jun 12 13:42:47.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 delete rc e2e-test-nginx-rc --namespace=kubectl-6947'
Jun 12 13:42:47.988: INFO: stderr: ""
Jun 12 13:42:47.988: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:42:47.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6947" for this suite.
Jun 12 13:43:10.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:43:10.099: INFO: namespace kubectl-6947 deletion completed in 22.079894995s

• [SLOW TEST:43.584 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:43:10.099: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-9958
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Jun 12 13:43:10.144: INFO: Found 0 stateful pods, waiting for 3
Jun 12 13:43:20.148: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 12 13:43:20.148: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 12 13:43:20.148: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jun 12 13:43:20.176: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 12 13:43:30.206: INFO: Updating stateful set ss2
Jun 12 13:43:30.210: INFO: Waiting for Pod statefulset-9958/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Jun 12 13:43:40.253: INFO: Found 2 stateful pods, waiting for 3
Jun 12 13:43:50.257: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 12 13:43:50.257: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 12 13:43:50.257: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 12 13:43:50.283: INFO: Updating stateful set ss2
Jun 12 13:43:50.287: INFO: Waiting for Pod statefulset-9958/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 12 13:44:00.311: INFO: Updating stateful set ss2
Jun 12 13:44:00.316: INFO: Waiting for StatefulSet statefulset-9958/ss2 to complete update
Jun 12 13:44:00.316: INFO: Waiting for Pod statefulset-9958/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 12 13:44:10.322: INFO: Waiting for StatefulSet statefulset-9958/ss2 to complete update
Jun 12 13:44:10.322: INFO: Waiting for Pod statefulset-9958/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 12 13:44:20.322: INFO: Waiting for StatefulSet statefulset-9958/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jun 12 13:44:30.323: INFO: Deleting all statefulset in ns statefulset-9958
Jun 12 13:44:30.325: INFO: Scaling statefulset ss2 to 0
Jun 12 13:45:00.337: INFO: Waiting for statefulset status.replicas updated to 0
Jun 12 13:45:00.339: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:45:00.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9958" for this suite.
Jun 12 13:45:06.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:45:06.445: INFO: namespace statefulset-9958 deletion completed in 6.080522415s

• [SLOW TEST:116.345 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:45:06.445: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-49950fc0-8d18-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume secrets
Jun 12 13:45:06.490: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-499661f4-8d18-11e9-b925-1ab852558ec8" in namespace "projected-4331" to be "success or failure"
Jun 12 13:45:06.496: INFO: Pod "pod-projected-secrets-499661f4-8d18-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.741234ms
Jun 12 13:45:08.500: INFO: Pod "pod-projected-secrets-499661f4-8d18-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009455448s
STEP: Saw pod success
Jun 12 13:45:08.500: INFO: Pod "pod-projected-secrets-499661f4-8d18-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:45:08.502: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-projected-secrets-499661f4-8d18-11e9-b925-1ab852558ec8 container secret-volume-test: <nil>
STEP: delete the pod
Jun 12 13:45:08.521: INFO: Waiting for pod pod-projected-secrets-499661f4-8d18-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:45:08.523: INFO: Pod pod-projected-secrets-499661f4-8d18-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:45:08.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4331" for this suite.
Jun 12 13:45:14.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:45:14.606: INFO: namespace projected-4331 deletion completed in 6.078817998s

• [SLOW TEST:8.162 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:45:14.607: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 12 13:45:14.635: INFO: Creating deployment "nginx-deployment"
Jun 12 13:45:14.653: INFO: Waiting for observed generation 1
Jun 12 13:45:16.658: INFO: Waiting for all required pods to come up
Jun 12 13:45:16.661: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 12 13:45:18.672: INFO: Waiting for deployment "nginx-deployment" to complete
Jun 12 13:45:18.677: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jun 12 13:45:18.685: INFO: Updating deployment nginx-deployment
Jun 12 13:45:18.685: INFO: Waiting for observed generation 2
Jun 12 13:45:20.690: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 12 13:45:20.692: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 12 13:45:20.694: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jun 12 13:45:20.700: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 12 13:45:20.700: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 12 13:45:20.702: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jun 12 13:45:20.705: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jun 12 13:45:20.705: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jun 12 13:45:20.711: INFO: Updating deployment nginx-deployment
Jun 12 13:45:20.712: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jun 12 13:45:20.716: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 12 13:45:20.719: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jun 12 13:45:22.728: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-3617,SelfLink:/apis/apps/v1/namespaces/deployment-3617/deployments/nginx-deployment,UID:4e74a117-8d18-11e9-afb2-00163e04c212,ResourceVersion:34306,Generation:3,CreationTimestamp:2019-06-12 13:45:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-06-12 13:45:20 +0000 UTC 2019-06-12 13:45:20 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-06-12 13:45:20 +0000 UTC 2019-06-12 13:45:14 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Jun 12 13:45:22.730: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-3617,SelfLink:/apis/apps/v1/namespaces/deployment-3617/replicasets/nginx-deployment-5f9595f595,UID:50dab78a-8d18-11e9-80ba-00163e04d588,ResourceVersion:34300,Generation:3,CreationTimestamp:2019-06-12 13:45:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 4e74a117-8d18-11e9-afb2-00163e04c212 0xc0028ff847 0xc0028ff848}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 12 13:45:22.730: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jun 12 13:45:22.730: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-3617,SelfLink:/apis/apps/v1/namespaces/deployment-3617/replicasets/nginx-deployment-6f478d8d8,UID:4e7154ad-8d18-11e9-80ba-00163e04d588,ResourceVersion:34287,Generation:3,CreationTimestamp:2019-06-12 13:45:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 4e74a117-8d18-11e9-afb2-00163e04c212 0xc0028ff917 0xc0028ff918}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jun 12 13:45:22.738: INFO: Pod "nginx-deployment-5f9595f595-4vsr5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-4vsr5,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-5f9595f595-4vsr5,UID:5212b56d-8d18-11e9-80ba-00163e04d588,ResourceVersion:34318,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 50dab78a-8d18-11e9-80ba-00163e04d588 0xc001b79ad7 0xc001b79ad8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.218,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b79b50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b79b80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.218,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.738: INFO: Pod "nginx-deployment-5f9595f595-69qgt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-69qgt,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-5f9595f595-69qgt,UID:52107b57-8d18-11e9-80ba-00163e04d588,ResourceVersion:34304,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 50dab78a-8d18-11e9-80ba-00163e04d588 0xc001b79c90 0xc001b79c91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.218,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b79d30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b79d60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.218,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.738: INFO: Pod "nginx-deployment-5f9595f595-9ll9z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-9ll9z,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-5f9595f595-9ll9z,UID:50dbca85-8d18-11e9-80ba-00163e04d588,ResourceVersion:34190,Generation:0,CreationTimestamp:2019-06-12 13:45:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 50dab78a-8d18-11e9-80ba-00163e04d588 0xc001b79e30 0xc001b79e31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b79ea0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b79ec0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.217,PodIP:,StartTime:2019-06-12 13:45:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.738: INFO: Pod "nginx-deployment-5f9595f595-9rlwt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-9rlwt,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-5f9595f595-9rlwt,UID:521296f7-8d18-11e9-80ba-00163e04d588,ResourceVersion:34344,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 50dab78a-8d18-11e9-80ba-00163e04d588 0xc001b79f90 0xc001b79f91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.219,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018c0060} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018c0080}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.219,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.738: INFO: Pod "nginx-deployment-5f9595f595-blqmx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-blqmx,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-5f9595f595-blqmx,UID:5212c0a4-8d18-11e9-80ba-00163e04d588,ResourceVersion:34336,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 50dab78a-8d18-11e9-80ba-00163e04d588 0xc0018c01c0 0xc0018c01c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018c0280} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018c02a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.217,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.738: INFO: Pod "nginx-deployment-5f9595f595-cs56p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-cs56p,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-5f9595f595-cs56p,UID:50e535bb-8d18-11e9-80ba-00163e04d588,ResourceVersion:34219,Generation:0,CreationTimestamp:2019-06-12 13:45:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 50dab78a-8d18-11e9-80ba-00163e04d588 0xc0018c03e0 0xc0018c03e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.218,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018c0510} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018c0530}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.218,PodIP:,StartTime:2019-06-12 13:45:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.738: INFO: Pod "nginx-deployment-5f9595f595-h7bht" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-h7bht,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-5f9595f595-h7bht,UID:520f1de8-8d18-11e9-80ba-00163e04d588,ResourceVersion:34308,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 50dab78a-8d18-11e9-80ba-00163e04d588 0xc0018c06b0 0xc0018c06b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.219,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018c0720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018c0740}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.219,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.739: INFO: Pod "nginx-deployment-5f9595f595-kwhbv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-kwhbv,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-5f9595f595-kwhbv,UID:5212c0e8-8d18-11e9-80ba-00163e04d588,ResourceVersion:34337,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 50dab78a-8d18-11e9-80ba-00163e04d588 0xc0018c0810 0xc0018c0811}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.218,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018c0890} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018c08c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.218,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.739: INFO: Pod "nginx-deployment-5f9595f595-mltpc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-mltpc,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-5f9595f595-mltpc,UID:50e6cae0-8d18-11e9-80ba-00163e04d588,ResourceVersion:34220,Generation:0,CreationTimestamp:2019-06-12 13:45:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 50dab78a-8d18-11e9-80ba-00163e04d588 0xc0018c09b0 0xc0018c09b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018c0a20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018c0a40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.217,PodIP:,StartTime:2019-06-12 13:45:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.739: INFO: Pod "nginx-deployment-5f9595f595-s4mz5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-s4mz5,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-5f9595f595-s4mz5,UID:50dca6ce-8d18-11e9-80ba-00163e04d588,ResourceVersion:34193,Generation:0,CreationTimestamp:2019-06-12 13:45:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 50dab78a-8d18-11e9-80ba-00163e04d588 0xc0018c0b10 0xc0018c0b11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.219,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018c0b80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018c0ba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.219,PodIP:,StartTime:2019-06-12 13:45:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.739: INFO: Pod "nginx-deployment-5f9595f595-t2g8c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-t2g8c,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-5f9595f595-t2g8c,UID:50dca63a-8d18-11e9-80ba-00163e04d588,ResourceVersion:34194,Generation:0,CreationTimestamp:2019-06-12 13:45:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 50dab78a-8d18-11e9-80ba-00163e04d588 0xc0018c0c70 0xc0018c0c71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.218,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018c0ce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018c0d00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:18 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.218,PodIP:,StartTime:2019-06-12 13:45:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.739: INFO: Pod "nginx-deployment-5f9595f595-trj9w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-trj9w,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-5f9595f595-trj9w,UID:5215b107-8d18-11e9-80ba-00163e04d588,ResourceVersion:34345,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 50dab78a-8d18-11e9-80ba-00163e04d588 0xc0018c0dd0 0xc0018c0dd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018c0ef0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018c0f60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.217,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.739: INFO: Pod "nginx-deployment-5f9595f595-vvtwp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-vvtwp,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-5f9595f595-vvtwp,UID:5210900c-8d18-11e9-80ba-00163e04d588,ResourceVersion:34315,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 50dab78a-8d18-11e9-80ba-00163e04d588 0xc0018c1060 0xc0018c1061}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018c1160} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018c1180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.217,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.739: INFO: Pod "nginx-deployment-6f478d8d8-4dntb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-4dntb,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-4dntb,UID:4e752c22-8d18-11e9-80ba-00163e04d588,ResourceVersion:34145,Generation:0,CreationTimestamp:2019-06-12 13:45:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc0018c1290 0xc0018c1291}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018c1320} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018c13b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:14 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.217,PodIP:172.20.2.156,StartTime:2019-06-12 13:45:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-12 13:45:16 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e1d274e0fb192b3750b9d80f51a12417f50de1cfec7fcde352286813707d0d3c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.739: INFO: Pod "nginx-deployment-6f478d8d8-5f75n" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-5f75n,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-5f75n,UID:4e731803-8d18-11e9-80ba-00163e04d588,ResourceVersion:34156,Generation:0,CreationTimestamp:2019-06-12 13:45:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc0018c1557 0xc0018c1558}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.218,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018c15f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018c1610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:14 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.218,PodIP:172.20.2.124,StartTime:2019-06-12 13:45:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-12 13:45:16 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://0414bde7fb1f4678051035cb4bbe6d32047f3b206377951e7f72eefa35ba27dd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.739: INFO: Pod "nginx-deployment-6f478d8d8-6st7s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-6st7s,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-6st7s,UID:520ea79e-8d18-11e9-80ba-00163e04d588,ResourceVersion:34290,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc0018c1737 0xc0018c1738}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.219,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018c17f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018c1810}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.219,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.740: INFO: Pod "nginx-deployment-6f478d8d8-7d7vj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7d7vj,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-7d7vj,UID:520fdbd6-8d18-11e9-80ba-00163e04d588,ResourceVersion:34298,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc0018c19f7 0xc0018c19f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.218,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018c1a60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018c1ab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.218,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.740: INFO: Pod "nginx-deployment-6f478d8d8-928px" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-928px,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-928px,UID:520d871a-8d18-11e9-80ba-00163e04d588,ResourceVersion:34268,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc0018c1b77 0xc0018c1b78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.219,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018c1be0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018c1c00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.219,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.740: INFO: Pod "nginx-deployment-6f478d8d8-9gxvn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-9gxvn,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-9gxvn,UID:4e753bdc-8d18-11e9-80ba-00163e04d588,ResourceVersion:34154,Generation:0,CreationTimestamp:2019-06-12 13:45:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc0018c1e37 0xc0018c1e38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.218,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018c1f20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018c1f50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:14 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.218,PodIP:172.20.2.125,StartTime:2019-06-12 13:45:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-12 13:45:16 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://91a678b9345befb23c48dea3ec5c0af8cf36631f449f5ff05a95880bcd910f4a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.740: INFO: Pod "nginx-deployment-6f478d8d8-bbb6p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-bbb6p,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-bbb6p,UID:5211cf6b-8d18-11e9-80ba-00163e04d588,ResourceVersion:34320,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc002c9e047 0xc002c9e048}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.219,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c9e0b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c9e0d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.219,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.740: INFO: Pod "nginx-deployment-6f478d8d8-bh8j9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-bh8j9,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-bh8j9,UID:4e76df69-8d18-11e9-80ba-00163e04d588,ResourceVersion:34142,Generation:0,CreationTimestamp:2019-06-12 13:45:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc002c9e197 0xc002c9e198}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c9e200} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c9e220}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:14 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.217,PodIP:172.20.2.157,StartTime:2019-06-12 13:45:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-12 13:45:16 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://06d6110c57fc663fdf8601f2c8fbd33cca2fd703a575dfc51f56a80e92865880}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.740: INFO: Pod "nginx-deployment-6f478d8d8-c5v92" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-c5v92,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-c5v92,UID:5211e10b-8d18-11e9-80ba-00163e04d588,ResourceVersion:34335,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc002c9e2f7 0xc002c9e2f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.219,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c9e360} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c9e380}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.219,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.740: INFO: Pod "nginx-deployment-6f478d8d8-d86p8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-d86p8,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-d86p8,UID:5211dcaa-8d18-11e9-80ba-00163e04d588,ResourceVersion:34321,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc002c9e447 0xc002c9e448}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c9e4b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c9e4d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.217,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.741: INFO: Pod "nginx-deployment-6f478d8d8-g67f2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-g67f2,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-g67f2,UID:520fd707-8d18-11e9-80ba-00163e04d588,ResourceVersion:34292,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc002c9e597 0xc002c9e598}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c9e600} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c9e620}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.217,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.741: INFO: Pod "nginx-deployment-6f478d8d8-jgkph" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-jgkph,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-jgkph,UID:520fd887-8d18-11e9-80ba-00163e04d588,ResourceVersion:34289,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc002c9e6e7 0xc002c9e6e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.218,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c9e750} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c9e770}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.218,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.741: INFO: Pod "nginx-deployment-6f478d8d8-jzfjz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-jzfjz,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-jzfjz,UID:520fe51b-8d18-11e9-80ba-00163e04d588,ResourceVersion:34307,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc002c9e837 0xc002c9e838}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c9e8a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c9e8c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.217,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.741: INFO: Pod "nginx-deployment-6f478d8d8-nmzp5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-nmzp5,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-nmzp5,UID:4e754992-8d18-11e9-80ba-00163e04d588,ResourceVersion:34138,Generation:0,CreationTimestamp:2019-06-12 13:45:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc002c9e987 0xc002c9e988}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.219,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c9e9f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c9ea10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:14 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.219,PodIP:172.20.1.169,StartTime:2019-06-12 13:45:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-12 13:45:16 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://5856ae84770085b8e28e36e3d237196ba4967c5bbd1abd6b5df74b1d617e3a0a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.741: INFO: Pod "nginx-deployment-6f478d8d8-q7d2w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-q7d2w,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-q7d2w,UID:5211dba5-8d18-11e9-80ba-00163e04d588,ResourceVersion:34312,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc002c9eae7 0xc002c9eae8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.218,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c9eb50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c9eb70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.218,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.741: INFO: Pod "nginx-deployment-6f478d8d8-qfj99" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-qfj99,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-qfj99,UID:4e7233b8-8d18-11e9-80ba-00163e04d588,ResourceVersion:34148,Generation:0,CreationTimestamp:2019-06-12 13:45:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc002c9ec37 0xc002c9ec38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c9ecb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c9ecd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:14 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.217,PodIP:172.20.2.155,StartTime:2019-06-12 13:45:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-12 13:45:16 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b3bc99c51a240dce9cc867c8b9b611f05105b6a368818427f150954206689e64}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.742: INFO: Pod "nginx-deployment-6f478d8d8-rjmjt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rjmjt,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-rjmjt,UID:520e9921-8d18-11e9-80ba-00163e04d588,ResourceVersion:34281,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc002c9eda7 0xc002c9eda8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.217,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c9ee10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c9ee30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.217,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.742: INFO: Pod "nginx-deployment-6f478d8d8-spp8c" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-spp8c,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-spp8c,UID:4e753b44-8d18-11e9-80ba-00163e04d588,ResourceVersion:34158,Generation:0,CreationTimestamp:2019-06-12 13:45:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc002c9eef7 0xc002c9eef8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.218,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c9ef60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c9ef80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:14 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.218,PodIP:172.20.2.5,StartTime:2019-06-12 13:45:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-12 13:45:16 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://a7cded4f4125c20ba68bbca75b22a6ee3fbff02aecafe774f320091b9cc6875d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.742: INFO: Pod "nginx-deployment-6f478d8d8-tpg5z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-tpg5z,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-tpg5z,UID:4e76d18f-8d18-11e9-80ba-00163e04d588,ResourceVersion:34163,Generation:0,CreationTimestamp:2019-06-12 13:45:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc002c9f050 0xc002c9f051}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.219,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c9f0b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c9f0d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:14 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.219,PodIP:172.20.1.170,StartTime:2019-06-12 13:45:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-12 13:45:16 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://6b827f5285e0d673999ce36559633b409a495d783f18fb180e234c31b382341d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 12 13:45:22.742: INFO: Pod "nginx-deployment-6f478d8d8-twcng" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-twcng,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3617,SelfLink:/api/v1/namespaces/deployment-3617/pods/nginx-deployment-6f478d8d8-twcng,UID:5211bda2-8d18-11e9-80ba-00163e04d588,ResourceVersion:34313,Generation:0,CreationTimestamp:2019-06-12 13:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 4e7154ad-8d18-11e9-80ba-00163e04d588 0xc002c9f1a7 0xc002c9f1a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dltzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dltzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dltzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.219,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c9f210} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c9f230}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-12 13:45:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.219,PodIP:,StartTime:2019-06-12 13:45:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:45:22.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3617" for this suite.
Jun 12 13:45:30.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:45:30.859: INFO: namespace deployment-3617 deletion completed in 8.095346357s

• [SLOW TEST:16.252 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:45:30.860: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:45:57.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6476" for this suite.
Jun 12 13:46:03.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:46:03.244: INFO: namespace container-runtime-6476 deletion completed in 6.080263265s

• [SLOW TEST:32.384 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:46:03.244: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 12 13:46:03.285: INFO: Waiting up to 5m0s for pod "pod-6b6fc178-8d18-11e9-b925-1ab852558ec8" in namespace "emptydir-7774" to be "success or failure"
Jun 12 13:46:03.288: INFO: Pod "pod-6b6fc178-8d18-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.117033ms
Jun 12 13:46:05.291: INFO: Pod "pod-6b6fc178-8d18-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00636097s
STEP: Saw pod success
Jun 12 13:46:05.291: INFO: Pod "pod-6b6fc178-8d18-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:46:05.294: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-6b6fc178-8d18-11e9-b925-1ab852558ec8 container test-container: <nil>
STEP: delete the pod
Jun 12 13:46:05.314: INFO: Waiting for pod pod-6b6fc178-8d18-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:46:05.316: INFO: Pod pod-6b6fc178-8d18-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:46:05.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7774" for this suite.
Jun 12 13:46:11.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:46:11.399: INFO: namespace emptydir-7774 deletion completed in 6.079112457s

• [SLOW TEST:8.155 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:46:11.399: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-2971
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Jun 12 13:46:11.470: INFO: Found 0 stateful pods, waiting for 3
Jun 12 13:46:21.473: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 12 13:46:21.473: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 12 13:46:21.473: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 12 13:46:21.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-2971 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 12 13:46:21.774: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 12 13:46:21.774: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 12 13:46:21.774: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jun 12 13:46:31.803: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 12 13:46:41.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-2971 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 12 13:46:42.108: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 12 13:46:42.108: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 12 13:46:42.108: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 12 13:46:52.125: INFO: Waiting for StatefulSet statefulset-2971/ss2 to complete update
Jun 12 13:46:52.125: INFO: Waiting for Pod statefulset-2971/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 12 13:46:52.125: INFO: Waiting for Pod statefulset-2971/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 12 13:46:52.125: INFO: Waiting for Pod statefulset-2971/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 12 13:47:02.131: INFO: Waiting for StatefulSet statefulset-2971/ss2 to complete update
Jun 12 13:47:02.131: INFO: Waiting for Pod statefulset-2971/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 12 13:47:02.131: INFO: Waiting for Pod statefulset-2971/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Jun 12 13:47:12.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-2971 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 12 13:47:12.425: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 12 13:47:12.425: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 12 13:47:12.425: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 12 13:47:22.452: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 12 13:47:32.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-652313554 exec --namespace=statefulset-2971 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 12 13:47:32.784: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 12 13:47:32.784: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 12 13:47:32.784: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 12 13:47:42.800: INFO: Waiting for StatefulSet statefulset-2971/ss2 to complete update
Jun 12 13:47:42.800: INFO: Waiting for Pod statefulset-2971/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Jun 12 13:47:42.800: INFO: Waiting for Pod statefulset-2971/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Jun 12 13:47:52.806: INFO: Waiting for StatefulSet statefulset-2971/ss2 to complete update
Jun 12 13:47:52.806: INFO: Waiting for Pod statefulset-2971/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Jun 12 13:48:02.812: INFO: Waiting for StatefulSet statefulset-2971/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jun 12 13:48:12.806: INFO: Deleting all statefulset in ns statefulset-2971
Jun 12 13:48:12.809: INFO: Scaling statefulset ss2 to 0
Jun 12 13:48:22.826: INFO: Waiting for statefulset status.replicas updated to 0
Jun 12 13:48:22.829: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:48:22.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2971" for this suite.
Jun 12 13:48:28.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:48:28.934: INFO: namespace statefulset-2971 deletion completed in 6.089911914s

• [SLOW TEST:137.535 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:48:28.934: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jun 12 13:48:28.968: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:48:32.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7806" for this suite.
Jun 12 13:48:38.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:48:38.151: INFO: namespace init-container-7806 deletion completed in 6.085036136s

• [SLOW TEST:9.217 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:48:38.152: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 12 13:48:38.188: INFO: Waiting up to 5m0s for pod "pod-c7c4cb65-8d18-11e9-b925-1ab852558ec8" in namespace "emptydir-5448" to be "success or failure"
Jun 12 13:48:38.190: INFO: Pod "pod-c7c4cb65-8d18-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.207493ms
Jun 12 13:48:40.193: INFO: Pod "pod-c7c4cb65-8d18-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005567393s
Jun 12 13:48:42.197: INFO: Pod "pod-c7c4cb65-8d18-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00897799s
STEP: Saw pod success
Jun 12 13:48:42.197: INFO: Pod "pod-c7c4cb65-8d18-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:48:42.199: INFO: Trying to get logs from node cn-hongkong.192.168.0.218 pod pod-c7c4cb65-8d18-11e9-b925-1ab852558ec8 container test-container: <nil>
STEP: delete the pod
Jun 12 13:48:42.219: INFO: Waiting for pod pod-c7c4cb65-8d18-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:48:42.221: INFO: Pod pod-c7c4cb65-8d18-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:48:42.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5448" for this suite.
Jun 12 13:48:48.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:48:48.306: INFO: namespace emptydir-5448 deletion completed in 6.081519981s

• [SLOW TEST:10.155 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:48:48.307: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 12 13:48:48.361: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:48.361: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:48.361: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:48.364: INFO: Number of nodes with available pods: 0
Jun 12 13:48:48.364: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 13:48:49.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:49.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:49.369: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:49.371: INFO: Number of nodes with available pods: 0
Jun 12 13:48:49.371: INFO: Node cn-hongkong.192.168.0.217 is running more than one daemon pod
Jun 12 13:48:50.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:50.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:50.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:50.370: INFO: Number of nodes with available pods: 2
Jun 12 13:48:50.370: INFO: Node cn-hongkong.192.168.0.218 is running more than one daemon pod
Jun 12 13:48:51.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:51.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:51.368: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:51.371: INFO: Number of nodes with available pods: 3
Jun 12 13:48:51.371: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 12 13:48:51.386: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:51.387: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:51.387: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:51.390: INFO: Number of nodes with available pods: 2
Jun 12 13:48:51.390: INFO: Node cn-hongkong.192.168.0.218 is running more than one daemon pod
Jun 12 13:48:52.395: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:52.395: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:52.395: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:52.397: INFO: Number of nodes with available pods: 2
Jun 12 13:48:52.398: INFO: Node cn-hongkong.192.168.0.218 is running more than one daemon pod
Jun 12 13:48:53.395: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.214 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:53.395: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.215 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:53.395: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.216 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 12 13:48:53.398: INFO: Number of nodes with available pods: 3
Jun 12 13:48:53.398: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1634, will wait for the garbage collector to delete the pods
Jun 12 13:48:53.461: INFO: Deleting DaemonSet.extensions daemon-set took: 6.319725ms
Jun 12 13:48:53.861: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.227818ms
Jun 12 13:49:02.864: INFO: Number of nodes with available pods: 0
Jun 12 13:49:02.864: INFO: Number of running nodes: 0, number of available pods: 0
Jun 12 13:49:02.867: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1634/daemonsets","resourceVersion":"35780"},"items":null}

Jun 12 13:49:02.869: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1634/pods","resourceVersion":"35780"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:49:02.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1634" for this suite.
Jun 12 13:49:08.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:49:08.963: INFO: namespace daemonsets-1634 deletion completed in 6.080007165s

• [SLOW TEST:20.657 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:49:08.963: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 12 13:49:09.012: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8576,SelfLink:/api/v1/namespaces/watch-8576/configmaps/e2e-watch-test-label-changed,UID:da24c7f2-8d18-11e9-afb2-00163e04c212,ResourceVersion:35833,Generation:0,CreationTimestamp:2019-06-12 13:49:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 12 13:49:09.012: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8576,SelfLink:/api/v1/namespaces/watch-8576/configmaps/e2e-watch-test-label-changed,UID:da24c7f2-8d18-11e9-afb2-00163e04c212,ResourceVersion:35834,Generation:0,CreationTimestamp:2019-06-12 13:49:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 12 13:49:09.012: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8576,SelfLink:/api/v1/namespaces/watch-8576/configmaps/e2e-watch-test-label-changed,UID:da24c7f2-8d18-11e9-afb2-00163e04c212,ResourceVersion:35835,Generation:0,CreationTimestamp:2019-06-12 13:49:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 12 13:49:19.046: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8576,SelfLink:/api/v1/namespaces/watch-8576/configmaps/e2e-watch-test-label-changed,UID:da24c7f2-8d18-11e9-afb2-00163e04c212,ResourceVersion:35863,Generation:0,CreationTimestamp:2019-06-12 13:49:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 12 13:49:19.046: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8576,SelfLink:/api/v1/namespaces/watch-8576/configmaps/e2e-watch-test-label-changed,UID:da24c7f2-8d18-11e9-afb2-00163e04c212,ResourceVersion:35864,Generation:0,CreationTimestamp:2019-06-12 13:49:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jun 12 13:49:19.046: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8576,SelfLink:/api/v1/namespaces/watch-8576/configmaps/e2e-watch-test-label-changed,UID:da24c7f2-8d18-11e9-afb2-00163e04c212,ResourceVersion:35865,Generation:0,CreationTimestamp:2019-06-12 13:49:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:49:19.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8576" for this suite.
Jun 12 13:49:25.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:49:25.129: INFO: namespace watch-8576 deletion completed in 6.079899002s

• [SLOW TEST:16.166 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:49:25.129: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-e3c6623e-8d18-11e9-b925-1ab852558ec8
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:49:27.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6137" for this suite.
Jun 12 13:49:49.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:49:49.303: INFO: namespace configmap-6137 deletion completed in 22.095099159s

• [SLOW TEST:24.174 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 12 13:49:49.303: INFO: >>> kubeConfig: /tmp/kubeconfig-652313554
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-f22ef57e-8d18-11e9-b925-1ab852558ec8
STEP: Creating a pod to test consume secrets
Jun 12 13:49:49.368: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f231657b-8d18-11e9-b925-1ab852558ec8" in namespace "projected-3042" to be "success or failure"
Jun 12 13:49:49.370: INFO: Pod "pod-projected-secrets-f231657b-8d18-11e9-b925-1ab852558ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.322085ms
Jun 12 13:49:51.373: INFO: Pod "pod-projected-secrets-f231657b-8d18-11e9-b925-1ab852558ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005946614s
STEP: Saw pod success
Jun 12 13:49:51.374: INFO: Pod "pod-projected-secrets-f231657b-8d18-11e9-b925-1ab852558ec8" satisfied condition "success or failure"
Jun 12 13:49:51.376: INFO: Trying to get logs from node cn-hongkong.192.168.0.217 pod pod-projected-secrets-f231657b-8d18-11e9-b925-1ab852558ec8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 12 13:49:51.402: INFO: Waiting for pod pod-projected-secrets-f231657b-8d18-11e9-b925-1ab852558ec8 to disappear
Jun 12 13:49:51.404: INFO: Pod pod-projected-secrets-f231657b-8d18-11e9-b925-1ab852558ec8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 12 13:49:51.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3042" for this suite.
Jun 12 13:49:57.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 12 13:49:57.493: INFO: namespace projected-3042 deletion completed in 6.084831616s

• [SLOW TEST:8.190 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSJun 12 13:49:57.494: INFO: Running AfterSuite actions on all nodes
Jun 12 13:49:57.494: INFO: Running AfterSuite actions on node 1
Jun 12 13:49:57.494: INFO: Skipping dumping logs from cluster

Ran 203 of 3585 Specs in 5274.568 seconds
SUCCESS! -- 203 Passed | 0 Failed | 0 Pending | 3382 Skipped PASS

Ginkgo ran 1 suite in 1h27m55.599368227s
Test Suite Passed
