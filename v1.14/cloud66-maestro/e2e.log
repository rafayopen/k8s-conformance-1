I1205 19:08:15.968313      15 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-935805400
I1205 19:08:15.968448      15 e2e.go:242] Starting e2e run "96c3a06f-1792-11ea-a680-be755c6bedde" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1575572894 - Will randomize all specs
Will run 204 of 3586 specs

Dec  5 19:08:16.056: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 19:08:16.059: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec  5 19:08:16.076: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec  5 19:08:16.105: INFO: 25 / 25 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec  5 19:08:16.105: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Dec  5 19:08:16.105: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec  5 19:08:16.115: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-amd64' (0 seconds elapsed)
Dec  5 19:08:16.115: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm' (0 seconds elapsed)
Dec  5 19:08:16.115: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm64' (0 seconds elapsed)
Dec  5 19:08:16.115: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-ppc64le' (0 seconds elapsed)
Dec  5 19:08:16.115: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-s390x' (0 seconds elapsed)
Dec  5 19:08:16.115: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec  5 19:08:16.115: INFO: e2e test version: v1.14.8
Dec  5 19:08:16.117: INFO: kube-apiserver version: v1.14.8
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:08:16.117: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename secrets
Dec  5 19:08:16.154: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-976f984e-1792-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume secrets
Dec  5 19:08:16.172: INFO: Waiting up to 5m0s for pod "pod-secrets-9770e1ef-1792-11ea-a680-be755c6bedde" in namespace "secrets-3545" to be "success or failure"
Dec  5 19:08:16.175: INFO: Pod "pod-secrets-9770e1ef-1792-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.512096ms
Dec  5 19:08:18.180: INFO: Pod "pod-secrets-9770e1ef-1792-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007945189s
Dec  5 19:08:20.184: INFO: Pod "pod-secrets-9770e1ef-1792-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012493644s
STEP: Saw pod success
Dec  5 19:08:20.184: INFO: Pod "pod-secrets-9770e1ef-1792-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:08:20.188: INFO: Trying to get logs from node puma pod pod-secrets-9770e1ef-1792-11ea-a680-be755c6bedde container secret-env-test: <nil>
STEP: delete the pod
Dec  5 19:08:20.224: INFO: Waiting for pod pod-secrets-9770e1ef-1792-11ea-a680-be755c6bedde to disappear
Dec  5 19:08:20.226: INFO: Pod pod-secrets-9770e1ef-1792-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:08:20.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3545" for this suite.
Dec  5 19:08:26.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:08:26.385: INFO: namespace secrets-3545 deletion completed in 6.155190615s

• [SLOW TEST:10.269 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:08:26.386: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-9537
Dec  5 19:08:28.442: INFO: Started pod liveness-exec in namespace container-probe-9537
STEP: checking the pod's current state and verifying that restartCount is present
Dec  5 19:08:28.446: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:12:29.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9537" for this suite.
Dec  5 19:12:35.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:12:35.310: INFO: namespace container-probe-9537 deletion completed in 6.139553079s

• [SLOW TEST:248.924 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:12:35.311: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Dec  5 19:12:35.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 create -f - --namespace=kubectl-7970'
Dec  5 19:12:35.760: INFO: stderr: ""
Dec  5 19:12:35.760: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  5 19:12:35.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7970'
Dec  5 19:12:35.848: INFO: stderr: ""
Dec  5 19:12:35.848: INFO: stdout: "update-demo-nautilus-wvvtw update-demo-nautilus-zkthz "
Dec  5 19:12:35.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-wvvtw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7970'
Dec  5 19:12:35.946: INFO: stderr: ""
Dec  5 19:12:35.946: INFO: stdout: ""
Dec  5 19:12:35.946: INFO: update-demo-nautilus-wvvtw is created but not running
Dec  5 19:12:40.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7970'
Dec  5 19:12:41.065: INFO: stderr: ""
Dec  5 19:12:41.065: INFO: stdout: "update-demo-nautilus-wvvtw update-demo-nautilus-zkthz "
Dec  5 19:12:41.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-wvvtw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7970'
Dec  5 19:12:41.159: INFO: stderr: ""
Dec  5 19:12:41.159: INFO: stdout: "true"
Dec  5 19:12:41.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-wvvtw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7970'
Dec  5 19:12:41.232: INFO: stderr: ""
Dec  5 19:12:41.232: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 19:12:41.232: INFO: validating pod update-demo-nautilus-wvvtw
Dec  5 19:12:41.238: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 19:12:41.238: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 19:12:41.238: INFO: update-demo-nautilus-wvvtw is verified up and running
Dec  5 19:12:41.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-zkthz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7970'
Dec  5 19:12:41.310: INFO: stderr: ""
Dec  5 19:12:41.310: INFO: stdout: "true"
Dec  5 19:12:41.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-zkthz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7970'
Dec  5 19:12:41.387: INFO: stderr: ""
Dec  5 19:12:41.387: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 19:12:41.387: INFO: validating pod update-demo-nautilus-zkthz
Dec  5 19:12:41.394: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 19:12:41.394: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 19:12:41.394: INFO: update-demo-nautilus-zkthz is verified up and running
STEP: scaling down the replication controller
Dec  5 19:12:41.396: INFO: scanned /root for discovery docs: <nil>
Dec  5 19:12:41.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7970'
Dec  5 19:12:42.524: INFO: stderr: ""
Dec  5 19:12:42.524: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  5 19:12:42.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7970'
Dec  5 19:12:42.638: INFO: stderr: ""
Dec  5 19:12:42.638: INFO: stdout: "update-demo-nautilus-wvvtw update-demo-nautilus-zkthz "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  5 19:12:47.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7970'
Dec  5 19:12:47.757: INFO: stderr: ""
Dec  5 19:12:47.757: INFO: stdout: "update-demo-nautilus-wvvtw update-demo-nautilus-zkthz "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  5 19:12:52.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7970'
Dec  5 19:12:52.882: INFO: stderr: ""
Dec  5 19:12:52.882: INFO: stdout: "update-demo-nautilus-wvvtw update-demo-nautilus-zkthz "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  5 19:12:57.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7970'
Dec  5 19:12:58.000: INFO: stderr: ""
Dec  5 19:12:58.000: INFO: stdout: "update-demo-nautilus-wvvtw "
Dec  5 19:12:58.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-wvvtw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7970'
Dec  5 19:12:58.076: INFO: stderr: ""
Dec  5 19:12:58.076: INFO: stdout: "true"
Dec  5 19:12:58.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-wvvtw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7970'
Dec  5 19:12:58.146: INFO: stderr: ""
Dec  5 19:12:58.146: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 19:12:58.146: INFO: validating pod update-demo-nautilus-wvvtw
Dec  5 19:12:58.153: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 19:12:58.153: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 19:12:58.153: INFO: update-demo-nautilus-wvvtw is verified up and running
STEP: scaling up the replication controller
Dec  5 19:12:58.155: INFO: scanned /root for discovery docs: <nil>
Dec  5 19:12:58.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7970'
Dec  5 19:12:59.258: INFO: stderr: ""
Dec  5 19:12:59.258: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  5 19:12:59.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7970'
Dec  5 19:12:59.378: INFO: stderr: ""
Dec  5 19:12:59.379: INFO: stdout: "update-demo-nautilus-ggj7g update-demo-nautilus-wvvtw "
Dec  5 19:12:59.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-ggj7g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7970'
Dec  5 19:12:59.476: INFO: stderr: ""
Dec  5 19:12:59.476: INFO: stdout: ""
Dec  5 19:12:59.476: INFO: update-demo-nautilus-ggj7g is created but not running
Dec  5 19:13:04.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7970'
Dec  5 19:13:04.585: INFO: stderr: ""
Dec  5 19:13:04.585: INFO: stdout: "update-demo-nautilus-ggj7g update-demo-nautilus-wvvtw "
Dec  5 19:13:04.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-ggj7g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7970'
Dec  5 19:13:04.679: INFO: stderr: ""
Dec  5 19:13:04.679: INFO: stdout: "true"
Dec  5 19:13:04.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-ggj7g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7970'
Dec  5 19:13:04.766: INFO: stderr: ""
Dec  5 19:13:04.766: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 19:13:04.766: INFO: validating pod update-demo-nautilus-ggj7g
Dec  5 19:13:04.773: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 19:13:04.773: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 19:13:04.773: INFO: update-demo-nautilus-ggj7g is verified up and running
Dec  5 19:13:04.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-wvvtw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7970'
Dec  5 19:13:04.841: INFO: stderr: ""
Dec  5 19:13:04.841: INFO: stdout: "true"
Dec  5 19:13:04.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-wvvtw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7970'
Dec  5 19:13:04.914: INFO: stderr: ""
Dec  5 19:13:04.914: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 19:13:04.914: INFO: validating pod update-demo-nautilus-wvvtw
Dec  5 19:13:04.922: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 19:13:04.922: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 19:13:04.922: INFO: update-demo-nautilus-wvvtw is verified up and running
STEP: using delete to clean up resources
Dec  5 19:13:04.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 delete --grace-period=0 --force -f - --namespace=kubectl-7970'
Dec  5 19:13:04.995: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 19:13:04.995: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  5 19:13:04.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7970'
Dec  5 19:13:05.083: INFO: stderr: "No resources found.\n"
Dec  5 19:13:05.083: INFO: stdout: ""
Dec  5 19:13:05.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -l name=update-demo --namespace=kubectl-7970 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  5 19:13:05.170: INFO: stderr: ""
Dec  5 19:13:05.170: INFO: stdout: "update-demo-nautilus-ggj7g\nupdate-demo-nautilus-wvvtw\n"
Dec  5 19:13:05.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7970'
Dec  5 19:13:05.772: INFO: stderr: "No resources found.\n"
Dec  5 19:13:05.772: INFO: stdout: ""
Dec  5 19:13:05.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -l name=update-demo --namespace=kubectl-7970 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  5 19:13:05.846: INFO: stderr: ""
Dec  5 19:13:05.846: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:13:05.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7970" for this suite.
Dec  5 19:13:27.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:13:28.002: INFO: namespace kubectl-7970 deletion completed in 22.150147602s

• [SLOW TEST:52.691 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:13:28.002: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 19:13:28.040: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec  5 19:13:28.053: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  5 19:13:33.059: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  5 19:13:33.059: INFO: Creating deployment "test-rolling-update-deployment"
Dec  5 19:13:33.066: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec  5 19:13:33.076: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec  5 19:13:35.086: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec  5 19:13:35.091: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  5 19:13:35.102: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-1711,SelfLink:/apis/apps/v1/namespaces/deployment-1711/deployments/test-rolling-update-deployment,UID:54538e67-1793-11ea-b857-a68228aedf28,ResourceVersion:33123,Generation:1,CreationTimestamp:2019-12-05 19:13:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-05 19:13:33 +0000 UTC 2019-12-05 19:13:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-05 19:13:35 +0000 UTC 2019-12-05 19:13:33 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-57b6b5bb54" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  5 19:13:35.106: INFO: New ReplicaSet "test-rolling-update-deployment-57b6b5bb54" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-57b6b5bb54,GenerateName:,Namespace:deployment-1711,SelfLink:/apis/apps/v1/namespaces/deployment-1711/replicasets/test-rolling-update-deployment-57b6b5bb54,UID:5456487e-1793-11ea-9811-dae9897b3b7b,ResourceVersion:33112,Generation:1,CreationTimestamp:2019-12-05 19:13:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 54538e67-1793-11ea-b857-a68228aedf28 0xc002192eb7 0xc002192eb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  5 19:13:35.106: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec  5 19:13:35.106: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-1711,SelfLink:/apis/apps/v1/namespaces/deployment-1711/replicasets/test-rolling-update-controller,UID:5155a12c-1793-11ea-b857-a68228aedf28,ResourceVersion:33122,Generation:2,CreationTimestamp:2019-12-05 19:13:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 54538e67-1793-11ea-b857-a68228aedf28 0xc002192de7 0xc002192de8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 19:13:35.110: INFO: Pod "test-rolling-update-deployment-57b6b5bb54-skghc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-57b6b5bb54-skghc,GenerateName:test-rolling-update-deployment-57b6b5bb54-,Namespace:deployment-1711,SelfLink:/api/v1/namespaces/deployment-1711/pods/test-rolling-update-deployment-57b6b5bb54-skghc,UID:54570d19-1793-11ea-9811-dae9897b3b7b,ResourceVersion:33111,Generation:0,CreationTimestamp:2019-12-05 19:13:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-57b6b5bb54 5456487e-1793-11ea-9811-dae9897b3b7b 0xc002193777 0xc002193778}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r95jr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r95jr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-r95jr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:puma,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021937f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002193810}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:13:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:13:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:13:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:13:33 +0000 UTC  }],Message:,Reason:,HostIP:178.62.90.156,PodIP:25.0.1.173,StartTime:2019-12-05 19:13:33 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-05 19:13:34 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://7ee0e58d7a3d83c35b929faa2638052a10625eefdb68bbf70a81f7c353291712}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:13:35.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1711" for this suite.
Dec  5 19:13:41.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:13:41.250: INFO: namespace deployment-1711 deletion completed in 6.135127796s

• [SLOW TEST:13.248 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:13:41.252: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-2541
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2541
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2541
Dec  5 19:13:41.306: INFO: Found 0 stateful pods, waiting for 1
Dec  5 19:13:51.313: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec  5 19:13:51.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-2541 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 19:13:51.596: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 19:13:51.596: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 19:13:51.596: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 19:13:51.602: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  5 19:14:01.608: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 19:14:01.608: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 19:14:01.632: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999564s
Dec  5 19:14:02.638: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994759055s
Dec  5 19:14:03.643: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988848259s
Dec  5 19:14:04.649: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983444004s
Dec  5 19:14:05.655: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.977681415s
Dec  5 19:14:06.661: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.971758136s
Dec  5 19:14:07.667: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.965761723s
Dec  5 19:14:08.673: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.959582195s
Dec  5 19:14:09.679: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.95393842s
Dec  5 19:14:10.684: INFO: Verifying statefulset ss doesn't scale past 1 for another 947.897552ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2541
Dec  5 19:14:11.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-2541 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 19:14:11.985: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  5 19:14:11.985: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 19:14:11.985: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 19:14:11.989: INFO: Found 1 stateful pods, waiting for 3
Dec  5 19:14:21.996: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 19:14:21.996: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 19:14:21.996: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec  5 19:14:22.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-2541 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 19:14:22.274: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 19:14:22.274: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 19:14:22.274: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 19:14:22.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-2541 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 19:14:22.484: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 19:14:22.484: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 19:14:22.484: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 19:14:22.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-2541 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 19:14:22.713: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 19:14:22.713: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 19:14:22.713: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 19:14:22.713: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 19:14:22.717: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec  5 19:14:32.730: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 19:14:32.730: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 19:14:32.730: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 19:14:32.747: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999471s
Dec  5 19:14:33.753: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993756648s
Dec  5 19:14:34.760: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987279206s
Dec  5 19:14:35.766: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980995935s
Dec  5 19:14:36.771: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975119141s
Dec  5 19:14:37.778: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.969509071s
Dec  5 19:14:38.784: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.96294763s
Dec  5 19:14:39.790: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.956361633s
Dec  5 19:14:40.796: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.950750293s
Dec  5 19:14:41.802: INFO: Verifying statefulset ss doesn't scale past 3 for another 944.804457ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2541
Dec  5 19:14:42.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-2541 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 19:14:43.070: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  5 19:14:43.070: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 19:14:43.070: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 19:14:43.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-2541 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 19:14:43.301: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  5 19:14:43.301: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 19:14:43.301: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 19:14:43.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-2541 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 19:14:43.553: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  5 19:14:43.553: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 19:14:43.553: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 19:14:43.553: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  5 19:15:13.575: INFO: Deleting all statefulset in ns statefulset-2541
Dec  5 19:15:13.579: INFO: Scaling statefulset ss to 0
Dec  5 19:15:13.592: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 19:15:13.596: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:15:13.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2541" for this suite.
Dec  5 19:15:19.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:15:19.756: INFO: namespace statefulset-2541 deletion completed in 6.138139725s

• [SLOW TEST:98.504 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:15:19.756: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-702
Dec  5 19:15:23.811: INFO: Started pod liveness-http in namespace container-probe-702
STEP: checking the pod's current state and verifying that restartCount is present
Dec  5 19:15:23.815: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:19:24.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-702" for this suite.
Dec  5 19:19:30.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:19:30.701: INFO: namespace container-probe-702 deletion completed in 6.149786426s

• [SLOW TEST:250.945 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:19:30.702: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-29843748-1794-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume secrets
Dec  5 19:19:30.751: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-29856efd-1794-11ea-a680-be755c6bedde" in namespace "projected-7959" to be "success or failure"
Dec  5 19:19:30.755: INFO: Pod "pod-projected-secrets-29856efd-1794-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.328846ms
Dec  5 19:19:32.761: INFO: Pod "pod-projected-secrets-29856efd-1794-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010060408s
STEP: Saw pod success
Dec  5 19:19:32.761: INFO: Pod "pod-projected-secrets-29856efd-1794-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:19:32.766: INFO: Trying to get logs from node crayfish pod pod-projected-secrets-29856efd-1794-11ea-a680-be755c6bedde container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  5 19:19:32.808: INFO: Waiting for pod pod-projected-secrets-29856efd-1794-11ea-a680-be755c6bedde to disappear
Dec  5 19:19:32.812: INFO: Pod pod-projected-secrets-29856efd-1794-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:19:32.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7959" for this suite.
Dec  5 19:19:38.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:19:38.959: INFO: namespace projected-7959 deletion completed in 6.143105395s

• [SLOW TEST:8.257 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:19:38.960: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:19:38.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8416" for this suite.
Dec  5 19:19:45.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:19:45.143: INFO: namespace services-8416 deletion completed in 6.140615623s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.183 seconds]
[sig-network] Services
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:19:45.145: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-322125a2-1794-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume configMaps
Dec  5 19:19:45.201: INFO: Waiting up to 5m0s for pod "pod-configmaps-32224243-1794-11ea-a680-be755c6bedde" in namespace "configmap-8328" to be "success or failure"
Dec  5 19:19:45.207: INFO: Pod "pod-configmaps-32224243-1794-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 5.336017ms
Dec  5 19:19:47.213: INFO: Pod "pod-configmaps-32224243-1794-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0117046s
Dec  5 19:19:49.218: INFO: Pod "pod-configmaps-32224243-1794-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016323654s
STEP: Saw pod success
Dec  5 19:19:49.218: INFO: Pod "pod-configmaps-32224243-1794-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:19:49.223: INFO: Trying to get logs from node crayfish pod pod-configmaps-32224243-1794-11ea-a680-be755c6bedde container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 19:19:49.250: INFO: Waiting for pod pod-configmaps-32224243-1794-11ea-a680-be755c6bedde to disappear
Dec  5 19:19:49.254: INFO: Pod pod-configmaps-32224243-1794-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:19:49.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8328" for this suite.
Dec  5 19:19:55.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:19:55.411: INFO: namespace configmap-8328 deletion completed in 6.151927372s

• [SLOW TEST:10.267 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:19:55.412: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-38402156-1794-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume secrets
Dec  5 19:19:55.470: INFO: Waiting up to 5m0s for pod "pod-secrets-38415f4f-1794-11ea-a680-be755c6bedde" in namespace "secrets-604" to be "success or failure"
Dec  5 19:19:55.474: INFO: Pod "pod-secrets-38415f4f-1794-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.123032ms
Dec  5 19:19:57.480: INFO: Pod "pod-secrets-38415f4f-1794-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010096606s
Dec  5 19:19:59.486: INFO: Pod "pod-secrets-38415f4f-1794-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015619969s
STEP: Saw pod success
Dec  5 19:19:59.486: INFO: Pod "pod-secrets-38415f4f-1794-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:19:59.490: INFO: Trying to get logs from node crayfish pod pod-secrets-38415f4f-1794-11ea-a680-be755c6bedde container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 19:19:59.517: INFO: Waiting for pod pod-secrets-38415f4f-1794-11ea-a680-be755c6bedde to disappear
Dec  5 19:19:59.521: INFO: Pod pod-secrets-38415f4f-1794-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:19:59.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-604" for this suite.
Dec  5 19:20:05.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:20:05.669: INFO: namespace secrets-604 deletion completed in 6.142224888s

• [SLOW TEST:10.258 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:20:05.674: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 19:20:05.711: INFO: Creating ReplicaSet my-hostname-basic-3e5d3892-1794-11ea-a680-be755c6bedde
Dec  5 19:20:05.721: INFO: Pod name my-hostname-basic-3e5d3892-1794-11ea-a680-be755c6bedde: Found 0 pods out of 1
Dec  5 19:20:10.728: INFO: Pod name my-hostname-basic-3e5d3892-1794-11ea-a680-be755c6bedde: Found 1 pods out of 1
Dec  5 19:20:10.728: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-3e5d3892-1794-11ea-a680-be755c6bedde" is running
Dec  5 19:20:10.733: INFO: Pod "my-hostname-basic-3e5d3892-1794-11ea-a680-be755c6bedde-wmx6f" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-05 19:20:05 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-05 19:20:07 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-05 19:20:07 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-05 19:20:05 +0000 UTC Reason: Message:}])
Dec  5 19:20:10.733: INFO: Trying to dial the pod
Dec  5 19:20:15.750: INFO: Controller my-hostname-basic-3e5d3892-1794-11ea-a680-be755c6bedde: Got expected result from replica 1 [my-hostname-basic-3e5d3892-1794-11ea-a680-be755c6bedde-wmx6f]: "my-hostname-basic-3e5d3892-1794-11ea-a680-be755c6bedde-wmx6f", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:20:15.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2418" for this suite.
Dec  5 19:20:21.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:20:21.896: INFO: namespace replicaset-2418 deletion completed in 6.139670976s

• [SLOW TEST:16.222 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:20:21.898: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Dec  5 19:20:21.950: INFO: Waiting up to 5m0s for pod "client-containers-4808b726-1794-11ea-a680-be755c6bedde" in namespace "containers-5499" to be "success or failure"
Dec  5 19:20:21.953: INFO: Pod "client-containers-4808b726-1794-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.592545ms
Dec  5 19:20:23.959: INFO: Pod "client-containers-4808b726-1794-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008658902s
Dec  5 19:20:25.966: INFO: Pod "client-containers-4808b726-1794-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015329076s
STEP: Saw pod success
Dec  5 19:20:25.966: INFO: Pod "client-containers-4808b726-1794-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:20:25.971: INFO: Trying to get logs from node crayfish pod client-containers-4808b726-1794-11ea-a680-be755c6bedde container test-container: <nil>
STEP: delete the pod
Dec  5 19:20:26.000: INFO: Waiting for pod client-containers-4808b726-1794-11ea-a680-be755c6bedde to disappear
Dec  5 19:20:26.004: INFO: Pod client-containers-4808b726-1794-11ea-a680-be755c6bedde no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:20:26.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5499" for this suite.
Dec  5 19:20:32.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:20:32.155: INFO: namespace containers-5499 deletion completed in 6.144819785s

• [SLOW TEST:10.257 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:20:32.155: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  5 19:20:32.202: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e25c77b-1794-11ea-a680-be755c6bedde" in namespace "downward-api-5690" to be "success or failure"
Dec  5 19:20:32.207: INFO: Pod "downwardapi-volume-4e25c77b-1794-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.669543ms
Dec  5 19:20:34.213: INFO: Pod "downwardapi-volume-4e25c77b-1794-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010552891s
Dec  5 19:20:36.219: INFO: Pod "downwardapi-volume-4e25c77b-1794-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016934967s
STEP: Saw pod success
Dec  5 19:20:36.220: INFO: Pod "downwardapi-volume-4e25c77b-1794-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:20:36.225: INFO: Trying to get logs from node crayfish pod downwardapi-volume-4e25c77b-1794-11ea-a680-be755c6bedde container client-container: <nil>
STEP: delete the pod
Dec  5 19:20:36.255: INFO: Waiting for pod downwardapi-volume-4e25c77b-1794-11ea-a680-be755c6bedde to disappear
Dec  5 19:20:36.259: INFO: Pod downwardapi-volume-4e25c77b-1794-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:20:36.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5690" for this suite.
Dec  5 19:20:42.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:20:42.409: INFO: namespace downward-api-5690 deletion completed in 6.144769959s

• [SLOW TEST:10.254 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:20:42.409: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-8g6s
STEP: Creating a pod to test atomic-volume-subpath
Dec  5 19:20:42.466: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8g6s" in namespace "subpath-521" to be "success or failure"
Dec  5 19:20:42.470: INFO: Pod "pod-subpath-test-secret-8g6s": Phase="Pending", Reason="", readiness=false. Elapsed: 3.542241ms
Dec  5 19:20:44.475: INFO: Pod "pod-subpath-test-secret-8g6s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009037616s
Dec  5 19:20:46.480: INFO: Pod "pod-subpath-test-secret-8g6s": Phase="Running", Reason="", readiness=true. Elapsed: 4.013571525s
Dec  5 19:20:48.486: INFO: Pod "pod-subpath-test-secret-8g6s": Phase="Running", Reason="", readiness=true. Elapsed: 6.019845428s
Dec  5 19:20:50.492: INFO: Pod "pod-subpath-test-secret-8g6s": Phase="Running", Reason="", readiness=true. Elapsed: 8.026136276s
Dec  5 19:20:52.498: INFO: Pod "pod-subpath-test-secret-8g6s": Phase="Running", Reason="", readiness=true. Elapsed: 10.03183977s
Dec  5 19:20:54.503: INFO: Pod "pod-subpath-test-secret-8g6s": Phase="Running", Reason="", readiness=true. Elapsed: 12.03726244s
Dec  5 19:20:56.510: INFO: Pod "pod-subpath-test-secret-8g6s": Phase="Running", Reason="", readiness=true. Elapsed: 14.043513896s
Dec  5 19:20:58.515: INFO: Pod "pod-subpath-test-secret-8g6s": Phase="Running", Reason="", readiness=true. Elapsed: 16.048503226s
Dec  5 19:21:00.520: INFO: Pod "pod-subpath-test-secret-8g6s": Phase="Running", Reason="", readiness=true. Elapsed: 18.053599034s
Dec  5 19:21:02.526: INFO: Pod "pod-subpath-test-secret-8g6s": Phase="Running", Reason="", readiness=true. Elapsed: 20.059823672s
Dec  5 19:21:04.531: INFO: Pod "pod-subpath-test-secret-8g6s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.06500235s
STEP: Saw pod success
Dec  5 19:21:04.531: INFO: Pod "pod-subpath-test-secret-8g6s" satisfied condition "success or failure"
Dec  5 19:21:04.535: INFO: Trying to get logs from node crayfish pod pod-subpath-test-secret-8g6s container test-container-subpath-secret-8g6s: <nil>
STEP: delete the pod
Dec  5 19:21:04.563: INFO: Waiting for pod pod-subpath-test-secret-8g6s to disappear
Dec  5 19:21:04.566: INFO: Pod pod-subpath-test-secret-8g6s no longer exists
STEP: Deleting pod pod-subpath-test-secret-8g6s
Dec  5 19:21:04.567: INFO: Deleting pod "pod-subpath-test-secret-8g6s" in namespace "subpath-521"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:21:04.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-521" for this suite.
Dec  5 19:21:10.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:21:10.715: INFO: namespace subpath-521 deletion completed in 6.13901725s

• [SLOW TEST:28.306 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:21:10.716: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-5851/secret-test-6521a72d-1794-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume secrets
Dec  5 19:21:10.765: INFO: Waiting up to 5m0s for pod "pod-configmaps-6522b1fa-1794-11ea-a680-be755c6bedde" in namespace "secrets-5851" to be "success or failure"
Dec  5 19:21:10.771: INFO: Pod "pod-configmaps-6522b1fa-1794-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 5.564564ms
Dec  5 19:21:12.777: INFO: Pod "pod-configmaps-6522b1fa-1794-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011109282s
STEP: Saw pod success
Dec  5 19:21:12.777: INFO: Pod "pod-configmaps-6522b1fa-1794-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:21:12.781: INFO: Trying to get logs from node puma pod pod-configmaps-6522b1fa-1794-11ea-a680-be755c6bedde container env-test: <nil>
STEP: delete the pod
Dec  5 19:21:12.807: INFO: Waiting for pod pod-configmaps-6522b1fa-1794-11ea-a680-be755c6bedde to disappear
Dec  5 19:21:12.811: INFO: Pod pod-configmaps-6522b1fa-1794-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:21:12.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5851" for this suite.
Dec  5 19:21:18.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:21:18.955: INFO: namespace secrets-5851 deletion completed in 6.139054071s

• [SLOW TEST:8.239 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:21:18.957: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 19:21:18.990: INFO: Creating deployment "nginx-deployment"
Dec  5 19:21:18.997: INFO: Waiting for observed generation 1
Dec  5 19:21:21.007: INFO: Waiting for all required pods to come up
Dec  5 19:21:21.011: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec  5 19:21:23.025: INFO: Waiting for deployment "nginx-deployment" to complete
Dec  5 19:21:23.034: INFO: Updating deployment "nginx-deployment" with a non-existent image
Dec  5 19:21:23.043: INFO: Updating deployment nginx-deployment
Dec  5 19:21:23.043: INFO: Waiting for observed generation 2
Dec  5 19:21:25.052: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec  5 19:21:25.057: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec  5 19:21:25.061: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec  5 19:21:25.073: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec  5 19:21:25.073: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec  5 19:21:25.077: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec  5 19:21:25.085: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Dec  5 19:21:25.085: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Dec  5 19:21:25.094: INFO: Updating deployment nginx-deployment
Dec  5 19:21:25.094: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Dec  5 19:21:25.103: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec  5 19:21:25.110: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  5 19:21:25.129: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-3812,SelfLink:/apis/apps/v1/namespaces/deployment-3812/deployments/nginx-deployment,UID:6a0a99a6-1794-11ea-b857-a68228aedf28,ResourceVersion:34698,Generation:3,CreationTimestamp:2019-12-05 19:21:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-12-05 19:21:23 +0000 UTC 2019-12-05 19:21:19 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-b79c9d74d" is progressing.} {Available False 2019-12-05 19:21:25 +0000 UTC 2019-12-05 19:21:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Dec  5 19:21:25.133: INFO: New ReplicaSet "nginx-deployment-b79c9d74d" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d,GenerateName:,Namespace:deployment-3812,SelfLink:/apis/apps/v1/namespaces/deployment-3812/replicasets/nginx-deployment-b79c9d74d,UID:6c75473d-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34679,Generation:3,CreationTimestamp:2019-12-05 19:21:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 6a0a99a6-1794-11ea-b857-a68228aedf28 0xc00319d177 0xc00319d178}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 19:21:25.133: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Dec  5 19:21:25.133: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5,GenerateName:,Namespace:deployment-3812,SelfLink:/apis/apps/v1/namespaces/deployment-3812/replicasets/nginx-deployment-85db8c99c5,UID:6a0bd629-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34676,Generation:3,CreationTimestamp:2019-12-05 19:21:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 6a0a99a6-1794-11ea-b857-a68228aedf28 0xc00319d0a7 0xc00319d0a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Dec  5 19:21:25.145: INFO: Pod "nginx-deployment-85db8c99c5-2hndz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-2hndz,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-85db8c99c5-2hndz,UID:6a0f2354-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34592,Generation:0,CreationTimestamp:2019-12-05 19:21:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 6a0bd629-1794-11ea-9811-dae9897b3b7b 0xc00256ced7 0xc00256ced8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:crayfish,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00256cf50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00256cf70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:19 +0000 UTC  }],Message:,Reason:,HostIP:178.62.90.178,PodIP:25.0.2.191,StartTime:2019-12-05 19:21:19 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-05 19:21:21 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://72a6c502084e580deab30136572d9e5b75361288ca21e57074a8e4680e339410}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.146: INFO: Pod "nginx-deployment-85db8c99c5-4tsw5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-4tsw5,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-85db8c99c5-4tsw5,UID:6a0d44c6-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34576,Generation:0,CreationTimestamp:2019-12-05 19:21:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 6a0bd629-1794-11ea-9811-dae9897b3b7b 0xc00256d040 0xc00256d041}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:crayfish,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00256d0b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00256d0d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:19 +0000 UTC  }],Message:,Reason:,HostIP:178.62.90.178,PodIP:25.0.2.187,StartTime:2019-12-05 19:21:19 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-05 19:21:20 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d46b74be92f171185f0a4510448bfe0a17cc290ae30e641a9b78df5e51e45f30}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.146: INFO: Pod "nginx-deployment-85db8c99c5-5stcq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-5stcq,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-85db8c99c5-5stcq,UID:6db1bd7c-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34697,Generation:0,CreationTimestamp:2019-12-05 19:21:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 6a0bd629-1794-11ea-9811-dae9897b3b7b 0xc00256d1a0 0xc00256d1a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00256d200} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00256d220}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.146: INFO: Pod "nginx-deployment-85db8c99c5-6t7h2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-6t7h2,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-85db8c99c5-6t7h2,UID:6a0dfca7-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34571,Generation:0,CreationTimestamp:2019-12-05 19:21:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 6a0bd629-1794-11ea-9811-dae9897b3b7b 0xc00256d287 0xc00256d288}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:crayfish,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00256d300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00256d320}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:19 +0000 UTC  }],Message:,Reason:,HostIP:178.62.90.178,PodIP:25.0.2.188,StartTime:2019-12-05 19:21:19 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-05 19:21:20 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d8d9dd522d59e3b1e66c48f9ccc7e03fa99f6a88bd9407ab38d0b37ac67e3211}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.146: INFO: Pod "nginx-deployment-85db8c99c5-85hwm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-85hwm,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-85db8c99c5-85hwm,UID:6dafb987-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34692,Generation:0,CreationTimestamp:2019-12-05 19:21:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 6a0bd629-1794-11ea-9811-dae9897b3b7b 0xc00256d400 0xc00256d401}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:puma,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00256d470} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00256d490}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:25 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.147: INFO: Pod "nginx-deployment-85db8c99c5-8v7d2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-8v7d2,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-85db8c99c5-8v7d2,UID:6a0f24ba-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34611,Generation:0,CreationTimestamp:2019-12-05 19:21:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 6a0bd629-1794-11ea-9811-dae9897b3b7b 0xc00256d510 0xc00256d511}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:puma,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00256d580} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00256d5a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:19 +0000 UTC  }],Message:,Reason:,HostIP:178.62.90.156,PodIP:25.0.1.180,StartTime:2019-12-05 19:21:19 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-05 19:21:20 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://84b43f02e679b2c68519a23d8d8b7eb992a85898f4d55e8bac646b49ffbc9a39}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.147: INFO: Pod "nginx-deployment-85db8c99c5-b7gn9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-b7gn9,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-85db8c99c5-b7gn9,UID:6a0f1f44-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34596,Generation:0,CreationTimestamp:2019-12-05 19:21:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 6a0bd629-1794-11ea-9811-dae9897b3b7b 0xc00256d670 0xc00256d671}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:crayfish,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00256d6e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00256d700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:19 +0000 UTC  }],Message:,Reason:,HostIP:178.62.90.178,PodIP:25.0.2.189,StartTime:2019-12-05 19:21:19 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-05 19:21:20 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c0f9fc1d2e10022e9ea6c7b0622d8ada5ddd0ee2dd2efe11cf6a9e88db15ee14}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.147: INFO: Pod "nginx-deployment-85db8c99c5-dq89z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-dq89z,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-85db8c99c5-dq89z,UID:6a104b27-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34599,Generation:0,CreationTimestamp:2019-12-05 19:21:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 6a0bd629-1794-11ea-9811-dae9897b3b7b 0xc00256d7d0 0xc00256d7d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:puma,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00256d840} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00256d860}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:19 +0000 UTC  }],Message:,Reason:,HostIP:178.62.90.156,PodIP:25.0.1.182,StartTime:2019-12-05 19:21:19 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-05 19:21:21 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://500cc6c20a653f74d42fc99cb1d51921232c410856e7ab18f6612d176fa362ad}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.147: INFO: Pod "nginx-deployment-85db8c99c5-jbh8x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-jbh8x,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-85db8c99c5-jbh8x,UID:6db1d26e-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34699,Generation:0,CreationTimestamp:2019-12-05 19:21:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 6a0bd629-1794-11ea-9811-dae9897b3b7b 0xc00256d930 0xc00256d931}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00256d990} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00256d9b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.147: INFO: Pod "nginx-deployment-85db8c99c5-kbq9l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-kbq9l,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-85db8c99c5-kbq9l,UID:6db1b285-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34695,Generation:0,CreationTimestamp:2019-12-05 19:21:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 6a0bd629-1794-11ea-9811-dae9897b3b7b 0xc00256da17 0xc00256da18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00256da80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00256daa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.148: INFO: Pod "nginx-deployment-85db8c99c5-mgwvl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-mgwvl,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-85db8c99c5-mgwvl,UID:6a105cf8-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34590,Generation:0,CreationTimestamp:2019-12-05 19:21:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 6a0bd629-1794-11ea-9811-dae9897b3b7b 0xc00256db07 0xc00256db08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:crayfish,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00256db80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00256dba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:19 +0000 UTC  }],Message:,Reason:,HostIP:178.62.90.178,PodIP:25.0.2.190,StartTime:2019-12-05 19:21:19 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-05 19:21:20 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e888975ccba3f683c73d8e9e10630a5e88509045463c31646f401e169e0fa435}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.148: INFO: Pod "nginx-deployment-85db8c99c5-p85qv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-p85qv,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-85db8c99c5-p85qv,UID:6dafb3da-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34688,Generation:0,CreationTimestamp:2019-12-05 19:21:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 6a0bd629-1794-11ea-9811-dae9897b3b7b 0xc00256dc70 0xc00256dc71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:puma,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00256dce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00256dd00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:25 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.148: INFO: Pod "nginx-deployment-85db8c99c5-r52k8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-r52k8,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-85db8c99c5-r52k8,UID:6a0f217d-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34602,Generation:0,CreationTimestamp:2019-12-05 19:21:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 6a0bd629-1794-11ea-9811-dae9897b3b7b 0xc00256dd80 0xc00256dd81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:puma,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00256ddf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00256de10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:19 +0000 UTC  }],Message:,Reason:,HostIP:178.62.90.156,PodIP:25.0.1.179,StartTime:2019-12-05 19:21:19 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-05 19:21:20 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://063c61c604205bdd3cafb5b1be6f21719d5e59612d2bbb8da85f3ac46f0063c6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.148: INFO: Pod "nginx-deployment-85db8c99c5-s884h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-s884h,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-85db8c99c5-s884h,UID:6db1d6f3-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34696,Generation:0,CreationTimestamp:2019-12-05 19:21:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 6a0bd629-1794-11ea-9811-dae9897b3b7b 0xc00256dee0 0xc00256dee1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00256df40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00256df60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.148: INFO: Pod "nginx-deployment-85db8c99c5-zj8lh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-zj8lh,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-85db8c99c5-zj8lh,UID:6daeb513-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34681,Generation:0,CreationTimestamp:2019-12-05 19:21:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 6a0bd629-1794-11ea-9811-dae9897b3b7b 0xc00256dfc7 0xc00256dfc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:puma,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000376050} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000376070}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:25 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.149: INFO: Pod "nginx-deployment-b79c9d74d-bp8pr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-bp8pr,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-b79c9d74d-bp8pr,UID:6c75e98d-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34633,Generation:0,CreationTimestamp:2019-12-05 19:21:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 6c75473d-1794-11ea-9811-dae9897b3b7b 0xc0003760f0 0xc0003760f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:crayfish,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000376170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000376190}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC  }],Message:,Reason:,HostIP:178.62.90.178,PodIP:,StartTime:2019-12-05 19:21:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.149: INFO: Pod "nginx-deployment-b79c9d74d-kf2qd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-kf2qd,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-b79c9d74d-kf2qd,UID:6c7c7b87-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34661,Generation:0,CreationTimestamp:2019-12-05 19:21:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 6c75473d-1794-11ea-9811-dae9897b3b7b 0xc000376260 0xc000376261}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:puma,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0003762e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000376300}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC  }],Message:,Reason:,HostIP:178.62.90.156,PodIP:,StartTime:2019-12-05 19:21:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.149: INFO: Pod "nginx-deployment-b79c9d74d-lnw28" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-lnw28,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-b79c9d74d-lnw28,UID:6db1841c-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34694,Generation:0,CreationTimestamp:2019-12-05 19:21:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 6c75473d-1794-11ea-9811-dae9897b3b7b 0xc0003763d0 0xc0003763d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000376440} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000376460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.149: INFO: Pod "nginx-deployment-b79c9d74d-pzlz2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-pzlz2,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-b79c9d74d-pzlz2,UID:6daff661-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34691,Generation:0,CreationTimestamp:2019-12-05 19:21:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 6c75473d-1794-11ea-9811-dae9897b3b7b 0xc0003764c7 0xc0003764c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:puma,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000376540} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000376560}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:25 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.149: INFO: Pod "nginx-deployment-b79c9d74d-sfhpv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-sfhpv,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-b79c9d74d-sfhpv,UID:6c76ed31-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34644,Generation:0,CreationTimestamp:2019-12-05 19:21:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 6c75473d-1794-11ea-9811-dae9897b3b7b 0xc0003765e0 0xc0003765e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:crayfish,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000376660} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000376680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC  }],Message:,Reason:,HostIP:178.62.90.178,PodIP:,StartTime:2019-12-05 19:21:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.149: INFO: Pod "nginx-deployment-b79c9d74d-v649k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-v649k,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-b79c9d74d-v649k,UID:6db177d1-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34693,Generation:0,CreationTimestamp:2019-12-05 19:21:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 6c75473d-1794-11ea-9811-dae9897b3b7b 0xc000376750 0xc000376751}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0003767c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0003767e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.150: INFO: Pod "nginx-deployment-b79c9d74d-v7hg2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-v7hg2,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-b79c9d74d-v7hg2,UID:6c76e038-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34641,Generation:0,CreationTimestamp:2019-12-05 19:21:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 6c75473d-1794-11ea-9811-dae9897b3b7b 0xc000376847 0xc000376848}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:puma,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0003768c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000376940}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC  }],Message:,Reason:,HostIP:178.62.90.156,PodIP:,StartTime:2019-12-05 19:21:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 19:21:25.150: INFO: Pod "nginx-deployment-b79c9d74d-whlk6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-whlk6,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-3812,SelfLink:/api/v1/namespaces/deployment-3812/pods/nginx-deployment-b79c9d74d-whlk6,UID:6c7d3e4e-1794-11ea-9811-dae9897b3b7b,ResourceVersion:34665,Generation:0,CreationTimestamp:2019-12-05 19:21:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 6c75473d-1794-11ea-9811-dae9897b3b7b 0xc000376a10 0xc000376a11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k45zb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k45zb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-k45zb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:crayfish,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000376ac0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000376ae0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:21:23 +0000 UTC  }],Message:,Reason:,HostIP:178.62.90.178,PodIP:,StartTime:2019-12-05 19:21:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:21:25.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3812" for this suite.
Dec  5 19:21:31.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:21:31.310: INFO: namespace deployment-3812 deletion completed in 6.149931352s

• [SLOW TEST:12.354 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:21:31.311: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  5 19:21:31.352: INFO: Waiting up to 5m0s for pod "downwardapi-volume-71678c84-1794-11ea-a680-be755c6bedde" in namespace "downward-api-3034" to be "success or failure"
Dec  5 19:21:31.357: INFO: Pod "downwardapi-volume-71678c84-1794-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 5.056285ms
Dec  5 19:21:33.362: INFO: Pod "downwardapi-volume-71678c84-1794-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009523744s
Dec  5 19:21:35.368: INFO: Pod "downwardapi-volume-71678c84-1794-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015161107s
Dec  5 19:21:37.374: INFO: Pod "downwardapi-volume-71678c84-1794-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022074172s
STEP: Saw pod success
Dec  5 19:21:37.375: INFO: Pod "downwardapi-volume-71678c84-1794-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:21:37.380: INFO: Trying to get logs from node puma pod downwardapi-volume-71678c84-1794-11ea-a680-be755c6bedde container client-container: <nil>
STEP: delete the pod
Dec  5 19:21:37.409: INFO: Waiting for pod downwardapi-volume-71678c84-1794-11ea-a680-be755c6bedde to disappear
Dec  5 19:21:37.413: INFO: Pod downwardapi-volume-71678c84-1794-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:21:37.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3034" for this suite.
Dec  5 19:21:43.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:21:43.571: INFO: namespace downward-api-3034 deletion completed in 6.152103267s

• [SLOW TEST:12.261 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:21:43.573: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Dec  5 19:21:43.609: INFO: namespace kubectl-798
Dec  5 19:21:43.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 create -f - --namespace=kubectl-798'
Dec  5 19:21:43.862: INFO: stderr: ""
Dec  5 19:21:43.862: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  5 19:21:44.868: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 19:21:44.868: INFO: Found 0 / 1
Dec  5 19:21:45.869: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 19:21:45.869: INFO: Found 0 / 1
Dec  5 19:21:46.868: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 19:21:46.868: INFO: Found 1 / 1
Dec  5 19:21:46.868: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  5 19:21:46.872: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 19:21:46.872: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  5 19:21:46.872: INFO: wait on redis-master startup in kubectl-798 
Dec  5 19:21:46.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 logs redis-master-rrhkp redis-master --namespace=kubectl-798'
Dec  5 19:21:47.018: INFO: stderr: ""
Dec  5 19:21:47.018: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Dec 19:21:45.175 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Dec 19:21:45.175 # Server started, Redis version 3.2.12\n1:M 05 Dec 19:21:45.175 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Dec 19:21:45.175 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Dec  5 19:21:47.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-798'
Dec  5 19:21:47.138: INFO: stderr: ""
Dec  5 19:21:47.138: INFO: stdout: "service/rm2 exposed\n"
Dec  5 19:21:47.145: INFO: Service rm2 in namespace kubectl-798 found.
STEP: exposing service
Dec  5 19:21:49.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-798'
Dec  5 19:21:49.293: INFO: stderr: ""
Dec  5 19:21:49.293: INFO: stdout: "service/rm3 exposed\n"
Dec  5 19:21:49.296: INFO: Service rm3 in namespace kubectl-798 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:21:51.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-798" for this suite.
Dec  5 19:22:13.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:22:13.461: INFO: namespace kubectl-798 deletion completed in 22.150936859s

• [SLOW TEST:29.889 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:22:13.462: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec  5 19:22:13.528: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-4390,SelfLink:/api/v1/namespaces/watch-4390/configmaps/e2e-watch-test-resource-version,UID:8a87b3c0-1794-11ea-b857-a68228aedf28,ResourceVersion:35127,Generation:0,CreationTimestamp:2019-12-05 19:22:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  5 19:22:13.528: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-4390,SelfLink:/api/v1/namespaces/watch-4390/configmaps/e2e-watch-test-resource-version,UID:8a87b3c0-1794-11ea-b857-a68228aedf28,ResourceVersion:35128,Generation:0,CreationTimestamp:2019-12-05 19:22:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:22:13.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4390" for this suite.
Dec  5 19:22:19.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:22:19.656: INFO: namespace watch-4390 deletion completed in 6.123044049s

• [SLOW TEST:6.194 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:22:19.656: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:22:25.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4022" for this suite.
Dec  5 19:22:31.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:22:31.956: INFO: namespace namespaces-4022 deletion completed in 6.151438955s
STEP: Destroying namespace "nsdeletetest-4041" for this suite.
Dec  5 19:22:31.960: INFO: Namespace nsdeletetest-4041 was already deleted
STEP: Destroying namespace "nsdeletetest-793" for this suite.
Dec  5 19:22:37.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:22:38.098: INFO: namespace nsdeletetest-793 deletion completed in 6.137879096s

• [SLOW TEST:18.442 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:22:38.098: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-9936f9b3-1794-11ea-a680-be755c6bedde
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:22:38.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7494" for this suite.
Dec  5 19:22:44.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:22:44.274: INFO: namespace configmap-7494 deletion completed in 6.133804662s

• [SLOW TEST:6.175 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:22:44.274: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 19:22:44.319: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec  5 19:22:49.324: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  5 19:22:49.324: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  5 19:22:53.366: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-5929,SelfLink:/apis/apps/v1/namespaces/deployment-5929/deployments/test-cleanup-deployment,UID:9fe38e87-1794-11ea-b857-a68228aedf28,ResourceVersion:35303,Generation:1,CreationTimestamp:2019-12-05 19:22:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-05 19:22:49 +0000 UTC 2019-12-05 19:22:49 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-05 19:22:51 +0000 UTC 2019-12-05 19:22:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-6865c98b76" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  5 19:22:53.372: INFO: New ReplicaSet "test-cleanup-deployment-6865c98b76" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-6865c98b76,GenerateName:,Namespace:deployment-5929,SelfLink:/apis/apps/v1/namespaces/deployment-5929/replicasets/test-cleanup-deployment-6865c98b76,UID:9fe64178-1794-11ea-9811-dae9897b3b7b,ResourceVersion:35292,Generation:1,CreationTimestamp:2019-12-05 19:22:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 9fe38e87-1794-11ea-b857-a68228aedf28 0xc001dac727 0xc001dac728}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  5 19:22:53.377: INFO: Pod "test-cleanup-deployment-6865c98b76-gf25q" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-6865c98b76-gf25q,GenerateName:test-cleanup-deployment-6865c98b76-,Namespace:deployment-5929,SelfLink:/api/v1/namespaces/deployment-5929/pods/test-cleanup-deployment-6865c98b76-gf25q,UID:9fe6ef18-1794-11ea-9811-dae9897b3b7b,ResourceVersion:35291,Generation:0,CreationTimestamp:2019-12-05 19:22:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-6865c98b76 9fe64178-1794-11ea-9811-dae9897b3b7b 0xc001dad0c7 0xc001dad0c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bghc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bghc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-bghc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:crayfish,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001dad1c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001dad1e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:22:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:22:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:22:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:22:49 +0000 UTC  }],Message:,Reason:,HostIP:178.62.90.178,PodIP:25.0.2.203,StartTime:2019-12-05 19:22:49 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-05 19:22:50 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://80b777d241586d5f4f9607867354a7cb3d62bfe2386a21a743159df98a25b724}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:22:53.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5929" for this suite.
Dec  5 19:22:59.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:22:59.518: INFO: namespace deployment-5929 deletion completed in 6.135079507s

• [SLOW TEST:15.244 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:22:59.522: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-a5fc54b9-1794-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume configMaps
Dec  5 19:22:59.576: INFO: Waiting up to 5m0s for pod "pod-configmaps-a5fd7790-1794-11ea-a680-be755c6bedde" in namespace "configmap-819" to be "success or failure"
Dec  5 19:22:59.581: INFO: Pod "pod-configmaps-a5fd7790-1794-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 5.074376ms
Dec  5 19:23:01.587: INFO: Pod "pod-configmaps-a5fd7790-1794-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011348084s
Dec  5 19:23:03.593: INFO: Pod "pod-configmaps-a5fd7790-1794-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017636045s
STEP: Saw pod success
Dec  5 19:23:03.593: INFO: Pod "pod-configmaps-a5fd7790-1794-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:23:03.598: INFO: Trying to get logs from node crayfish pod pod-configmaps-a5fd7790-1794-11ea-a680-be755c6bedde container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 19:23:03.628: INFO: Waiting for pod pod-configmaps-a5fd7790-1794-11ea-a680-be755c6bedde to disappear
Dec  5 19:23:03.632: INFO: Pod pod-configmaps-a5fd7790-1794-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:23:03.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-819" for this suite.
Dec  5 19:23:09.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:23:09.787: INFO: namespace configmap-819 deletion completed in 6.148552313s

• [SLOW TEST:10.265 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:23:09.787: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 19:23:09.821: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:23:14.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4945" for this suite.
Dec  5 19:23:56.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:23:56.147: INFO: namespace pods-4945 deletion completed in 42.141459579s

• [SLOW TEST:46.360 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:23:56.148: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-4296
Dec  5 19:23:58.203: INFO: Started pod liveness-http in namespace container-probe-4296
STEP: checking the pod's current state and verifying that restartCount is present
Dec  5 19:23:58.208: INFO: Initial restart count of pod liveness-http is 0
Dec  5 19:24:16.263: INFO: Restart count of pod container-probe-4296/liveness-http is now 1 (18.054936864s elapsed)
Dec  5 19:24:34.317: INFO: Restart count of pod container-probe-4296/liveness-http is now 2 (36.108998769s elapsed)
Dec  5 19:24:54.381: INFO: Restart count of pod container-probe-4296/liveness-http is now 3 (56.17234936s elapsed)
Dec  5 19:25:16.450: INFO: Restart count of pod container-probe-4296/liveness-http is now 4 (1m18.241329534s elapsed)
Dec  5 19:26:18.624: INFO: Restart count of pod container-probe-4296/liveness-http is now 5 (2m20.415756564s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:26:18.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4296" for this suite.
Dec  5 19:26:24.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:26:24.786: INFO: namespace container-probe-4296 deletion completed in 6.13783878s

• [SLOW TEST:148.638 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:26:24.786: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  5 19:26:28.887: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 19:26:28.892: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 19:26:30.892: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 19:26:30.898: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 19:26:32.892: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 19:26:32.898: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 19:26:34.892: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 19:26:34.898: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 19:26:36.892: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 19:26:36.899: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 19:26:38.892: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 19:26:38.897: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 19:26:40.892: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 19:26:40.898: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 19:26:42.892: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 19:26:42.898: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 19:26:44.892: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 19:26:44.898: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 19:26:46.892: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 19:26:46.898: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 19:26:48.892: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 19:26:48.898: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:26:48.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4655" for this suite.
Dec  5 19:27:10.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:27:11.044: INFO: namespace container-lifecycle-hook-4655 deletion completed in 22.139650559s

• [SLOW TEST:46.258 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:27:11.045: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Dec  5 19:27:11.080: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:27:15.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4313" for this suite.
Dec  5 19:27:21.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:27:21.312: INFO: namespace init-container-4313 deletion completed in 6.14211837s

• [SLOW TEST:10.268 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:27:21.314: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-42071a16-1795-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume secrets
Dec  5 19:27:21.404: INFO: Waiting up to 5m0s for pod "pod-secrets-420d9a1c-1795-11ea-a680-be755c6bedde" in namespace "secrets-3" to be "success or failure"
Dec  5 19:27:21.410: INFO: Pod "pod-secrets-420d9a1c-1795-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 5.695876ms
Dec  5 19:27:23.418: INFO: Pod "pod-secrets-420d9a1c-1795-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013463061s
STEP: Saw pod success
Dec  5 19:27:23.418: INFO: Pod "pod-secrets-420d9a1c-1795-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:27:23.422: INFO: Trying to get logs from node crayfish pod pod-secrets-420d9a1c-1795-11ea-a680-be755c6bedde container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 19:27:23.454: INFO: Waiting for pod pod-secrets-420d9a1c-1795-11ea-a680-be755c6bedde to disappear
Dec  5 19:27:23.458: INFO: Pod pod-secrets-420d9a1c-1795-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:27:23.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3" for this suite.
Dec  5 19:27:29.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:27:29.597: INFO: namespace secrets-3 deletion completed in 6.13395922s
STEP: Destroying namespace "secret-namespace-8284" for this suite.
Dec  5 19:27:35.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:27:35.736: INFO: namespace secret-namespace-8284 deletion completed in 6.138599939s

• [SLOW TEST:14.422 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:27:35.736: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Dec  5 19:27:35.771: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-935805400 proxy --unix-socket=/tmp/kubectl-proxy-unix823197015/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:27:35.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1716" for this suite.
Dec  5 19:27:41.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:27:42.020: INFO: namespace kubectl-1716 deletion completed in 6.148745427s

• [SLOW TEST:6.283 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:27:42.020: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Dec  5 19:27:42.059: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:27:45.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4030" for this suite.
Dec  5 19:28:07.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:28:07.605: INFO: namespace init-container-4030 deletion completed in 22.143134627s

• [SLOW TEST:25.585 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:28:07.605: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  5 19:28:07.654: INFO: Waiting up to 5m0s for pod "pod-5d9e5683-1795-11ea-a680-be755c6bedde" in namespace "emptydir-8845" to be "success or failure"
Dec  5 19:28:07.660: INFO: Pod "pod-5d9e5683-1795-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 5.403085ms
Dec  5 19:28:09.666: INFO: Pod "pod-5d9e5683-1795-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011408562s
Dec  5 19:28:11.671: INFO: Pod "pod-5d9e5683-1795-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017139105s
STEP: Saw pod success
Dec  5 19:28:11.672: INFO: Pod "pod-5d9e5683-1795-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:28:11.676: INFO: Trying to get logs from node crayfish pod pod-5d9e5683-1795-11ea-a680-be755c6bedde container test-container: <nil>
STEP: delete the pod
Dec  5 19:28:11.707: INFO: Waiting for pod pod-5d9e5683-1795-11ea-a680-be755c6bedde to disappear
Dec  5 19:28:11.710: INFO: Pod pod-5d9e5683-1795-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:28:11.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8845" for this suite.
Dec  5 19:28:17.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:28:17.852: INFO: namespace emptydir-8845 deletion completed in 6.136189292s

• [SLOW TEST:10.247 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:28:17.852: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  5 19:28:17.918: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:17.918: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:17.918: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:17.921: INFO: Number of nodes with available pods: 0
Dec  5 19:28:17.921: INFO: Node crayfish is running more than one daemon pod
Dec  5 19:28:18.982: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:18.982: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:18.982: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:18.988: INFO: Number of nodes with available pods: 0
Dec  5 19:28:18.988: INFO: Node crayfish is running more than one daemon pod
Dec  5 19:28:19.928: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:19.928: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:19.928: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:19.933: INFO: Number of nodes with available pods: 1
Dec  5 19:28:19.933: INFO: Node crayfish is running more than one daemon pod
Dec  5 19:28:20.937: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:20.937: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:20.937: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:20.942: INFO: Number of nodes with available pods: 2
Dec  5 19:28:20.942: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec  5 19:28:20.968: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:20.969: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:20.969: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:20.973: INFO: Number of nodes with available pods: 1
Dec  5 19:28:20.973: INFO: Node puma is running more than one daemon pod
Dec  5 19:28:21.980: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:21.980: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:21.980: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:21.985: INFO: Number of nodes with available pods: 1
Dec  5 19:28:21.985: INFO: Node puma is running more than one daemon pod
Dec  5 19:28:22.980: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:22.980: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:22.980: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:22.985: INFO: Number of nodes with available pods: 1
Dec  5 19:28:22.985: INFO: Node puma is running more than one daemon pod
Dec  5 19:28:23.980: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:23.980: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:23.980: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:23.985: INFO: Number of nodes with available pods: 1
Dec  5 19:28:23.985: INFO: Node puma is running more than one daemon pod
Dec  5 19:28:24.980: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:24.980: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:24.980: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:24.985: INFO: Number of nodes with available pods: 1
Dec  5 19:28:24.985: INFO: Node puma is running more than one daemon pod
Dec  5 19:28:25.981: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:25.981: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:25.981: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:25.986: INFO: Number of nodes with available pods: 1
Dec  5 19:28:25.986: INFO: Node puma is running more than one daemon pod
Dec  5 19:28:26.980: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:26.980: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:26.980: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:26.985: INFO: Number of nodes with available pods: 1
Dec  5 19:28:26.985: INFO: Node puma is running more than one daemon pod
Dec  5 19:28:27.980: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:27.980: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:27.980: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:27.985: INFO: Number of nodes with available pods: 1
Dec  5 19:28:27.985: INFO: Node puma is running more than one daemon pod
Dec  5 19:28:28.980: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:28.980: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:28.980: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:28.984: INFO: Number of nodes with available pods: 1
Dec  5 19:28:28.984: INFO: Node puma is running more than one daemon pod
Dec  5 19:28:29.980: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:29.980: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:29.980: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:29.985: INFO: Number of nodes with available pods: 1
Dec  5 19:28:29.985: INFO: Node puma is running more than one daemon pod
Dec  5 19:28:30.980: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:30.980: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:30.980: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:30.985: INFO: Number of nodes with available pods: 1
Dec  5 19:28:30.985: INFO: Node puma is running more than one daemon pod
Dec  5 19:28:31.980: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:31.980: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:31.980: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:31.985: INFO: Number of nodes with available pods: 1
Dec  5 19:28:31.985: INFO: Node puma is running more than one daemon pod
Dec  5 19:28:32.980: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:32.980: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:32.980: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:32.985: INFO: Number of nodes with available pods: 1
Dec  5 19:28:32.985: INFO: Node puma is running more than one daemon pod
Dec  5 19:28:33.980: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:33.980: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:33.980: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:33.984: INFO: Number of nodes with available pods: 1
Dec  5 19:28:33.985: INFO: Node puma is running more than one daemon pod
Dec  5 19:28:34.982: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:34.982: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:34.982: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:34.987: INFO: Number of nodes with available pods: 1
Dec  5 19:28:34.987: INFO: Node puma is running more than one daemon pod
Dec  5 19:28:35.980: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:35.980: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:35.980: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:35.985: INFO: Number of nodes with available pods: 1
Dec  5 19:28:35.985: INFO: Node puma is running more than one daemon pod
Dec  5 19:28:36.980: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:36.980: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:36.980: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:28:36.985: INFO: Number of nodes with available pods: 2
Dec  5 19:28:36.985: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1972, will wait for the garbage collector to delete the pods
Dec  5 19:28:37.056: INFO: Deleting DaemonSet.extensions daemon-set took: 10.657365ms
Dec  5 19:28:37.356: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.470208ms
Dec  5 19:28:44.262: INFO: Number of nodes with available pods: 0
Dec  5 19:28:44.262: INFO: Number of running nodes: 0, number of available pods: 0
Dec  5 19:28:44.269: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1972/daemonsets","resourceVersion":"36347"},"items":null}

Dec  5 19:28:44.273: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1972/pods","resourceVersion":"36347"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:28:44.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1972" for this suite.
Dec  5 19:28:50.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:28:50.428: INFO: namespace daemonsets-1972 deletion completed in 6.134609264s

• [SLOW TEST:32.576 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:28:50.428: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  5 19:28:50.474: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77241da2-1795-11ea-a680-be755c6bedde" in namespace "downward-api-9760" to be "success or failure"
Dec  5 19:28:50.478: INFO: Pod "downwardapi-volume-77241da2-1795-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.564723ms
Dec  5 19:28:52.485: INFO: Pod "downwardapi-volume-77241da2-1795-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010832855s
STEP: Saw pod success
Dec  5 19:28:52.485: INFO: Pod "downwardapi-volume-77241da2-1795-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:28:52.489: INFO: Trying to get logs from node puma pod downwardapi-volume-77241da2-1795-11ea-a680-be755c6bedde container client-container: <nil>
STEP: delete the pod
Dec  5 19:28:52.519: INFO: Waiting for pod downwardapi-volume-77241da2-1795-11ea-a680-be755c6bedde to disappear
Dec  5 19:28:52.523: INFO: Pod downwardapi-volume-77241da2-1795-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:28:52.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9760" for this suite.
Dec  5 19:28:58.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:28:58.658: INFO: namespace downward-api-9760 deletion completed in 6.128763519s

• [SLOW TEST:8.229 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:28:58.658: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:29:22.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5198" for this suite.
Dec  5 19:29:29.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:29:29.122: INFO: namespace container-runtime-5198 deletion completed in 6.134457564s

• [SLOW TEST:30.464 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:29:29.136: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Dec  5 19:29:29.182: INFO: Waiting up to 5m0s for pod "downward-api-8e36180e-1795-11ea-a680-be755c6bedde" in namespace "downward-api-9226" to be "success or failure"
Dec  5 19:29:29.187: INFO: Pod "downward-api-8e36180e-1795-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 5.086259ms
Dec  5 19:29:31.193: INFO: Pod "downward-api-8e36180e-1795-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010693952s
STEP: Saw pod success
Dec  5 19:29:31.193: INFO: Pod "downward-api-8e36180e-1795-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:29:31.198: INFO: Trying to get logs from node puma pod downward-api-8e36180e-1795-11ea-a680-be755c6bedde container dapi-container: <nil>
STEP: delete the pod
Dec  5 19:29:31.222: INFO: Waiting for pod downward-api-8e36180e-1795-11ea-a680-be755c6bedde to disappear
Dec  5 19:29:31.226: INFO: Pod downward-api-8e36180e-1795-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:29:31.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9226" for this suite.
Dec  5 19:29:37.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:29:37.375: INFO: namespace downward-api-9226 deletion completed in 6.143794151s

• [SLOW TEST:8.239 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:29:37.375: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Dec  5 19:29:37.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 create -f - --namespace=kubectl-9295'
Dec  5 19:29:37.805: INFO: stderr: ""
Dec  5 19:29:37.805: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  5 19:29:37.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9295'
Dec  5 19:29:37.878: INFO: stderr: ""
Dec  5 19:29:37.878: INFO: stdout: "update-demo-nautilus-64j46 update-demo-nautilus-plkth "
Dec  5 19:29:37.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-64j46 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9295'
Dec  5 19:29:37.951: INFO: stderr: ""
Dec  5 19:29:37.951: INFO: stdout: ""
Dec  5 19:29:37.951: INFO: update-demo-nautilus-64j46 is created but not running
Dec  5 19:29:42.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9295'
Dec  5 19:29:43.079: INFO: stderr: ""
Dec  5 19:29:43.079: INFO: stdout: "update-demo-nautilus-64j46 update-demo-nautilus-plkth "
Dec  5 19:29:43.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-64j46 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9295'
Dec  5 19:29:43.165: INFO: stderr: ""
Dec  5 19:29:43.165: INFO: stdout: "true"
Dec  5 19:29:43.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-64j46 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9295'
Dec  5 19:29:43.244: INFO: stderr: ""
Dec  5 19:29:43.244: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 19:29:43.244: INFO: validating pod update-demo-nautilus-64j46
Dec  5 19:29:43.250: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 19:29:43.250: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 19:29:43.250: INFO: update-demo-nautilus-64j46 is verified up and running
Dec  5 19:29:43.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-plkth -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9295'
Dec  5 19:29:43.321: INFO: stderr: ""
Dec  5 19:29:43.321: INFO: stdout: "true"
Dec  5 19:29:43.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-plkth -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9295'
Dec  5 19:29:43.398: INFO: stderr: ""
Dec  5 19:29:43.398: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 19:29:43.398: INFO: validating pod update-demo-nautilus-plkth
Dec  5 19:29:43.405: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 19:29:43.405: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 19:29:43.405: INFO: update-demo-nautilus-plkth is verified up and running
STEP: using delete to clean up resources
Dec  5 19:29:43.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 delete --grace-period=0 --force -f - --namespace=kubectl-9295'
Dec  5 19:29:43.501: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 19:29:43.501: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  5 19:29:43.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9295'
Dec  5 19:29:43.583: INFO: stderr: "No resources found.\n"
Dec  5 19:29:43.583: INFO: stdout: ""
Dec  5 19:29:43.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -l name=update-demo --namespace=kubectl-9295 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  5 19:29:43.659: INFO: stderr: ""
Dec  5 19:29:43.659: INFO: stdout: "update-demo-nautilus-64j46\nupdate-demo-nautilus-plkth\n"
Dec  5 19:29:44.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9295'
Dec  5 19:29:44.249: INFO: stderr: "No resources found.\n"
Dec  5 19:29:44.249: INFO: stdout: ""
Dec  5 19:29:44.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -l name=update-demo --namespace=kubectl-9295 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  5 19:29:44.335: INFO: stderr: ""
Dec  5 19:29:44.335: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:29:44.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9295" for this suite.
Dec  5 19:30:06.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:30:06.494: INFO: namespace kubectl-9295 deletion completed in 22.153710215s

• [SLOW TEST:29.119 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:30:06.494: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Dec  5 19:30:06.536: INFO: Waiting up to 5m0s for pod "client-containers-a47a4d18-1795-11ea-a680-be755c6bedde" in namespace "containers-3065" to be "success or failure"
Dec  5 19:30:06.539: INFO: Pod "client-containers-a47a4d18-1795-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.122682ms
Dec  5 19:30:08.544: INFO: Pod "client-containers-a47a4d18-1795-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008098015s
STEP: Saw pod success
Dec  5 19:30:08.544: INFO: Pod "client-containers-a47a4d18-1795-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:30:08.548: INFO: Trying to get logs from node crayfish pod client-containers-a47a4d18-1795-11ea-a680-be755c6bedde container test-container: <nil>
STEP: delete the pod
Dec  5 19:30:08.572: INFO: Waiting for pod client-containers-a47a4d18-1795-11ea-a680-be755c6bedde to disappear
Dec  5 19:30:08.575: INFO: Pod client-containers-a47a4d18-1795-11ea-a680-be755c6bedde no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:30:08.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3065" for this suite.
Dec  5 19:30:14.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:30:14.733: INFO: namespace containers-3065 deletion completed in 6.152472159s

• [SLOW TEST:8.239 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:30:14.733: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1384
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 19:30:14.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-9059'
Dec  5 19:30:14.882: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  5 19:30:14.882: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1390
Dec  5 19:30:14.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 delete deployment e2e-test-nginx-deployment --namespace=kubectl-9059'
Dec  5 19:30:14.982: INFO: stderr: ""
Dec  5 19:30:14.982: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:30:14.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9059" for this suite.
Dec  5 19:30:37.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:30:37.124: INFO: namespace kubectl-9059 deletion completed in 22.136848821s

• [SLOW TEST:22.391 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:30:37.129: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  5 19:30:37.180: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6be52e6-1795-11ea-a680-be755c6bedde" in namespace "projected-4270" to be "success or failure"
Dec  5 19:30:37.183: INFO: Pod "downwardapi-volume-b6be52e6-1795-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.487689ms
Dec  5 19:30:39.189: INFO: Pod "downwardapi-volume-b6be52e6-1795-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008723815s
STEP: Saw pod success
Dec  5 19:30:39.189: INFO: Pod "downwardapi-volume-b6be52e6-1795-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:30:39.194: INFO: Trying to get logs from node crayfish pod downwardapi-volume-b6be52e6-1795-11ea-a680-be755c6bedde container client-container: <nil>
STEP: delete the pod
Dec  5 19:30:39.218: INFO: Waiting for pod downwardapi-volume-b6be52e6-1795-11ea-a680-be755c6bedde to disappear
Dec  5 19:30:39.222: INFO: Pod downwardapi-volume-b6be52e6-1795-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:30:39.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4270" for this suite.
Dec  5 19:30:45.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:30:45.357: INFO: namespace projected-4270 deletion completed in 6.130659375s

• [SLOW TEST:8.229 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:30:45.361: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Dec  5 19:30:47.952: INFO: Successfully updated pod "annotationupdatebba56db9-1795-11ea-a680-be755c6bedde"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:30:49.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4752" for this suite.
Dec  5 19:31:12.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:31:12.127: INFO: namespace projected-4752 deletion completed in 22.143988958s

• [SLOW TEST:26.767 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:31:12.127: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-7017
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7017 to expose endpoints map[]
Dec  5 19:31:12.186: INFO: successfully validated that service endpoint-test2 in namespace services-7017 exposes endpoints map[] (4.09065ms elapsed)
STEP: Creating pod pod1 in namespace services-7017
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7017 to expose endpoints map[pod1:[80]]
Dec  5 19:31:14.224: INFO: successfully validated that service endpoint-test2 in namespace services-7017 exposes endpoints map[pod1:[80]] (2.028259443s elapsed)
STEP: Creating pod pod2 in namespace services-7017
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7017 to expose endpoints map[pod1:[80] pod2:[80]]
Dec  5 19:31:17.286: INFO: successfully validated that service endpoint-test2 in namespace services-7017 exposes endpoints map[pod1:[80] pod2:[80]] (3.055219507s elapsed)
STEP: Deleting pod pod1 in namespace services-7017
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7017 to expose endpoints map[pod2:[80]]
Dec  5 19:31:17.307: INFO: successfully validated that service endpoint-test2 in namespace services-7017 exposes endpoints map[pod2:[80]] (11.2132ms elapsed)
STEP: Deleting pod pod2 in namespace services-7017
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7017 to expose endpoints map[]
Dec  5 19:31:17.320: INFO: successfully validated that service endpoint-test2 in namespace services-7017 exposes endpoints map[] (3.741506ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:31:17.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7017" for this suite.
Dec  5 19:31:39.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:31:39.508: INFO: namespace services-7017 deletion completed in 22.155118234s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:27.380 seconds]
[sig-network] Services
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:31:39.508: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  5 19:31:39.554: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dbeb8989-1795-11ea-a680-be755c6bedde" in namespace "projected-2130" to be "success or failure"
Dec  5 19:31:39.557: INFO: Pod "downwardapi-volume-dbeb8989-1795-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.227138ms
Dec  5 19:31:41.563: INFO: Pod "downwardapi-volume-dbeb8989-1795-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008486423s
STEP: Saw pod success
Dec  5 19:31:41.563: INFO: Pod "downwardapi-volume-dbeb8989-1795-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:31:41.567: INFO: Trying to get logs from node crayfish pod downwardapi-volume-dbeb8989-1795-11ea-a680-be755c6bedde container client-container: <nil>
STEP: delete the pod
Dec  5 19:31:41.592: INFO: Waiting for pod downwardapi-volume-dbeb8989-1795-11ea-a680-be755c6bedde to disappear
Dec  5 19:31:41.595: INFO: Pod downwardapi-volume-dbeb8989-1795-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:31:41.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2130" for this suite.
Dec  5 19:31:47.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:31:47.745: INFO: namespace projected-2130 deletion completed in 6.145230294s

• [SLOW TEST:8.237 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:31:47.750: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec  5 19:31:47.794: INFO: Waiting up to 5m0s for pod "pod-e0d519ad-1795-11ea-a680-be755c6bedde" in namespace "emptydir-586" to be "success or failure"
Dec  5 19:31:47.799: INFO: Pod "pod-e0d519ad-1795-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.976746ms
Dec  5 19:31:49.804: INFO: Pod "pod-e0d519ad-1795-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01061216s
STEP: Saw pod success
Dec  5 19:31:49.805: INFO: Pod "pod-e0d519ad-1795-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:31:49.809: INFO: Trying to get logs from node puma pod pod-e0d519ad-1795-11ea-a680-be755c6bedde container test-container: <nil>
STEP: delete the pod
Dec  5 19:31:49.837: INFO: Waiting for pod pod-e0d519ad-1795-11ea-a680-be755c6bedde to disappear
Dec  5 19:31:49.841: INFO: Pod pod-e0d519ad-1795-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:31:49.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-586" for this suite.
Dec  5 19:31:55.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:31:55.977: INFO: namespace emptydir-586 deletion completed in 6.130865097s

• [SLOW TEST:8.227 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:31:55.977: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Dec  5 19:31:58.559: INFO: Successfully updated pod "labelsupdatee5bbdb25-1795-11ea-a680-be755c6bedde"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:32:00.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1947" for this suite.
Dec  5 19:32:22.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:32:22.722: INFO: namespace downward-api-1947 deletion completed in 22.132229858s

• [SLOW TEST:26.744 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:32:22.722: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 19:32:22.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 create -f - --namespace=kubectl-6024'
Dec  5 19:32:23.073: INFO: stderr: ""
Dec  5 19:32:23.073: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec  5 19:32:23.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 create -f - --namespace=kubectl-6024'
Dec  5 19:32:23.345: INFO: stderr: ""
Dec  5 19:32:23.345: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  5 19:32:24.352: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 19:32:24.352: INFO: Found 0 / 1
Dec  5 19:32:25.351: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 19:32:25.351: INFO: Found 1 / 1
Dec  5 19:32:25.351: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  5 19:32:25.356: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 19:32:25.356: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  5 19:32:25.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 describe pod redis-master-7gm75 --namespace=kubectl-6024'
Dec  5 19:32:25.489: INFO: stderr: ""
Dec  5 19:32:25.489: INFO: stdout: "Name:               redis-master-7gm75\nNamespace:          kubectl-6024\nPriority:           0\nPriorityClassName:  <none>\nNode:               puma/178.62.90.156\nStart Time:         Thu, 05 Dec 2019 19:32:23 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 25.0.1.208\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://f1a5cd7bcfdac1bf276f38660afbefc5cfd887ecc8cb50d2a8321bfba8bf4161\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 05 Dec 2019 19:32:24 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-xlgrq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-xlgrq:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-xlgrq\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-6024/redis-master-7gm75 to puma\n  Normal  Pulled     1s    kubelet, puma      Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, puma      Created container redis-master\n  Normal  Started    1s    kubelet, puma      Started container redis-master\n"
Dec  5 19:32:25.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 describe rc redis-master --namespace=kubectl-6024'
Dec  5 19:32:25.637: INFO: stderr: ""
Dec  5 19:32:25.637: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-6024\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-7gm75\n"
Dec  5 19:32:25.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 describe service redis-master --namespace=kubectl-6024'
Dec  5 19:32:25.722: INFO: stderr: ""
Dec  5 19:32:25.722: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-6024\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.109.91.190\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         25.0.1.208:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec  5 19:32:25.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 describe node crayfish'
Dec  5 19:32:25.827: INFO: stderr: ""
Dec  5 19:32:25.827: INFO: stdout: "Name:               crayfish\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=crayfish\n                    kubernetes.io/os=linux\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"32:31:6b:0b:df:b5\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 178.62.90.178\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 05 Dec 2019 15:20:56 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 05 Dec 2019 19:31:30 +0000   Thu, 05 Dec 2019 15:20:56 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 05 Dec 2019 19:31:30 +0000   Thu, 05 Dec 2019 15:20:56 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 05 Dec 2019 19:31:30 +0000   Thu, 05 Dec 2019 15:20:56 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 05 Dec 2019 19:31:30 +0000   Thu, 05 Dec 2019 15:21:06 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  178.62.90.178\n  Hostname:    crayfish\nCapacity:\n cpu:                8\n ephemeral-storage:  101445540Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16424836Ki\n pods:               110\nAllocatable:\n cpu:                8\n ephemeral-storage:  93492209510\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16322436Ki\n pods:               110\nSystem Info:\n Machine ID:                 a88ae08dee6146edbe6068a95daeff83\n System UUID:                A88AE08D-EE61-46ED-BE60-68A95DAEFF83\n Boot ID:                    e82db44d-548d-4c5b-94a8-0954470509c0\n Kernel Version:             4.15.0-72-generic\n OS Image:                   Ubuntu 18.04.3 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.6\n Kubelet Version:            v1.14.8\n Kube-Proxy Version:         v1.14.8\nPodCIDR:                     25.0.2.0/24\nNon-terminated Pods:         (4 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                kube-flannel-ds-amd64-zqml7                                100m (1%)     100m (1%)   50Mi (0%)        50Mi (0%)      4h11m\n  kube-system                kube-proxy-xb74r                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h11m\n  sonobuoy                   sonobuoy-e2e-job-69046a85250743c3                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         24m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-892d09b6f39b465a-wx8xw    0 (0%)        0 (0%)      0 (0%)           0 (0%)         24m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                100m (1%)  100m (1%)\n  memory             50Mi (0%)  50Mi (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\nEvents:              <none>\n"
Dec  5 19:32:25.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 describe namespace kubectl-6024'
Dec  5 19:32:25.909: INFO: stderr: ""
Dec  5 19:32:25.909: INFO: stdout: "Name:         kubectl-6024\nLabels:       e2e-framework=kubectl\n              e2e-run=96c3a06f-1792-11ea-a680-be755c6bedde\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:32:25.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6024" for this suite.
Dec  5 19:32:47.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:32:48.059: INFO: namespace kubectl-6024 deletion completed in 22.145959553s

• [SLOW TEST:25.337 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:32:48.060: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-1243
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1243 to expose endpoints map[]
Dec  5 19:32:48.113: INFO: Get endpoints failed (3.462467ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Dec  5 19:32:49.118: INFO: successfully validated that service multi-endpoint-test in namespace services-1243 exposes endpoints map[] (1.00797145s elapsed)
STEP: Creating pod pod1 in namespace services-1243
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1243 to expose endpoints map[pod1:[100]]
Dec  5 19:32:51.160: INFO: successfully validated that service multi-endpoint-test in namespace services-1243 exposes endpoints map[pod1:[100]] (2.032566508s elapsed)
STEP: Creating pod pod2 in namespace services-1243
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1243 to expose endpoints map[pod1:[100] pod2:[101]]
Dec  5 19:32:54.222: INFO: successfully validated that service multi-endpoint-test in namespace services-1243 exposes endpoints map[pod1:[100] pod2:[101]] (3.05597827s elapsed)
STEP: Deleting pod pod1 in namespace services-1243
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1243 to expose endpoints map[pod2:[101]]
Dec  5 19:32:54.242: INFO: successfully validated that service multi-endpoint-test in namespace services-1243 exposes endpoints map[pod2:[101]] (10.554334ms elapsed)
STEP: Deleting pod pod2 in namespace services-1243
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1243 to expose endpoints map[]
Dec  5 19:32:54.256: INFO: successfully validated that service multi-endpoint-test in namespace services-1243 exposes endpoints map[] (4.779316ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:32:54.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1243" for this suite.
Dec  5 19:33:16.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:33:16.427: INFO: namespace services-1243 deletion completed in 22.139627662s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:28.368 seconds]
[sig-network] Services
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:33:16.428: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-15b07a2a-1796-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume secrets
Dec  5 19:33:16.480: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-15b1a71a-1796-11ea-a680-be755c6bedde" in namespace "projected-1507" to be "success or failure"
Dec  5 19:33:16.484: INFO: Pod "pod-projected-secrets-15b1a71a-1796-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.871553ms
Dec  5 19:33:18.488: INFO: Pod "pod-projected-secrets-15b1a71a-1796-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008655469s
STEP: Saw pod success
Dec  5 19:33:18.488: INFO: Pod "pod-projected-secrets-15b1a71a-1796-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:33:18.493: INFO: Trying to get logs from node crayfish pod pod-projected-secrets-15b1a71a-1796-11ea-a680-be755c6bedde container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  5 19:33:18.518: INFO: Waiting for pod pod-projected-secrets-15b1a71a-1796-11ea-a680-be755c6bedde to disappear
Dec  5 19:33:18.521: INFO: Pod pod-projected-secrets-15b1a71a-1796-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:33:18.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1507" for this suite.
Dec  5 19:33:24.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:33:24.656: INFO: namespace projected-1507 deletion completed in 6.130558542s

• [SLOW TEST:8.229 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:33:24.657: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  5 19:33:24.707: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a98b269-1796-11ea-a680-be755c6bedde" in namespace "projected-6184" to be "success or failure"
Dec  5 19:33:24.711: INFO: Pod "downwardapi-volume-1a98b269-1796-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.986602ms
Dec  5 19:33:26.717: INFO: Pod "downwardapi-volume-1a98b269-1796-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010066341s
STEP: Saw pod success
Dec  5 19:33:26.717: INFO: Pod "downwardapi-volume-1a98b269-1796-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:33:26.722: INFO: Trying to get logs from node puma pod downwardapi-volume-1a98b269-1796-11ea-a680-be755c6bedde container client-container: <nil>
STEP: delete the pod
Dec  5 19:33:26.750: INFO: Waiting for pod downwardapi-volume-1a98b269-1796-11ea-a680-be755c6bedde to disappear
Dec  5 19:33:26.754: INFO: Pod downwardapi-volume-1a98b269-1796-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:33:26.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6184" for this suite.
Dec  5 19:33:32.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:33:32.897: INFO: namespace projected-6184 deletion completed in 6.134667845s

• [SLOW TEST:8.241 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:33:32.898: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Dec  5 19:33:32.943: INFO: Waiting up to 5m0s for pod "downward-api-1f81a1b9-1796-11ea-a680-be755c6bedde" in namespace "downward-api-2345" to be "success or failure"
Dec  5 19:33:32.947: INFO: Pod "downward-api-1f81a1b9-1796-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.851425ms
Dec  5 19:33:34.954: INFO: Pod "downward-api-1f81a1b9-1796-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010167159s
STEP: Saw pod success
Dec  5 19:33:34.954: INFO: Pod "downward-api-1f81a1b9-1796-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:33:34.959: INFO: Trying to get logs from node puma pod downward-api-1f81a1b9-1796-11ea-a680-be755c6bedde container dapi-container: <nil>
STEP: delete the pod
Dec  5 19:33:34.987: INFO: Waiting for pod downward-api-1f81a1b9-1796-11ea-a680-be755c6bedde to disappear
Dec  5 19:33:34.991: INFO: Pod downward-api-1f81a1b9-1796-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:33:34.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2345" for this suite.
Dec  5 19:33:41.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:33:41.133: INFO: namespace downward-api-2345 deletion completed in 6.136889427s

• [SLOW TEST:8.236 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:33:41.137: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Dec  5 19:33:41.175: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  5 19:33:41.184: INFO: Waiting for terminating namespaces to be deleted...
Dec  5 19:33:41.187: INFO: 
Logging pods the kubelet thinks is on node crayfish before test
Dec  5 19:33:41.194: INFO: kube-proxy-xb74r from kube-system started at 2019-12-05 15:20:56 +0000 UTC (1 container statuses recorded)
Dec  5 19:33:41.194: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  5 19:33:41.194: INFO: sonobuoy-e2e-job-69046a85250743c3 from sonobuoy started at 2019-12-05 19:08:12 +0000 UTC (2 container statuses recorded)
Dec  5 19:33:41.194: INFO: 	Container e2e ready: true, restart count 0
Dec  5 19:33:41.194: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 19:33:41.194: INFO: sonobuoy-systemd-logs-daemon-set-892d09b6f39b465a-wx8xw from sonobuoy started at 2019-12-05 19:08:12 +0000 UTC (2 container statuses recorded)
Dec  5 19:33:41.194: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 19:33:41.194: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  5 19:33:41.194: INFO: kube-flannel-ds-amd64-zqml7 from kube-system started at 2019-12-05 15:20:56 +0000 UTC (1 container statuses recorded)
Dec  5 19:33:41.194: INFO: 	Container kube-flannel ready: true, restart count 0
Dec  5 19:33:41.194: INFO: 
Logging pods the kubelet thinks is on node puma before test
Dec  5 19:33:41.201: INFO: sonobuoy-systemd-logs-daemon-set-892d09b6f39b465a-74vqz from sonobuoy started at 2019-12-05 19:08:12 +0000 UTC (2 container statuses recorded)
Dec  5 19:33:41.201: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 19:33:41.201: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  5 19:33:41.201: INFO: kube-proxy-wwvd5 from kube-system started at 2019-12-05 15:20:44 +0000 UTC (1 container statuses recorded)
Dec  5 19:33:41.201: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  5 19:33:41.201: INFO: sonobuoy from sonobuoy started at 2019-12-05 19:08:11 +0000 UTC (1 container statuses recorded)
Dec  5 19:33:41.201: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  5 19:33:41.201: INFO: kube-flannel-ds-amd64-5ql7s from kube-system started at 2019-12-05 15:20:44 +0000 UTC (1 container statuses recorded)
Dec  5 19:33:41.201: INFO: 	Container kube-flannel ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node crayfish
STEP: verifying the node has the label node puma
Dec  5 19:33:41.245: INFO: Pod kube-flannel-ds-amd64-5ql7s requesting resource cpu=100m on Node puma
Dec  5 19:33:41.245: INFO: Pod kube-flannel-ds-amd64-zqml7 requesting resource cpu=100m on Node crayfish
Dec  5 19:33:41.245: INFO: Pod kube-proxy-wwvd5 requesting resource cpu=0m on Node puma
Dec  5 19:33:41.245: INFO: Pod kube-proxy-xb74r requesting resource cpu=0m on Node crayfish
Dec  5 19:33:41.245: INFO: Pod sonobuoy requesting resource cpu=0m on Node puma
Dec  5 19:33:41.245: INFO: Pod sonobuoy-e2e-job-69046a85250743c3 requesting resource cpu=0m on Node crayfish
Dec  5 19:33:41.245: INFO: Pod sonobuoy-systemd-logs-daemon-set-892d09b6f39b465a-74vqz requesting resource cpu=0m on Node puma
Dec  5 19:33:41.245: INFO: Pod sonobuoy-systemd-logs-daemon-set-892d09b6f39b465a-wx8xw requesting resource cpu=0m on Node crayfish
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2475c4c7-1796-11ea-a680-be755c6bedde.15dd9096a0635f72], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8884/filler-pod-2475c4c7-1796-11ea-a680-be755c6bedde to crayfish]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2475c4c7-1796-11ea-a680-be755c6bedde.15dd9096dcb91040], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2475c4c7-1796-11ea-a680-be755c6bedde.15dd9096e00c8e79], Reason = [Created], Message = [Created container filler-pod-2475c4c7-1796-11ea-a680-be755c6bedde]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2475c4c7-1796-11ea-a680-be755c6bedde.15dd9096ed3bac5a], Reason = [Started], Message = [Started container filler-pod-2475c4c7-1796-11ea-a680-be755c6bedde]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2477492c-1796-11ea-a680-be755c6bedde.15dd9096a0d2e7e1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8884/filler-pod-2477492c-1796-11ea-a680-be755c6bedde to puma]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2477492c-1796-11ea-a680-be755c6bedde.15dd9096d95bbc4f], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2477492c-1796-11ea-a680-be755c6bedde.15dd9096dd6c7a8f], Reason = [Created], Message = [Created container filler-pod-2477492c-1796-11ea-a680-be755c6bedde]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2477492c-1796-11ea-a680-be755c6bedde.15dd9096e8394552], Reason = [Started], Message = [Started container filler-pod-2477492c-1796-11ea-a680-be755c6bedde]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15dd909719683447], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node crayfish
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node puma
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:33:44.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8884" for this suite.
Dec  5 19:33:50.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:33:50.466: INFO: namespace sched-pred-8884 deletion completed in 6.122707112s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:9.330 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:33:50.467: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Dec  5 19:33:50.506: INFO: Waiting up to 5m0s for pod "client-containers-29f9648e-1796-11ea-a680-be755c6bedde" in namespace "containers-784" to be "success or failure"
Dec  5 19:33:50.511: INFO: Pod "client-containers-29f9648e-1796-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 5.397439ms
Dec  5 19:33:52.517: INFO: Pod "client-containers-29f9648e-1796-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011408281s
STEP: Saw pod success
Dec  5 19:33:52.518: INFO: Pod "client-containers-29f9648e-1796-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:33:52.522: INFO: Trying to get logs from node puma pod client-containers-29f9648e-1796-11ea-a680-be755c6bedde container test-container: <nil>
STEP: delete the pod
Dec  5 19:33:52.546: INFO: Waiting for pod client-containers-29f9648e-1796-11ea-a680-be755c6bedde to disappear
Dec  5 19:33:52.549: INFO: Pod client-containers-29f9648e-1796-11ea-a680-be755c6bedde no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:33:52.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-784" for this suite.
Dec  5 19:33:58.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:33:58.691: INFO: namespace containers-784 deletion completed in 6.136945966s

• [SLOW TEST:8.225 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:33:58.691: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Dec  5 19:33:58.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 create -f - --namespace=kubectl-8517'
Dec  5 19:33:58.933: INFO: stderr: ""
Dec  5 19:33:58.933: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  5 19:33:58.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8517'
Dec  5 19:33:59.043: INFO: stderr: ""
Dec  5 19:33:59.043: INFO: stdout: "update-demo-nautilus-qxwwp update-demo-nautilus-s52h4 "
Dec  5 19:33:59.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-qxwwp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8517'
Dec  5 19:33:59.136: INFO: stderr: ""
Dec  5 19:33:59.136: INFO: stdout: ""
Dec  5 19:33:59.136: INFO: update-demo-nautilus-qxwwp is created but not running
Dec  5 19:34:04.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8517'
Dec  5 19:34:04.261: INFO: stderr: ""
Dec  5 19:34:04.261: INFO: stdout: "update-demo-nautilus-qxwwp update-demo-nautilus-s52h4 "
Dec  5 19:34:04.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-qxwwp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8517'
Dec  5 19:34:04.376: INFO: stderr: ""
Dec  5 19:34:04.376: INFO: stdout: "true"
Dec  5 19:34:04.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-qxwwp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8517'
Dec  5 19:34:04.463: INFO: stderr: ""
Dec  5 19:34:04.463: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 19:34:04.463: INFO: validating pod update-demo-nautilus-qxwwp
Dec  5 19:34:04.469: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 19:34:04.469: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 19:34:04.469: INFO: update-demo-nautilus-qxwwp is verified up and running
Dec  5 19:34:04.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-s52h4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8517'
Dec  5 19:34:04.539: INFO: stderr: ""
Dec  5 19:34:04.539: INFO: stdout: "true"
Dec  5 19:34:04.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-nautilus-s52h4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8517'
Dec  5 19:34:04.644: INFO: stderr: ""
Dec  5 19:34:04.644: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 19:34:04.644: INFO: validating pod update-demo-nautilus-s52h4
Dec  5 19:34:04.651: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 19:34:04.651: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 19:34:04.651: INFO: update-demo-nautilus-s52h4 is verified up and running
STEP: rolling-update to new replication controller
Dec  5 19:34:04.652: INFO: scanned /root for discovery docs: <nil>
Dec  5 19:34:04.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-8517'
Dec  5 19:34:27.119: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  5 19:34:27.119: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  5 19:34:27.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8517'
Dec  5 19:34:27.240: INFO: stderr: ""
Dec  5 19:34:27.240: INFO: stdout: "update-demo-kitten-x9nqv update-demo-kitten-z8kpb "
Dec  5 19:34:27.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-kitten-x9nqv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8517'
Dec  5 19:34:27.323: INFO: stderr: ""
Dec  5 19:34:27.323: INFO: stdout: "true"
Dec  5 19:34:27.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-kitten-x9nqv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8517'
Dec  5 19:34:27.392: INFO: stderr: ""
Dec  5 19:34:27.392: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  5 19:34:27.392: INFO: validating pod update-demo-kitten-x9nqv
Dec  5 19:34:27.399: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  5 19:34:27.399: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  5 19:34:27.399: INFO: update-demo-kitten-x9nqv is verified up and running
Dec  5 19:34:27.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-kitten-z8kpb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8517'
Dec  5 19:34:27.476: INFO: stderr: ""
Dec  5 19:34:27.476: INFO: stdout: "true"
Dec  5 19:34:27.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods update-demo-kitten-z8kpb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8517'
Dec  5 19:34:27.556: INFO: stderr: ""
Dec  5 19:34:27.556: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  5 19:34:27.556: INFO: validating pod update-demo-kitten-z8kpb
Dec  5 19:34:27.562: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  5 19:34:27.562: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  5 19:34:27.562: INFO: update-demo-kitten-z8kpb is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:34:27.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8517" for this suite.
Dec  5 19:34:49.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:34:49.711: INFO: namespace kubectl-8517 deletion completed in 22.142979402s

• [SLOW TEST:51.020 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:34:49.716: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 19:34:49.779: INFO: Create a RollingUpdate DaemonSet
Dec  5 19:34:49.786: INFO: Check that daemon pods launch on every node of the cluster
Dec  5 19:34:49.791: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:34:49.791: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:34:49.791: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:34:49.793: INFO: Number of nodes with available pods: 0
Dec  5 19:34:49.793: INFO: Node crayfish is running more than one daemon pod
Dec  5 19:34:50.800: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:34:50.800: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:34:50.801: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:34:50.806: INFO: Number of nodes with available pods: 0
Dec  5 19:34:50.806: INFO: Node crayfish is running more than one daemon pod
Dec  5 19:34:51.800: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:34:51.800: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:34:51.800: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:34:51.805: INFO: Number of nodes with available pods: 1
Dec  5 19:34:51.805: INFO: Node crayfish is running more than one daemon pod
Dec  5 19:34:52.801: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:34:52.801: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:34:52.801: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:34:52.806: INFO: Number of nodes with available pods: 2
Dec  5 19:34:52.806: INFO: Number of running nodes: 2, number of available pods: 2
Dec  5 19:34:52.806: INFO: Update the DaemonSet to trigger a rollout
Dec  5 19:34:52.817: INFO: Updating DaemonSet daemon-set
Dec  5 19:35:04.840: INFO: Roll back the DaemonSet before rollout is complete
Dec  5 19:35:04.851: INFO: Updating DaemonSet daemon-set
Dec  5 19:35:04.851: INFO: Make sure DaemonSet rollback is complete
Dec  5 19:35:04.856: INFO: Wrong image for pod: daemon-set-8bx7j. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  5 19:35:04.856: INFO: Pod daemon-set-8bx7j is not available
Dec  5 19:35:04.863: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:35:04.863: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:35:04.863: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:35:05.868: INFO: Wrong image for pod: daemon-set-8bx7j. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  5 19:35:05.868: INFO: Pod daemon-set-8bx7j is not available
Dec  5 19:35:05.874: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:35:05.874: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:35:05.874: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:35:06.868: INFO: Wrong image for pod: daemon-set-8bx7j. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  5 19:35:06.868: INFO: Pod daemon-set-8bx7j is not available
Dec  5 19:35:06.875: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:35:06.875: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:35:06.875: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:35:07.869: INFO: Pod daemon-set-httgl is not available
Dec  5 19:35:07.875: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:35:07.875: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 19:35:07.875: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6896, will wait for the garbage collector to delete the pods
Dec  5 19:35:07.951: INFO: Deleting DaemonSet.extensions daemon-set took: 12.19225ms
Dec  5 19:35:08.251: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.370622ms
Dec  5 19:35:14.258: INFO: Number of nodes with available pods: 0
Dec  5 19:35:14.258: INFO: Number of running nodes: 0, number of available pods: 0
Dec  5 19:35:14.262: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6896/daemonsets","resourceVersion":"37958"},"items":null}

Dec  5 19:35:14.266: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6896/pods","resourceVersion":"37958"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:35:14.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6896" for this suite.
Dec  5 19:35:20.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:35:20.427: INFO: namespace daemonsets-6896 deletion completed in 6.140597211s

• [SLOW TEST:30.711 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:35:20.427: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 19:35:20.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-113'
Dec  5 19:35:20.584: INFO: stderr: ""
Dec  5 19:35:20.584: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Dec  5 19:35:25.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pod e2e-test-nginx-pod --namespace=kubectl-113 -o json'
Dec  5 19:35:25.758: INFO: stderr: ""
Dec  5 19:35:25.758: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-12-05T19:35:20Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-113\",\n        \"resourceVersion\": \"38016\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-113/pods/e2e-test-nginx-pod\",\n        \"uid\": \"5fa89231-1796-11ea-9811-dae9897b3b7b\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-4759f\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"crayfish\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-4759f\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-4759f\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-05T19:35:20Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-05T19:35:22Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-05T19:35:22Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-05T19:35:20Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://9cf35925a442089d344f7b61b3ed92e0b82606e40fff6ca90d3fdb25f4e83555\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-12-05T19:35:21Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"178.62.90.178\",\n        \"phase\": \"Running\",\n        \"podIP\": \"25.0.2.225\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-12-05T19:35:20Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec  5 19:35:25.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 replace -f - --namespace=kubectl-113'
Dec  5 19:35:26.044: INFO: stderr: ""
Dec  5 19:35:26.044: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Dec  5 19:35:26.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 delete pods e2e-test-nginx-pod --namespace=kubectl-113'
Dec  5 19:35:27.626: INFO: stderr: ""
Dec  5 19:35:27.626: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:35:27.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-113" for this suite.
Dec  5 19:35:33.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:35:33.772: INFO: namespace kubectl-113 deletion completed in 6.140803982s

• [SLOW TEST:13.345 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:35:33.773: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec  5 19:35:33.824: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2918,SelfLink:/api/v1/namespaces/watch-2918/configmaps/e2e-watch-test-watch-closed,UID:678dc46f-1796-11ea-b857-a68228aedf28,ResourceVersion:38063,Generation:0,CreationTimestamp:2019-12-05 19:35:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  5 19:35:33.824: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2918,SelfLink:/api/v1/namespaces/watch-2918/configmaps/e2e-watch-test-watch-closed,UID:678dc46f-1796-11ea-b857-a68228aedf28,ResourceVersion:38064,Generation:0,CreationTimestamp:2019-12-05 19:35:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec  5 19:35:33.840: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2918,SelfLink:/api/v1/namespaces/watch-2918/configmaps/e2e-watch-test-watch-closed,UID:678dc46f-1796-11ea-b857-a68228aedf28,ResourceVersion:38065,Generation:0,CreationTimestamp:2019-12-05 19:35:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  5 19:35:33.840: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2918,SelfLink:/api/v1/namespaces/watch-2918/configmaps/e2e-watch-test-watch-closed,UID:678dc46f-1796-11ea-b857-a68228aedf28,ResourceVersion:38066,Generation:0,CreationTimestamp:2019-12-05 19:35:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:35:33.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2918" for this suite.
Dec  5 19:35:39.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:35:39.970: INFO: namespace watch-2918 deletion completed in 6.126542489s

• [SLOW TEST:6.197 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:35:39.970: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Dec  5 19:35:40.003: INFO: PodSpec: initContainers in spec.initContainers
Dec  5 19:36:25.524: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-6b3ed7a7-1796-11ea-a680-be755c6bedde", GenerateName:"", Namespace:"init-container-3214", SelfLink:"/api/v1/namespaces/init-container-3214/pods/pod-init-6b3ed7a7-1796-11ea-a680-be755c6bedde", UID:"6b3f6790-1796-11ea-b857-a68228aedf28", ResourceVersion:"38197", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63711171340, loc:(*time.Location)(0x882f100)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"3530494"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-v9ggg", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002fa4ac0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-v9ggg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-v9ggg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-v9ggg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002192298), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"puma", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002c940c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002192320)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002192340)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002192348), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00219234c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711171340, loc:(*time.Location)(0x882f100)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711171340, loc:(*time.Location)(0x882f100)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711171340, loc:(*time.Location)(0x882f100)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711171340, loc:(*time.Location)(0x882f100)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"178.62.90.156", PodIP:"25.0.1.219", StartTime:(*v1.Time)(0xc0016f6ac0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002708150)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0027081c0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://2711b39309334ae276283fced8280b0bd02d731d0dead43e81522fa486ef9e2a"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0016f6c00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0016f6b40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:36:25.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3214" for this suite.
Dec  5 19:36:47.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:36:47.669: INFO: namespace init-container-3214 deletion completed in 22.136932088s

• [SLOW TEST:67.699 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:36:47.669: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Dec  5 19:36:50.256: INFO: Successfully updated pod "annotationupdate9398af3b-1796-11ea-a680-be755c6bedde"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:36:52.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6336" for this suite.
Dec  5 19:37:14.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:37:14.433: INFO: namespace downward-api-6336 deletion completed in 22.143970113s

• [SLOW TEST:26.764 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:37:14.435: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Dec  5 19:37:14.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 create -f - --namespace=kubectl-51'
Dec  5 19:37:14.768: INFO: stderr: ""
Dec  5 19:37:14.768: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  5 19:37:15.774: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 19:37:15.774: INFO: Found 0 / 1
Dec  5 19:37:16.774: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 19:37:16.774: INFO: Found 0 / 1
Dec  5 19:37:17.774: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 19:37:17.774: INFO: Found 1 / 1
Dec  5 19:37:17.774: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec  5 19:37:17.787: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 19:37:17.787: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  5 19:37:17.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 patch pod redis-master-nbrzv --namespace=kubectl-51 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec  5 19:37:17.928: INFO: stderr: ""
Dec  5 19:37:17.928: INFO: stdout: "pod/redis-master-nbrzv patched\n"
STEP: checking annotations
Dec  5 19:37:17.933: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 19:37:17.933: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:37:17.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-51" for this suite.
Dec  5 19:37:39.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:37:40.080: INFO: namespace kubectl-51 deletion completed in 22.141276457s

• [SLOW TEST:25.645 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:37:40.081: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1521
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1521
STEP: Creating statefulset with conflicting port in namespace statefulset-1521
STEP: Waiting until pod test-pod will start running in namespace statefulset-1521
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1521
Dec  5 19:37:44.158: INFO: Observed stateful pod in namespace: statefulset-1521, name: ss-0, uid: b4e7d713-1796-11ea-9811-dae9897b3b7b, status phase: Pending. Waiting for statefulset controller to delete.
Dec  5 19:37:53.544: INFO: Observed stateful pod in namespace: statefulset-1521, name: ss-0, uid: b4e7d713-1796-11ea-9811-dae9897b3b7b, status phase: Failed. Waiting for statefulset controller to delete.
Dec  5 19:37:53.554: INFO: Observed stateful pod in namespace: statefulset-1521, name: ss-0, uid: b4e7d713-1796-11ea-9811-dae9897b3b7b, status phase: Failed. Waiting for statefulset controller to delete.
Dec  5 19:37:53.559: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1521
STEP: Removing pod with conflicting port in namespace statefulset-1521
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1521 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  5 19:37:57.598: INFO: Deleting all statefulset in ns statefulset-1521
Dec  5 19:37:57.603: INFO: Scaling statefulset ss to 0
Dec  5 19:38:07.624: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 19:38:07.628: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:38:07.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1521" for this suite.
Dec  5 19:38:13.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:38:13.793: INFO: namespace statefulset-1521 deletion completed in 6.138687695s

• [SLOW TEST:33.713 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:38:13.794: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  5 19:38:13.843: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c6ef4526-1796-11ea-a680-be755c6bedde" in namespace "downward-api-7212" to be "success or failure"
Dec  5 19:38:13.847: INFO: Pod "downwardapi-volume-c6ef4526-1796-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014172ms
Dec  5 19:38:15.853: INFO: Pod "downwardapi-volume-c6ef4526-1796-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00948729s
STEP: Saw pod success
Dec  5 19:38:15.853: INFO: Pod "downwardapi-volume-c6ef4526-1796-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:38:15.858: INFO: Trying to get logs from node puma pod downwardapi-volume-c6ef4526-1796-11ea-a680-be755c6bedde container client-container: <nil>
STEP: delete the pod
Dec  5 19:38:15.884: INFO: Waiting for pod downwardapi-volume-c6ef4526-1796-11ea-a680-be755c6bedde to disappear
Dec  5 19:38:15.887: INFO: Pod downwardapi-volume-c6ef4526-1796-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:38:15.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7212" for this suite.
Dec  5 19:38:21.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:38:22.035: INFO: namespace downward-api-7212 deletion completed in 6.141800674s

• [SLOW TEST:8.241 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:38:22.035: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:38:26.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-355" for this suite.
Dec  5 19:38:32.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:38:32.233: INFO: namespace kubelet-test-355 deletion completed in 6.138036593s

• [SLOW TEST:10.198 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:38:32.234: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1342
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Dec  5 19:38:32.280: INFO: Found 0 stateful pods, waiting for 3
Dec  5 19:38:42.286: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 19:38:42.286: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 19:38:42.286: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 19:38:42.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-1342 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 19:38:42.581: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 19:38:42.581: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 19:38:42.581: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec  5 19:38:52.625: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec  5 19:39:02.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-1342 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 19:39:02.913: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  5 19:39:02.913: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 19:39:02.913: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 19:39:22.947: INFO: Waiting for StatefulSet statefulset-1342/ss2 to complete update
Dec  5 19:39:22.947: INFO: Waiting for Pod statefulset-1342/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Dec  5 19:39:32.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-1342 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 19:39:33.242: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 19:39:33.242: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 19:39:33.242: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 19:39:43.285: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec  5 19:39:53.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-1342 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 19:39:53.586: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  5 19:39:53.586: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 19:39:53.586: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 19:40:03.621: INFO: Waiting for StatefulSet statefulset-1342/ss2 to complete update
Dec  5 19:40:03.622: INFO: Waiting for Pod statefulset-1342/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec  5 19:40:03.622: INFO: Waiting for Pod statefulset-1342/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec  5 19:40:03.622: INFO: Waiting for Pod statefulset-1342/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec  5 19:40:13.632: INFO: Waiting for StatefulSet statefulset-1342/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  5 19:40:23.633: INFO: Deleting all statefulset in ns statefulset-1342
Dec  5 19:40:23.637: INFO: Scaling statefulset ss2 to 0
Dec  5 19:40:53.662: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 19:40:53.667: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:40:53.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1342" for this suite.
Dec  5 19:40:59.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:40:59.822: INFO: namespace statefulset-1342 deletion completed in 6.128914484s

• [SLOW TEST:147.589 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:40:59.823: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-29e50932-1797-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume configMaps
Dec  5 19:40:59.874: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-29e60364-1797-11ea-a680-be755c6bedde" in namespace "projected-8972" to be "success or failure"
Dec  5 19:40:59.880: INFO: Pod "pod-projected-configmaps-29e60364-1797-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 5.250001ms
Dec  5 19:41:01.885: INFO: Pod "pod-projected-configmaps-29e60364-1797-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010782406s
STEP: Saw pod success
Dec  5 19:41:01.885: INFO: Pod "pod-projected-configmaps-29e60364-1797-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:41:01.890: INFO: Trying to get logs from node puma pod pod-projected-configmaps-29e60364-1797-11ea-a680-be755c6bedde container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 19:41:01.917: INFO: Waiting for pod pod-projected-configmaps-29e60364-1797-11ea-a680-be755c6bedde to disappear
Dec  5 19:41:01.920: INFO: Pod pod-projected-configmaps-29e60364-1797-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:41:01.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8972" for this suite.
Dec  5 19:41:07.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:41:08.068: INFO: namespace projected-8972 deletion completed in 6.142784917s

• [SLOW TEST:8.245 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:41:08.068: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7387
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Dec  5 19:41:08.131: INFO: Found 0 stateful pods, waiting for 3
Dec  5 19:41:18.138: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 19:41:18.138: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 19:41:18.138: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec  5 19:41:18.172: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec  5 19:41:28.214: INFO: Updating stateful set ss2
Dec  5 19:41:28.223: INFO: Waiting for Pod statefulset-7387/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Dec  5 19:41:38.282: INFO: Found 2 stateful pods, waiting for 3
Dec  5 19:41:48.289: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 19:41:48.289: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 19:41:48.289: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec  5 19:41:48.320: INFO: Updating stateful set ss2
Dec  5 19:41:48.331: INFO: Waiting for Pod statefulset-7387/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 19:41:58.364: INFO: Updating stateful set ss2
Dec  5 19:41:58.373: INFO: Waiting for StatefulSet statefulset-7387/ss2 to complete update
Dec  5 19:41:58.373: INFO: Waiting for Pod statefulset-7387/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  5 19:42:08.386: INFO: Deleting all statefulset in ns statefulset-7387
Dec  5 19:42:08.391: INFO: Scaling statefulset ss2 to 0
Dec  5 19:42:38.414: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 19:42:38.419: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:42:38.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7387" for this suite.
Dec  5 19:42:44.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:42:44.587: INFO: namespace statefulset-7387 deletion completed in 6.140188214s

• [SLOW TEST:96.519 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:42:44.587: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  5 19:42:44.632: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68562368-1797-11ea-a680-be755c6bedde" in namespace "projected-8208" to be "success or failure"
Dec  5 19:42:44.635: INFO: Pod "downwardapi-volume-68562368-1797-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.476398ms
Dec  5 19:42:46.640: INFO: Pod "downwardapi-volume-68562368-1797-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0087642s
Dec  5 19:42:48.647: INFO: Pod "downwardapi-volume-68562368-1797-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014867144s
STEP: Saw pod success
Dec  5 19:42:48.647: INFO: Pod "downwardapi-volume-68562368-1797-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:42:48.651: INFO: Trying to get logs from node crayfish pod downwardapi-volume-68562368-1797-11ea-a680-be755c6bedde container client-container: <nil>
STEP: delete the pod
Dec  5 19:42:48.683: INFO: Waiting for pod downwardapi-volume-68562368-1797-11ea-a680-be755c6bedde to disappear
Dec  5 19:42:48.687: INFO: Pod downwardapi-volume-68562368-1797-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:42:48.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8208" for this suite.
Dec  5 19:42:54.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:42:54.812: INFO: namespace projected-8208 deletion completed in 6.119797622s

• [SLOW TEST:10.225 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:42:54.814: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 19:42:54.852: INFO: Creating deployment "test-recreate-deployment"
Dec  5 19:42:54.858: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec  5 19:42:54.868: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec  5 19:42:56.879: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec  5 19:42:56.883: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711171774, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711171774, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711171774, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711171774, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6566d46b4b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 19:42:58.889: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec  5 19:42:58.903: INFO: Updating deployment test-recreate-deployment
Dec  5 19:42:58.903: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  5 19:42:58.969: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-3420,SelfLink:/apis/apps/v1/namespaces/deployment-3420/deployments/test-recreate-deployment,UID:6e6f72c2-1797-11ea-b857-a68228aedf28,ResourceVersion:39871,Generation:2,CreationTimestamp:2019-12-05 19:42:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-12-05 19:42:58 +0000 UTC 2019-12-05 19:42:58 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-12-05 19:42:58 +0000 UTC 2019-12-05 19:42:54 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-745fb9c84c" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Dec  5 19:42:58.973: INFO: New ReplicaSet "test-recreate-deployment-745fb9c84c" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-745fb9c84c,GenerateName:,Namespace:deployment-3420,SelfLink:/apis/apps/v1/namespaces/deployment-3420/replicasets/test-recreate-deployment-745fb9c84c,UID:70dea892-1797-11ea-9811-dae9897b3b7b,ResourceVersion:39869,Generation:1,CreationTimestamp:2019-12-05 19:42:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 6e6f72c2-1797-11ea-b857-a68228aedf28 0xc002be3187 0xc002be3188}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 19:42:58.973: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec  5 19:42:58.973: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6566d46b4b,GenerateName:,Namespace:deployment-3420,SelfLink:/apis/apps/v1/namespaces/deployment-3420/replicasets/test-recreate-deployment-6566d46b4b,UID:6e70907e-1797-11ea-9811-dae9897b3b7b,ResourceVersion:39860,Generation:2,CreationTimestamp:2019-12-05 19:42:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 6e6f72c2-1797-11ea-b857-a68228aedf28 0xc002be30b7 0xc002be30b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 19:42:58.977: INFO: Pod "test-recreate-deployment-745fb9c84c-sjwmr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-745fb9c84c-sjwmr,GenerateName:test-recreate-deployment-745fb9c84c-,Namespace:deployment-3420,SelfLink:/api/v1/namespaces/deployment-3420/pods/test-recreate-deployment-745fb9c84c-sjwmr,UID:70df5e60-1797-11ea-9811-dae9897b3b7b,ResourceVersion:39872,Generation:0,CreationTimestamp:2019-12-05 19:42:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-745fb9c84c 70dea892-1797-11ea-9811-dae9897b3b7b 0xc002be3fc7 0xc002be3fc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bszjm {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bszjm,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bszjm true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:crayfish,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028d4040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028d4060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:42:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:42:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:42:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 19:42:58 +0000 UTC  }],Message:,Reason:,HostIP:178.62.90.178,PodIP:,StartTime:2019-12-05 19:42:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:42:58.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3420" for this suite.
Dec  5 19:43:04.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:43:05.102: INFO: namespace deployment-3420 deletion completed in 6.120182265s

• [SLOW TEST:10.289 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:43:05.103: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-749060b5-1797-11ea-a680-be755c6bedde
STEP: Creating secret with name secret-projected-all-test-volume-7490608e-1797-11ea-a680-be755c6bedde
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec  5 19:43:05.152: INFO: Waiting up to 5m0s for pod "projected-volume-74906047-1797-11ea-a680-be755c6bedde" in namespace "projected-8143" to be "success or failure"
Dec  5 19:43:05.156: INFO: Pod "projected-volume-74906047-1797-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.712037ms
Dec  5 19:43:07.163: INFO: Pod "projected-volume-74906047-1797-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011159708s
Dec  5 19:43:09.169: INFO: Pod "projected-volume-74906047-1797-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016919385s
STEP: Saw pod success
Dec  5 19:43:09.169: INFO: Pod "projected-volume-74906047-1797-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:43:09.173: INFO: Trying to get logs from node puma pod projected-volume-74906047-1797-11ea-a680-be755c6bedde container projected-all-volume-test: <nil>
STEP: delete the pod
Dec  5 19:43:09.202: INFO: Waiting for pod projected-volume-74906047-1797-11ea-a680-be755c6bedde to disappear
Dec  5 19:43:09.206: INFO: Pod projected-volume-74906047-1797-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:43:09.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8143" for this suite.
Dec  5 19:43:15.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:43:15.408: INFO: namespace projected-8143 deletion completed in 6.196218827s

• [SLOW TEST:10.305 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:43:15.413: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-590/configmap-test-7ab62f01-1797-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume configMaps
Dec  5 19:43:15.463: INFO: Waiting up to 5m0s for pod "pod-configmaps-7ab75f26-1797-11ea-a680-be755c6bedde" in namespace "configmap-590" to be "success or failure"
Dec  5 19:43:15.466: INFO: Pod "pod-configmaps-7ab75f26-1797-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.244878ms
Dec  5 19:43:17.472: INFO: Pod "pod-configmaps-7ab75f26-1797-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009084746s
Dec  5 19:43:19.478: INFO: Pod "pod-configmaps-7ab75f26-1797-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01526002s
STEP: Saw pod success
Dec  5 19:43:19.478: INFO: Pod "pod-configmaps-7ab75f26-1797-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:43:19.482: INFO: Trying to get logs from node puma pod pod-configmaps-7ab75f26-1797-11ea-a680-be755c6bedde container env-test: <nil>
STEP: delete the pod
Dec  5 19:43:19.511: INFO: Waiting for pod pod-configmaps-7ab75f26-1797-11ea-a680-be755c6bedde to disappear
Dec  5 19:43:19.515: INFO: Pod pod-configmaps-7ab75f26-1797-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:43:19.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-590" for this suite.
Dec  5 19:43:25.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:43:25.665: INFO: namespace configmap-590 deletion completed in 6.144965597s

• [SLOW TEST:10.252 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:43:25.665: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  5 19:43:29.759: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 19:43:29.763: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 19:43:31.763: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 19:43:31.768: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 19:43:33.763: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 19:43:33.769: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 19:43:35.763: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 19:43:35.768: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 19:43:37.763: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 19:43:37.768: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 19:43:39.763: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 19:43:39.769: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 19:43:41.763: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 19:43:41.770: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 19:43:43.763: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 19:43:43.769: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 19:43:45.763: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 19:43:45.768: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 19:43:47.763: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 19:43:47.768: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 19:43:49.763: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 19:43:49.768: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 19:43:51.763: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 19:43:51.770: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 19:43:53.763: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 19:43:53.769: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:43:53.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3177" for this suite.
Dec  5 19:44:15.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:44:15.936: INFO: namespace container-lifecycle-hook-3177 deletion completed in 22.147384582s

• [SLOW TEST:50.271 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:44:15.937: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Dec  5 19:44:16.506: INFO: created pod pod-service-account-defaultsa
Dec  5 19:44:16.506: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec  5 19:44:16.514: INFO: created pod pod-service-account-mountsa
Dec  5 19:44:16.514: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec  5 19:44:16.521: INFO: created pod pod-service-account-nomountsa
Dec  5 19:44:16.521: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec  5 19:44:16.533: INFO: created pod pod-service-account-defaultsa-mountspec
Dec  5 19:44:16.533: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec  5 19:44:16.541: INFO: created pod pod-service-account-mountsa-mountspec
Dec  5 19:44:16.541: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec  5 19:44:16.547: INFO: created pod pod-service-account-nomountsa-mountspec
Dec  5 19:44:16.547: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec  5 19:44:16.553: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec  5 19:44:16.553: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec  5 19:44:16.558: INFO: created pod pod-service-account-mountsa-nomountspec
Dec  5 19:44:16.558: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec  5 19:44:16.566: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec  5 19:44:16.566: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:44:16.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2924" for this suite.
Dec  5 19:44:22.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:44:22.723: INFO: namespace svcaccounts-2924 deletion completed in 6.150833642s

• [SLOW TEST:6.787 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:44:22.726: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  5 19:44:22.771: INFO: Waiting up to 5m0s for pod "pod-a2d50724-1797-11ea-a680-be755c6bedde" in namespace "emptydir-911" to be "success or failure"
Dec  5 19:44:22.776: INFO: Pod "pod-a2d50724-1797-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.695856ms
Dec  5 19:44:24.782: INFO: Pod "pod-a2d50724-1797-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010660352s
Dec  5 19:44:26.788: INFO: Pod "pod-a2d50724-1797-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016682888s
STEP: Saw pod success
Dec  5 19:44:26.788: INFO: Pod "pod-a2d50724-1797-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:44:26.793: INFO: Trying to get logs from node crayfish pod pod-a2d50724-1797-11ea-a680-be755c6bedde container test-container: <nil>
STEP: delete the pod
Dec  5 19:44:26.823: INFO: Waiting for pod pod-a2d50724-1797-11ea-a680-be755c6bedde to disappear
Dec  5 19:44:26.828: INFO: Pod pod-a2d50724-1797-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:44:26.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-911" for this suite.
Dec  5 19:44:32.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:44:32.956: INFO: namespace emptydir-911 deletion completed in 6.123249051s

• [SLOW TEST:10.231 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:44:32.957: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  5 19:44:32.998: INFO: Waiting up to 5m0s for pod "pod-a8ededbf-1797-11ea-a680-be755c6bedde" in namespace "emptydir-7596" to be "success or failure"
Dec  5 19:44:33.003: INFO: Pod "pod-a8ededbf-1797-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 5.036433ms
Dec  5 19:44:35.011: INFO: Pod "pod-a8ededbf-1797-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013721498s
Dec  5 19:44:37.018: INFO: Pod "pod-a8ededbf-1797-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020174739s
STEP: Saw pod success
Dec  5 19:44:37.018: INFO: Pod "pod-a8ededbf-1797-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:44:37.023: INFO: Trying to get logs from node crayfish pod pod-a8ededbf-1797-11ea-a680-be755c6bedde container test-container: <nil>
STEP: delete the pod
Dec  5 19:44:37.056: INFO: Waiting for pod pod-a8ededbf-1797-11ea-a680-be755c6bedde to disappear
Dec  5 19:44:37.061: INFO: Pod pod-a8ededbf-1797-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:44:37.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7596" for this suite.
Dec  5 19:44:43.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:44:43.214: INFO: namespace emptydir-7596 deletion completed in 6.146874846s

• [SLOW TEST:10.258 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:44:43.216: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  5 19:44:43.263: INFO: Waiting up to 5m0s for pod "pod-af0c285f-1797-11ea-a680-be755c6bedde" in namespace "emptydir-6819" to be "success or failure"
Dec  5 19:44:43.266: INFO: Pod "pod-af0c285f-1797-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.977446ms
Dec  5 19:44:45.272: INFO: Pod "pod-af0c285f-1797-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008908378s
Dec  5 19:44:47.278: INFO: Pod "pod-af0c285f-1797-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014189076s
STEP: Saw pod success
Dec  5 19:44:47.278: INFO: Pod "pod-af0c285f-1797-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:44:47.283: INFO: Trying to get logs from node crayfish pod pod-af0c285f-1797-11ea-a680-be755c6bedde container test-container: <nil>
STEP: delete the pod
Dec  5 19:44:47.311: INFO: Waiting for pod pod-af0c285f-1797-11ea-a680-be755c6bedde to disappear
Dec  5 19:44:47.315: INFO: Pod pod-af0c285f-1797-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:44:47.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6819" for this suite.
Dec  5 19:44:53.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:44:53.456: INFO: namespace emptydir-6819 deletion completed in 6.136261815s

• [SLOW TEST:10.240 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:44:53.457: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:44:55.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-107" for this suite.
Dec  5 19:45:33.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:45:33.666: INFO: namespace kubelet-test-107 deletion completed in 38.126378086s

• [SLOW TEST:40.208 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:45:33.666: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Dec  5 19:45:43.783: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 13
	[quantile=0.9] = 94
	[quantile=0.99] = 94
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 215121
	[quantile=0.9] = 221723
	[quantile=0.99] = 221723
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 5
	[quantile=0.9] = 12
	[quantile=0.99] = 39
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 18
	[quantile=0.9] = 33
	[quantile=0.99] = 71
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 16
	[quantile=0.9] = 33
	[quantile=0.99] = 47
For namespace_queue_latency_sum:
	[] = 3258
For namespace_queue_latency_count:
	[] = 187
For namespace_retries:
	[] = 190
For namespace_work_duration:
	[quantile=0.5] = 162665
	[quantile=0.9] = 282230
	[quantile=0.99] = 376195
For namespace_work_duration_sum:
	[] = 23940076
For namespace_work_duration_count:
	[] = 187
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:45:43.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7125" for this suite.
Dec  5 19:45:49.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:45:49.930: INFO: namespace gc-7125 deletion completed in 6.141141356s

• [SLOW TEST:16.264 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:45:49.930: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec  5 19:45:49.976: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-320,SelfLink:/api/v1/namespaces/watch-320/configmaps/e2e-watch-test-configmap-a,UID:d6d03cd5-1797-11ea-b857-a68228aedf28,ResourceVersion:40572,Generation:0,CreationTimestamp:2019-12-05 19:45:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  5 19:45:49.976: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-320,SelfLink:/api/v1/namespaces/watch-320/configmaps/e2e-watch-test-configmap-a,UID:d6d03cd5-1797-11ea-b857-a68228aedf28,ResourceVersion:40572,Generation:0,CreationTimestamp:2019-12-05 19:45:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec  5 19:45:59.990: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-320,SelfLink:/api/v1/namespaces/watch-320/configmaps/e2e-watch-test-configmap-a,UID:d6d03cd5-1797-11ea-b857-a68228aedf28,ResourceVersion:40591,Generation:0,CreationTimestamp:2019-12-05 19:45:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  5 19:45:59.991: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-320,SelfLink:/api/v1/namespaces/watch-320/configmaps/e2e-watch-test-configmap-a,UID:d6d03cd5-1797-11ea-b857-a68228aedf28,ResourceVersion:40591,Generation:0,CreationTimestamp:2019-12-05 19:45:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec  5 19:46:10.003: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-320,SelfLink:/api/v1/namespaces/watch-320/configmaps/e2e-watch-test-configmap-a,UID:d6d03cd5-1797-11ea-b857-a68228aedf28,ResourceVersion:40610,Generation:0,CreationTimestamp:2019-12-05 19:45:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  5 19:46:10.003: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-320,SelfLink:/api/v1/namespaces/watch-320/configmaps/e2e-watch-test-configmap-a,UID:d6d03cd5-1797-11ea-b857-a68228aedf28,ResourceVersion:40610,Generation:0,CreationTimestamp:2019-12-05 19:45:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec  5 19:46:20.017: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-320,SelfLink:/api/v1/namespaces/watch-320/configmaps/e2e-watch-test-configmap-a,UID:d6d03cd5-1797-11ea-b857-a68228aedf28,ResourceVersion:40629,Generation:0,CreationTimestamp:2019-12-05 19:45:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  5 19:46:20.017: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-320,SelfLink:/api/v1/namespaces/watch-320/configmaps/e2e-watch-test-configmap-a,UID:d6d03cd5-1797-11ea-b857-a68228aedf28,ResourceVersion:40629,Generation:0,CreationTimestamp:2019-12-05 19:45:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec  5 19:46:30.028: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-320,SelfLink:/api/v1/namespaces/watch-320/configmaps/e2e-watch-test-configmap-b,UID:eeaf427c-1797-11ea-b857-a68228aedf28,ResourceVersion:40649,Generation:0,CreationTimestamp:2019-12-05 19:46:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  5 19:46:30.028: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-320,SelfLink:/api/v1/namespaces/watch-320/configmaps/e2e-watch-test-configmap-b,UID:eeaf427c-1797-11ea-b857-a68228aedf28,ResourceVersion:40649,Generation:0,CreationTimestamp:2019-12-05 19:46:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec  5 19:46:40.040: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-320,SelfLink:/api/v1/namespaces/watch-320/configmaps/e2e-watch-test-configmap-b,UID:eeaf427c-1797-11ea-b857-a68228aedf28,ResourceVersion:40672,Generation:0,CreationTimestamp:2019-12-05 19:46:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  5 19:46:40.040: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-320,SelfLink:/api/v1/namespaces/watch-320/configmaps/e2e-watch-test-configmap-b,UID:eeaf427c-1797-11ea-b857-a68228aedf28,ResourceVersion:40672,Generation:0,CreationTimestamp:2019-12-05 19:46:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:46:50.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-320" for this suite.
Dec  5 19:46:56.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:46:56.181: INFO: namespace watch-320 deletion completed in 6.132252263s

• [SLOW TEST:66.250 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:46:56.181: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6335
I1205 19:46:56.227983      15 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6335, replica count: 1
I1205 19:46:57.278635      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1205 19:46:58.279147      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1205 19:46:59.279531      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  5 19:46:59.401: INFO: Created: latency-svc-6r5qk
Dec  5 19:46:59.407: INFO: Got endpoints: latency-svc-6r5qk [27.751213ms]
Dec  5 19:46:59.429: INFO: Created: latency-svc-d9hl7
Dec  5 19:46:59.433: INFO: Got endpoints: latency-svc-d9hl7 [25.202417ms]
Dec  5 19:46:59.445: INFO: Created: latency-svc-tz5gd
Dec  5 19:46:59.448: INFO: Got endpoints: latency-svc-tz5gd [40.121983ms]
Dec  5 19:46:59.458: INFO: Created: latency-svc-vmk44
Dec  5 19:46:59.462: INFO: Got endpoints: latency-svc-vmk44 [53.624392ms]
Dec  5 19:46:59.481: INFO: Created: latency-svc-jdtn8
Dec  5 19:46:59.481: INFO: Got endpoints: latency-svc-jdtn8 [72.479729ms]
Dec  5 19:46:59.494: INFO: Created: latency-svc-btbhk
Dec  5 19:46:59.498: INFO: Got endpoints: latency-svc-btbhk [89.14425ms]
Dec  5 19:46:59.509: INFO: Created: latency-svc-b2mzg
Dec  5 19:46:59.512: INFO: Got endpoints: latency-svc-b2mzg [103.160037ms]
Dec  5 19:46:59.521: INFO: Created: latency-svc-r7r4s
Dec  5 19:46:59.526: INFO: Got endpoints: latency-svc-r7r4s [116.676765ms]
Dec  5 19:46:59.534: INFO: Created: latency-svc-hqvhj
Dec  5 19:46:59.536: INFO: Got endpoints: latency-svc-hqvhj [126.548248ms]
Dec  5 19:46:59.544: INFO: Created: latency-svc-rxk4g
Dec  5 19:46:59.547: INFO: Got endpoints: latency-svc-rxk4g [138.10068ms]
Dec  5 19:46:59.560: INFO: Created: latency-svc-6zv7l
Dec  5 19:46:59.562: INFO: Got endpoints: latency-svc-6zv7l [153.039548ms]
Dec  5 19:46:59.575: INFO: Created: latency-svc-ngd8q
Dec  5 19:46:59.578: INFO: Got endpoints: latency-svc-ngd8q [168.343294ms]
Dec  5 19:46:59.589: INFO: Created: latency-svc-56dgl
Dec  5 19:46:59.592: INFO: Got endpoints: latency-svc-56dgl [182.503297ms]
Dec  5 19:46:59.601: INFO: Created: latency-svc-ksjsf
Dec  5 19:46:59.604: INFO: Got endpoints: latency-svc-ksjsf [195.007896ms]
Dec  5 19:46:59.619: INFO: Created: latency-svc-8sd84
Dec  5 19:46:59.623: INFO: Got endpoints: latency-svc-8sd84 [213.128966ms]
Dec  5 19:46:59.632: INFO: Created: latency-svc-bmzsn
Dec  5 19:46:59.635: INFO: Got endpoints: latency-svc-bmzsn [225.331149ms]
Dec  5 19:46:59.645: INFO: Created: latency-svc-qgzxn
Dec  5 19:46:59.648: INFO: Got endpoints: latency-svc-qgzxn [215.388816ms]
Dec  5 19:46:59.655: INFO: Created: latency-svc-klgq8
Dec  5 19:46:59.662: INFO: Got endpoints: latency-svc-klgq8 [214.657052ms]
Dec  5 19:46:59.667: INFO: Created: latency-svc-pqdp9
Dec  5 19:46:59.669: INFO: Got endpoints: latency-svc-pqdp9 [207.430835ms]
Dec  5 19:46:59.679: INFO: Created: latency-svc-v82m5
Dec  5 19:46:59.682: INFO: Got endpoints: latency-svc-v82m5 [201.486867ms]
Dec  5 19:46:59.695: INFO: Created: latency-svc-fkvrj
Dec  5 19:46:59.698: INFO: Got endpoints: latency-svc-fkvrj [199.804844ms]
Dec  5 19:46:59.705: INFO: Created: latency-svc-r7r9c
Dec  5 19:46:59.708: INFO: Got endpoints: latency-svc-r7r9c [195.483084ms]
Dec  5 19:46:59.716: INFO: Created: latency-svc-9fr2b
Dec  5 19:46:59.719: INFO: Got endpoints: latency-svc-9fr2b [193.174362ms]
Dec  5 19:46:59.728: INFO: Created: latency-svc-mg7r2
Dec  5 19:46:59.729: INFO: Got endpoints: latency-svc-mg7r2 [193.744668ms]
Dec  5 19:46:59.739: INFO: Created: latency-svc-h7dc7
Dec  5 19:46:59.742: INFO: Got endpoints: latency-svc-h7dc7 [194.735771ms]
Dec  5 19:46:59.750: INFO: Created: latency-svc-sdsqq
Dec  5 19:46:59.753: INFO: Got endpoints: latency-svc-sdsqq [190.820624ms]
Dec  5 19:46:59.762: INFO: Created: latency-svc-686vm
Dec  5 19:46:59.765: INFO: Got endpoints: latency-svc-686vm [187.307461ms]
Dec  5 19:46:59.780: INFO: Created: latency-svc-cfrnt
Dec  5 19:46:59.786: INFO: Got endpoints: latency-svc-cfrnt [194.389413ms]
Dec  5 19:46:59.799: INFO: Created: latency-svc-vn6xx
Dec  5 19:46:59.803: INFO: Got endpoints: latency-svc-vn6xx [198.203995ms]
Dec  5 19:46:59.810: INFO: Created: latency-svc-kq6d7
Dec  5 19:46:59.812: INFO: Got endpoints: latency-svc-kq6d7 [189.407744ms]
Dec  5 19:46:59.826: INFO: Created: latency-svc-5dzz2
Dec  5 19:46:59.828: INFO: Got endpoints: latency-svc-5dzz2 [193.212855ms]
Dec  5 19:46:59.838: INFO: Created: latency-svc-8mpjd
Dec  5 19:46:59.841: INFO: Got endpoints: latency-svc-8mpjd [192.650578ms]
Dec  5 19:46:59.851: INFO: Created: latency-svc-bh74b
Dec  5 19:46:59.854: INFO: Got endpoints: latency-svc-bh74b [191.270634ms]
Dec  5 19:46:59.862: INFO: Created: latency-svc-hh8fx
Dec  5 19:46:59.864: INFO: Got endpoints: latency-svc-hh8fx [195.169283ms]
Dec  5 19:46:59.873: INFO: Created: latency-svc-tdkms
Dec  5 19:46:59.875: INFO: Got endpoints: latency-svc-tdkms [192.857652ms]
Dec  5 19:46:59.884: INFO: Created: latency-svc-94hwq
Dec  5 19:46:59.887: INFO: Got endpoints: latency-svc-94hwq [189.187003ms]
Dec  5 19:46:59.895: INFO: Created: latency-svc-v4wfn
Dec  5 19:46:59.907: INFO: Got endpoints: latency-svc-v4wfn [199.088283ms]
Dec  5 19:46:59.911: INFO: Created: latency-svc-nxkhz
Dec  5 19:46:59.913: INFO: Got endpoints: latency-svc-nxkhz [193.9993ms]
Dec  5 19:46:59.922: INFO: Created: latency-svc-sq87b
Dec  5 19:46:59.925: INFO: Got endpoints: latency-svc-sq87b [195.345343ms]
Dec  5 19:46:59.932: INFO: Created: latency-svc-km8f7
Dec  5 19:46:59.934: INFO: Got endpoints: latency-svc-km8f7 [192.205723ms]
Dec  5 19:46:59.943: INFO: Created: latency-svc-zd47b
Dec  5 19:46:59.953: INFO: Created: latency-svc-22kd2
Dec  5 19:46:59.955: INFO: Got endpoints: latency-svc-zd47b [201.615445ms]
Dec  5 19:46:59.964: INFO: Created: latency-svc-c7twb
Dec  5 19:46:59.975: INFO: Created: latency-svc-bqm28
Dec  5 19:46:59.985: INFO: Created: latency-svc-nsjdq
Dec  5 19:47:00.000: INFO: Created: latency-svc-wvtcf
Dec  5 19:47:00.006: INFO: Got endpoints: latency-svc-22kd2 [241.141293ms]
Dec  5 19:47:00.012: INFO: Created: latency-svc-5sbdc
Dec  5 19:47:00.022: INFO: Created: latency-svc-k6fg2
Dec  5 19:47:00.032: INFO: Created: latency-svc-9jg7s
Dec  5 19:47:00.058: INFO: Got endpoints: latency-svc-c7twb [271.118409ms]
Dec  5 19:47:00.062: INFO: Created: latency-svc-pszbb
Dec  5 19:47:00.071: INFO: Created: latency-svc-86grn
Dec  5 19:47:00.083: INFO: Created: latency-svc-9w7vm
Dec  5 19:47:00.091: INFO: Created: latency-svc-g4nfq
Dec  5 19:47:00.101: INFO: Created: latency-svc-d84tz
Dec  5 19:47:00.108: INFO: Got endpoints: latency-svc-bqm28 [305.302878ms]
Dec  5 19:47:00.117: INFO: Created: latency-svc-cwwd7
Dec  5 19:47:00.124: INFO: Created: latency-svc-6255x
Dec  5 19:47:00.133: INFO: Created: latency-svc-g58jl
Dec  5 19:47:00.142: INFO: Created: latency-svc-knmft
Dec  5 19:47:00.152: INFO: Created: latency-svc-g9n56
Dec  5 19:47:00.157: INFO: Got endpoints: latency-svc-nsjdq [344.734075ms]
Dec  5 19:47:00.171: INFO: Created: latency-svc-p5nwr
Dec  5 19:47:00.205: INFO: Got endpoints: latency-svc-wvtcf [376.928895ms]
Dec  5 19:47:00.218: INFO: Created: latency-svc-72xl7
Dec  5 19:47:00.255: INFO: Got endpoints: latency-svc-5sbdc [413.872835ms]
Dec  5 19:47:00.267: INFO: Created: latency-svc-k88q8
Dec  5 19:47:00.305: INFO: Got endpoints: latency-svc-k6fg2 [450.748251ms]
Dec  5 19:47:00.319: INFO: Created: latency-svc-4mh65
Dec  5 19:47:00.355: INFO: Got endpoints: latency-svc-9jg7s [490.623787ms]
Dec  5 19:47:00.368: INFO: Created: latency-svc-vlhx4
Dec  5 19:47:00.406: INFO: Got endpoints: latency-svc-pszbb [530.754067ms]
Dec  5 19:47:00.419: INFO: Created: latency-svc-h5m5g
Dec  5 19:47:00.455: INFO: Got endpoints: latency-svc-86grn [568.159141ms]
Dec  5 19:47:00.467: INFO: Created: latency-svc-rv2bq
Dec  5 19:47:00.505: INFO: Got endpoints: latency-svc-9w7vm [598.341594ms]
Dec  5 19:47:00.518: INFO: Created: latency-svc-hcrh8
Dec  5 19:47:00.554: INFO: Got endpoints: latency-svc-g4nfq [641.691523ms]
Dec  5 19:47:00.571: INFO: Created: latency-svc-xsr54
Dec  5 19:47:00.605: INFO: Got endpoints: latency-svc-d84tz [680.15569ms]
Dec  5 19:47:00.620: INFO: Created: latency-svc-vz4p5
Dec  5 19:47:00.655: INFO: Got endpoints: latency-svc-cwwd7 [721.036767ms]
Dec  5 19:47:00.671: INFO: Created: latency-svc-hrgch
Dec  5 19:47:00.707: INFO: Got endpoints: latency-svc-6255x [751.620783ms]
Dec  5 19:47:00.720: INFO: Created: latency-svc-fp48n
Dec  5 19:47:00.755: INFO: Got endpoints: latency-svc-g58jl [748.363882ms]
Dec  5 19:47:00.768: INFO: Created: latency-svc-mjr98
Dec  5 19:47:00.805: INFO: Got endpoints: latency-svc-knmft [747.629902ms]
Dec  5 19:47:00.818: INFO: Created: latency-svc-g9xjb
Dec  5 19:47:00.855: INFO: Got endpoints: latency-svc-g9n56 [746.374444ms]
Dec  5 19:47:00.869: INFO: Created: latency-svc-qz9n9
Dec  5 19:47:00.905: INFO: Got endpoints: latency-svc-p5nwr [748.198136ms]
Dec  5 19:47:00.927: INFO: Created: latency-svc-s6zqm
Dec  5 19:47:00.955: INFO: Got endpoints: latency-svc-72xl7 [749.992468ms]
Dec  5 19:47:00.969: INFO: Created: latency-svc-h2d8l
Dec  5 19:47:01.005: INFO: Got endpoints: latency-svc-k88q8 [749.891299ms]
Dec  5 19:47:01.018: INFO: Created: latency-svc-2hwlf
Dec  5 19:47:01.055: INFO: Got endpoints: latency-svc-4mh65 [750.647087ms]
Dec  5 19:47:01.071: INFO: Created: latency-svc-h2p4l
Dec  5 19:47:01.105: INFO: Got endpoints: latency-svc-vlhx4 [749.444963ms]
Dec  5 19:47:01.118: INFO: Created: latency-svc-5lcz8
Dec  5 19:47:01.155: INFO: Got endpoints: latency-svc-h5m5g [749.31043ms]
Dec  5 19:47:01.170: INFO: Created: latency-svc-55zph
Dec  5 19:47:01.206: INFO: Got endpoints: latency-svc-rv2bq [750.752982ms]
Dec  5 19:47:01.221: INFO: Created: latency-svc-8kg25
Dec  5 19:47:01.259: INFO: Got endpoints: latency-svc-hcrh8 [753.358197ms]
Dec  5 19:47:01.273: INFO: Created: latency-svc-wx8fr
Dec  5 19:47:01.306: INFO: Got endpoints: latency-svc-xsr54 [751.003919ms]
Dec  5 19:47:01.320: INFO: Created: latency-svc-btkrk
Dec  5 19:47:01.356: INFO: Got endpoints: latency-svc-vz4p5 [750.702163ms]
Dec  5 19:47:01.369: INFO: Created: latency-svc-xj2kx
Dec  5 19:47:01.405: INFO: Got endpoints: latency-svc-hrgch [749.492004ms]
Dec  5 19:47:01.419: INFO: Created: latency-svc-gqq79
Dec  5 19:47:01.455: INFO: Got endpoints: latency-svc-fp48n [748.14155ms]
Dec  5 19:47:01.470: INFO: Created: latency-svc-86t92
Dec  5 19:47:01.505: INFO: Got endpoints: latency-svc-mjr98 [750.054393ms]
Dec  5 19:47:01.518: INFO: Created: latency-svc-nwb8k
Dec  5 19:47:01.556: INFO: Got endpoints: latency-svc-g9xjb [750.352715ms]
Dec  5 19:47:01.570: INFO: Created: latency-svc-zphcn
Dec  5 19:47:01.605: INFO: Got endpoints: latency-svc-qz9n9 [750.089748ms]
Dec  5 19:47:01.620: INFO: Created: latency-svc-jzprg
Dec  5 19:47:01.655: INFO: Got endpoints: latency-svc-s6zqm [749.613681ms]
Dec  5 19:47:01.669: INFO: Created: latency-svc-d8lmv
Dec  5 19:47:01.705: INFO: Got endpoints: latency-svc-h2d8l [750.024431ms]
Dec  5 19:47:01.719: INFO: Created: latency-svc-5wzfx
Dec  5 19:47:01.755: INFO: Got endpoints: latency-svc-2hwlf [750.934439ms]
Dec  5 19:47:01.771: INFO: Created: latency-svc-566nn
Dec  5 19:47:01.806: INFO: Got endpoints: latency-svc-h2p4l [750.311509ms]
Dec  5 19:47:01.820: INFO: Created: latency-svc-7vjs6
Dec  5 19:47:01.857: INFO: Got endpoints: latency-svc-5lcz8 [752.21472ms]
Dec  5 19:47:01.877: INFO: Created: latency-svc-sf8zx
Dec  5 19:47:01.905: INFO: Got endpoints: latency-svc-55zph [749.80571ms]
Dec  5 19:47:01.925: INFO: Created: latency-svc-ptxzd
Dec  5 19:47:01.955: INFO: Got endpoints: latency-svc-8kg25 [749.443835ms]
Dec  5 19:47:01.969: INFO: Created: latency-svc-xsslw
Dec  5 19:47:02.005: INFO: Got endpoints: latency-svc-wx8fr [746.379984ms]
Dec  5 19:47:02.018: INFO: Created: latency-svc-cw8pb
Dec  5 19:47:02.055: INFO: Got endpoints: latency-svc-btkrk [748.98175ms]
Dec  5 19:47:02.067: INFO: Created: latency-svc-7pc48
Dec  5 19:47:02.105: INFO: Got endpoints: latency-svc-xj2kx [749.193665ms]
Dec  5 19:47:02.118: INFO: Created: latency-svc-2f6qq
Dec  5 19:47:02.155: INFO: Got endpoints: latency-svc-gqq79 [750.156021ms]
Dec  5 19:47:02.193: INFO: Created: latency-svc-55dx8
Dec  5 19:47:02.205: INFO: Got endpoints: latency-svc-86t92 [749.858732ms]
Dec  5 19:47:02.217: INFO: Created: latency-svc-649wq
Dec  5 19:47:02.255: INFO: Got endpoints: latency-svc-nwb8k [749.801623ms]
Dec  5 19:47:02.269: INFO: Created: latency-svc-dlzcw
Dec  5 19:47:02.305: INFO: Got endpoints: latency-svc-zphcn [749.202122ms]
Dec  5 19:47:02.319: INFO: Created: latency-svc-6n25j
Dec  5 19:47:02.355: INFO: Got endpoints: latency-svc-jzprg [749.847651ms]
Dec  5 19:47:02.369: INFO: Created: latency-svc-vgn46
Dec  5 19:47:02.405: INFO: Got endpoints: latency-svc-d8lmv [749.952566ms]
Dec  5 19:47:02.420: INFO: Created: latency-svc-hszml
Dec  5 19:47:02.455: INFO: Got endpoints: latency-svc-5wzfx [749.246483ms]
Dec  5 19:47:02.469: INFO: Created: latency-svc-ntd26
Dec  5 19:47:02.505: INFO: Got endpoints: latency-svc-566nn [749.138662ms]
Dec  5 19:47:02.518: INFO: Created: latency-svc-sdnbb
Dec  5 19:47:02.555: INFO: Got endpoints: latency-svc-7vjs6 [748.9076ms]
Dec  5 19:47:02.568: INFO: Created: latency-svc-ldbvk
Dec  5 19:47:02.605: INFO: Got endpoints: latency-svc-sf8zx [747.888079ms]
Dec  5 19:47:02.618: INFO: Created: latency-svc-f5xvt
Dec  5 19:47:02.655: INFO: Got endpoints: latency-svc-ptxzd [749.657921ms]
Dec  5 19:47:02.669: INFO: Created: latency-svc-4c5mz
Dec  5 19:47:02.705: INFO: Got endpoints: latency-svc-xsslw [749.291858ms]
Dec  5 19:47:02.720: INFO: Created: latency-svc-bhmhh
Dec  5 19:47:02.755: INFO: Got endpoints: latency-svc-cw8pb [749.518038ms]
Dec  5 19:47:02.768: INFO: Created: latency-svc-bzr9m
Dec  5 19:47:02.806: INFO: Got endpoints: latency-svc-7pc48 [750.934045ms]
Dec  5 19:47:02.819: INFO: Created: latency-svc-xvk5n
Dec  5 19:47:02.855: INFO: Got endpoints: latency-svc-2f6qq [749.644011ms]
Dec  5 19:47:02.884: INFO: Created: latency-svc-62vt5
Dec  5 19:47:02.905: INFO: Got endpoints: latency-svc-55dx8 [750.099784ms]
Dec  5 19:47:02.922: INFO: Created: latency-svc-st8dw
Dec  5 19:47:02.955: INFO: Got endpoints: latency-svc-649wq [750.204165ms]
Dec  5 19:47:02.971: INFO: Created: latency-svc-6nbhw
Dec  5 19:47:03.005: INFO: Got endpoints: latency-svc-dlzcw [750.485938ms]
Dec  5 19:47:03.018: INFO: Created: latency-svc-q7b6x
Dec  5 19:47:03.055: INFO: Got endpoints: latency-svc-6n25j [750.338191ms]
Dec  5 19:47:03.069: INFO: Created: latency-svc-9tzr4
Dec  5 19:47:03.105: INFO: Got endpoints: latency-svc-vgn46 [749.621058ms]
Dec  5 19:47:03.119: INFO: Created: latency-svc-dbb82
Dec  5 19:47:03.155: INFO: Got endpoints: latency-svc-hszml [749.945655ms]
Dec  5 19:47:03.169: INFO: Created: latency-svc-jng96
Dec  5 19:47:03.207: INFO: Got endpoints: latency-svc-ntd26 [752.15837ms]
Dec  5 19:47:03.222: INFO: Created: latency-svc-dz5gg
Dec  5 19:47:03.255: INFO: Got endpoints: latency-svc-sdnbb [750.49516ms]
Dec  5 19:47:03.272: INFO: Created: latency-svc-cd54s
Dec  5 19:47:03.307: INFO: Got endpoints: latency-svc-ldbvk [752.161634ms]
Dec  5 19:47:03.322: INFO: Created: latency-svc-d7j8h
Dec  5 19:47:03.355: INFO: Got endpoints: latency-svc-f5xvt [749.788718ms]
Dec  5 19:47:03.367: INFO: Created: latency-svc-bgjc4
Dec  5 19:47:03.406: INFO: Got endpoints: latency-svc-4c5mz [750.795429ms]
Dec  5 19:47:03.420: INFO: Created: latency-svc-xx4h6
Dec  5 19:47:03.455: INFO: Got endpoints: latency-svc-bhmhh [750.588248ms]
Dec  5 19:47:03.469: INFO: Created: latency-svc-zqz6w
Dec  5 19:47:03.505: INFO: Got endpoints: latency-svc-bzr9m [749.847723ms]
Dec  5 19:47:03.517: INFO: Created: latency-svc-pp9sx
Dec  5 19:47:03.556: INFO: Got endpoints: latency-svc-xvk5n [749.755591ms]
Dec  5 19:47:03.569: INFO: Created: latency-svc-45vmc
Dec  5 19:47:03.606: INFO: Got endpoints: latency-svc-62vt5 [750.904313ms]
Dec  5 19:47:03.619: INFO: Created: latency-svc-pthcx
Dec  5 19:47:03.655: INFO: Got endpoints: latency-svc-st8dw [749.575809ms]
Dec  5 19:47:03.668: INFO: Created: latency-svc-4sxmw
Dec  5 19:47:03.705: INFO: Got endpoints: latency-svc-6nbhw [750.241666ms]
Dec  5 19:47:03.718: INFO: Created: latency-svc-hqvnh
Dec  5 19:47:03.756: INFO: Got endpoints: latency-svc-q7b6x [750.858996ms]
Dec  5 19:47:03.771: INFO: Created: latency-svc-vtkjx
Dec  5 19:47:03.805: INFO: Got endpoints: latency-svc-9tzr4 [749.286085ms]
Dec  5 19:47:03.820: INFO: Created: latency-svc-zhfkm
Dec  5 19:47:03.856: INFO: Got endpoints: latency-svc-dbb82 [751.670629ms]
Dec  5 19:47:03.870: INFO: Created: latency-svc-xvpnn
Dec  5 19:47:03.905: INFO: Got endpoints: latency-svc-jng96 [750.03545ms]
Dec  5 19:47:03.923: INFO: Created: latency-svc-s7jxd
Dec  5 19:47:03.957: INFO: Got endpoints: latency-svc-dz5gg [749.966954ms]
Dec  5 19:47:03.970: INFO: Created: latency-svc-z7ljt
Dec  5 19:47:04.005: INFO: Got endpoints: latency-svc-cd54s [749.860059ms]
Dec  5 19:47:04.020: INFO: Created: latency-svc-r68vt
Dec  5 19:47:04.057: INFO: Got endpoints: latency-svc-d7j8h [749.686111ms]
Dec  5 19:47:04.070: INFO: Created: latency-svc-fwhd8
Dec  5 19:47:04.105: INFO: Got endpoints: latency-svc-bgjc4 [750.683748ms]
Dec  5 19:47:04.119: INFO: Created: latency-svc-5jxlc
Dec  5 19:47:04.157: INFO: Got endpoints: latency-svc-xx4h6 [751.282186ms]
Dec  5 19:47:04.173: INFO: Created: latency-svc-m9t98
Dec  5 19:47:04.206: INFO: Got endpoints: latency-svc-zqz6w [750.797036ms]
Dec  5 19:47:04.219: INFO: Created: latency-svc-tb2gp
Dec  5 19:47:04.255: INFO: Got endpoints: latency-svc-pp9sx [750.359382ms]
Dec  5 19:47:04.268: INFO: Created: latency-svc-4zzxv
Dec  5 19:47:04.305: INFO: Got endpoints: latency-svc-45vmc [749.172988ms]
Dec  5 19:47:04.320: INFO: Created: latency-svc-bfrlv
Dec  5 19:47:04.355: INFO: Got endpoints: latency-svc-pthcx [749.205312ms]
Dec  5 19:47:04.370: INFO: Created: latency-svc-tdxgz
Dec  5 19:47:04.406: INFO: Got endpoints: latency-svc-4sxmw [751.222307ms]
Dec  5 19:47:04.420: INFO: Created: latency-svc-flv26
Dec  5 19:47:04.455: INFO: Got endpoints: latency-svc-hqvnh [749.647298ms]
Dec  5 19:47:04.468: INFO: Created: latency-svc-s9l8w
Dec  5 19:47:04.505: INFO: Got endpoints: latency-svc-vtkjx [749.306182ms]
Dec  5 19:47:04.519: INFO: Created: latency-svc-zvxjf
Dec  5 19:47:04.558: INFO: Got endpoints: latency-svc-zhfkm [753.705282ms]
Dec  5 19:47:04.571: INFO: Created: latency-svc-z94vf
Dec  5 19:47:04.605: INFO: Got endpoints: latency-svc-xvpnn [748.432432ms]
Dec  5 19:47:04.619: INFO: Created: latency-svc-65fvb
Dec  5 19:47:04.655: INFO: Got endpoints: latency-svc-s7jxd [749.780575ms]
Dec  5 19:47:04.685: INFO: Created: latency-svc-f27kk
Dec  5 19:47:04.705: INFO: Got endpoints: latency-svc-z7ljt [748.039678ms]
Dec  5 19:47:04.719: INFO: Created: latency-svc-99wpv
Dec  5 19:47:04.755: INFO: Got endpoints: latency-svc-r68vt [749.598042ms]
Dec  5 19:47:04.767: INFO: Created: latency-svc-js798
Dec  5 19:47:04.805: INFO: Got endpoints: latency-svc-fwhd8 [748.649235ms]
Dec  5 19:47:04.821: INFO: Created: latency-svc-82cq4
Dec  5 19:47:04.855: INFO: Got endpoints: latency-svc-5jxlc [749.659855ms]
Dec  5 19:47:04.870: INFO: Created: latency-svc-9dxtp
Dec  5 19:47:04.905: INFO: Got endpoints: latency-svc-m9t98 [748.169319ms]
Dec  5 19:47:04.921: INFO: Created: latency-svc-8ctr6
Dec  5 19:47:04.955: INFO: Got endpoints: latency-svc-tb2gp [748.610453ms]
Dec  5 19:47:04.967: INFO: Created: latency-svc-64glr
Dec  5 19:47:05.005: INFO: Got endpoints: latency-svc-4zzxv [749.349568ms]
Dec  5 19:47:05.024: INFO: Created: latency-svc-mltqw
Dec  5 19:47:05.055: INFO: Got endpoints: latency-svc-bfrlv [750.605962ms]
Dec  5 19:47:05.070: INFO: Created: latency-svc-g58w5
Dec  5 19:47:05.105: INFO: Got endpoints: latency-svc-tdxgz [749.897397ms]
Dec  5 19:47:05.118: INFO: Created: latency-svc-dgkfm
Dec  5 19:47:05.155: INFO: Got endpoints: latency-svc-flv26 [748.579897ms]
Dec  5 19:47:05.168: INFO: Created: latency-svc-qzsfk
Dec  5 19:47:05.205: INFO: Got endpoints: latency-svc-s9l8w [749.981845ms]
Dec  5 19:47:05.218: INFO: Created: latency-svc-jkd6q
Dec  5 19:47:05.255: INFO: Got endpoints: latency-svc-zvxjf [749.042248ms]
Dec  5 19:47:05.268: INFO: Created: latency-svc-bmh76
Dec  5 19:47:05.305: INFO: Got endpoints: latency-svc-z94vf [746.221359ms]
Dec  5 19:47:05.318: INFO: Created: latency-svc-87qb7
Dec  5 19:47:05.355: INFO: Got endpoints: latency-svc-65fvb [750.0882ms]
Dec  5 19:47:05.369: INFO: Created: latency-svc-nzqxp
Dec  5 19:47:05.406: INFO: Got endpoints: latency-svc-f27kk [751.075508ms]
Dec  5 19:47:05.423: INFO: Created: latency-svc-wh8kc
Dec  5 19:47:05.455: INFO: Got endpoints: latency-svc-99wpv [750.171336ms]
Dec  5 19:47:05.468: INFO: Created: latency-svc-tj6rh
Dec  5 19:47:05.505: INFO: Got endpoints: latency-svc-js798 [750.274461ms]
Dec  5 19:47:05.524: INFO: Created: latency-svc-47d9p
Dec  5 19:47:05.556: INFO: Got endpoints: latency-svc-82cq4 [750.746821ms]
Dec  5 19:47:05.570: INFO: Created: latency-svc-bkp67
Dec  5 19:47:05.605: INFO: Got endpoints: latency-svc-9dxtp [749.954072ms]
Dec  5 19:47:05.619: INFO: Created: latency-svc-lftqt
Dec  5 19:47:05.654: INFO: Got endpoints: latency-svc-8ctr6 [748.972025ms]
Dec  5 19:47:05.667: INFO: Created: latency-svc-z2qzf
Dec  5 19:47:05.706: INFO: Got endpoints: latency-svc-64glr [750.506199ms]
Dec  5 19:47:05.719: INFO: Created: latency-svc-hxhs5
Dec  5 19:47:05.755: INFO: Got endpoints: latency-svc-mltqw [750.554724ms]
Dec  5 19:47:05.771: INFO: Created: latency-svc-65l2k
Dec  5 19:47:05.805: INFO: Got endpoints: latency-svc-g58w5 [749.699007ms]
Dec  5 19:47:05.819: INFO: Created: latency-svc-769gp
Dec  5 19:47:05.855: INFO: Got endpoints: latency-svc-dgkfm [750.224039ms]
Dec  5 19:47:05.868: INFO: Created: latency-svc-sxfjw
Dec  5 19:47:05.905: INFO: Got endpoints: latency-svc-qzsfk [750.09923ms]
Dec  5 19:47:05.919: INFO: Created: latency-svc-tvjqh
Dec  5 19:47:05.955: INFO: Got endpoints: latency-svc-jkd6q [749.920095ms]
Dec  5 19:47:05.968: INFO: Created: latency-svc-hp4j6
Dec  5 19:47:06.006: INFO: Got endpoints: latency-svc-bmh76 [751.879637ms]
Dec  5 19:47:06.020: INFO: Created: latency-svc-pjcmn
Dec  5 19:47:06.055: INFO: Got endpoints: latency-svc-87qb7 [750.199644ms]
Dec  5 19:47:06.069: INFO: Created: latency-svc-ff4jj
Dec  5 19:47:06.105: INFO: Got endpoints: latency-svc-nzqxp [749.619521ms]
Dec  5 19:47:06.119: INFO: Created: latency-svc-mc4qf
Dec  5 19:47:06.155: INFO: Got endpoints: latency-svc-wh8kc [748.711881ms]
Dec  5 19:47:06.168: INFO: Created: latency-svc-mt2kh
Dec  5 19:47:06.205: INFO: Got endpoints: latency-svc-tj6rh [749.621661ms]
Dec  5 19:47:06.221: INFO: Created: latency-svc-9x65n
Dec  5 19:47:06.255: INFO: Got endpoints: latency-svc-47d9p [749.87042ms]
Dec  5 19:47:06.267: INFO: Created: latency-svc-lcbfv
Dec  5 19:47:06.305: INFO: Got endpoints: latency-svc-bkp67 [748.602133ms]
Dec  5 19:47:06.321: INFO: Created: latency-svc-f5vbz
Dec  5 19:47:06.355: INFO: Got endpoints: latency-svc-lftqt [749.76194ms]
Dec  5 19:47:06.369: INFO: Created: latency-svc-rtwdg
Dec  5 19:47:06.405: INFO: Got endpoints: latency-svc-z2qzf [750.709629ms]
Dec  5 19:47:06.419: INFO: Created: latency-svc-cngr6
Dec  5 19:47:06.455: INFO: Got endpoints: latency-svc-hxhs5 [749.484165ms]
Dec  5 19:47:06.469: INFO: Created: latency-svc-n2vjr
Dec  5 19:47:06.505: INFO: Got endpoints: latency-svc-65l2k [749.459301ms]
Dec  5 19:47:06.520: INFO: Created: latency-svc-22bl6
Dec  5 19:47:06.555: INFO: Got endpoints: latency-svc-769gp [749.852909ms]
Dec  5 19:47:06.567: INFO: Created: latency-svc-6mf7p
Dec  5 19:47:06.605: INFO: Got endpoints: latency-svc-sxfjw [749.703005ms]
Dec  5 19:47:06.618: INFO: Created: latency-svc-xvmgs
Dec  5 19:47:06.655: INFO: Got endpoints: latency-svc-tvjqh [750.289141ms]
Dec  5 19:47:06.669: INFO: Created: latency-svc-67vq4
Dec  5 19:47:06.707: INFO: Got endpoints: latency-svc-hp4j6 [751.850491ms]
Dec  5 19:47:06.722: INFO: Created: latency-svc-vb7tg
Dec  5 19:47:06.755: INFO: Got endpoints: latency-svc-pjcmn [748.521423ms]
Dec  5 19:47:06.768: INFO: Created: latency-svc-pdpvq
Dec  5 19:47:06.806: INFO: Got endpoints: latency-svc-ff4jj [750.550537ms]
Dec  5 19:47:06.820: INFO: Created: latency-svc-7mq6m
Dec  5 19:47:06.855: INFO: Got endpoints: latency-svc-mc4qf [749.990463ms]
Dec  5 19:47:06.868: INFO: Created: latency-svc-mfzs4
Dec  5 19:47:06.906: INFO: Got endpoints: latency-svc-mt2kh [751.101873ms]
Dec  5 19:47:06.921: INFO: Created: latency-svc-9bpht
Dec  5 19:47:06.955: INFO: Got endpoints: latency-svc-9x65n [750.264694ms]
Dec  5 19:47:06.968: INFO: Created: latency-svc-kllht
Dec  5 19:47:07.005: INFO: Got endpoints: latency-svc-lcbfv [749.518887ms]
Dec  5 19:47:07.018: INFO: Created: latency-svc-8tkms
Dec  5 19:47:07.055: INFO: Got endpoints: latency-svc-f5vbz [749.719668ms]
Dec  5 19:47:07.070: INFO: Created: latency-svc-94w4b
Dec  5 19:47:07.104: INFO: Got endpoints: latency-svc-rtwdg [749.461849ms]
Dec  5 19:47:07.117: INFO: Created: latency-svc-rcb9d
Dec  5 19:47:07.155: INFO: Got endpoints: latency-svc-cngr6 [749.863156ms]
Dec  5 19:47:07.186: INFO: Created: latency-svc-vvvt9
Dec  5 19:47:07.205: INFO: Got endpoints: latency-svc-n2vjr [749.752221ms]
Dec  5 19:47:07.219: INFO: Created: latency-svc-7hrzj
Dec  5 19:47:07.255: INFO: Got endpoints: latency-svc-22bl6 [749.93607ms]
Dec  5 19:47:07.306: INFO: Got endpoints: latency-svc-6mf7p [751.176278ms]
Dec  5 19:47:07.355: INFO: Got endpoints: latency-svc-xvmgs [750.148872ms]
Dec  5 19:47:07.405: INFO: Got endpoints: latency-svc-67vq4 [749.379785ms]
Dec  5 19:47:07.455: INFO: Got endpoints: latency-svc-vb7tg [748.290414ms]
Dec  5 19:47:07.505: INFO: Got endpoints: latency-svc-pdpvq [749.756868ms]
Dec  5 19:47:07.555: INFO: Got endpoints: latency-svc-7mq6m [749.900964ms]
Dec  5 19:47:07.605: INFO: Got endpoints: latency-svc-mfzs4 [750.535762ms]
Dec  5 19:47:07.654: INFO: Got endpoints: latency-svc-9bpht [748.431818ms]
Dec  5 19:47:07.705: INFO: Got endpoints: latency-svc-kllht [749.531256ms]
Dec  5 19:47:07.755: INFO: Got endpoints: latency-svc-8tkms [750.055735ms]
Dec  5 19:47:07.805: INFO: Got endpoints: latency-svc-94w4b [750.70378ms]
Dec  5 19:47:07.855: INFO: Got endpoints: latency-svc-rcb9d [750.56373ms]
Dec  5 19:47:07.907: INFO: Got endpoints: latency-svc-vvvt9 [752.00352ms]
Dec  5 19:47:07.955: INFO: Got endpoints: latency-svc-7hrzj [750.502978ms]
Dec  5 19:47:07.956: INFO: Latencies: [25.202417ms 40.121983ms 53.624392ms 72.479729ms 89.14425ms 103.160037ms 116.676765ms 126.548248ms 138.10068ms 153.039548ms 168.343294ms 182.503297ms 187.307461ms 189.187003ms 189.407744ms 190.820624ms 191.270634ms 192.205723ms 192.650578ms 192.857652ms 193.174362ms 193.212855ms 193.744668ms 193.9993ms 194.389413ms 194.735771ms 195.007896ms 195.169283ms 195.345343ms 195.483084ms 198.203995ms 199.088283ms 199.804844ms 201.486867ms 201.615445ms 207.430835ms 213.128966ms 214.657052ms 215.388816ms 225.331149ms 241.141293ms 271.118409ms 305.302878ms 344.734075ms 376.928895ms 413.872835ms 450.748251ms 490.623787ms 530.754067ms 568.159141ms 598.341594ms 641.691523ms 680.15569ms 721.036767ms 746.221359ms 746.374444ms 746.379984ms 747.629902ms 747.888079ms 748.039678ms 748.14155ms 748.169319ms 748.198136ms 748.290414ms 748.363882ms 748.431818ms 748.432432ms 748.521423ms 748.579897ms 748.602133ms 748.610453ms 748.649235ms 748.711881ms 748.9076ms 748.972025ms 748.98175ms 749.042248ms 749.138662ms 749.172988ms 749.193665ms 749.202122ms 749.205312ms 749.246483ms 749.286085ms 749.291858ms 749.306182ms 749.31043ms 749.349568ms 749.379785ms 749.443835ms 749.444963ms 749.459301ms 749.461849ms 749.484165ms 749.492004ms 749.518038ms 749.518887ms 749.531256ms 749.575809ms 749.598042ms 749.613681ms 749.619521ms 749.621058ms 749.621661ms 749.644011ms 749.647298ms 749.657921ms 749.659855ms 749.686111ms 749.699007ms 749.703005ms 749.719668ms 749.752221ms 749.755591ms 749.756868ms 749.76194ms 749.780575ms 749.788718ms 749.801623ms 749.80571ms 749.847651ms 749.847723ms 749.852909ms 749.858732ms 749.860059ms 749.863156ms 749.87042ms 749.891299ms 749.897397ms 749.900964ms 749.920095ms 749.93607ms 749.945655ms 749.952566ms 749.954072ms 749.966954ms 749.981845ms 749.990463ms 749.992468ms 750.024431ms 750.03545ms 750.054393ms 750.055735ms 750.0882ms 750.089748ms 750.09923ms 750.099784ms 750.148872ms 750.156021ms 750.171336ms 750.199644ms 750.204165ms 750.224039ms 750.241666ms 750.264694ms 750.274461ms 750.289141ms 750.311509ms 750.338191ms 750.352715ms 750.359382ms 750.485938ms 750.49516ms 750.502978ms 750.506199ms 750.535762ms 750.550537ms 750.554724ms 750.56373ms 750.588248ms 750.605962ms 750.647087ms 750.683748ms 750.702163ms 750.70378ms 750.709629ms 750.746821ms 750.752982ms 750.795429ms 750.797036ms 750.858996ms 750.904313ms 750.934045ms 750.934439ms 751.003919ms 751.075508ms 751.101873ms 751.176278ms 751.222307ms 751.282186ms 751.620783ms 751.670629ms 751.850491ms 751.879637ms 752.00352ms 752.15837ms 752.161634ms 752.21472ms 753.358197ms 753.705282ms]
Dec  5 19:47:07.956: INFO: 50 %ile: 749.613681ms
Dec  5 19:47:07.956: INFO: 90 %ile: 750.858996ms
Dec  5 19:47:07.956: INFO: 99 %ile: 753.358197ms
Dec  5 19:47:07.956: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:47:07.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6335" for this suite.
Dec  5 19:47:29.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:47:30.102: INFO: namespace svc-latency-6335 deletion completed in 22.140774278s

• [SLOW TEST:33.922 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:47:30.103: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-7557
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7557
STEP: Deleting pre-stop pod
Dec  5 19:47:39.208: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:47:39.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7557" for this suite.
Dec  5 19:48:17.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:48:17.372: INFO: namespace prestop-7557 deletion completed in 38.146365406s

• [SLOW TEST:47.269 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:48:17.373: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Dec  5 19:48:48.010: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 13
	[quantile=0.9] = 94
	[quantile=0.99] = 94
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 215121
	[quantile=0.9] = 221723
	[quantile=0.99] = 221723
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = 10
	[quantile=0.9] = 10
	[quantile=0.99] = 10
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = 2832
	[quantile=0.9] = 2832
	[quantile=0.99] = 2832
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 4
	[quantile=0.9] = 10
	[quantile=0.99] = 43
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 12
	[quantile=0.9] = 32
	[quantile=0.99] = 71
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 14
	[quantile=0.9] = 31
	[quantile=0.99] = 65
For namespace_queue_latency_sum:
	[] = 3613
For namespace_queue_latency_count:
	[] = 208
For namespace_retries:
	[] = 211
For namespace_work_duration:
	[quantile=0.5] = 160607
	[quantile=0.9] = 246517
	[quantile=0.99] = 437513
For namespace_work_duration_sum:
	[] = 30990955
For namespace_work_duration_count:
	[] = 208
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:48:48.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9715" for this suite.
Dec  5 19:48:54.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:48:54.164: INFO: namespace gc-9715 deletion completed in 6.148656086s

• [SLOW TEST:36.791 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:48:54.164: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-449f3f31-1798-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume configMaps
Dec  5 19:48:54.211: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-44a03d8b-1798-11ea-a680-be755c6bedde" in namespace "projected-3421" to be "success or failure"
Dec  5 19:48:54.214: INFO: Pod "pod-projected-configmaps-44a03d8b-1798-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.506469ms
Dec  5 19:48:56.219: INFO: Pod "pod-projected-configmaps-44a03d8b-1798-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008714121s
STEP: Saw pod success
Dec  5 19:48:56.219: INFO: Pod "pod-projected-configmaps-44a03d8b-1798-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:48:56.224: INFO: Trying to get logs from node puma pod pod-projected-configmaps-44a03d8b-1798-11ea-a680-be755c6bedde container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 19:48:56.252: INFO: Waiting for pod pod-projected-configmaps-44a03d8b-1798-11ea-a680-be755c6bedde to disappear
Dec  5 19:48:56.256: INFO: Pod pod-projected-configmaps-44a03d8b-1798-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:48:56.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3421" for this suite.
Dec  5 19:49:02.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:49:02.396: INFO: namespace projected-3421 deletion completed in 6.135015888s

• [SLOW TEST:8.232 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:49:02.397: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec  5 19:49:02.434: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:49:13.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9297" for this suite.
Dec  5 19:49:19.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:49:19.697: INFO: namespace pods-9297 deletion completed in 6.128510819s

• [SLOW TEST:17.300 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:49:19.698: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  5 19:49:19.743: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53d7a8e8-1798-11ea-a680-be755c6bedde" in namespace "projected-8502" to be "success or failure"
Dec  5 19:49:19.747: INFO: Pod "downwardapi-volume-53d7a8e8-1798-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.664659ms
Dec  5 19:49:21.751: INFO: Pod "downwardapi-volume-53d7a8e8-1798-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008619754s
STEP: Saw pod success
Dec  5 19:49:21.752: INFO: Pod "downwardapi-volume-53d7a8e8-1798-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:49:21.756: INFO: Trying to get logs from node puma pod downwardapi-volume-53d7a8e8-1798-11ea-a680-be755c6bedde container client-container: <nil>
STEP: delete the pod
Dec  5 19:49:21.781: INFO: Waiting for pod downwardapi-volume-53d7a8e8-1798-11ea-a680-be755c6bedde to disappear
Dec  5 19:49:21.784: INFO: Pod downwardapi-volume-53d7a8e8-1798-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:49:21.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8502" for this suite.
Dec  5 19:49:27.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:49:27.928: INFO: namespace projected-8502 deletion completed in 6.138770679s

• [SLOW TEST:8.230 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:49:27.928: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-58c01f05-1798-11ea-a680-be755c6bedde
STEP: Creating secret with name s-test-opt-upd-58c01f48-1798-11ea-a680-be755c6bedde
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-58c01f05-1798-11ea-a680-be755c6bedde
STEP: Updating secret s-test-opt-upd-58c01f48-1798-11ea-a680-be755c6bedde
STEP: Creating secret with name s-test-opt-create-58c01f62-1798-11ea-a680-be755c6bedde
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:49:34.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6732" for this suite.
Dec  5 19:49:56.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:49:56.256: INFO: namespace projected-6732 deletion completed in 22.137219661s

• [SLOW TEST:28.328 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:49:56.256: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  5 19:49:56.302: INFO: Waiting up to 5m0s for pod "downwardapi-volume-69a1fad1-1798-11ea-a680-be755c6bedde" in namespace "downward-api-769" to be "success or failure"
Dec  5 19:49:56.308: INFO: Pod "downwardapi-volume-69a1fad1-1798-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 6.060856ms
Dec  5 19:49:58.316: INFO: Pod "downwardapi-volume-69a1fad1-1798-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013478088s
Dec  5 19:50:00.321: INFO: Pod "downwardapi-volume-69a1fad1-1798-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018868045s
STEP: Saw pod success
Dec  5 19:50:00.321: INFO: Pod "downwardapi-volume-69a1fad1-1798-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:50:00.328: INFO: Trying to get logs from node puma pod downwardapi-volume-69a1fad1-1798-11ea-a680-be755c6bedde container client-container: <nil>
STEP: delete the pod
Dec  5 19:50:00.353: INFO: Waiting for pod downwardapi-volume-69a1fad1-1798-11ea-a680-be755c6bedde to disappear
Dec  5 19:50:00.357: INFO: Pod downwardapi-volume-69a1fad1-1798-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:50:00.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-769" for this suite.
Dec  5 19:50:06.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:50:06.486: INFO: namespace downward-api-769 deletion completed in 6.123342803s

• [SLOW TEST:10.230 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:50:06.486: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Dec  5 19:50:06.525: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-935805400 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:50:06.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1570" for this suite.
Dec  5 19:50:12.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:50:12.779: INFO: namespace kubectl-1570 deletion completed in 6.134570245s

• [SLOW TEST:6.293 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:50:12.780: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-737ad390-1798-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume configMaps
Dec  5 19:50:12.826: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-737be043-1798-11ea-a680-be755c6bedde" in namespace "projected-3482" to be "success or failure"
Dec  5 19:50:12.831: INFO: Pod "pod-projected-configmaps-737be043-1798-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 5.211022ms
Dec  5 19:50:14.836: INFO: Pod "pod-projected-configmaps-737be043-1798-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009608781s
STEP: Saw pod success
Dec  5 19:50:14.836: INFO: Pod "pod-projected-configmaps-737be043-1798-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:50:14.840: INFO: Trying to get logs from node puma pod pod-projected-configmaps-737be043-1798-11ea-a680-be755c6bedde container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 19:50:14.861: INFO: Waiting for pod pod-projected-configmaps-737be043-1798-11ea-a680-be755c6bedde to disappear
Dec  5 19:50:14.863: INFO: Pod pod-projected-configmaps-737be043-1798-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:50:14.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3482" for this suite.
Dec  5 19:50:20.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:50:21.008: INFO: namespace projected-3482 deletion completed in 6.140287913s

• [SLOW TEST:8.228 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:50:21.011: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec  5 19:50:21.278: INFO: Pod name wrapped-volume-race-78815637-1798-11ea-a680-be755c6bedde: Found 3 pods out of 5
Dec  5 19:50:26.287: INFO: Pod name wrapped-volume-race-78815637-1798-11ea-a680-be755c6bedde: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-78815637-1798-11ea-a680-be755c6bedde in namespace emptydir-wrapper-1770, will wait for the garbage collector to delete the pods
Dec  5 19:50:36.401: INFO: Deleting ReplicationController wrapped-volume-race-78815637-1798-11ea-a680-be755c6bedde took: 16.206456ms
Dec  5 19:50:36.702: INFO: Terminating ReplicationController wrapped-volume-race-78815637-1798-11ea-a680-be755c6bedde pods took: 300.391634ms
STEP: Creating RC which spawns configmap-volume pods
Dec  5 19:51:24.026: INFO: Pod name wrapped-volume-race-9de9ad42-1798-11ea-a680-be755c6bedde: Found 0 pods out of 5
Dec  5 19:51:29.033: INFO: Pod name wrapped-volume-race-9de9ad42-1798-11ea-a680-be755c6bedde: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9de9ad42-1798-11ea-a680-be755c6bedde in namespace emptydir-wrapper-1770, will wait for the garbage collector to delete the pods
Dec  5 19:51:39.138: INFO: Deleting ReplicationController wrapped-volume-race-9de9ad42-1798-11ea-a680-be755c6bedde took: 12.75552ms
Dec  5 19:51:39.439: INFO: Terminating ReplicationController wrapped-volume-race-9de9ad42-1798-11ea-a680-be755c6bedde pods took: 300.386307ms
STEP: Creating RC which spawns configmap-volume pods
Dec  5 19:52:24.063: INFO: Pod name wrapped-volume-race-c1b29b82-1798-11ea-a680-be755c6bedde: Found 0 pods out of 5
Dec  5 19:52:29.071: INFO: Pod name wrapped-volume-race-c1b29b82-1798-11ea-a680-be755c6bedde: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c1b29b82-1798-11ea-a680-be755c6bedde in namespace emptydir-wrapper-1770, will wait for the garbage collector to delete the pods
Dec  5 19:52:41.196: INFO: Deleting ReplicationController wrapped-volume-race-c1b29b82-1798-11ea-a680-be755c6bedde took: 16.908723ms
Dec  5 19:52:41.496: INFO: Terminating ReplicationController wrapped-volume-race-c1b29b82-1798-11ea-a680-be755c6bedde pods took: 300.273516ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:53:23.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1770" for this suite.
Dec  5 19:53:29.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:53:30.053: INFO: namespace emptydir-wrapper-1770 deletion completed in 6.134959772s

• [SLOW TEST:189.042 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:53:30.053: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec  5 19:53:30.108: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1232,SelfLink:/api/v1/namespaces/watch-1232/configmaps/e2e-watch-test-label-changed,UID:e910a364-1798-11ea-b857-a68228aedf28,ResourceVersion:43889,Generation:0,CreationTimestamp:2019-12-05 19:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  5 19:53:30.109: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1232,SelfLink:/api/v1/namespaces/watch-1232/configmaps/e2e-watch-test-label-changed,UID:e910a364-1798-11ea-b857-a68228aedf28,ResourceVersion:43890,Generation:0,CreationTimestamp:2019-12-05 19:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  5 19:53:30.109: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1232,SelfLink:/api/v1/namespaces/watch-1232/configmaps/e2e-watch-test-label-changed,UID:e910a364-1798-11ea-b857-a68228aedf28,ResourceVersion:43891,Generation:0,CreationTimestamp:2019-12-05 19:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec  5 19:53:40.150: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1232,SelfLink:/api/v1/namespaces/watch-1232/configmaps/e2e-watch-test-label-changed,UID:e910a364-1798-11ea-b857-a68228aedf28,ResourceVersion:43915,Generation:0,CreationTimestamp:2019-12-05 19:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  5 19:53:40.150: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1232,SelfLink:/api/v1/namespaces/watch-1232/configmaps/e2e-watch-test-label-changed,UID:e910a364-1798-11ea-b857-a68228aedf28,ResourceVersion:43916,Generation:0,CreationTimestamp:2019-12-05 19:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec  5 19:53:40.150: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1232,SelfLink:/api/v1/namespaces/watch-1232/configmaps/e2e-watch-test-label-changed,UID:e910a364-1798-11ea-b857-a68228aedf28,ResourceVersion:43917,Generation:0,CreationTimestamp:2019-12-05 19:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:53:40.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1232" for this suite.
Dec  5 19:53:46.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:53:46.293: INFO: namespace watch-1232 deletion completed in 6.136889659s

• [SLOW TEST:16.240 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:53:46.294: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-7054
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  5 19:53:46.326: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  5 19:54:04.443: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 25.0.2.14 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7054 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 19:54:04.443: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 19:54:05.623: INFO: Found all expected endpoints: [netserver-0]
Dec  5 19:54:05.628: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 25.0.1.251 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7054 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 19:54:05.628: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 19:54:06.782: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:54:06.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7054" for this suite.
Dec  5 19:54:28.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:54:28.916: INFO: namespace pod-network-test-7054 deletion completed in 22.127434867s

• [SLOW TEST:42.623 seconds]
[sig-network] Networking
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:54:28.918: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-0c26f7fb-1799-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume configMaps
Dec  5 19:54:28.966: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0c2800b8-1799-11ea-a680-be755c6bedde" in namespace "projected-4871" to be "success or failure"
Dec  5 19:54:28.970: INFO: Pod "pod-projected-configmaps-0c2800b8-1799-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.197008ms
Dec  5 19:54:30.975: INFO: Pod "pod-projected-configmaps-0c2800b8-1799-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008831094s
STEP: Saw pod success
Dec  5 19:54:30.975: INFO: Pod "pod-projected-configmaps-0c2800b8-1799-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:54:30.980: INFO: Trying to get logs from node puma pod pod-projected-configmaps-0c2800b8-1799-11ea-a680-be755c6bedde container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 19:54:31.008: INFO: Waiting for pod pod-projected-configmaps-0c2800b8-1799-11ea-a680-be755c6bedde to disappear
Dec  5 19:54:31.011: INFO: Pod pod-projected-configmaps-0c2800b8-1799-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:54:31.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4871" for this suite.
Dec  5 19:54:37.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:54:37.159: INFO: namespace projected-4871 deletion completed in 6.142464876s

• [SLOW TEST:8.241 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:54:37.161: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-pgdw
STEP: Creating a pod to test atomic-volume-subpath
Dec  5 19:54:37.218: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pgdw" in namespace "subpath-2198" to be "success or failure"
Dec  5 19:54:37.223: INFO: Pod "pod-subpath-test-configmap-pgdw": Phase="Pending", Reason="", readiness=false. Elapsed: 5.773961ms
Dec  5 19:54:39.229: INFO: Pod "pod-subpath-test-configmap-pgdw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011402503s
Dec  5 19:54:41.235: INFO: Pod "pod-subpath-test-configmap-pgdw": Phase="Running", Reason="", readiness=true. Elapsed: 4.017574832s
Dec  5 19:54:43.241: INFO: Pod "pod-subpath-test-configmap-pgdw": Phase="Running", Reason="", readiness=true. Elapsed: 6.023815318s
Dec  5 19:54:45.247: INFO: Pod "pod-subpath-test-configmap-pgdw": Phase="Running", Reason="", readiness=true. Elapsed: 8.029343769s
Dec  5 19:54:47.253: INFO: Pod "pod-subpath-test-configmap-pgdw": Phase="Running", Reason="", readiness=true. Elapsed: 10.035430117s
Dec  5 19:54:49.258: INFO: Pod "pod-subpath-test-configmap-pgdw": Phase="Running", Reason="", readiness=true. Elapsed: 12.040634435s
Dec  5 19:54:51.264: INFO: Pod "pod-subpath-test-configmap-pgdw": Phase="Running", Reason="", readiness=true. Elapsed: 14.046734826s
Dec  5 19:54:53.271: INFO: Pod "pod-subpath-test-configmap-pgdw": Phase="Running", Reason="", readiness=true. Elapsed: 16.053270478s
Dec  5 19:54:55.277: INFO: Pod "pod-subpath-test-configmap-pgdw": Phase="Running", Reason="", readiness=true. Elapsed: 18.058919371s
Dec  5 19:54:57.283: INFO: Pod "pod-subpath-test-configmap-pgdw": Phase="Running", Reason="", readiness=true. Elapsed: 20.065395291s
Dec  5 19:54:59.289: INFO: Pod "pod-subpath-test-configmap-pgdw": Phase="Running", Reason="", readiness=true. Elapsed: 22.071054229s
Dec  5 19:55:01.294: INFO: Pod "pod-subpath-test-configmap-pgdw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.076299932s
STEP: Saw pod success
Dec  5 19:55:01.294: INFO: Pod "pod-subpath-test-configmap-pgdw" satisfied condition "success or failure"
Dec  5 19:55:01.299: INFO: Trying to get logs from node crayfish pod pod-subpath-test-configmap-pgdw container test-container-subpath-configmap-pgdw: <nil>
STEP: delete the pod
Dec  5 19:55:01.327: INFO: Waiting for pod pod-subpath-test-configmap-pgdw to disappear
Dec  5 19:55:01.332: INFO: Pod pod-subpath-test-configmap-pgdw no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pgdw
Dec  5 19:55:01.332: INFO: Deleting pod "pod-subpath-test-configmap-pgdw" in namespace "subpath-2198"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:55:01.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2198" for this suite.
Dec  5 19:55:07.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:55:07.473: INFO: namespace subpath-2198 deletion completed in 6.133712947s

• [SLOW TEST:30.312 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:55:07.473: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  5 19:55:12.072: INFO: Successfully updated pod "pod-update-2321b3ad-1799-11ea-a680-be755c6bedde"
STEP: verifying the updated pod is in kubernetes
Dec  5 19:55:12.082: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:55:12.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4825" for this suite.
Dec  5 19:55:34.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:55:34.239: INFO: namespace pods-4825 deletion completed in 22.150517121s

• [SLOW TEST:26.766 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:55:34.240: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-33172b32-1799-11ea-a680-be755c6bedde
STEP: Creating configMap with name cm-test-opt-upd-33172b6f-1799-11ea-a680-be755c6bedde
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-33172b32-1799-11ea-a680-be755c6bedde
STEP: Updating configmap cm-test-opt-upd-33172b6f-1799-11ea-a680-be755c6bedde
STEP: Creating configMap with name cm-test-opt-create-33172b86-1799-11ea-a680-be755c6bedde
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:56:58.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6902" for this suite.
Dec  5 19:57:20.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:57:21.107: INFO: namespace projected-6902 deletion completed in 22.141849007s

• [SLOW TEST:106.867 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:57:21.107: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5220
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  5 19:57:21.143: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  5 19:57:45.239: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://25.0.2.17:8080/dial?request=hostName&protocol=http&host=25.0.2.16&port=8080&tries=1'] Namespace:pod-network-test-5220 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 19:57:45.239: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 19:57:45.392: INFO: Waiting for endpoints: map[]
Dec  5 19:57:45.397: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://25.0.2.17:8080/dial?request=hostName&protocol=http&host=25.0.1.3&port=8080&tries=1'] Namespace:pod-network-test-5220 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 19:57:45.397: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 19:57:45.516: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:57:45.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5220" for this suite.
Dec  5 19:58:07.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:58:07.658: INFO: namespace pod-network-test-5220 deletion completed in 22.135285614s

• [SLOW TEST:46.551 seconds]
[sig-network] Networking
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:58:07.658: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2239.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2239.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  5 19:58:09.760: INFO: DNS probes using dns-2239/dns-test-8e8852f8-1799-11ea-a680-be755c6bedde succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:58:09.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2239" for this suite.
Dec  5 19:58:15.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:58:15.915: INFO: namespace dns-2239 deletion completed in 6.134225929s

• [SLOW TEST:8.257 seconds]
[sig-network] DNS
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:58:15.918: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Dec  5 19:58:15.954: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  5 19:58:15.963: INFO: Waiting for terminating namespaces to be deleted...
Dec  5 19:58:15.966: INFO: 
Logging pods the kubelet thinks is on node crayfish before test
Dec  5 19:58:15.973: INFO: kube-proxy-xb74r from kube-system started at 2019-12-05 15:20:56 +0000 UTC (1 container statuses recorded)
Dec  5 19:58:15.973: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  5 19:58:15.973: INFO: sonobuoy-e2e-job-69046a85250743c3 from sonobuoy started at 2019-12-05 19:08:12 +0000 UTC (2 container statuses recorded)
Dec  5 19:58:15.973: INFO: 	Container e2e ready: true, restart count 0
Dec  5 19:58:15.973: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 19:58:15.973: INFO: sonobuoy-systemd-logs-daemon-set-892d09b6f39b465a-wx8xw from sonobuoy started at 2019-12-05 19:08:12 +0000 UTC (2 container statuses recorded)
Dec  5 19:58:15.973: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 19:58:15.973: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  5 19:58:15.973: INFO: kube-flannel-ds-amd64-zqml7 from kube-system started at 2019-12-05 15:20:56 +0000 UTC (1 container statuses recorded)
Dec  5 19:58:15.973: INFO: 	Container kube-flannel ready: true, restart count 0
Dec  5 19:58:15.973: INFO: 
Logging pods the kubelet thinks is on node puma before test
Dec  5 19:58:15.980: INFO: kube-flannel-ds-amd64-5ql7s from kube-system started at 2019-12-05 15:20:44 +0000 UTC (1 container statuses recorded)
Dec  5 19:58:15.980: INFO: 	Container kube-flannel ready: true, restart count 0
Dec  5 19:58:15.980: INFO: kube-proxy-wwvd5 from kube-system started at 2019-12-05 15:20:44 +0000 UTC (1 container statuses recorded)
Dec  5 19:58:15.980: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  5 19:58:15.980: INFO: sonobuoy from sonobuoy started at 2019-12-05 19:08:11 +0000 UTC (1 container statuses recorded)
Dec  5 19:58:15.980: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  5 19:58:15.980: INFO: sonobuoy-systemd-logs-daemon-set-892d09b6f39b465a-74vqz from sonobuoy started at 2019-12-05 19:08:12 +0000 UTC (2 container statuses recorded)
Dec  5 19:58:15.980: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 19:58:15.980: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-94b04d56-1799-11ea-a680-be755c6bedde 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-94b04d56-1799-11ea-a680-be755c6bedde off the node crayfish
STEP: verifying the node doesn't have the label kubernetes.io/e2e-94b04d56-1799-11ea-a680-be755c6bedde
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:58:22.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3101" for this suite.
Dec  5 19:58:30.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:58:30.226: INFO: namespace sched-pred-3101 deletion completed in 8.142829218s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:14.308 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:58:30.226: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  5 19:58:30.269: INFO: Waiting up to 5m0s for pod "pod-9bfb5129-1799-11ea-a680-be755c6bedde" in namespace "emptydir-8327" to be "success or failure"
Dec  5 19:58:30.274: INFO: Pod "pod-9bfb5129-1799-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.584851ms
Dec  5 19:58:32.279: INFO: Pod "pod-9bfb5129-1799-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010207014s
STEP: Saw pod success
Dec  5 19:58:32.280: INFO: Pod "pod-9bfb5129-1799-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:58:32.285: INFO: Trying to get logs from node puma pod pod-9bfb5129-1799-11ea-a680-be755c6bedde container test-container: <nil>
STEP: delete the pod
Dec  5 19:58:32.313: INFO: Waiting for pod pod-9bfb5129-1799-11ea-a680-be755c6bedde to disappear
Dec  5 19:58:32.317: INFO: Pod pod-9bfb5129-1799-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:58:32.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8327" for this suite.
Dec  5 19:58:38.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:58:38.469: INFO: namespace emptydir-8327 deletion completed in 6.14692699s

• [SLOW TEST:8.243 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:58:38.470: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-547/configmap-test-a0e5fc27-1799-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume configMaps
Dec  5 19:58:38.525: INFO: Waiting up to 5m0s for pod "pod-configmaps-a0e73923-1799-11ea-a680-be755c6bedde" in namespace "configmap-547" to be "success or failure"
Dec  5 19:58:38.528: INFO: Pod "pod-configmaps-a0e73923-1799-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.769134ms
Dec  5 19:58:40.535: INFO: Pod "pod-configmaps-a0e73923-1799-11ea-a680-be755c6bedde": Phase="Running", Reason="", readiness=true. Elapsed: 2.010043738s
Dec  5 19:58:42.541: INFO: Pod "pod-configmaps-a0e73923-1799-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016041553s
STEP: Saw pod success
Dec  5 19:58:42.541: INFO: Pod "pod-configmaps-a0e73923-1799-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:58:42.546: INFO: Trying to get logs from node crayfish pod pod-configmaps-a0e73923-1799-11ea-a680-be755c6bedde container env-test: <nil>
STEP: delete the pod
Dec  5 19:58:42.573: INFO: Waiting for pod pod-configmaps-a0e73923-1799-11ea-a680-be755c6bedde to disappear
Dec  5 19:58:42.577: INFO: Pod pod-configmaps-a0e73923-1799-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:58:42.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-547" for this suite.
Dec  5 19:58:48.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:58:48.718: INFO: namespace configmap-547 deletion completed in 6.136296683s

• [SLOW TEST:10.249 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:58:48.720: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 19:58:50.804: INFO: Waiting up to 5m0s for pod "client-envvars-a8390f6a-1799-11ea-a680-be755c6bedde" in namespace "pods-6636" to be "success or failure"
Dec  5 19:58:50.810: INFO: Pod "client-envvars-a8390f6a-1799-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 5.788218ms
Dec  5 19:58:52.816: INFO: Pod "client-envvars-a8390f6a-1799-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011737805s
Dec  5 19:58:54.822: INFO: Pod "client-envvars-a8390f6a-1799-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017725462s
STEP: Saw pod success
Dec  5 19:58:54.822: INFO: Pod "client-envvars-a8390f6a-1799-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:58:54.826: INFO: Trying to get logs from node crayfish pod client-envvars-a8390f6a-1799-11ea-a680-be755c6bedde container env3cont: <nil>
STEP: delete the pod
Dec  5 19:58:54.855: INFO: Waiting for pod client-envvars-a8390f6a-1799-11ea-a680-be755c6bedde to disappear
Dec  5 19:58:54.859: INFO: Pod client-envvars-a8390f6a-1799-11ea-a680-be755c6bedde no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:58:54.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6636" for this suite.
Dec  5 19:59:36.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:59:36.995: INFO: namespace pods-6636 deletion completed in 42.129792222s

• [SLOW TEST:48.275 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:59:36.995: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-c3c75b52-1799-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume configMaps
Dec  5 19:59:37.043: INFO: Waiting up to 5m0s for pod "pod-configmaps-c3c87917-1799-11ea-a680-be755c6bedde" in namespace "configmap-2428" to be "success or failure"
Dec  5 19:59:37.048: INFO: Pod "pod-configmaps-c3c87917-1799-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.866859ms
Dec  5 19:59:39.054: INFO: Pod "pod-configmaps-c3c87917-1799-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010610834s
Dec  5 19:59:41.060: INFO: Pod "pod-configmaps-c3c87917-1799-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016786777s
STEP: Saw pod success
Dec  5 19:59:41.060: INFO: Pod "pod-configmaps-c3c87917-1799-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 19:59:41.065: INFO: Trying to get logs from node crayfish pod pod-configmaps-c3c87917-1799-11ea-a680-be755c6bedde container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 19:59:41.095: INFO: Waiting for pod pod-configmaps-c3c87917-1799-11ea-a680-be755c6bedde to disappear
Dec  5 19:59:41.098: INFO: Pod pod-configmaps-c3c87917-1799-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:59:41.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2428" for this suite.
Dec  5 19:59:47.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 19:59:47.246: INFO: namespace configmap-2428 deletion completed in 6.142335349s

• [SLOW TEST:10.251 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 19:59:47.246: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 19:59:47.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-3095'
Dec  5 19:59:47.536: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  5 19:59:47.537: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Dec  5 19:59:47.544: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-rcs7d]
Dec  5 19:59:47.545: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-rcs7d" in namespace "kubectl-3095" to be "running and ready"
Dec  5 19:59:47.548: INFO: Pod "e2e-test-nginx-rc-rcs7d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.584373ms
Dec  5 19:59:49.554: INFO: Pod "e2e-test-nginx-rc-rcs7d": Phase="Running", Reason="", readiness=true. Elapsed: 2.009200274s
Dec  5 19:59:49.554: INFO: Pod "e2e-test-nginx-rc-rcs7d" satisfied condition "running and ready"
Dec  5 19:59:49.554: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-rcs7d]
Dec  5 19:59:49.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 logs rc/e2e-test-nginx-rc --namespace=kubectl-3095'
Dec  5 19:59:49.675: INFO: stderr: ""
Dec  5 19:59:49.675: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1425
Dec  5 19:59:49.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 delete rc e2e-test-nginx-rc --namespace=kubectl-3095'
Dec  5 19:59:49.797: INFO: stderr: ""
Dec  5 19:59:49.797: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 19:59:49.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3095" for this suite.
Dec  5 20:00:11.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:00:11.956: INFO: namespace kubectl-3095 deletion completed in 22.151959102s

• [SLOW TEST:24.710 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:00:11.957: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-d89f0214-1799-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume configMaps
Dec  5 20:00:12.012: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d8a0488b-1799-11ea-a680-be755c6bedde" in namespace "projected-2109" to be "success or failure"
Dec  5 20:00:12.016: INFO: Pod "pod-projected-configmaps-d8a0488b-1799-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.804492ms
Dec  5 20:00:14.022: INFO: Pod "pod-projected-configmaps-d8a0488b-1799-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009600491s
STEP: Saw pod success
Dec  5 20:00:14.022: INFO: Pod "pod-projected-configmaps-d8a0488b-1799-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:00:14.026: INFO: Trying to get logs from node puma pod pod-projected-configmaps-d8a0488b-1799-11ea-a680-be755c6bedde container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 20:00:14.058: INFO: Waiting for pod pod-projected-configmaps-d8a0488b-1799-11ea-a680-be755c6bedde to disappear
Dec  5 20:00:14.062: INFO: Pod pod-projected-configmaps-d8a0488b-1799-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:00:14.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2109" for this suite.
Dec  5 20:00:20.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:00:20.210: INFO: namespace projected-2109 deletion completed in 6.143453582s

• [SLOW TEST:8.253 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:00:20.211: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Dec  5 20:00:20.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 --namespace=kubectl-984 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec  5 20:00:22.539: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec  5 20:00:22.539: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:00:24.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-984" for this suite.
Dec  5 20:00:34.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:00:34.702: INFO: namespace kubectl-984 deletion completed in 10.147058601s

• [SLOW TEST:14.491 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:00:34.703: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-e62d9b78-1799-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume secrets
Dec  5 20:00:34.754: INFO: Waiting up to 5m0s for pod "pod-secrets-e62ea5c6-1799-11ea-a680-be755c6bedde" in namespace "secrets-8907" to be "success or failure"
Dec  5 20:00:34.758: INFO: Pod "pod-secrets-e62ea5c6-1799-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006555ms
Dec  5 20:00:36.763: INFO: Pod "pod-secrets-e62ea5c6-1799-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009426742s
STEP: Saw pod success
Dec  5 20:00:36.763: INFO: Pod "pod-secrets-e62ea5c6-1799-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:00:36.768: INFO: Trying to get logs from node puma pod pod-secrets-e62ea5c6-1799-11ea-a680-be755c6bedde container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 20:00:36.795: INFO: Waiting for pod pod-secrets-e62ea5c6-1799-11ea-a680-be755c6bedde to disappear
Dec  5 20:00:36.801: INFO: Pod pod-secrets-e62ea5c6-1799-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:00:36.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8907" for this suite.
Dec  5 20:00:42.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:00:42.949: INFO: namespace secrets-8907 deletion completed in 6.141758427s

• [SLOW TEST:8.246 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:00:42.950: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-eb18b7b2-1799-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume secrets
Dec  5 20:00:43.009: INFO: Waiting up to 5m0s for pod "pod-secrets-eb19e38e-1799-11ea-a680-be755c6bedde" in namespace "secrets-9043" to be "success or failure"
Dec  5 20:00:43.013: INFO: Pod "pod-secrets-eb19e38e-1799-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.822544ms
Dec  5 20:00:45.017: INFO: Pod "pod-secrets-eb19e38e-1799-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008477488s
STEP: Saw pod success
Dec  5 20:00:45.017: INFO: Pod "pod-secrets-eb19e38e-1799-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:00:45.022: INFO: Trying to get logs from node crayfish pod pod-secrets-eb19e38e-1799-11ea-a680-be755c6bedde container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 20:00:45.050: INFO: Waiting for pod pod-secrets-eb19e38e-1799-11ea-a680-be755c6bedde to disappear
Dec  5 20:00:45.054: INFO: Pod pod-secrets-eb19e38e-1799-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:00:45.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9043" for this suite.
Dec  5 20:00:51.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:00:51.204: INFO: namespace secrets-9043 deletion completed in 6.144601551s

• [SLOW TEST:8.254 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:00:51.207: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  5 20:00:51.251: INFO: Waiting up to 5m0s for pod "pod-f0038cc5-1799-11ea-a680-be755c6bedde" in namespace "emptydir-280" to be "success or failure"
Dec  5 20:00:51.255: INFO: Pod "pod-f0038cc5-1799-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.733944ms
Dec  5 20:00:53.261: INFO: Pod "pod-f0038cc5-1799-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010181348s
STEP: Saw pod success
Dec  5 20:00:53.261: INFO: Pod "pod-f0038cc5-1799-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:00:53.266: INFO: Trying to get logs from node puma pod pod-f0038cc5-1799-11ea-a680-be755c6bedde container test-container: <nil>
STEP: delete the pod
Dec  5 20:00:53.289: INFO: Waiting for pod pod-f0038cc5-1799-11ea-a680-be755c6bedde to disappear
Dec  5 20:00:53.292: INFO: Pod pod-f0038cc5-1799-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:00:53.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-280" for this suite.
Dec  5 20:00:59.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:00:59.451: INFO: namespace emptydir-280 deletion completed in 6.153521436s

• [SLOW TEST:8.244 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:00:59.451: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  5 20:00:59.499: INFO: Waiting up to 5m0s for pod "pod-f4ee3a69-1799-11ea-a680-be755c6bedde" in namespace "emptydir-280" to be "success or failure"
Dec  5 20:00:59.502: INFO: Pod "pod-f4ee3a69-1799-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.039012ms
Dec  5 20:01:01.507: INFO: Pod "pod-f4ee3a69-1799-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008345811s
STEP: Saw pod success
Dec  5 20:01:01.507: INFO: Pod "pod-f4ee3a69-1799-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:01:01.511: INFO: Trying to get logs from node crayfish pod pod-f4ee3a69-1799-11ea-a680-be755c6bedde container test-container: <nil>
STEP: delete the pod
Dec  5 20:01:01.538: INFO: Waiting for pod pod-f4ee3a69-1799-11ea-a680-be755c6bedde to disappear
Dec  5 20:01:01.541: INFO: Pod pod-f4ee3a69-1799-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:01:01.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-280" for this suite.
Dec  5 20:01:05.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:01:05.677: INFO: namespace emptydir-280 deletion completed in 4.131131611s

• [SLOW TEST:6.226 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:01:05.677: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec  5 20:01:09.776: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9432 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 20:01:09.776: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 20:01:09.929: INFO: Exec stderr: ""
Dec  5 20:01:09.929: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9432 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 20:01:09.929: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 20:01:10.051: INFO: Exec stderr: ""
Dec  5 20:01:10.051: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9432 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 20:01:10.051: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 20:01:10.172: INFO: Exec stderr: ""
Dec  5 20:01:10.172: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9432 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 20:01:10.172: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 20:01:10.291: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec  5 20:01:10.291: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9432 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 20:01:10.291: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 20:01:10.414: INFO: Exec stderr: ""
Dec  5 20:01:10.414: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9432 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 20:01:10.414: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 20:01:10.543: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec  5 20:01:10.543: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9432 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 20:01:10.543: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 20:01:10.699: INFO: Exec stderr: ""
Dec  5 20:01:10.699: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9432 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 20:01:10.699: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 20:01:10.823: INFO: Exec stderr: ""
Dec  5 20:01:10.823: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9432 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 20:01:10.823: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 20:01:10.958: INFO: Exec stderr: ""
Dec  5 20:01:10.958: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9432 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 20:01:10.958: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 20:01:11.082: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:01:11.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9432" for this suite.
Dec  5 20:01:57.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:01:57.227: INFO: namespace e2e-kubelet-etc-hosts-9432 deletion completed in 46.13939569s

• [SLOW TEST:51.550 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:01:57.228: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  5 20:01:57.278: INFO: Waiting up to 5m0s for pod "pod-175d80ac-179a-11ea-a680-be755c6bedde" in namespace "emptydir-8419" to be "success or failure"
Dec  5 20:01:57.281: INFO: Pod "pod-175d80ac-179a-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.209399ms
Dec  5 20:01:59.286: INFO: Pod "pod-175d80ac-179a-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008908454s
STEP: Saw pod success
Dec  5 20:01:59.287: INFO: Pod "pod-175d80ac-179a-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:01:59.291: INFO: Trying to get logs from node puma pod pod-175d80ac-179a-11ea-a680-be755c6bedde container test-container: <nil>
STEP: delete the pod
Dec  5 20:01:59.316: INFO: Waiting for pod pod-175d80ac-179a-11ea-a680-be755c6bedde to disappear
Dec  5 20:01:59.320: INFO: Pod pod-175d80ac-179a-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:01:59.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8419" for this suite.
Dec  5 20:02:05.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:02:05.470: INFO: namespace emptydir-8419 deletion completed in 6.144786225s

• [SLOW TEST:8.242 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:02:05.470: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-z575j in namespace proxy-7650
I1205 20:02:05.530680      15 runners.go:184] Created replication controller with name: proxy-service-z575j, namespace: proxy-7650, replica count: 1
I1205 20:02:06.581099      15 runners.go:184] proxy-service-z575j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1205 20:02:07.581290      15 runners.go:184] proxy-service-z575j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1205 20:02:08.581678      15 runners.go:184] proxy-service-z575j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1205 20:02:09.582071      15 runners.go:184] proxy-service-z575j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1205 20:02:10.582381      15 runners.go:184] proxy-service-z575j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1205 20:02:11.582721      15 runners.go:184] proxy-service-z575j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1205 20:02:12.583074      15 runners.go:184] proxy-service-z575j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1205 20:02:13.583548      15 runners.go:184] proxy-service-z575j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1205 20:02:14.583921      15 runners.go:184] proxy-service-z575j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1205 20:02:15.584348      15 runners.go:184] proxy-service-z575j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1205 20:02:16.584864      15 runners.go:184] proxy-service-z575j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1205 20:02:17.585265      15 runners.go:184] proxy-service-z575j Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  5 20:02:17.590: INFO: setup took 12.085242279s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec  5 20:02:17.603: INFO: (0) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 11.899785ms)
Dec  5 20:02:17.603: INFO: (0) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 11.860255ms)
Dec  5 20:02:17.604: INFO: (0) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 13.298676ms)
Dec  5 20:02:17.604: INFO: (0) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 13.396655ms)
Dec  5 20:02:17.605: INFO: (0) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 13.888693ms)
Dec  5 20:02:17.605: INFO: (0) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 13.616443ms)
Dec  5 20:02:17.605: INFO: (0) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 13.60526ms)
Dec  5 20:02:17.606: INFO: (0) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 15.419346ms)
Dec  5 20:02:17.606: INFO: (0) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 15.507353ms)
Dec  5 20:02:17.606: INFO: (0) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 15.422224ms)
Dec  5 20:02:17.606: INFO: (0) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 16.005857ms)
Dec  5 20:02:17.610: INFO: (0) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 19.116562ms)
Dec  5 20:02:17.611: INFO: (0) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 19.8081ms)
Dec  5 20:02:17.611: INFO: (0) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 19.984411ms)
Dec  5 20:02:17.614: INFO: (0) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 23.128454ms)
Dec  5 20:02:17.616: INFO: (0) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 25.309558ms)
Dec  5 20:02:17.622: INFO: (1) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 4.905516ms)
Dec  5 20:02:17.622: INFO: (1) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 5.767139ms)
Dec  5 20:02:17.624: INFO: (1) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 6.837539ms)
Dec  5 20:02:17.624: INFO: (1) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 6.868417ms)
Dec  5 20:02:17.624: INFO: (1) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 7.417047ms)
Dec  5 20:02:17.625: INFO: (1) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 7.929225ms)
Dec  5 20:02:17.625: INFO: (1) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 8.288715ms)
Dec  5 20:02:17.625: INFO: (1) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 8.789204ms)
Dec  5 20:02:17.625: INFO: (1) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 8.63514ms)
Dec  5 20:02:17.625: INFO: (1) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 8.667495ms)
Dec  5 20:02:17.626: INFO: (1) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 9.265129ms)
Dec  5 20:02:17.627: INFO: (1) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 11.054622ms)
Dec  5 20:02:17.627: INFO: (1) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 10.854201ms)
Dec  5 20:02:17.628: INFO: (1) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 10.941412ms)
Dec  5 20:02:17.628: INFO: (1) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 11.322459ms)
Dec  5 20:02:17.628: INFO: (1) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 11.273021ms)
Dec  5 20:02:17.638: INFO: (2) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 9.327499ms)
Dec  5 20:02:17.638: INFO: (2) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 9.306529ms)
Dec  5 20:02:17.638: INFO: (2) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 9.586814ms)
Dec  5 20:02:17.638: INFO: (2) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 9.694983ms)
Dec  5 20:02:17.638: INFO: (2) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 10.110628ms)
Dec  5 20:02:17.639: INFO: (2) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 10.28138ms)
Dec  5 20:02:17.639: INFO: (2) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 10.523351ms)
Dec  5 20:02:17.639: INFO: (2) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 10.612774ms)
Dec  5 20:02:17.639: INFO: (2) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 10.744745ms)
Dec  5 20:02:17.639: INFO: (2) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 10.793063ms)
Dec  5 20:02:17.639: INFO: (2) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 11.221699ms)
Dec  5 20:02:17.641: INFO: (2) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 12.283334ms)
Dec  5 20:02:17.642: INFO: (2) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 13.538221ms)
Dec  5 20:02:17.642: INFO: (2) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 13.870899ms)
Dec  5 20:02:17.642: INFO: (2) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 14.061354ms)
Dec  5 20:02:17.642: INFO: (2) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 14.266468ms)
Dec  5 20:02:17.648: INFO: (3) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 5.683784ms)
Dec  5 20:02:17.649: INFO: (3) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 5.535907ms)
Dec  5 20:02:17.649: INFO: (3) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 5.940742ms)
Dec  5 20:02:17.649: INFO: (3) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 6.224294ms)
Dec  5 20:02:17.649: INFO: (3) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 6.329864ms)
Dec  5 20:02:17.649: INFO: (3) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 6.459102ms)
Dec  5 20:02:17.650: INFO: (3) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 6.78213ms)
Dec  5 20:02:17.650: INFO: (3) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 6.579134ms)
Dec  5 20:02:17.649: INFO: (3) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 6.614356ms)
Dec  5 20:02:17.650: INFO: (3) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 6.677413ms)
Dec  5 20:02:17.650: INFO: (3) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 6.871651ms)
Dec  5 20:02:17.652: INFO: (3) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 9.209623ms)
Dec  5 20:02:17.652: INFO: (3) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 9.172923ms)
Dec  5 20:02:17.653: INFO: (3) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 9.650899ms)
Dec  5 20:02:17.653: INFO: (3) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 10.146158ms)
Dec  5 20:02:17.653: INFO: (3) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 10.288874ms)
Dec  5 20:02:17.658: INFO: (4) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 4.872507ms)
Dec  5 20:02:17.662: INFO: (4) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 8.743808ms)
Dec  5 20:02:17.662: INFO: (4) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 8.459727ms)
Dec  5 20:02:17.662: INFO: (4) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 8.843151ms)
Dec  5 20:02:17.663: INFO: (4) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 9.186506ms)
Dec  5 20:02:17.663: INFO: (4) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 9.261686ms)
Dec  5 20:02:17.663: INFO: (4) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 9.397459ms)
Dec  5 20:02:17.663: INFO: (4) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 9.95295ms)
Dec  5 20:02:17.664: INFO: (4) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 9.989824ms)
Dec  5 20:02:17.664: INFO: (4) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 10.174366ms)
Dec  5 20:02:17.664: INFO: (4) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 10.044065ms)
Dec  5 20:02:17.665: INFO: (4) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 10.936533ms)
Dec  5 20:02:17.665: INFO: (4) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 11.285955ms)
Dec  5 20:02:17.666: INFO: (4) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 12.078869ms)
Dec  5 20:02:17.666: INFO: (4) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 12.376163ms)
Dec  5 20:02:17.666: INFO: (4) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 12.402672ms)
Dec  5 20:02:17.671: INFO: (5) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 4.603987ms)
Dec  5 20:02:17.672: INFO: (5) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 5.784428ms)
Dec  5 20:02:17.672: INFO: (5) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 5.549457ms)
Dec  5 20:02:17.672: INFO: (5) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 5.802237ms)
Dec  5 20:02:17.672: INFO: (5) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 5.969052ms)
Dec  5 20:02:17.672: INFO: (5) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 6.236197ms)
Dec  5 20:02:17.672: INFO: (5) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 6.10583ms)
Dec  5 20:02:17.673: INFO: (5) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 6.06026ms)
Dec  5 20:02:17.673: INFO: (5) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 6.436449ms)
Dec  5 20:02:17.672: INFO: (5) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 5.731345ms)
Dec  5 20:02:17.674: INFO: (5) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 6.865772ms)
Dec  5 20:02:17.674: INFO: (5) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 7.440653ms)
Dec  5 20:02:17.675: INFO: (5) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 8.124291ms)
Dec  5 20:02:17.675: INFO: (5) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 8.620667ms)
Dec  5 20:02:17.675: INFO: (5) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 8.139531ms)
Dec  5 20:02:17.675: INFO: (5) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 8.348751ms)
Dec  5 20:02:17.682: INFO: (6) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 6.418248ms)
Dec  5 20:02:17.682: INFO: (6) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 6.483962ms)
Dec  5 20:02:17.683: INFO: (6) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 7.810375ms)
Dec  5 20:02:17.683: INFO: (6) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 7.90536ms)
Dec  5 20:02:17.684: INFO: (6) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 8.641189ms)
Dec  5 20:02:17.684: INFO: (6) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 8.597856ms)
Dec  5 20:02:17.684: INFO: (6) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 8.817993ms)
Dec  5 20:02:17.684: INFO: (6) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 8.943126ms)
Dec  5 20:02:17.684: INFO: (6) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 9.042815ms)
Dec  5 20:02:17.684: INFO: (6) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 9.102268ms)
Dec  5 20:02:17.684: INFO: (6) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 9.233248ms)
Dec  5 20:02:17.685: INFO: (6) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 9.298226ms)
Dec  5 20:02:17.685: INFO: (6) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 9.281308ms)
Dec  5 20:02:17.685: INFO: (6) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 9.410514ms)
Dec  5 20:02:17.685: INFO: (6) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 9.385773ms)
Dec  5 20:02:17.685: INFO: (6) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 10.058242ms)
Dec  5 20:02:17.689: INFO: (7) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 3.646536ms)
Dec  5 20:02:17.693: INFO: (7) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 6.946082ms)
Dec  5 20:02:17.693: INFO: (7) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 7.010394ms)
Dec  5 20:02:17.693: INFO: (7) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 7.262873ms)
Dec  5 20:02:17.693: INFO: (7) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 7.546977ms)
Dec  5 20:02:17.694: INFO: (7) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 7.402216ms)
Dec  5 20:02:17.694: INFO: (7) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 8.043001ms)
Dec  5 20:02:17.694: INFO: (7) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 7.292319ms)
Dec  5 20:02:17.694: INFO: (7) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 8.044406ms)
Dec  5 20:02:17.694: INFO: (7) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 7.993682ms)
Dec  5 20:02:17.695: INFO: (7) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 9.102634ms)
Dec  5 20:02:17.696: INFO: (7) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 10.587576ms)
Dec  5 20:02:17.696: INFO: (7) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 10.154618ms)
Dec  5 20:02:17.696: INFO: (7) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 10.236541ms)
Dec  5 20:02:17.696: INFO: (7) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 10.926218ms)
Dec  5 20:02:17.697: INFO: (7) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 11.001277ms)
Dec  5 20:02:17.700: INFO: (8) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 3.077913ms)
Dec  5 20:02:17.701: INFO: (8) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 4.066756ms)
Dec  5 20:02:17.702: INFO: (8) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 5.328652ms)
Dec  5 20:02:17.702: INFO: (8) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 5.503985ms)
Dec  5 20:02:17.702: INFO: (8) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 5.366485ms)
Dec  5 20:02:17.702: INFO: (8) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 5.516666ms)
Dec  5 20:02:17.703: INFO: (8) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 5.686925ms)
Dec  5 20:02:17.703: INFO: (8) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 5.781539ms)
Dec  5 20:02:17.703: INFO: (8) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 5.939721ms)
Dec  5 20:02:17.703: INFO: (8) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 5.984326ms)
Dec  5 20:02:17.703: INFO: (8) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 6.348791ms)
Dec  5 20:02:17.704: INFO: (8) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 6.924113ms)
Dec  5 20:02:17.704: INFO: (8) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 6.959905ms)
Dec  5 20:02:17.704: INFO: (8) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 7.265787ms)
Dec  5 20:02:17.704: INFO: (8) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 7.180311ms)
Dec  5 20:02:17.704: INFO: (8) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 7.283609ms)
Dec  5 20:02:17.709: INFO: (9) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 4.683377ms)
Dec  5 20:02:17.709: INFO: (9) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 4.984381ms)
Dec  5 20:02:17.710: INFO: (9) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 4.90963ms)
Dec  5 20:02:17.710: INFO: (9) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 5.126439ms)
Dec  5 20:02:17.710: INFO: (9) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 5.011227ms)
Dec  5 20:02:17.710: INFO: (9) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 5.225502ms)
Dec  5 20:02:17.710: INFO: (9) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 5.313283ms)
Dec  5 20:02:17.710: INFO: (9) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 5.200047ms)
Dec  5 20:02:17.710: INFO: (9) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 5.365797ms)
Dec  5 20:02:17.710: INFO: (9) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 5.359941ms)
Dec  5 20:02:17.711: INFO: (9) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 6.011869ms)
Dec  5 20:02:17.712: INFO: (9) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 7.022928ms)
Dec  5 20:02:17.713: INFO: (9) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 8.516103ms)
Dec  5 20:02:17.713: INFO: (9) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 8.596485ms)
Dec  5 20:02:17.713: INFO: (9) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 8.498643ms)
Dec  5 20:02:17.713: INFO: (9) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 8.506691ms)
Dec  5 20:02:17.717: INFO: (10) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 3.43975ms)
Dec  5 20:02:17.720: INFO: (10) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 6.402774ms)
Dec  5 20:02:17.720: INFO: (10) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 6.523599ms)
Dec  5 20:02:17.720: INFO: (10) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 6.622801ms)
Dec  5 20:02:17.720: INFO: (10) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 6.405546ms)
Dec  5 20:02:17.721: INFO: (10) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 7.009912ms)
Dec  5 20:02:17.721: INFO: (10) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 7.066676ms)
Dec  5 20:02:17.721: INFO: (10) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 7.051608ms)
Dec  5 20:02:17.721: INFO: (10) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 7.643971ms)
Dec  5 20:02:17.721: INFO: (10) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 7.547874ms)
Dec  5 20:02:17.722: INFO: (10) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 8.296755ms)
Dec  5 20:02:17.723: INFO: (10) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 9.502872ms)
Dec  5 20:02:17.723: INFO: (10) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 9.701318ms)
Dec  5 20:02:17.723: INFO: (10) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 9.8683ms)
Dec  5 20:02:17.723: INFO: (10) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 9.839233ms)
Dec  5 20:02:17.723: INFO: (10) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 9.951653ms)
Dec  5 20:02:17.728: INFO: (11) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 4.865067ms)
Dec  5 20:02:17.729: INFO: (11) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 5.611029ms)
Dec  5 20:02:17.729: INFO: (11) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 5.552265ms)
Dec  5 20:02:17.729: INFO: (11) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 5.64169ms)
Dec  5 20:02:17.729: INFO: (11) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 5.705328ms)
Dec  5 20:02:17.729: INFO: (11) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 5.940091ms)
Dec  5 20:02:17.729: INFO: (11) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 5.889193ms)
Dec  5 20:02:17.729: INFO: (11) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 5.721731ms)
Dec  5 20:02:17.729: INFO: (11) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 5.880295ms)
Dec  5 20:02:17.730: INFO: (11) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 6.024454ms)
Dec  5 20:02:17.730: INFO: (11) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 6.605147ms)
Dec  5 20:02:17.732: INFO: (11) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 8.453245ms)
Dec  5 20:02:17.732: INFO: (11) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 8.472329ms)
Dec  5 20:02:17.732: INFO: (11) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 8.733294ms)
Dec  5 20:02:17.732: INFO: (11) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 8.723403ms)
Dec  5 20:02:17.732: INFO: (11) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 8.596165ms)
Dec  5 20:02:17.738: INFO: (12) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 5.688245ms)
Dec  5 20:02:17.739: INFO: (12) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 6.113691ms)
Dec  5 20:02:17.739: INFO: (12) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 6.461779ms)
Dec  5 20:02:17.739: INFO: (12) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 6.596406ms)
Dec  5 20:02:17.739: INFO: (12) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 6.746632ms)
Dec  5 20:02:17.739: INFO: (12) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 6.694328ms)
Dec  5 20:02:17.739: INFO: (12) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 6.768872ms)
Dec  5 20:02:17.739: INFO: (12) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 6.83457ms)
Dec  5 20:02:17.739: INFO: (12) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 6.95678ms)
Dec  5 20:02:17.739: INFO: (12) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 6.874421ms)
Dec  5 20:02:17.739: INFO: (12) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 6.893393ms)
Dec  5 20:02:17.740: INFO: (12) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 6.890504ms)
Dec  5 20:02:17.740: INFO: (12) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 6.89813ms)
Dec  5 20:02:17.740: INFO: (12) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 7.761914ms)
Dec  5 20:02:17.740: INFO: (12) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 7.759706ms)
Dec  5 20:02:17.740: INFO: (12) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 7.774602ms)
Dec  5 20:02:17.746: INFO: (13) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 5.398079ms)
Dec  5 20:02:17.747: INFO: (13) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 6.693982ms)
Dec  5 20:02:17.748: INFO: (13) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 7.02494ms)
Dec  5 20:02:17.748: INFO: (13) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 7.123278ms)
Dec  5 20:02:17.748: INFO: (13) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 6.922168ms)
Dec  5 20:02:17.748: INFO: (13) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 7.152115ms)
Dec  5 20:02:17.748: INFO: (13) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 7.122594ms)
Dec  5 20:02:17.748: INFO: (13) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 7.224802ms)
Dec  5 20:02:17.748: INFO: (13) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 7.173068ms)
Dec  5 20:02:17.748: INFO: (13) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 7.23791ms)
Dec  5 20:02:17.748: INFO: (13) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 7.908101ms)
Dec  5 20:02:17.750: INFO: (13) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 9.219322ms)
Dec  5 20:02:17.750: INFO: (13) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 9.384949ms)
Dec  5 20:02:17.750: INFO: (13) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 9.488082ms)
Dec  5 20:02:17.750: INFO: (13) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 9.63018ms)
Dec  5 20:02:17.750: INFO: (13) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 9.900825ms)
Dec  5 20:02:17.755: INFO: (14) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 4.743211ms)
Dec  5 20:02:17.756: INFO: (14) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 4.884315ms)
Dec  5 20:02:17.756: INFO: (14) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 4.815664ms)
Dec  5 20:02:17.756: INFO: (14) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 5.164509ms)
Dec  5 20:02:17.756: INFO: (14) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 5.372377ms)
Dec  5 20:02:17.756: INFO: (14) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 5.670029ms)
Dec  5 20:02:17.756: INFO: (14) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 5.933192ms)
Dec  5 20:02:17.757: INFO: (14) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 5.871663ms)
Dec  5 20:02:17.757: INFO: (14) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 5.983109ms)
Dec  5 20:02:17.757: INFO: (14) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 6.07417ms)
Dec  5 20:02:17.757: INFO: (14) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 6.503819ms)
Dec  5 20:02:17.757: INFO: (14) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 6.439102ms)
Dec  5 20:02:17.759: INFO: (14) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 7.997763ms)
Dec  5 20:02:17.759: INFO: (14) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 8.673839ms)
Dec  5 20:02:17.760: INFO: (14) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 9.049134ms)
Dec  5 20:02:17.760: INFO: (14) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 8.929871ms)
Dec  5 20:02:17.764: INFO: (15) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 3.828988ms)
Dec  5 20:02:17.765: INFO: (15) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 5.532523ms)
Dec  5 20:02:17.765: INFO: (15) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 5.664573ms)
Dec  5 20:02:17.765: INFO: (15) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 5.646703ms)
Dec  5 20:02:17.765: INFO: (15) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 5.436798ms)
Dec  5 20:02:17.765: INFO: (15) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 5.541748ms)
Dec  5 20:02:17.765: INFO: (15) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 5.621093ms)
Dec  5 20:02:17.765: INFO: (15) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 5.547431ms)
Dec  5 20:02:17.766: INFO: (15) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 5.847547ms)
Dec  5 20:02:17.766: INFO: (15) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 6.132155ms)
Dec  5 20:02:17.766: INFO: (15) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 6.56396ms)
Dec  5 20:02:17.767: INFO: (15) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 7.499437ms)
Dec  5 20:02:17.768: INFO: (15) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 7.857538ms)
Dec  5 20:02:17.768: INFO: (15) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 7.843265ms)
Dec  5 20:02:17.768: INFO: (15) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 7.785593ms)
Dec  5 20:02:17.768: INFO: (15) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 7.825567ms)
Dec  5 20:02:17.772: INFO: (16) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 3.792652ms)
Dec  5 20:02:17.776: INFO: (16) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 7.714223ms)
Dec  5 20:02:17.776: INFO: (16) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 7.779134ms)
Dec  5 20:02:17.776: INFO: (16) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 7.83449ms)
Dec  5 20:02:17.776: INFO: (16) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 7.923046ms)
Dec  5 20:02:17.776: INFO: (16) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 7.8392ms)
Dec  5 20:02:17.776: INFO: (16) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 7.892413ms)
Dec  5 20:02:17.776: INFO: (16) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 7.935616ms)
Dec  5 20:02:17.776: INFO: (16) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 7.959467ms)
Dec  5 20:02:17.776: INFO: (16) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 8.039982ms)
Dec  5 20:02:17.776: INFO: (16) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 8.132558ms)
Dec  5 20:02:17.778: INFO: (16) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 10.438442ms)
Dec  5 20:02:17.779: INFO: (16) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 10.907198ms)
Dec  5 20:02:17.779: INFO: (16) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 10.821627ms)
Dec  5 20:02:17.779: INFO: (16) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 10.83018ms)
Dec  5 20:02:17.779: INFO: (16) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 10.902641ms)
Dec  5 20:02:17.783: INFO: (17) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 4.301115ms)
Dec  5 20:02:17.784: INFO: (17) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 5.252891ms)
Dec  5 20:02:17.784: INFO: (17) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 5.319183ms)
Dec  5 20:02:17.784: INFO: (17) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 5.284268ms)
Dec  5 20:02:17.784: INFO: (17) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 5.22602ms)
Dec  5 20:02:17.784: INFO: (17) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 5.431242ms)
Dec  5 20:02:17.784: INFO: (17) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 5.240459ms)
Dec  5 20:02:17.784: INFO: (17) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 5.353194ms)
Dec  5 20:02:17.784: INFO: (17) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 5.353825ms)
Dec  5 20:02:17.784: INFO: (17) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 5.218185ms)
Dec  5 20:02:17.785: INFO: (17) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 6.201132ms)
Dec  5 20:02:17.786: INFO: (17) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 6.872189ms)
Dec  5 20:02:17.787: INFO: (17) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 7.739125ms)
Dec  5 20:02:17.787: INFO: (17) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 7.918544ms)
Dec  5 20:02:17.787: INFO: (17) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 7.977598ms)
Dec  5 20:02:17.787: INFO: (17) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 8.155759ms)
Dec  5 20:02:17.791: INFO: (18) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 3.40429ms)
Dec  5 20:02:17.792: INFO: (18) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 4.4079ms)
Dec  5 20:02:17.792: INFO: (18) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 4.777022ms)
Dec  5 20:02:17.792: INFO: (18) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 4.908278ms)
Dec  5 20:02:17.792: INFO: (18) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 4.967533ms)
Dec  5 20:02:17.792: INFO: (18) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 4.811781ms)
Dec  5 20:02:17.792: INFO: (18) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 4.891086ms)
Dec  5 20:02:17.792: INFO: (18) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 5.152226ms)
Dec  5 20:02:17.792: INFO: (18) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 5.061694ms)
Dec  5 20:02:17.792: INFO: (18) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 4.989539ms)
Dec  5 20:02:17.794: INFO: (18) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 6.608132ms)
Dec  5 20:02:17.795: INFO: (18) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 7.687274ms)
Dec  5 20:02:17.795: INFO: (18) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 7.797112ms)
Dec  5 20:02:17.795: INFO: (18) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 7.880165ms)
Dec  5 20:02:17.795: INFO: (18) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 7.862426ms)
Dec  5 20:02:17.795: INFO: (18) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 8.019565ms)
Dec  5 20:02:17.799: INFO: (19) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:162/proxy/: bar (200; 3.838991ms)
Dec  5 20:02:17.800: INFO: (19) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:160/proxy/: foo (200; 4.248826ms)
Dec  5 20:02:17.800: INFO: (19) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:160/proxy/: foo (200; 4.447081ms)
Dec  5 20:02:17.800: INFO: (19) /api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/http:proxy-service-z575j-kvd2t:1080/proxy/rewriteme">... (200; 4.530803ms)
Dec  5 20:02:17.800: INFO: (19) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:460/proxy/: tls baz (200; 4.516269ms)
Dec  5 20:02:17.800: INFO: (19) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:443/proxy/tlsrewritem... (200; 4.501031ms)
Dec  5 20:02:17.801: INFO: (19) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:162/proxy/: bar (200; 5.317833ms)
Dec  5 20:02:17.801: INFO: (19) /api/v1/namespaces/proxy-7650/pods/https:proxy-service-z575j-kvd2t:462/proxy/: tls qux (200; 5.599191ms)
Dec  5 20:02:17.801: INFO: (19) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t/proxy/rewriteme">test</a> (200; 5.587747ms)
Dec  5 20:02:17.801: INFO: (19) /api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/: <a href="/api/v1/namespaces/proxy-7650/pods/proxy-service-z575j-kvd2t:1080/proxy/rewriteme">test<... (200; 5.851679ms)
Dec  5 20:02:17.802: INFO: (19) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname2/proxy/: bar (200; 6.839547ms)
Dec  5 20:02:17.804: INFO: (19) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname2/proxy/: bar (200; 8.292271ms)
Dec  5 20:02:17.804: INFO: (19) /api/v1/namespaces/proxy-7650/services/http:proxy-service-z575j:portname1/proxy/: foo (200; 8.404983ms)
Dec  5 20:02:17.804: INFO: (19) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname1/proxy/: tls baz (200; 8.426277ms)
Dec  5 20:02:17.804: INFO: (19) /api/v1/namespaces/proxy-7650/services/proxy-service-z575j:portname1/proxy/: foo (200; 8.413874ms)
Dec  5 20:02:17.804: INFO: (19) /api/v1/namespaces/proxy-7650/services/https:proxy-service-z575j:tlsportname2/proxy/: tls qux (200; 8.514423ms)
STEP: deleting ReplicationController proxy-service-z575j in namespace proxy-7650, will wait for the garbage collector to delete the pods
Dec  5 20:02:17.868: INFO: Deleting ReplicationController proxy-service-z575j took: 11.118802ms
Dec  5 20:02:18.168: INFO: Terminating ReplicationController proxy-service-z575j pods took: 300.266134ms
[AfterEach] version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:02:19.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7650" for this suite.
Dec  5 20:02:25.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:02:25.911: INFO: namespace proxy-7650 deletion completed in 6.135748825s

• [SLOW TEST:20.441 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:02:25.911: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec  5 20:02:25.951: INFO: Pod name pod-release: Found 0 pods out of 1
Dec  5 20:02:30.957: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:02:30.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1551" for this suite.
Dec  5 20:02:37.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:02:37.118: INFO: namespace replication-controller-1551 deletion completed in 6.134615073s

• [SLOW TEST:11.207 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:02:37.122: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Dec  5 20:02:39.701: INFO: Successfully updated pod "labelsupdate2f2425b5-179a-11ea-a680-be755c6bedde"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:02:41.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1519" for this suite.
Dec  5 20:03:03.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:03:03.878: INFO: namespace projected-1519 deletion completed in 22.144213556s

• [SLOW TEST:26.756 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:03:03.878: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Dec  5 20:03:03.923: INFO: Waiting up to 5m0s for pod "var-expansion-3f17518d-179a-11ea-a680-be755c6bedde" in namespace "var-expansion-8924" to be "success or failure"
Dec  5 20:03:03.927: INFO: Pod "var-expansion-3f17518d-179a-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.895581ms
Dec  5 20:03:05.933: INFO: Pod "var-expansion-3f17518d-179a-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00977415s
STEP: Saw pod success
Dec  5 20:03:05.933: INFO: Pod "var-expansion-3f17518d-179a-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:03:05.937: INFO: Trying to get logs from node crayfish pod var-expansion-3f17518d-179a-11ea-a680-be755c6bedde container dapi-container: <nil>
STEP: delete the pod
Dec  5 20:03:05.977: INFO: Waiting for pod var-expansion-3f17518d-179a-11ea-a680-be755c6bedde to disappear
Dec  5 20:03:05.981: INFO: Pod var-expansion-3f17518d-179a-11ea-a680-be755c6bedde no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:03:05.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8924" for this suite.
Dec  5 20:03:12.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:03:12.124: INFO: namespace var-expansion-8924 deletion completed in 6.138431382s

• [SLOW TEST:8.246 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:03:12.124: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  5 20:03:12.171: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44021f20-179a-11ea-a680-be755c6bedde" in namespace "downward-api-7979" to be "success or failure"
Dec  5 20:03:12.176: INFO: Pod "downwardapi-volume-44021f20-179a-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007684ms
Dec  5 20:03:14.182: INFO: Pod "downwardapi-volume-44021f20-179a-11ea-a680-be755c6bedde": Phase="Running", Reason="", readiness=true. Elapsed: 2.010345671s
Dec  5 20:03:16.188: INFO: Pod "downwardapi-volume-44021f20-179a-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016614478s
STEP: Saw pod success
Dec  5 20:03:16.188: INFO: Pod "downwardapi-volume-44021f20-179a-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:03:16.194: INFO: Trying to get logs from node puma pod downwardapi-volume-44021f20-179a-11ea-a680-be755c6bedde container client-container: <nil>
STEP: delete the pod
Dec  5 20:03:16.222: INFO: Waiting for pod downwardapi-volume-44021f20-179a-11ea-a680-be755c6bedde to disappear
Dec  5 20:03:16.226: INFO: Pod downwardapi-volume-44021f20-179a-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:03:16.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7979" for this suite.
Dec  5 20:03:22.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:03:22.364: INFO: namespace downward-api-7979 deletion completed in 6.132692831s

• [SLOW TEST:10.240 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:03:22.365: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-4a1c38de-179a-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume secrets
Dec  5 20:03:22.413: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4a1d666b-179a-11ea-a680-be755c6bedde" in namespace "projected-6059" to be "success or failure"
Dec  5 20:03:22.416: INFO: Pod "pod-projected-secrets-4a1d666b-179a-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.694144ms
Dec  5 20:03:24.423: INFO: Pod "pod-projected-secrets-4a1d666b-179a-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010045558s
Dec  5 20:03:26.428: INFO: Pod "pod-projected-secrets-4a1d666b-179a-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015516158s
STEP: Saw pod success
Dec  5 20:03:26.428: INFO: Pod "pod-projected-secrets-4a1d666b-179a-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:03:26.432: INFO: Trying to get logs from node puma pod pod-projected-secrets-4a1d666b-179a-11ea-a680-be755c6bedde container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  5 20:03:26.457: INFO: Waiting for pod pod-projected-secrets-4a1d666b-179a-11ea-a680-be755c6bedde to disappear
Dec  5 20:03:26.461: INFO: Pod pod-projected-secrets-4a1d666b-179a-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:03:26.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6059" for this suite.
Dec  5 20:03:32.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:03:32.610: INFO: namespace projected-6059 deletion completed in 6.144202179s

• [SLOW TEST:10.245 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:03:32.610: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  5 20:03:32.655: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5037bda9-179a-11ea-a680-be755c6bedde" in namespace "projected-7986" to be "success or failure"
Dec  5 20:03:32.660: INFO: Pod "downwardapi-volume-5037bda9-179a-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.850417ms
Dec  5 20:03:34.667: INFO: Pod "downwardapi-volume-5037bda9-179a-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011277254s
Dec  5 20:03:36.671: INFO: Pod "downwardapi-volume-5037bda9-179a-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015606741s
STEP: Saw pod success
Dec  5 20:03:36.671: INFO: Pod "downwardapi-volume-5037bda9-179a-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:03:36.675: INFO: Trying to get logs from node puma pod downwardapi-volume-5037bda9-179a-11ea-a680-be755c6bedde container client-container: <nil>
STEP: delete the pod
Dec  5 20:03:36.700: INFO: Waiting for pod downwardapi-volume-5037bda9-179a-11ea-a680-be755c6bedde to disappear
Dec  5 20:03:36.704: INFO: Pod downwardapi-volume-5037bda9-179a-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:03:36.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7986" for this suite.
Dec  5 20:03:42.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:03:42.835: INFO: namespace projected-7986 deletion completed in 6.126804119s

• [SLOW TEST:10.225 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:03:42.835: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Dec  5 20:03:42.879: INFO: Waiting up to 5m0s for pod "downward-api-564fb501-179a-11ea-a680-be755c6bedde" in namespace "downward-api-5271" to be "success or failure"
Dec  5 20:03:42.881: INFO: Pod "downward-api-564fb501-179a-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.647575ms
Dec  5 20:03:44.888: INFO: Pod "downward-api-564fb501-179a-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009190981s
Dec  5 20:03:46.893: INFO: Pod "downward-api-564fb501-179a-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01451283s
STEP: Saw pod success
Dec  5 20:03:46.893: INFO: Pod "downward-api-564fb501-179a-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:03:46.898: INFO: Trying to get logs from node puma pod downward-api-564fb501-179a-11ea-a680-be755c6bedde container dapi-container: <nil>
STEP: delete the pod
Dec  5 20:03:46.923: INFO: Waiting for pod downward-api-564fb501-179a-11ea-a680-be755c6bedde to disappear
Dec  5 20:03:46.927: INFO: Pod downward-api-564fb501-179a-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:03:46.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5271" for this suite.
Dec  5 20:03:52.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:03:53.076: INFO: namespace downward-api-5271 deletion completed in 6.143440197s

• [SLOW TEST:10.241 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:03:53.076: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1174
STEP: creating the pod
Dec  5 20:03:53.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 create -f - --namespace=kubectl-5397'
Dec  5 20:03:53.300: INFO: stderr: ""
Dec  5 20:03:53.300: INFO: stdout: "pod/pause created\n"
Dec  5 20:03:53.300: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec  5 20:03:53.300: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5397" to be "running and ready"
Dec  5 20:03:53.304: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.499588ms
Dec  5 20:03:55.310: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010119324s
Dec  5 20:03:55.310: INFO: Pod "pause" satisfied condition "running and ready"
Dec  5 20:03:55.310: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Dec  5 20:03:55.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 label pods pause testing-label=testing-label-value --namespace=kubectl-5397'
Dec  5 20:03:55.442: INFO: stderr: ""
Dec  5 20:03:55.442: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec  5 20:03:55.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pod pause -L testing-label --namespace=kubectl-5397'
Dec  5 20:03:55.520: INFO: stderr: ""
Dec  5 20:03:55.520: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec  5 20:03:55.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 label pods pause testing-label- --namespace=kubectl-5397'
Dec  5 20:03:55.603: INFO: stderr: ""
Dec  5 20:03:55.603: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec  5 20:03:55.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pod pause -L testing-label --namespace=kubectl-5397'
Dec  5 20:03:55.711: INFO: stderr: ""
Dec  5 20:03:55.711: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1181
STEP: using delete to clean up resources
Dec  5 20:03:55.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 delete --grace-period=0 --force -f - --namespace=kubectl-5397'
Dec  5 20:03:55.790: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 20:03:55.790: INFO: stdout: "pod \"pause\" force deleted\n"
Dec  5 20:03:55.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get rc,svc -l name=pause --no-headers --namespace=kubectl-5397'
Dec  5 20:03:55.873: INFO: stderr: "No resources found.\n"
Dec  5 20:03:55.873: INFO: stdout: ""
Dec  5 20:03:55.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -l name=pause --namespace=kubectl-5397 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  5 20:03:55.945: INFO: stderr: ""
Dec  5 20:03:55.945: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:03:55.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5397" for this suite.
Dec  5 20:04:01.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:04:02.078: INFO: namespace kubectl-5397 deletion completed in 6.127570928s

• [SLOW TEST:9.002 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:04:02.078: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 20:04:24.142: INFO: Container started at 2019-12-05 20:04:03 +0000 UTC, pod became ready at 2019-12-05 20:04:22 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:04:24.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9827" for this suite.
Dec  5 20:04:46.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:04:46.290: INFO: namespace container-probe-9827 deletion completed in 22.141324033s

• [SLOW TEST:44.212 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:04:46.293: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec  5 20:04:52.412: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 90
	[quantile=0.9] = 49203
	[quantile=0.99] = 60073
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 169791
	[quantile=0.9] = 258686
	[quantile=0.99] = 259281
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 5
	[quantile=0.9] = 12
	[quantile=0.99] = 41
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 16
	[quantile=0.9] = 33
	[quantile=0.99] = 66
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 12
	[quantile=0.9] = 27
	[quantile=0.99] = 70
For namespace_queue_latency_sum:
	[] = 6428
For namespace_queue_latency_count:
	[] = 336
For namespace_retries:
	[] = 343
For namespace_work_duration:
	[quantile=0.5] = 160234
	[quantile=0.9] = 220457
	[quantile=0.99] = 278474
For namespace_work_duration_sum:
	[] = 48423361
For namespace_work_duration_count:
	[] = 336
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:04:52.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1453" for this suite.
Dec  5 20:04:58.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:04:58.551: INFO: namespace gc-1453 deletion completed in 6.134137716s

• [SLOW TEST:12.258 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:04:58.552: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-8371beb4-179a-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume secrets
Dec  5 20:04:58.602: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8372bdd8-179a-11ea-a680-be755c6bedde" in namespace "projected-1532" to be "success or failure"
Dec  5 20:04:58.608: INFO: Pod "pod-projected-secrets-8372bdd8-179a-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 5.313478ms
Dec  5 20:05:00.614: INFO: Pod "pod-projected-secrets-8372bdd8-179a-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011414443s
STEP: Saw pod success
Dec  5 20:05:00.614: INFO: Pod "pod-projected-secrets-8372bdd8-179a-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:05:00.619: INFO: Trying to get logs from node puma pod pod-projected-secrets-8372bdd8-179a-11ea-a680-be755c6bedde container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  5 20:05:00.648: INFO: Waiting for pod pod-projected-secrets-8372bdd8-179a-11ea-a680-be755c6bedde to disappear
Dec  5 20:05:00.652: INFO: Pod pod-projected-secrets-8372bdd8-179a-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:05:00.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1532" for this suite.
Dec  5 20:05:06.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:05:06.809: INFO: namespace projected-1532 deletion completed in 6.13759507s

• [SLOW TEST:8.257 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:05:06.809: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8048.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8048.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8048.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8048.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8048.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8048.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  5 20:05:10.914: INFO: DNS probes using dns-8048/dns-test-885d7589-179a-11ea-a680-be755c6bedde succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:05:10.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8048" for this suite.
Dec  5 20:05:16.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:05:17.080: INFO: namespace dns-8048 deletion completed in 6.143619372s

• [SLOW TEST:10.271 seconds]
[sig-network] DNS
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:05:17.080: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:06:17.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9670" for this suite.
Dec  5 20:06:39.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:06:39.512: INFO: namespace container-probe-9670 deletion completed in 22.153390768s

• [SLOW TEST:82.431 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:06:39.512: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-bf9f6cb4-179a-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume configMaps
Dec  5 20:06:39.569: INFO: Waiting up to 5m0s for pod "pod-configmaps-bfa07bc0-179a-11ea-a680-be755c6bedde" in namespace "configmap-8575" to be "success or failure"
Dec  5 20:06:39.572: INFO: Pod "pod-configmaps-bfa07bc0-179a-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.601677ms
Dec  5 20:06:41.578: INFO: Pod "pod-configmaps-bfa07bc0-179a-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009046375s
STEP: Saw pod success
Dec  5 20:06:41.578: INFO: Pod "pod-configmaps-bfa07bc0-179a-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:06:41.582: INFO: Trying to get logs from node crayfish pod pod-configmaps-bfa07bc0-179a-11ea-a680-be755c6bedde container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 20:06:41.611: INFO: Waiting for pod pod-configmaps-bfa07bc0-179a-11ea-a680-be755c6bedde to disappear
Dec  5 20:06:41.615: INFO: Pod pod-configmaps-bfa07bc0-179a-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:06:41.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8575" for this suite.
Dec  5 20:06:47.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:06:47.755: INFO: namespace configmap-8575 deletion completed in 6.133545558s

• [SLOW TEST:8.243 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:06:47.755: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 20:06:47.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-7781'
Dec  5 20:06:47.927: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  5 20:06:47.927: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Dec  5 20:06:47.938: INFO: scanned /root for discovery docs: <nil>
Dec  5 20:06:47.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-7781'
Dec  5 20:07:03.753: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  5 20:07:03.753: INFO: stdout: "Created e2e-test-nginx-rc-4601ab9472eac05a658ae7e541b7d749\nScaling up e2e-test-nginx-rc-4601ab9472eac05a658ae7e541b7d749 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-4601ab9472eac05a658ae7e541b7d749 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-4601ab9472eac05a658ae7e541b7d749 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Dec  5 20:07:03.753: INFO: stdout: "Created e2e-test-nginx-rc-4601ab9472eac05a658ae7e541b7d749\nScaling up e2e-test-nginx-rc-4601ab9472eac05a658ae7e541b7d749 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-4601ab9472eac05a658ae7e541b7d749 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-4601ab9472eac05a658ae7e541b7d749 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Dec  5 20:07:03.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-7781'
Dec  5 20:07:03.890: INFO: stderr: ""
Dec  5 20:07:03.890: INFO: stdout: "e2e-test-nginx-rc-4601ab9472eac05a658ae7e541b7d749-xjn5s e2e-test-nginx-rc-ht59v "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Dec  5 20:07:08.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-7781'
Dec  5 20:07:09.010: INFO: stderr: ""
Dec  5 20:07:09.010: INFO: stdout: "e2e-test-nginx-rc-4601ab9472eac05a658ae7e541b7d749-xjn5s "
Dec  5 20:07:09.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods e2e-test-nginx-rc-4601ab9472eac05a658ae7e541b7d749-xjn5s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7781'
Dec  5 20:07:09.089: INFO: stderr: ""
Dec  5 20:07:09.089: INFO: stdout: "true"
Dec  5 20:07:09.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods e2e-test-nginx-rc-4601ab9472eac05a658ae7e541b7d749-xjn5s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7781'
Dec  5 20:07:09.163: INFO: stderr: ""
Dec  5 20:07:09.163: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Dec  5 20:07:09.163: INFO: e2e-test-nginx-rc-4601ab9472eac05a658ae7e541b7d749-xjn5s is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1486
Dec  5 20:07:09.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 delete rc e2e-test-nginx-rc --namespace=kubectl-7781'
Dec  5 20:07:09.244: INFO: stderr: ""
Dec  5 20:07:09.244: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:07:09.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7781" for this suite.
Dec  5 20:07:15.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:07:15.396: INFO: namespace kubectl-7781 deletion completed in 6.145187543s

• [SLOW TEST:27.641 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:07:15.397: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Dec  5 20:07:15.442: INFO: Waiting up to 5m0s for pod "downward-api-d5024a9e-179a-11ea-a680-be755c6bedde" in namespace "downward-api-3241" to be "success or failure"
Dec  5 20:07:15.447: INFO: Pod "downward-api-d5024a9e-179a-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.950471ms
Dec  5 20:07:17.453: INFO: Pod "downward-api-d5024a9e-179a-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011128428s
Dec  5 20:07:19.460: INFO: Pod "downward-api-d5024a9e-179a-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017827328s
STEP: Saw pod success
Dec  5 20:07:19.460: INFO: Pod "downward-api-d5024a9e-179a-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:07:19.466: INFO: Trying to get logs from node puma pod downward-api-d5024a9e-179a-11ea-a680-be755c6bedde container dapi-container: <nil>
STEP: delete the pod
Dec  5 20:07:19.497: INFO: Waiting for pod downward-api-d5024a9e-179a-11ea-a680-be755c6bedde to disappear
Dec  5 20:07:19.501: INFO: Pod downward-api-d5024a9e-179a-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:07:19.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3241" for this suite.
Dec  5 20:07:25.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:07:25.639: INFO: namespace downward-api-3241 deletion completed in 6.132966649s

• [SLOW TEST:10.243 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:07:25.642: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:07:25.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-610" for this suite.
Dec  5 20:07:47.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:07:47.830: INFO: namespace pods-610 deletion completed in 22.131253963s

• [SLOW TEST:22.188 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:07:47.831: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-e8587f1a-179a-11ea-a680-be755c6bedde
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-e8587f1a-179a-11ea-a680-be755c6bedde
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:07:51.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2285" for this suite.
Dec  5 20:08:13.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:08:14.072: INFO: namespace configmap-2285 deletion completed in 22.126908243s

• [SLOW TEST:26.242 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:08:14.073: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:08:16.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7990" for this suite.
Dec  5 20:08:56.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:08:56.298: INFO: namespace kubelet-test-7990 deletion completed in 40.149683312s

• [SLOW TEST:42.225 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:08:56.299: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec  5 20:09:01.380: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:09:01.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7101" for this suite.
Dec  5 20:09:23.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:09:23.559: INFO: namespace replicaset-7101 deletion completed in 22.152983621s

• [SLOW TEST:27.261 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:09:23.560: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1576
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 20:09:23.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-368'
Dec  5 20:09:23.732: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  5 20:09:23.732: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
Dec  5 20:09:23.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 delete jobs e2e-test-nginx-job --namespace=kubectl-368'
Dec  5 20:09:23.857: INFO: stderr: ""
Dec  5 20:09:23.857: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:09:23.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-368" for this suite.
Dec  5 20:09:45.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:09:46.011: INFO: namespace kubectl-368 deletion completed in 22.147714412s

• [SLOW TEST:22.451 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:09:46.012: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5752
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  5 20:09:46.049: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  5 20:10:10.152: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://25.0.1.33:8080/dial?request=hostName&protocol=udp&host=25.0.1.32&port=8081&tries=1'] Namespace:pod-network-test-5752 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 20:10:10.152: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 20:10:10.330: INFO: Waiting for endpoints: map[]
Dec  5 20:10:10.335: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://25.0.1.33:8080/dial?request=hostName&protocol=udp&host=25.0.2.42&port=8081&tries=1'] Namespace:pod-network-test-5752 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 20:10:10.335: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 20:10:10.459: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:10:10.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5752" for this suite.
Dec  5 20:10:32.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:10:32.606: INFO: namespace pod-network-test-5752 deletion completed in 22.141902256s

• [SLOW TEST:46.594 seconds]
[sig-network] Networking
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:10:32.607: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-4a8e323a-179b-11ea-a680-be755c6bedde
STEP: Creating secret with name s-test-opt-upd-4a8e3275-179b-11ea-a680-be755c6bedde
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-4a8e323a-179b-11ea-a680-be755c6bedde
STEP: Updating secret s-test-opt-upd-4a8e3275-179b-11ea-a680-be755c6bedde
STEP: Creating secret with name s-test-opt-create-4a8e3290-179b-11ea-a680-be755c6bedde
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:11:53.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6876" for this suite.
Dec  5 20:12:15.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:12:15.422: INFO: namespace secrets-6876 deletion completed in 22.13689423s

• [SLOW TEST:102.815 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:12:15.423: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:12:20.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8910" for this suite.
Dec  5 20:12:42.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:12:42.660: INFO: namespace replication-controller-8910 deletion completed in 22.148727962s

• [SLOW TEST:27.238 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:12:42.661: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 20:12:42.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 version'
Dec  5 20:12:42.799: INFO: stderr: ""
Dec  5 20:12:42.799: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.8\", GitCommit:\"211047e9a1922595eaa3a1127ed365e9299a6c23\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T12:11:03Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.8\", GitCommit:\"211047e9a1922595eaa3a1127ed365e9299a6c23\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T12:02:12Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:12:42.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3048" for this suite.
Dec  5 20:12:48.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:12:48.950: INFO: namespace kubectl-3048 deletion completed in 6.145553448s

• [SLOW TEST:6.289 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:12:48.952: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Dec  5 20:12:48.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 cluster-info'
Dec  5 20:12:49.206: INFO: stderr: ""
Dec  5 20:12:49.206: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:12:49.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1264" for this suite.
Dec  5 20:12:55.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:12:55.340: INFO: namespace kubectl-1264 deletion completed in 6.12870523s

• [SLOW TEST:6.389 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:12:55.341: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  5 20:12:55.391: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9fa23bc0-179b-11ea-a680-be755c6bedde" in namespace "downward-api-294" to be "success or failure"
Dec  5 20:12:55.396: INFO: Pod "downwardapi-volume-9fa23bc0-179b-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.897951ms
Dec  5 20:12:57.401: INFO: Pod "downwardapi-volume-9fa23bc0-179b-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00993813s
Dec  5 20:12:59.407: INFO: Pod "downwardapi-volume-9fa23bc0-179b-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015758676s
STEP: Saw pod success
Dec  5 20:12:59.407: INFO: Pod "downwardapi-volume-9fa23bc0-179b-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:12:59.413: INFO: Trying to get logs from node puma pod downwardapi-volume-9fa23bc0-179b-11ea-a680-be755c6bedde container client-container: <nil>
STEP: delete the pod
Dec  5 20:12:59.451: INFO: Waiting for pod downwardapi-volume-9fa23bc0-179b-11ea-a680-be755c6bedde to disappear
Dec  5 20:12:59.455: INFO: Pod downwardapi-volume-9fa23bc0-179b-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:12:59.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-294" for this suite.
Dec  5 20:13:05.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:13:05.603: INFO: namespace downward-api-294 deletion completed in 6.142198598s

• [SLOW TEST:10.261 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:13:05.603: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-5735
Dec  5 20:13:07.662: INFO: Started pod liveness-exec in namespace container-probe-5735
STEP: checking the pod's current state and verifying that restartCount is present
Dec  5 20:13:07.667: INFO: Initial restart count of pod liveness-exec is 0
Dec  5 20:13:53.804: INFO: Restart count of pod container-probe-5735/liveness-exec is now 1 (46.13673172s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:13:53.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5735" for this suite.
Dec  5 20:13:59.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:13:59.976: INFO: namespace container-probe-5735 deletion completed in 6.148510717s

• [SLOW TEST:54.374 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:13:59.978: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-c628e7e6-179b-11ea-a680-be755c6bedde
Dec  5 20:14:00.026: INFO: Pod name my-hostname-basic-c628e7e6-179b-11ea-a680-be755c6bedde: Found 0 pods out of 1
Dec  5 20:14:05.032: INFO: Pod name my-hostname-basic-c628e7e6-179b-11ea-a680-be755c6bedde: Found 1 pods out of 1
Dec  5 20:14:05.032: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c628e7e6-179b-11ea-a680-be755c6bedde" are running
Dec  5 20:14:05.037: INFO: Pod "my-hostname-basic-c628e7e6-179b-11ea-a680-be755c6bedde-vt7zh" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-05 20:14:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-05 20:14:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-05 20:14:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-05 20:14:00 +0000 UTC Reason: Message:}])
Dec  5 20:14:05.037: INFO: Trying to dial the pod
Dec  5 20:14:10.054: INFO: Controller my-hostname-basic-c628e7e6-179b-11ea-a680-be755c6bedde: Got expected result from replica 1 [my-hostname-basic-c628e7e6-179b-11ea-a680-be755c6bedde-vt7zh]: "my-hostname-basic-c628e7e6-179b-11ea-a680-be755c6bedde-vt7zh", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:14:10.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8066" for this suite.
Dec  5 20:14:16.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:14:16.198: INFO: namespace replication-controller-8066 deletion completed in 6.13780977s

• [SLOW TEST:16.220 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:14:16.198: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-9707
Dec  5 20:14:20.249: INFO: Started pod liveness-http in namespace container-probe-9707
STEP: checking the pod's current state and verifying that restartCount is present
Dec  5 20:14:20.253: INFO: Initial restart count of pod liveness-http is 0
Dec  5 20:14:38.309: INFO: Restart count of pod container-probe-9707/liveness-http is now 1 (18.055671898s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:14:38.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9707" for this suite.
Dec  5 20:14:44.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:14:44.461: INFO: namespace container-probe-9707 deletion completed in 6.126122468s

• [SLOW TEST:28.263 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:14:44.461: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  5 20:14:44.507: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e0ac3dff-179b-11ea-a680-be755c6bedde" in namespace "projected-7120" to be "success or failure"
Dec  5 20:14:44.510: INFO: Pod "downwardapi-volume-e0ac3dff-179b-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.081082ms
Dec  5 20:14:46.516: INFO: Pod "downwardapi-volume-e0ac3dff-179b-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009354096s
STEP: Saw pod success
Dec  5 20:14:46.517: INFO: Pod "downwardapi-volume-e0ac3dff-179b-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:14:46.521: INFO: Trying to get logs from node puma pod downwardapi-volume-e0ac3dff-179b-11ea-a680-be755c6bedde container client-container: <nil>
STEP: delete the pod
Dec  5 20:14:46.551: INFO: Waiting for pod downwardapi-volume-e0ac3dff-179b-11ea-a680-be755c6bedde to disappear
Dec  5 20:14:46.554: INFO: Pod downwardapi-volume-e0ac3dff-179b-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:14:46.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7120" for this suite.
Dec  5 20:14:52.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:14:52.702: INFO: namespace projected-7120 deletion completed in 6.142628141s

• [SLOW TEST:8.241 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:14:52.704: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:14:56.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5347" for this suite.
Dec  5 20:15:34.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:15:34.922: INFO: namespace kubelet-test-5347 deletion completed in 38.139707347s

• [SLOW TEST:42.219 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:15:34.923: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 20:15:34.957: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:15:36.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8274" for this suite.
Dec  5 20:15:42.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:15:42.170: INFO: namespace custom-resource-definition-8274 deletion completed in 6.140816289s

• [SLOW TEST:7.247 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:32
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:15:42.170: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Dec  5 20:16:22.289: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 11
	[quantile=0.9] = 13
	[quantile=0.99] = 16
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 6
	[quantile=0.9] = 207181
	[quantile=0.99] = 207614
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = 11
	[quantile=0.9] = 11
	[quantile=0.99] = 11
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = 242008
	[quantile=0.9] = 242008
	[quantile=0.99] = 242008
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 6
	[quantile=0.9] = 13
	[quantile=0.99] = 38
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 18
	[quantile=0.9] = 35
	[quantile=0.99] = 76
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 12
	[quantile=0.9] = 25
	[quantile=0.99] = 76
For namespace_queue_latency_sum:
	[] = 7550
For namespace_queue_latency_count:
	[] = 406
For namespace_retries:
	[] = 414
For namespace_work_duration:
	[quantile=0.5] = 174436
	[quantile=0.9] = 215411
	[quantile=0.99] = 267132
For namespace_work_duration_sum:
	[] = 57721568
For namespace_work_duration_count:
	[] = 406
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:16:22.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4197" for this suite.
Dec  5 20:16:28.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:16:28.429: INFO: namespace gc-4197 deletion completed in 6.133839052s

• [SLOW TEST:46.259 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:16:28.429: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-1ea33d79-179c-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume configMaps
Dec  5 20:16:28.471: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ea45560-179c-11ea-a680-be755c6bedde" in namespace "configmap-6613" to be "success or failure"
Dec  5 20:16:28.475: INFO: Pod "pod-configmaps-1ea45560-179c-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.552336ms
Dec  5 20:16:30.480: INFO: Pod "pod-configmaps-1ea45560-179c-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008659795s
Dec  5 20:16:32.485: INFO: Pod "pod-configmaps-1ea45560-179c-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0133159s
STEP: Saw pod success
Dec  5 20:16:32.485: INFO: Pod "pod-configmaps-1ea45560-179c-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:16:32.489: INFO: Trying to get logs from node crayfish pod pod-configmaps-1ea45560-179c-11ea-a680-be755c6bedde container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 20:16:32.516: INFO: Waiting for pod pod-configmaps-1ea45560-179c-11ea-a680-be755c6bedde to disappear
Dec  5 20:16:32.519: INFO: Pod pod-configmaps-1ea45560-179c-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:16:32.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6613" for this suite.
Dec  5 20:16:38.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:16:38.664: INFO: namespace configmap-6613 deletion completed in 6.140522093s

• [SLOW TEST:10.235 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:16:38.665: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-24be4e96-179c-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume configMaps
Dec  5 20:16:38.716: INFO: Waiting up to 5m0s for pod "pod-configmaps-24bf739e-179c-11ea-a680-be755c6bedde" in namespace "configmap-5977" to be "success or failure"
Dec  5 20:16:38.723: INFO: Pod "pod-configmaps-24bf739e-179c-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 6.411091ms
Dec  5 20:16:40.728: INFO: Pod "pod-configmaps-24bf739e-179c-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011406418s
STEP: Saw pod success
Dec  5 20:16:40.728: INFO: Pod "pod-configmaps-24bf739e-179c-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:16:40.733: INFO: Trying to get logs from node puma pod pod-configmaps-24bf739e-179c-11ea-a680-be755c6bedde container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 20:16:40.764: INFO: Waiting for pod pod-configmaps-24bf739e-179c-11ea-a680-be755c6bedde to disappear
Dec  5 20:16:40.768: INFO: Pod pod-configmaps-24bf739e-179c-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:16:40.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5977" for this suite.
Dec  5 20:16:46.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:16:46.901: INFO: namespace configmap-5977 deletion completed in 6.128952487s

• [SLOW TEST:8.237 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:16:46.904: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  5 20:16:53.000: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 20:16:53.003: INFO: Pod pod-with-poststart-http-hook still exists
Dec  5 20:16:55.003: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 20:16:55.008: INFO: Pod pod-with-poststart-http-hook still exists
Dec  5 20:16:57.003: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 20:16:57.009: INFO: Pod pod-with-poststart-http-hook still exists
Dec  5 20:16:59.003: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 20:16:59.008: INFO: Pod pod-with-poststart-http-hook still exists
Dec  5 20:17:01.003: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 20:17:01.008: INFO: Pod pod-with-poststart-http-hook still exists
Dec  5 20:17:03.003: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 20:17:03.009: INFO: Pod pod-with-poststart-http-hook still exists
Dec  5 20:17:05.003: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 20:17:05.008: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:17:05.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1807" for this suite.
Dec  5 20:17:27.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:17:27.151: INFO: namespace container-lifecycle-hook-1807 deletion completed in 22.137212283s

• [SLOW TEST:40.246 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:17:27.151: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-7k5h
STEP: Creating a pod to test atomic-volume-subpath
Dec  5 20:17:27.202: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-7k5h" in namespace "subpath-777" to be "success or failure"
Dec  5 20:17:27.206: INFO: Pod "pod-subpath-test-downwardapi-7k5h": Phase="Pending", Reason="", readiness=false. Elapsed: 3.56903ms
Dec  5 20:17:29.211: INFO: Pod "pod-subpath-test-downwardapi-7k5h": Phase="Running", Reason="", readiness=true. Elapsed: 2.008758612s
Dec  5 20:17:31.216: INFO: Pod "pod-subpath-test-downwardapi-7k5h": Phase="Running", Reason="", readiness=true. Elapsed: 4.013945523s
Dec  5 20:17:33.221: INFO: Pod "pod-subpath-test-downwardapi-7k5h": Phase="Running", Reason="", readiness=true. Elapsed: 6.019119501s
Dec  5 20:17:35.228: INFO: Pod "pod-subpath-test-downwardapi-7k5h": Phase="Running", Reason="", readiness=true. Elapsed: 8.025711566s
Dec  5 20:17:37.234: INFO: Pod "pod-subpath-test-downwardapi-7k5h": Phase="Running", Reason="", readiness=true. Elapsed: 10.031632458s
Dec  5 20:17:39.239: INFO: Pod "pod-subpath-test-downwardapi-7k5h": Phase="Running", Reason="", readiness=true. Elapsed: 12.036512787s
Dec  5 20:17:41.244: INFO: Pod "pod-subpath-test-downwardapi-7k5h": Phase="Running", Reason="", readiness=true. Elapsed: 14.041901697s
Dec  5 20:17:43.250: INFO: Pod "pod-subpath-test-downwardapi-7k5h": Phase="Running", Reason="", readiness=true. Elapsed: 16.047367844s
Dec  5 20:17:45.255: INFO: Pod "pod-subpath-test-downwardapi-7k5h": Phase="Running", Reason="", readiness=true. Elapsed: 18.053018437s
Dec  5 20:17:47.261: INFO: Pod "pod-subpath-test-downwardapi-7k5h": Phase="Running", Reason="", readiness=true. Elapsed: 20.058731818s
Dec  5 20:17:49.265: INFO: Pod "pod-subpath-test-downwardapi-7k5h": Phase="Running", Reason="", readiness=true. Elapsed: 22.063023846s
Dec  5 20:17:51.271: INFO: Pod "pod-subpath-test-downwardapi-7k5h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.068585758s
STEP: Saw pod success
Dec  5 20:17:51.271: INFO: Pod "pod-subpath-test-downwardapi-7k5h" satisfied condition "success or failure"
Dec  5 20:17:51.276: INFO: Trying to get logs from node puma pod pod-subpath-test-downwardapi-7k5h container test-container-subpath-downwardapi-7k5h: <nil>
STEP: delete the pod
Dec  5 20:17:51.306: INFO: Waiting for pod pod-subpath-test-downwardapi-7k5h to disappear
Dec  5 20:17:51.310: INFO: Pod pod-subpath-test-downwardapi-7k5h no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-7k5h
Dec  5 20:17:51.310: INFO: Deleting pod "pod-subpath-test-downwardapi-7k5h" in namespace "subpath-777"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:17:51.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-777" for this suite.
Dec  5 20:17:57.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:17:57.441: INFO: namespace subpath-777 deletion completed in 6.121454171s

• [SLOW TEST:30.290 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:17:57.444: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Dec  5 20:17:57.498: INFO: Waiting up to 5m0s for pod "client-containers-53b42986-179c-11ea-a680-be755c6bedde" in namespace "containers-8059" to be "success or failure"
Dec  5 20:17:57.501: INFO: Pod "client-containers-53b42986-179c-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.703609ms
Dec  5 20:17:59.508: INFO: Pod "client-containers-53b42986-179c-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00995638s
STEP: Saw pod success
Dec  5 20:17:59.508: INFO: Pod "client-containers-53b42986-179c-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:17:59.513: INFO: Trying to get logs from node puma pod client-containers-53b42986-179c-11ea-a680-be755c6bedde container test-container: <nil>
STEP: delete the pod
Dec  5 20:17:59.543: INFO: Waiting for pod client-containers-53b42986-179c-11ea-a680-be755c6bedde to disappear
Dec  5 20:17:59.548: INFO: Pod client-containers-53b42986-179c-11ea-a680-be755c6bedde no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:17:59.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8059" for this suite.
Dec  5 20:18:05.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:18:05.699: INFO: namespace containers-8059 deletion completed in 6.145206419s

• [SLOW TEST:8.256 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:18:05.700: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 20:18:05.747: INFO: (0) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 6.486205ms)
Dec  5 20:18:05.753: INFO: (1) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 5.241151ms)
Dec  5 20:18:05.758: INFO: (2) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.7392ms)
Dec  5 20:18:05.763: INFO: (3) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 5.336693ms)
Dec  5 20:18:05.769: INFO: (4) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 5.568587ms)
Dec  5 20:18:05.773: INFO: (5) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.269531ms)
Dec  5 20:18:05.778: INFO: (6) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.773304ms)
Dec  5 20:18:05.782: INFO: (7) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.351057ms)
Dec  5 20:18:05.788: INFO: (8) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 5.303155ms)
Dec  5 20:18:05.792: INFO: (9) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.469124ms)
Dec  5 20:18:05.797: INFO: (10) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.545157ms)
Dec  5 20:18:05.803: INFO: (11) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 5.825061ms)
Dec  5 20:18:05.807: INFO: (12) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.43248ms)
Dec  5 20:18:05.812: INFO: (13) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.408189ms)
Dec  5 20:18:05.817: INFO: (14) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.527009ms)
Dec  5 20:18:05.821: INFO: (15) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.52988ms)
Dec  5 20:18:05.825: INFO: (16) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.256531ms)
Dec  5 20:18:05.830: INFO: (17) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.787449ms)
Dec  5 20:18:05.835: INFO: (18) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.61673ms)
Dec  5 20:18:05.840: INFO: (19) /api/v1/nodes/crayfish:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.433099ms)
[AfterEach] version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:18:05.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3489" for this suite.
Dec  5 20:18:11.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:18:11.988: INFO: namespace proxy-3489 deletion completed in 6.143959242s

• [SLOW TEST:6.288 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:18:11.988: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-5c5f0797-179c-11ea-a680-be755c6bedde
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:18:14.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9207" for this suite.
Dec  5 20:18:32.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:18:32.252: INFO: namespace configmap-9207 deletion completed in 18.163596315s

• [SLOW TEST:20.263 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:18:32.256: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-687317ab-179c-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume secrets
Dec  5 20:18:32.310: INFO: Waiting up to 5m0s for pod "pod-secrets-6874480c-179c-11ea-a680-be755c6bedde" in namespace "secrets-4319" to be "success or failure"
Dec  5 20:18:32.313: INFO: Pod "pod-secrets-6874480c-179c-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.606583ms
Dec  5 20:18:34.319: INFO: Pod "pod-secrets-6874480c-179c-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009555726s
STEP: Saw pod success
Dec  5 20:18:34.319: INFO: Pod "pod-secrets-6874480c-179c-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:18:34.324: INFO: Trying to get logs from node puma pod pod-secrets-6874480c-179c-11ea-a680-be755c6bedde container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 20:18:34.351: INFO: Waiting for pod pod-secrets-6874480c-179c-11ea-a680-be755c6bedde to disappear
Dec  5 20:18:34.355: INFO: Pod pod-secrets-6874480c-179c-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:18:34.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4319" for this suite.
Dec  5 20:18:40.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:18:40.507: INFO: namespace secrets-4319 deletion completed in 6.147173116s

• [SLOW TEST:8.251 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:18:40.508: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 20:18:40.568: INFO: (0) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 17.485617ms)
Dec  5 20:18:40.573: INFO: (1) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.382261ms)
Dec  5 20:18:40.577: INFO: (2) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.33278ms)
Dec  5 20:18:40.582: INFO: (3) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.443692ms)
Dec  5 20:18:40.586: INFO: (4) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.274625ms)
Dec  5 20:18:40.591: INFO: (5) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.711799ms)
Dec  5 20:18:40.595: INFO: (6) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.649923ms)
Dec  5 20:18:40.600: INFO: (7) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.548362ms)
Dec  5 20:18:40.604: INFO: (8) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.282016ms)
Dec  5 20:18:40.608: INFO: (9) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 3.999698ms)
Dec  5 20:18:40.612: INFO: (10) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 3.969157ms)
Dec  5 20:18:40.616: INFO: (11) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.01374ms)
Dec  5 20:18:40.621: INFO: (12) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.779607ms)
Dec  5 20:18:40.626: INFO: (13) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.370735ms)
Dec  5 20:18:40.630: INFO: (14) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.735058ms)
Dec  5 20:18:40.634: INFO: (15) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 3.952065ms)
Dec  5 20:18:40.638: INFO: (16) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 3.778196ms)
Dec  5 20:18:40.643: INFO: (17) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.319649ms)
Dec  5 20:18:40.647: INFO: (18) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.515548ms)
Dec  5 20:18:40.652: INFO: (19) /api/v1/nodes/crayfish/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="archive/">... (200; 4.413197ms)
[AfterEach] version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:18:40.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7589" for this suite.
Dec  5 20:18:46.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:18:46.804: INFO: namespace proxy-7589 deletion completed in 6.139002831s

• [SLOW TEST:6.297 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:18:46.804: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  5 20:18:46.853: INFO: Waiting up to 5m0s for pod "pod-711f4999-179c-11ea-a680-be755c6bedde" in namespace "emptydir-8182" to be "success or failure"
Dec  5 20:18:46.859: INFO: Pod "pod-711f4999-179c-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 5.711982ms
Dec  5 20:18:48.865: INFO: Pod "pod-711f4999-179c-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011761158s
Dec  5 20:18:50.871: INFO: Pod "pod-711f4999-179c-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017456453s
STEP: Saw pod success
Dec  5 20:18:50.871: INFO: Pod "pod-711f4999-179c-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:18:50.875: INFO: Trying to get logs from node crayfish pod pod-711f4999-179c-11ea-a680-be755c6bedde container test-container: <nil>
STEP: delete the pod
Dec  5 20:18:50.904: INFO: Waiting for pod pod-711f4999-179c-11ea-a680-be755c6bedde to disappear
Dec  5 20:18:50.909: INFO: Pod pod-711f4999-179c-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:18:50.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8182" for this suite.
Dec  5 20:18:56.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:18:57.069: INFO: namespace emptydir-8182 deletion completed in 6.154555532s

• [SLOW TEST:10.264 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:18:57.069: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Dec  5 20:18:57.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 api-versions'
Dec  5 20:18:57.203: INFO: stderr: ""
Dec  5 20:18:57.203: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:18:57.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1950" for this suite.
Dec  5 20:19:03.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:19:03.328: INFO: namespace kubectl-1950 deletion completed in 6.119533259s

• [SLOW TEST:6.259 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:19:03.329: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-7af80d1d-179c-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume configMaps
Dec  5 20:19:03.377: INFO: Waiting up to 5m0s for pod "pod-configmaps-7af9081c-179c-11ea-a680-be755c6bedde" in namespace "configmap-6280" to be "success or failure"
Dec  5 20:19:03.381: INFO: Pod "pod-configmaps-7af9081c-179c-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.390808ms
Dec  5 20:19:05.387: INFO: Pod "pod-configmaps-7af9081c-179c-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009963589s
Dec  5 20:19:07.393: INFO: Pod "pod-configmaps-7af9081c-179c-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015932925s
STEP: Saw pod success
Dec  5 20:19:07.393: INFO: Pod "pod-configmaps-7af9081c-179c-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:19:07.398: INFO: Trying to get logs from node puma pod pod-configmaps-7af9081c-179c-11ea-a680-be755c6bedde container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 20:19:07.432: INFO: Waiting for pod pod-configmaps-7af9081c-179c-11ea-a680-be755c6bedde to disappear
Dec  5 20:19:07.436: INFO: Pod pod-configmaps-7af9081c-179c-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:19:07.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6280" for this suite.
Dec  5 20:19:13.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:19:13.581: INFO: namespace configmap-6280 deletion completed in 6.139253085s

• [SLOW TEST:10.252 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:19:13.582: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  5 20:19:13.628: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81149b1a-179c-11ea-a680-be755c6bedde" in namespace "projected-457" to be "success or failure"
Dec  5 20:19:13.632: INFO: Pod "downwardapi-volume-81149b1a-179c-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.423815ms
Dec  5 20:19:15.638: INFO: Pod "downwardapi-volume-81149b1a-179c-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010272024s
Dec  5 20:19:17.653: INFO: Pod "downwardapi-volume-81149b1a-179c-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025560812s
STEP: Saw pod success
Dec  5 20:19:17.653: INFO: Pod "downwardapi-volume-81149b1a-179c-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:19:17.658: INFO: Trying to get logs from node crayfish pod downwardapi-volume-81149b1a-179c-11ea-a680-be755c6bedde container client-container: <nil>
STEP: delete the pod
Dec  5 20:19:17.686: INFO: Waiting for pod downwardapi-volume-81149b1a-179c-11ea-a680-be755c6bedde to disappear
Dec  5 20:19:17.690: INFO: Pod downwardapi-volume-81149b1a-179c-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:19:17.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-457" for this suite.
Dec  5 20:19:23.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:19:23.820: INFO: namespace projected-457 deletion completed in 6.124048613s

• [SLOW TEST:10.238 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:19:23.820: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Dec  5 20:19:25.886: INFO: Pod pod-hostip-872f121f-179c-11ea-a680-be755c6bedde has hostIP: 178.62.90.156
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:19:25.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1414" for this suite.
Dec  5 20:19:47.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:19:48.034: INFO: namespace pods-1414 deletion completed in 22.142028643s

• [SLOW TEST:24.214 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:19:48.035: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Dec  5 20:19:48.083: INFO: Waiting up to 5m0s for pod "var-expansion-959dfd94-179c-11ea-a680-be755c6bedde" in namespace "var-expansion-7222" to be "success or failure"
Dec  5 20:19:48.087: INFO: Pod "var-expansion-959dfd94-179c-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.583652ms
Dec  5 20:19:50.093: INFO: Pod "var-expansion-959dfd94-179c-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010504827s
Dec  5 20:19:52.099: INFO: Pod "var-expansion-959dfd94-179c-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016340853s
STEP: Saw pod success
Dec  5 20:19:52.099: INFO: Pod "var-expansion-959dfd94-179c-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:19:52.104: INFO: Trying to get logs from node crayfish pod var-expansion-959dfd94-179c-11ea-a680-be755c6bedde container dapi-container: <nil>
STEP: delete the pod
Dec  5 20:19:52.134: INFO: Waiting for pod var-expansion-959dfd94-179c-11ea-a680-be755c6bedde to disappear
Dec  5 20:19:52.138: INFO: Pod var-expansion-959dfd94-179c-11ea-a680-be755c6bedde no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:19:52.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7222" for this suite.
Dec  5 20:19:58.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:19:58.268: INFO: namespace var-expansion-7222 deletion completed in 6.124271737s

• [SLOW TEST:10.234 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:19:58.268: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-8f5f
STEP: Creating a pod to test atomic-volume-subpath
Dec  5 20:19:58.325: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-8f5f" in namespace "subpath-2499" to be "success or failure"
Dec  5 20:19:58.329: INFO: Pod "pod-subpath-test-projected-8f5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.136965ms
Dec  5 20:20:00.335: INFO: Pod "pod-subpath-test-projected-8f5f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009890538s
Dec  5 20:20:02.341: INFO: Pod "pod-subpath-test-projected-8f5f": Phase="Running", Reason="", readiness=true. Elapsed: 4.01577399s
Dec  5 20:20:04.346: INFO: Pod "pod-subpath-test-projected-8f5f": Phase="Running", Reason="", readiness=true. Elapsed: 6.020985093s
Dec  5 20:20:06.351: INFO: Pod "pod-subpath-test-projected-8f5f": Phase="Running", Reason="", readiness=true. Elapsed: 8.026457614s
Dec  5 20:20:08.357: INFO: Pod "pod-subpath-test-projected-8f5f": Phase="Running", Reason="", readiness=true. Elapsed: 10.032396788s
Dec  5 20:20:10.364: INFO: Pod "pod-subpath-test-projected-8f5f": Phase="Running", Reason="", readiness=true. Elapsed: 12.03949639s
Dec  5 20:20:12.371: INFO: Pod "pod-subpath-test-projected-8f5f": Phase="Running", Reason="", readiness=true. Elapsed: 14.045948943s
Dec  5 20:20:14.377: INFO: Pod "pod-subpath-test-projected-8f5f": Phase="Running", Reason="", readiness=true. Elapsed: 16.052097904s
Dec  5 20:20:16.385: INFO: Pod "pod-subpath-test-projected-8f5f": Phase="Running", Reason="", readiness=true. Elapsed: 18.060273888s
Dec  5 20:20:18.391: INFO: Pod "pod-subpath-test-projected-8f5f": Phase="Running", Reason="", readiness=true. Elapsed: 20.066024569s
Dec  5 20:20:20.397: INFO: Pod "pod-subpath-test-projected-8f5f": Phase="Running", Reason="", readiness=true. Elapsed: 22.072213929s
Dec  5 20:20:22.403: INFO: Pod "pod-subpath-test-projected-8f5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.078021885s
STEP: Saw pod success
Dec  5 20:20:22.403: INFO: Pod "pod-subpath-test-projected-8f5f" satisfied condition "success or failure"
Dec  5 20:20:22.408: INFO: Trying to get logs from node crayfish pod pod-subpath-test-projected-8f5f container test-container-subpath-projected-8f5f: <nil>
STEP: delete the pod
Dec  5 20:20:22.438: INFO: Waiting for pod pod-subpath-test-projected-8f5f to disappear
Dec  5 20:20:22.442: INFO: Pod pod-subpath-test-projected-8f5f no longer exists
STEP: Deleting pod pod-subpath-test-projected-8f5f
Dec  5 20:20:22.442: INFO: Deleting pod "pod-subpath-test-projected-8f5f" in namespace "subpath-2499"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:20:22.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2499" for this suite.
Dec  5 20:20:28.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:20:28.596: INFO: namespace subpath-2499 deletion completed in 6.143960558s

• [SLOW TEST:30.327 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:20:28.597: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Dec  5 20:20:28.631: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec  5 20:20:28.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 create -f - --namespace=kubectl-5341'
Dec  5 20:20:28.881: INFO: stderr: ""
Dec  5 20:20:28.881: INFO: stdout: "service/redis-slave created\n"
Dec  5 20:20:28.881: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec  5 20:20:28.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 create -f - --namespace=kubectl-5341'
Dec  5 20:20:29.163: INFO: stderr: ""
Dec  5 20:20:29.163: INFO: stdout: "service/redis-master created\n"
Dec  5 20:20:29.164: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec  5 20:20:29.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 create -f - --namespace=kubectl-5341'
Dec  5 20:20:29.415: INFO: stderr: ""
Dec  5 20:20:29.415: INFO: stdout: "service/frontend created\n"
Dec  5 20:20:29.416: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec  5 20:20:29.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 create -f - --namespace=kubectl-5341'
Dec  5 20:20:29.581: INFO: stderr: ""
Dec  5 20:20:29.581: INFO: stdout: "deployment.apps/frontend created\n"
Dec  5 20:20:29.581: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec  5 20:20:29.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 create -f - --namespace=kubectl-5341'
Dec  5 20:20:29.740: INFO: stderr: ""
Dec  5 20:20:29.740: INFO: stdout: "deployment.apps/redis-master created\n"
Dec  5 20:20:29.740: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec  5 20:20:29.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 create -f - --namespace=kubectl-5341'
Dec  5 20:20:29.912: INFO: stderr: ""
Dec  5 20:20:29.912: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Dec  5 20:20:29.912: INFO: Waiting for all frontend pods to be Running.
Dec  5 20:20:34.963: INFO: Waiting for frontend to serve content.
Dec  5 20:20:34.987: INFO: Trying to add a new entry to the guestbook.
Dec  5 20:20:35.005: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec  5 20:20:35.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 delete --grace-period=0 --force -f - --namespace=kubectl-5341'
Dec  5 20:20:35.170: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 20:20:35.170: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec  5 20:20:35.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 delete --grace-period=0 --force -f - --namespace=kubectl-5341'
Dec  5 20:20:35.284: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 20:20:35.284: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  5 20:20:35.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 delete --grace-period=0 --force -f - --namespace=kubectl-5341'
Dec  5 20:20:35.388: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 20:20:35.388: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  5 20:20:35.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 delete --grace-period=0 --force -f - --namespace=kubectl-5341'
Dec  5 20:20:35.468: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 20:20:35.468: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  5 20:20:35.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 delete --grace-period=0 --force -f - --namespace=kubectl-5341'
Dec  5 20:20:35.549: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 20:20:35.549: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  5 20:20:35.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 delete --grace-period=0 --force -f - --namespace=kubectl-5341'
Dec  5 20:20:35.628: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 20:20:35.628: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:20:35.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5341" for this suite.
Dec  5 20:21:15.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:21:15.769: INFO: namespace kubectl-5341 deletion completed in 40.136836728s

• [SLOW TEST:47.173 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:21:15.770: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-c9e96e63-179c-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume configMaps
Dec  5 20:21:15.823: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c9ea9710-179c-11ea-a680-be755c6bedde" in namespace "projected-4187" to be "success or failure"
Dec  5 20:21:15.828: INFO: Pod "pod-projected-configmaps-c9ea9710-179c-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 5.394538ms
Dec  5 20:21:17.845: INFO: Pod "pod-projected-configmaps-c9ea9710-179c-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02167603s
Dec  5 20:21:19.851: INFO: Pod "pod-projected-configmaps-c9ea9710-179c-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028233542s
STEP: Saw pod success
Dec  5 20:21:19.851: INFO: Pod "pod-projected-configmaps-c9ea9710-179c-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:21:19.856: INFO: Trying to get logs from node crayfish pod pod-projected-configmaps-c9ea9710-179c-11ea-a680-be755c6bedde container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 20:21:19.883: INFO: Waiting for pod pod-projected-configmaps-c9ea9710-179c-11ea-a680-be755c6bedde to disappear
Dec  5 20:21:19.888: INFO: Pod pod-projected-configmaps-c9ea9710-179c-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:21:19.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4187" for this suite.
Dec  5 20:21:25.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:21:26.030: INFO: namespace projected-4187 deletion completed in 6.137347769s

• [SLOW TEST:10.260 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:21:26.030: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Dec  5 20:21:26.093: INFO: Waiting up to 5m0s for pod "pod-d00941cc-179c-11ea-a680-be755c6bedde" in namespace "emptydir-9111" to be "success or failure"
Dec  5 20:21:26.097: INFO: Pod "pod-d00941cc-179c-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.261275ms
Dec  5 20:21:28.102: INFO: Pod "pod-d00941cc-179c-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008334047s
STEP: Saw pod success
Dec  5 20:21:28.102: INFO: Pod "pod-d00941cc-179c-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:21:28.106: INFO: Trying to get logs from node puma pod pod-d00941cc-179c-11ea-a680-be755c6bedde container test-container: <nil>
STEP: delete the pod
Dec  5 20:21:28.131: INFO: Waiting for pod pod-d00941cc-179c-11ea-a680-be755c6bedde to disappear
Dec  5 20:21:28.135: INFO: Pod pod-d00941cc-179c-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:21:28.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9111" for this suite.
Dec  5 20:21:34.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:21:34.274: INFO: namespace emptydir-9111 deletion completed in 6.134083257s

• [SLOW TEST:8.244 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:21:34.275: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  5 20:21:34.328: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d4f1b6fb-179c-11ea-a680-be755c6bedde" in namespace "downward-api-9146" to be "success or failure"
Dec  5 20:21:34.332: INFO: Pod "downwardapi-volume-d4f1b6fb-179c-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.672314ms
Dec  5 20:21:36.337: INFO: Pod "downwardapi-volume-d4f1b6fb-179c-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009293707s
Dec  5 20:21:38.343: INFO: Pod "downwardapi-volume-d4f1b6fb-179c-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015238887s
STEP: Saw pod success
Dec  5 20:21:38.343: INFO: Pod "downwardapi-volume-d4f1b6fb-179c-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:21:38.347: INFO: Trying to get logs from node crayfish pod downwardapi-volume-d4f1b6fb-179c-11ea-a680-be755c6bedde container client-container: <nil>
STEP: delete the pod
Dec  5 20:21:38.378: INFO: Waiting for pod downwardapi-volume-d4f1b6fb-179c-11ea-a680-be755c6bedde to disappear
Dec  5 20:21:38.381: INFO: Pod downwardapi-volume-d4f1b6fb-179c-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:21:38.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9146" for this suite.
Dec  5 20:21:44.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:21:44.521: INFO: namespace downward-api-9146 deletion completed in 6.134528233s

• [SLOW TEST:10.246 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:21:44.522: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 20:21:44.578: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec  5 20:21:44.591: INFO: Number of nodes with available pods: 0
Dec  5 20:21:44.591: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec  5 20:21:44.612: INFO: Number of nodes with available pods: 0
Dec  5 20:21:44.612: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:21:45.617: INFO: Number of nodes with available pods: 0
Dec  5 20:21:45.617: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:21:46.617: INFO: Number of nodes with available pods: 0
Dec  5 20:21:46.617: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:21:47.617: INFO: Number of nodes with available pods: 1
Dec  5 20:21:47.617: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec  5 20:21:47.638: INFO: Number of nodes with available pods: 1
Dec  5 20:21:47.638: INFO: Number of running nodes: 0, number of available pods: 1
Dec  5 20:21:48.644: INFO: Number of nodes with available pods: 0
Dec  5 20:21:48.644: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec  5 20:21:48.661: INFO: Number of nodes with available pods: 0
Dec  5 20:21:48.661: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:21:49.667: INFO: Number of nodes with available pods: 0
Dec  5 20:21:49.667: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:21:50.668: INFO: Number of nodes with available pods: 0
Dec  5 20:21:50.668: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:21:51.667: INFO: Number of nodes with available pods: 0
Dec  5 20:21:51.667: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:21:52.673: INFO: Number of nodes with available pods: 0
Dec  5 20:21:52.673: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:21:53.667: INFO: Number of nodes with available pods: 0
Dec  5 20:21:53.667: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:21:54.671: INFO: Number of nodes with available pods: 0
Dec  5 20:21:54.671: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:21:55.666: INFO: Number of nodes with available pods: 0
Dec  5 20:21:55.666: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:21:56.672: INFO: Number of nodes with available pods: 1
Dec  5 20:21:56.672: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4248, will wait for the garbage collector to delete the pods
Dec  5 20:21:56.747: INFO: Deleting DaemonSet.extensions daemon-set took: 10.507834ms
Dec  5 20:21:57.048: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.706236ms
Dec  5 20:22:00.054: INFO: Number of nodes with available pods: 0
Dec  5 20:22:00.054: INFO: Number of running nodes: 0, number of available pods: 0
Dec  5 20:22:00.059: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4248/daemonsets","resourceVersion":"50085"},"items":null}

Dec  5 20:22:00.064: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4248/pods","resourceVersion":"50085"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:22:00.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4248" for this suite.
Dec  5 20:22:06.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:22:06.231: INFO: namespace daemonsets-4248 deletion completed in 6.135045268s

• [SLOW TEST:21.709 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:22:06.232: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1521
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 20:22:06.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-2589'
Dec  5 20:22:06.364: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  5 20:22:06.364: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1526
Dec  5 20:22:08.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 delete deployment e2e-test-nginx-deployment --namespace=kubectl-2589'
Dec  5 20:22:08.508: INFO: stderr: ""
Dec  5 20:22:08.508: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:22:08.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2589" for this suite.
Dec  5 20:24:10.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:24:10.660: INFO: namespace kubectl-2589 deletion completed in 2m2.144963487s

• [SLOW TEST:124.428 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:24:10.660: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  5 20:24:10.709: INFO: Waiting up to 5m0s for pod "pod-3227d767-179d-11ea-a680-be755c6bedde" in namespace "emptydir-2568" to be "success or failure"
Dec  5 20:24:10.714: INFO: Pod "pod-3227d767-179d-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.955473ms
Dec  5 20:24:12.720: INFO: Pod "pod-3227d767-179d-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010970172s
STEP: Saw pod success
Dec  5 20:24:12.720: INFO: Pod "pod-3227d767-179d-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:24:12.725: INFO: Trying to get logs from node puma pod pod-3227d767-179d-11ea-a680-be755c6bedde container test-container: <nil>
STEP: delete the pod
Dec  5 20:24:12.753: INFO: Waiting for pod pod-3227d767-179d-11ea-a680-be755c6bedde to disappear
Dec  5 20:24:12.757: INFO: Pod pod-3227d767-179d-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:24:12.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2568" for this suite.
Dec  5 20:24:18.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:24:18.912: INFO: namespace emptydir-2568 deletion completed in 6.149863689s

• [SLOW TEST:8.252 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:24:18.913: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  5 20:24:18.957: INFO: Waiting up to 5m0s for pod "downwardapi-volume-371264df-179d-11ea-a680-be755c6bedde" in namespace "downward-api-7665" to be "success or failure"
Dec  5 20:24:18.962: INFO: Pod "downwardapi-volume-371264df-179d-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.588801ms
Dec  5 20:24:20.968: INFO: Pod "downwardapi-volume-371264df-179d-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010455739s
STEP: Saw pod success
Dec  5 20:24:20.968: INFO: Pod "downwardapi-volume-371264df-179d-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:24:20.973: INFO: Trying to get logs from node puma pod downwardapi-volume-371264df-179d-11ea-a680-be755c6bedde container client-container: <nil>
STEP: delete the pod
Dec  5 20:24:20.997: INFO: Waiting for pod downwardapi-volume-371264df-179d-11ea-a680-be755c6bedde to disappear
Dec  5 20:24:21.001: INFO: Pod downwardapi-volume-371264df-179d-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:24:21.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7665" for this suite.
Dec  5 20:24:27.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:24:27.154: INFO: namespace downward-api-7665 deletion completed in 6.149121696s

• [SLOW TEST:8.242 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:24:27.156: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 20:24:27.220: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"3bfeb9e5-179d-11ea-b857-a68228aedf28", Controller:(*bool)(0xc0023f9e9a), BlockOwnerDeletion:(*bool)(0xc0023f9e9b)}}
Dec  5 20:24:27.226: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3bfcdad4-179d-11ea-b857-a68228aedf28", Controller:(*bool)(0xc002e423d6), BlockOwnerDeletion:(*bool)(0xc002e423d7)}}
Dec  5 20:24:27.230: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3bfdf187-179d-11ea-b857-a68228aedf28", Controller:(*bool)(0xc000870bb6), BlockOwnerDeletion:(*bool)(0xc000870bb7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:24:32.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1866" for this suite.
Dec  5 20:24:38.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:24:38.396: INFO: namespace gc-1866 deletion completed in 6.14486497s

• [SLOW TEST:11.241 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:24:38.397: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7180
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-7180
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7180
Dec  5 20:24:38.448: INFO: Found 0 stateful pods, waiting for 1
Dec  5 20:24:48.455: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec  5 20:24:48.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 20:24:48.763: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 20:24:48.763: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 20:24:48.763: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 20:24:48.769: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  5 20:24:58.776: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 20:24:58.776: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 20:24:58.799: INFO: POD   NODE  PHASE    GRACE  CONDITIONS
Dec  5 20:24:58.799: INFO: ss-0  puma  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  }]
Dec  5 20:24:58.799: INFO: ss-1        Pending         []
Dec  5 20:24:58.799: INFO: 
Dec  5 20:24:58.799: INFO: StatefulSet ss has not reached scale 3, at 2
Dec  5 20:24:59.805: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99414621s
Dec  5 20:25:00.812: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988267016s
Dec  5 20:25:01.816: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981587917s
Dec  5 20:25:02.822: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976709365s
Dec  5 20:25:03.828: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.970921745s
Dec  5 20:25:04.835: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964767334s
Dec  5 20:25:05.841: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.958212324s
Dec  5 20:25:06.848: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.951676228s
Dec  5 20:25:07.853: INFO: Verifying statefulset ss doesn't scale past 3 for another 945.286613ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7180
Dec  5 20:25:08.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:25:09.127: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  5 20:25:09.127: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 20:25:09.128: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 20:25:09.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:25:09.383: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  5 20:25:09.383: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 20:25:09.383: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 20:25:09.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:25:09.624: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  5 20:25:09.624: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 20:25:09.624: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 20:25:09.629: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Dec  5 20:25:19.636: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 20:25:19.636: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 20:25:19.636: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec  5 20:25:19.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 20:25:19.924: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 20:25:19.924: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 20:25:19.924: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 20:25:19.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 20:25:20.219: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 20:25:20.219: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 20:25:20.219: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 20:25:20.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 20:25:20.431: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 20:25:20.431: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 20:25:20.431: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 20:25:20.431: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 20:25:20.435: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec  5 20:25:30.446: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 20:25:30.446: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 20:25:30.446: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 20:25:30.466: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec  5 20:25:30.466: INFO: ss-0  puma      Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  }]
Dec  5 20:25:30.466: INFO: ss-1  crayfish  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  }]
Dec  5 20:25:30.466: INFO: ss-2  puma      Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  }]
Dec  5 20:25:30.466: INFO: 
Dec  5 20:25:30.466: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  5 20:25:31.471: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec  5 20:25:31.471: INFO: ss-0  puma      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  }]
Dec  5 20:25:31.471: INFO: ss-1  crayfish  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  }]
Dec  5 20:25:31.472: INFO: ss-2  puma      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  }]
Dec  5 20:25:31.472: INFO: 
Dec  5 20:25:31.472: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  5 20:25:32.477: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec  5 20:25:32.477: INFO: ss-0  puma      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  }]
Dec  5 20:25:32.477: INFO: ss-1  crayfish  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  }]
Dec  5 20:25:32.477: INFO: ss-2  puma      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  }]
Dec  5 20:25:32.477: INFO: 
Dec  5 20:25:32.477: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  5 20:25:33.482: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec  5 20:25:33.482: INFO: ss-0  puma      Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  }]
Dec  5 20:25:33.482: INFO: ss-1  crayfish  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  }]
Dec  5 20:25:33.482: INFO: 
Dec  5 20:25:33.482: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  5 20:25:34.488: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec  5 20:25:34.488: INFO: ss-0  puma      Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  }]
Dec  5 20:25:34.488: INFO: ss-1  crayfish  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  }]
Dec  5 20:25:34.488: INFO: 
Dec  5 20:25:34.488: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  5 20:25:35.494: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec  5 20:25:35.494: INFO: ss-0  puma      Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  }]
Dec  5 20:25:35.494: INFO: ss-1  crayfish  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  }]
Dec  5 20:25:35.494: INFO: 
Dec  5 20:25:35.494: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  5 20:25:36.499: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec  5 20:25:36.499: INFO: ss-0  puma      Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  }]
Dec  5 20:25:36.499: INFO: ss-1  crayfish  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  }]
Dec  5 20:25:36.499: INFO: 
Dec  5 20:25:36.499: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  5 20:25:37.505: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec  5 20:25:37.505: INFO: ss-0  puma      Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  }]
Dec  5 20:25:37.505: INFO: ss-1  crayfish  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  }]
Dec  5 20:25:37.505: INFO: 
Dec  5 20:25:37.505: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  5 20:25:38.511: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec  5 20:25:38.511: INFO: ss-0  puma      Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  }]
Dec  5 20:25:38.512: INFO: ss-1  crayfish  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  }]
Dec  5 20:25:38.512: INFO: 
Dec  5 20:25:38.512: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  5 20:25:39.518: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Dec  5 20:25:39.518: INFO: ss-0  puma      Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:38 +0000 UTC  }]
Dec  5 20:25:39.518: INFO: ss-1  crayfish  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:25:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:24:58 +0000 UTC  }]
Dec  5 20:25:39.518: INFO: 
Dec  5 20:25:39.518: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7180
Dec  5 20:25:40.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:25:40.690: INFO: rc: 1
Dec  5 20:25:40.690: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0025655f0 exit status 1 <nil> <nil> true [0xc0009ac678 0xc0009ad060 0xc0009ad500] [0xc0009ac678 0xc0009ad060 0xc0009ad500] [0xc0009acc08 0xc0009ad408] [0xb916c0 0xb916c0] 0xc002e1c180 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Dec  5 20:25:50.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:25:50.809: INFO: rc: 1
Dec  5 20:25:50.809: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002a725a0 exit status 1 <nil> <nil> true [0xc0007bb348 0xc0007bb458 0xc0007bb608] [0xc0007bb348 0xc0007bb458 0xc0007bb608] [0xc0007bb430 0xc0007bb578] [0xb916c0 0xb916c0] 0xc0025b25a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:26:00.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:26:00.915: INFO: rc: 1
Dec  5 20:26:00.915: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002a72a50 exit status 1 <nil> <nil> true [0xc0007bb650 0xc0007bb748 0xc0007bb838] [0xc0007bb650 0xc0007bb748 0xc0007bb838] [0xc0007bb730 0xc0007bb7b8] [0xb916c0 0xb916c0] 0xc0025b2cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:26:10.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:26:11.030: INFO: rc: 1
Dec  5 20:26:11.030: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002a72d80 exit status 1 <nil> <nil> true [0xc0007bb8d0 0xc0007bba30 0xc0007bbaa0] [0xc0007bb8d0 0xc0007bba30 0xc0007bbaa0] [0xc0007bb9a0 0xc0007bba80] [0xb916c0 0xb916c0] 0xc0025b3740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:26:21.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:26:21.140: INFO: rc: 1
Dec  5 20:26:21.140: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002a73110 exit status 1 <nil> <nil> true [0xc0007bbb00 0xc0007bbbe0 0xc0007bbc20] [0xc0007bbb00 0xc0007bbbe0 0xc0007bbc20] [0xc0007bbbc8 0xc0007bbc10] [0xb916c0 0xb916c0] 0xc002940120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:26:31.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:26:31.236: INFO: rc: 1
Dec  5 20:26:31.236: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0025659b0 exit status 1 <nil> <nil> true [0xc0009ad5a0 0xc0009ad888 0xc0009ad9f0] [0xc0009ad5a0 0xc0009ad888 0xc0009ad9f0] [0xc0009ad790 0xc0009ad9a0] [0xb916c0 0xb916c0] 0xc002e1c5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:26:41.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:26:41.351: INFO: rc: 1
Dec  5 20:26:41.351: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0023d0330 exit status 1 <nil> <nil> true [0xc002ea6000 0xc002ea6018 0xc002ea6030] [0xc002ea6000 0xc002ea6018 0xc002ea6030] [0xc002ea6010 0xc002ea6028] [0xb916c0 0xb916c0] 0xc001918360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:26:51.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:26:51.440: INFO: rc: 1
Dec  5 20:26:51.440: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00232df20 exit status 1 <nil> <nil> true [0xc0001ca9a8 0xc0001ca9f8 0xc0001caa38] [0xc0001ca9a8 0xc0001ca9f8 0xc0001caa38] [0xc0001ca9f0 0xc0001caa28] [0xb916c0 0xb916c0] 0xc00371f440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:27:01.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:27:01.554: INFO: rc: 1
Dec  5 20:27:01.554: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001676300 exit status 1 <nil> <nil> true [0xc0001caa90 0xc0001caae0 0xc0001caaf8] [0xc0001caa90 0xc0001caae0 0xc0001caaf8] [0xc0001caad8 0xc0001caaf0] [0xb916c0 0xb916c0] 0xc00371f860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:27:11.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:27:11.654: INFO: rc: 1
Dec  5 20:27:11.654: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000345a10 exit status 1 <nil> <nil> true [0xc0009ada98 0xc0009adc20 0xc0009addb0] [0xc0009ada98 0xc0009adc20 0xc0009addb0] [0xc0009adbd0 0xc0009add30] [0xb916c0 0xb916c0] 0xc002e1cba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:27:21.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:27:21.758: INFO: rc: 1
Dec  5 20:27:21.758: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002564300 exit status 1 <nil> <nil> true [0xc0000100d0 0xc0000db420 0xc0000db5b8] [0xc0000100d0 0xc0000db420 0xc0000db5b8] [0xc0000da5b8 0xc0000db578] [0xb916c0 0xb916c0] 0xc0025b27e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:27:31.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:27:31.869: INFO: rc: 1
Dec  5 20:27:31.869: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002b74300 exit status 1 <nil> <nil> true [0xc0007ba098 0xc0007ba4e8 0xc0007ba580] [0xc0007ba098 0xc0007ba4e8 0xc0007ba580] [0xc0007ba3b8 0xc0007ba570] [0xb916c0 0xb916c0] 0xc0022ce780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:27:41.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:27:41.976: INFO: rc: 1
Dec  5 20:27:41.976: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002ee2450 exit status 1 <nil> <nil> true [0xc002ea6000 0xc002ea6018 0xc002ea6030] [0xc002ea6000 0xc002ea6018 0xc002ea6030] [0xc002ea6010 0xc002ea6028] [0xb916c0 0xb916c0] 0xc0028845a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:27:51.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:27:52.080: INFO: rc: 1
Dec  5 20:27:52.080: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002b747e0 exit status 1 <nil> <nil> true [0xc0007ba638 0xc0007ba9b8 0xc0007baaf8] [0xc0007ba638 0xc0007ba9b8 0xc0007baaf8] [0xc0007ba938 0xc0007baa88] [0xb916c0 0xb916c0] 0xc0022cf140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:28:02.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:28:02.191: INFO: rc: 1
Dec  5 20:28:02.191: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002ee27b0 exit status 1 <nil> <nil> true [0xc002ea6038 0xc002ea6050 0xc002ea6068] [0xc002ea6038 0xc002ea6050 0xc002ea6068] [0xc002ea6048 0xc002ea6060] [0xb916c0 0xb916c0] 0xc002884d80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:28:12.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:28:12.302: INFO: rc: 1
Dec  5 20:28:12.302: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002b74b40 exit status 1 <nil> <nil> true [0xc0007babd8 0xc0007bacb8 0xc0007baf08] [0xc0007babd8 0xc0007bacb8 0xc0007baf08] [0xc0007bac58 0xc0007bae68] [0xb916c0 0xb916c0] 0xc0022cff20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:28:22.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:28:22.420: INFO: rc: 1
Dec  5 20:28:22.420: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002ee2b10 exit status 1 <nil> <nil> true [0xc002ea6070 0xc002ea6088 0xc002ea60a0] [0xc002ea6070 0xc002ea6088 0xc002ea60a0] [0xc002ea6080 0xc002ea6098] [0xb916c0 0xb916c0] 0xc002885440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:28:32.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:28:32.525: INFO: rc: 1
Dec  5 20:28:32.525: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002ee2e40 exit status 1 <nil> <nil> true [0xc002ea60a8 0xc002ea60c0 0xc002ea60d8] [0xc002ea60a8 0xc002ea60c0 0xc002ea60d8] [0xc002ea60b8 0xc002ea60d0] [0xb916c0 0xb916c0] 0xc002885b00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:28:42.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:28:42.648: INFO: rc: 1
Dec  5 20:28:42.648: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002ee31a0 exit status 1 <nil> <nil> true [0xc002ea60e0 0xc002ea60f8 0xc002ea6110] [0xc002ea60e0 0xc002ea60f8 0xc002ea6110] [0xc002ea60f0 0xc002ea6108] [0xb916c0 0xb916c0] 0xc001918240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:28:52.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:28:52.751: INFO: rc: 1
Dec  5 20:28:52.751: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002b74f00 exit status 1 <nil> <nil> true [0xc0007baf60 0xc0007bb088 0xc0007bb1a8] [0xc0007baf60 0xc0007bb088 0xc0007bb1a8] [0xc0007bafb0 0xc0007bb148] [0xb916c0 0xb916c0] 0xc002940300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:29:02.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:29:02.870: INFO: rc: 1
Dec  5 20:29:02.870: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002ee3500 exit status 1 <nil> <nil> true [0xc002ea6118 0xc002ea6130 0xc002ea6148] [0xc002ea6118 0xc002ea6130 0xc002ea6148] [0xc002ea6128 0xc002ea6140] [0xb916c0 0xb916c0] 0xc001918600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:29:12.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:29:12.956: INFO: rc: 1
Dec  5 20:29:12.956: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002b75230 exit status 1 <nil> <nil> true [0xc0007bb270 0xc0007bb348 0xc0007bb458] [0xc0007bb270 0xc0007bb348 0xc0007bb458] [0xc0007bb330 0xc0007bb430] [0xb916c0 0xb916c0] 0xc002940ae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:29:22.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:29:23.076: INFO: rc: 1
Dec  5 20:29:23.076: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002b74330 exit status 1 <nil> <nil> true [0xc0000100d0 0xc0007ba188 0xc0007ba550] [0xc0000100d0 0xc0007ba188 0xc0007ba550] [0xc0007ba098 0xc0007ba4e8] [0xb916c0 0xb916c0] 0xc0028845a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:29:33.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:29:33.191: INFO: rc: 1
Dec  5 20:29:33.191: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002b74810 exit status 1 <nil> <nil> true [0xc0007ba570 0xc0007ba758 0xc0007baa38] [0xc0007ba570 0xc0007ba758 0xc0007baa38] [0xc0007ba638 0xc0007ba9b8] [0xb916c0 0xb916c0] 0xc002884d80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:29:43.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:29:43.298: INFO: rc: 1
Dec  5 20:29:43.298: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002b74b70 exit status 1 <nil> <nil> true [0xc0007baa88 0xc0007bac10 0xc0007bae20] [0xc0007baa88 0xc0007bac10 0xc0007bae20] [0xc0007babd8 0xc0007bacb8] [0xb916c0 0xb916c0] 0xc002885440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:29:53.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:29:53.416: INFO: rc: 1
Dec  5 20:29:53.416: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002564360 exit status 1 <nil> <nil> true [0xc0000da5b8 0xc0000db578 0xc0000db720] [0xc0000da5b8 0xc0000db578 0xc0000db720] [0xc0000db498 0xc0000db680] [0xb916c0 0xb916c0] 0xc0022ce780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:30:03.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:30:03.526: INFO: rc: 1
Dec  5 20:30:03.526: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002b74ea0 exit status 1 <nil> <nil> true [0xc0007bae68 0xc0007baf88 0xc0007bb120] [0xc0007bae68 0xc0007baf88 0xc0007bb120] [0xc0007baf60 0xc0007bb088] [0xb916c0 0xb916c0] 0xc002885b00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:30:13.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:30:13.645: INFO: rc: 1
Dec  5 20:30:13.645: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00232c300 exit status 1 <nil> <nil> true [0xc002ea6000 0xc002ea6018 0xc002ea6030] [0xc002ea6000 0xc002ea6018 0xc002ea6030] [0xc002ea6010 0xc002ea6028] [0xb916c0 0xb916c0] 0xc0029402a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:30:23.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:30:23.742: INFO: rc: 1
Dec  5 20:30:23.742: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00232c660 exit status 1 <nil> <nil> true [0xc002ea6038 0xc002ea6050 0xc002ea6068] [0xc002ea6038 0xc002ea6050 0xc002ea6068] [0xc002ea6048 0xc002ea6060] [0xb916c0 0xb916c0] 0xc002940960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:30:33.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:30:33.854: INFO: rc: 1
Dec  5 20:30:33.854: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00232c9c0 exit status 1 <nil> <nil> true [0xc002ea6070 0xc002ea6088 0xc002ea60a0] [0xc002ea6070 0xc002ea6088 0xc002ea60a0] [0xc002ea6080 0xc002ea6098] [0xb916c0 0xb916c0] 0xc002941a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Dec  5 20:30:43.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 exec --namespace=statefulset-7180 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 20:30:43.961: INFO: rc: 1
Dec  5 20:30:43.961: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Dec  5 20:30:43.961: INFO: Scaling statefulset ss to 0
Dec  5 20:30:43.978: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  5 20:30:43.983: INFO: Deleting all statefulset in ns statefulset-7180
Dec  5 20:30:43.987: INFO: Scaling statefulset ss to 0
Dec  5 20:30:43.999: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 20:30:44.004: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:30:44.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7180" for this suite.
Dec  5 20:30:50.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:30:50.159: INFO: namespace statefulset-7180 deletion completed in 6.133821191s

• [SLOW TEST:371.762 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:30:50.161: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-2045e088-179e-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume configMaps
Dec  5 20:30:50.209: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2046e4cc-179e-11ea-a680-be755c6bedde" in namespace "projected-6640" to be "success or failure"
Dec  5 20:30:50.213: INFO: Pod "pod-projected-configmaps-2046e4cc-179e-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.483534ms
Dec  5 20:30:52.219: INFO: Pod "pod-projected-configmaps-2046e4cc-179e-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010144205s
STEP: Saw pod success
Dec  5 20:30:52.219: INFO: Pod "pod-projected-configmaps-2046e4cc-179e-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:30:52.223: INFO: Trying to get logs from node puma pod pod-projected-configmaps-2046e4cc-179e-11ea-a680-be755c6bedde container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 20:30:52.250: INFO: Waiting for pod pod-projected-configmaps-2046e4cc-179e-11ea-a680-be755c6bedde to disappear
Dec  5 20:30:52.254: INFO: Pod pod-projected-configmaps-2046e4cc-179e-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:30:52.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6640" for this suite.
Dec  5 20:30:58.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:30:58.409: INFO: namespace projected-6640 deletion completed in 6.150081015s

• [SLOW TEST:8.248 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:30:58.409: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Dec  5 20:30:58.458: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-6114" to be "success or failure"
Dec  5 20:30:58.462: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076603ms
Dec  5 20:31:00.469: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010749165s
STEP: Saw pod success
Dec  5 20:31:00.469: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec  5 20:31:00.474: INFO: Trying to get logs from node puma pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec  5 20:31:00.500: INFO: Waiting for pod pod-host-path-test to disappear
Dec  5 20:31:00.503: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:31:00.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-6114" for this suite.
Dec  5 20:31:06.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:31:06.632: INFO: namespace hostpath-6114 deletion completed in 6.123979233s

• [SLOW TEST:8.223 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:31:06.632: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-mpsz
STEP: Creating a pod to test atomic-volume-subpath
Dec  5 20:31:06.685: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mpsz" in namespace "subpath-2136" to be "success or failure"
Dec  5 20:31:06.690: INFO: Pod "pod-subpath-test-configmap-mpsz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.812884ms
Dec  5 20:31:08.696: INFO: Pod "pod-subpath-test-configmap-mpsz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010265328s
Dec  5 20:31:10.702: INFO: Pod "pod-subpath-test-configmap-mpsz": Phase="Running", Reason="", readiness=true. Elapsed: 4.016472135s
Dec  5 20:31:12.707: INFO: Pod "pod-subpath-test-configmap-mpsz": Phase="Running", Reason="", readiness=true. Elapsed: 6.021789237s
Dec  5 20:31:14.713: INFO: Pod "pod-subpath-test-configmap-mpsz": Phase="Running", Reason="", readiness=true. Elapsed: 8.028049386s
Dec  5 20:31:16.720: INFO: Pod "pod-subpath-test-configmap-mpsz": Phase="Running", Reason="", readiness=true. Elapsed: 10.034965118s
Dec  5 20:31:18.726: INFO: Pod "pod-subpath-test-configmap-mpsz": Phase="Running", Reason="", readiness=true. Elapsed: 12.040682601s
Dec  5 20:31:20.732: INFO: Pod "pod-subpath-test-configmap-mpsz": Phase="Running", Reason="", readiness=true. Elapsed: 14.046950995s
Dec  5 20:31:22.738: INFO: Pod "pod-subpath-test-configmap-mpsz": Phase="Running", Reason="", readiness=true. Elapsed: 16.05311268s
Dec  5 20:31:24.758: INFO: Pod "pod-subpath-test-configmap-mpsz": Phase="Running", Reason="", readiness=true. Elapsed: 18.072752744s
Dec  5 20:31:26.765: INFO: Pod "pod-subpath-test-configmap-mpsz": Phase="Running", Reason="", readiness=true. Elapsed: 20.079178864s
Dec  5 20:31:28.771: INFO: Pod "pod-subpath-test-configmap-mpsz": Phase="Running", Reason="", readiness=true. Elapsed: 22.085180133s
Dec  5 20:31:30.776: INFO: Pod "pod-subpath-test-configmap-mpsz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.090704268s
STEP: Saw pod success
Dec  5 20:31:30.776: INFO: Pod "pod-subpath-test-configmap-mpsz" satisfied condition "success or failure"
Dec  5 20:31:30.781: INFO: Trying to get logs from node crayfish pod pod-subpath-test-configmap-mpsz container test-container-subpath-configmap-mpsz: <nil>
STEP: delete the pod
Dec  5 20:31:30.810: INFO: Waiting for pod pod-subpath-test-configmap-mpsz to disappear
Dec  5 20:31:30.814: INFO: Pod pod-subpath-test-configmap-mpsz no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mpsz
Dec  5 20:31:30.814: INFO: Deleting pod "pod-subpath-test-configmap-mpsz" in namespace "subpath-2136"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:31:30.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2136" for this suite.
Dec  5 20:31:36.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:31:36.970: INFO: namespace subpath-2136 deletion completed in 6.147609461s

• [SLOW TEST:30.338 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:31:36.975: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec  5 20:31:41.549: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9863 pod-service-account-3c7c7acb-179e-11ea-a680-be755c6bedde -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec  5 20:31:41.809: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9863 pod-service-account-3c7c7acb-179e-11ea-a680-be755c6bedde -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec  5 20:31:42.046: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9863 pod-service-account-3c7c7acb-179e-11ea-a680-be755c6bedde -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:31:42.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9863" for this suite.
Dec  5 20:31:48.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:31:48.449: INFO: namespace svcaccounts-9863 deletion completed in 6.132834938s

• [SLOW TEST:11.474 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:31:48.449: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1256
STEP: creating an rc
Dec  5 20:31:48.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 create -f - --namespace=kubectl-6860'
Dec  5 20:31:48.836: INFO: stderr: ""
Dec  5 20:31:48.837: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Dec  5 20:31:49.843: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 20:31:49.843: INFO: Found 0 / 1
Dec  5 20:31:50.842: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 20:31:50.842: INFO: Found 0 / 1
Dec  5 20:31:51.842: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 20:31:51.842: INFO: Found 1 / 1
Dec  5 20:31:51.842: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  5 20:31:51.847: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 20:31:51.847: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Dec  5 20:31:51.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 logs redis-master-jkl5d redis-master --namespace=kubectl-6860'
Dec  5 20:31:51.985: INFO: stderr: ""
Dec  5 20:31:51.985: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Dec 20:31:50.150 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Dec 20:31:50.150 # Server started, Redis version 3.2.12\n1:M 05 Dec 20:31:50.150 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Dec 20:31:50.151 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Dec  5 20:31:51.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 log redis-master-jkl5d redis-master --namespace=kubectl-6860 --tail=1'
Dec  5 20:31:52.089: INFO: stderr: ""
Dec  5 20:31:52.089: INFO: stdout: "1:M 05 Dec 20:31:50.151 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Dec  5 20:31:52.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 log redis-master-jkl5d redis-master --namespace=kubectl-6860 --limit-bytes=1'
Dec  5 20:31:52.198: INFO: stderr: ""
Dec  5 20:31:52.198: INFO: stdout: " "
STEP: exposing timestamps
Dec  5 20:31:52.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 log redis-master-jkl5d redis-master --namespace=kubectl-6860 --tail=1 --timestamps'
Dec  5 20:31:52.313: INFO: stderr: ""
Dec  5 20:31:52.313: INFO: stdout: "2019-12-05T20:31:50.151149551Z 1:M 05 Dec 20:31:50.151 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Dec  5 20:31:54.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 log redis-master-jkl5d redis-master --namespace=kubectl-6860 --since=1s'
Dec  5 20:31:54.956: INFO: stderr: ""
Dec  5 20:31:54.956: INFO: stdout: ""
Dec  5 20:31:54.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 log redis-master-jkl5d redis-master --namespace=kubectl-6860 --since=24h'
Dec  5 20:31:55.076: INFO: stderr: ""
Dec  5 20:31:55.077: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Dec 20:31:50.150 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Dec 20:31:50.150 # Server started, Redis version 3.2.12\n1:M 05 Dec 20:31:50.150 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Dec 20:31:50.151 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
STEP: using delete to clean up resources
Dec  5 20:31:55.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 delete --grace-period=0 --force -f - --namespace=kubectl-6860'
Dec  5 20:31:55.177: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 20:31:55.177: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Dec  5 20:31:55.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get rc,svc -l name=nginx --no-headers --namespace=kubectl-6860'
Dec  5 20:31:55.259: INFO: stderr: "No resources found.\n"
Dec  5 20:31:55.259: INFO: stdout: ""
Dec  5 20:31:55.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 get pods -l name=nginx --namespace=kubectl-6860 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  5 20:31:55.332: INFO: stderr: ""
Dec  5 20:31:55.332: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:31:55.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6860" for this suite.
Dec  5 20:32:01.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:32:01.486: INFO: namespace kubectl-6860 deletion completed in 6.1484898s

• [SLOW TEST:13.037 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:32:01.488: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec  5 20:32:11.655: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 705
	[quantile=0.9] = 198685
	[quantile=0.99] = 447784
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 51539
	[quantile=0.9] = 547278
	[quantile=0.99] = 597523
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 6
	[quantile=0.9] = 13
	[quantile=0.99] = 47
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 21
	[quantile=0.9] = 38
	[quantile=0.99] = 79
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 15
	[quantile=0.9] = 42
	[quantile=0.99] = 51
For namespace_queue_latency_sum:
	[] = 9132
For namespace_queue_latency_count:
	[] = 497
For namespace_retries:
	[] = 507
For namespace_work_duration:
	[quantile=0.5] = 360
	[quantile=0.9] = 237833
	[quantile=0.99] = 258278
For namespace_work_duration_sum:
	[] = 69550757
For namespace_work_duration_count:
	[] = 497
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:32:11.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2999" for this suite.
Dec  5 20:32:17.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:32:17.787: INFO: namespace gc-2999 deletion completed in 6.125509273s

• [SLOW TEST:16.299 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:32:17.790: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8140.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8140.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8140.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8140.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 214.29.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.29.214_udp@PTR;check="$$(dig +tcp +noall +answer +search 214.29.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.29.214_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8140.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8140.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8140.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8140.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 214.29.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.29.214_udp@PTR;check="$$(dig +tcp +noall +answer +search 214.29.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.29.214_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  5 20:32:21.874: INFO: Unable to read wheezy_udp@dns-test-service.dns-8140.svc.cluster.local from pod dns-8140/dns-test-5483d9d2-179e-11ea-a680-be755c6bedde: the server could not find the requested resource (get pods dns-test-5483d9d2-179e-11ea-a680-be755c6bedde)
Dec  5 20:32:21.880: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8140.svc.cluster.local from pod dns-8140/dns-test-5483d9d2-179e-11ea-a680-be755c6bedde: the server could not find the requested resource (get pods dns-test-5483d9d2-179e-11ea-a680-be755c6bedde)
Dec  5 20:32:21.886: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8140.svc.cluster.local from pod dns-8140/dns-test-5483d9d2-179e-11ea-a680-be755c6bedde: the server could not find the requested resource (get pods dns-test-5483d9d2-179e-11ea-a680-be755c6bedde)
Dec  5 20:32:21.893: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8140.svc.cluster.local from pod dns-8140/dns-test-5483d9d2-179e-11ea-a680-be755c6bedde: the server could not find the requested resource (get pods dns-test-5483d9d2-179e-11ea-a680-be755c6bedde)
Dec  5 20:32:21.928: INFO: Unable to read jessie_udp@dns-test-service.dns-8140.svc.cluster.local from pod dns-8140/dns-test-5483d9d2-179e-11ea-a680-be755c6bedde: the server could not find the requested resource (get pods dns-test-5483d9d2-179e-11ea-a680-be755c6bedde)
Dec  5 20:32:21.933: INFO: Unable to read jessie_tcp@dns-test-service.dns-8140.svc.cluster.local from pod dns-8140/dns-test-5483d9d2-179e-11ea-a680-be755c6bedde: the server could not find the requested resource (get pods dns-test-5483d9d2-179e-11ea-a680-be755c6bedde)
Dec  5 20:32:21.938: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8140.svc.cluster.local from pod dns-8140/dns-test-5483d9d2-179e-11ea-a680-be755c6bedde: the server could not find the requested resource (get pods dns-test-5483d9d2-179e-11ea-a680-be755c6bedde)
Dec  5 20:32:21.950: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8140.svc.cluster.local from pod dns-8140/dns-test-5483d9d2-179e-11ea-a680-be755c6bedde: the server could not find the requested resource (get pods dns-test-5483d9d2-179e-11ea-a680-be755c6bedde)
Dec  5 20:32:21.976: INFO: Lookups using dns-8140/dns-test-5483d9d2-179e-11ea-a680-be755c6bedde failed for: [wheezy_udp@dns-test-service.dns-8140.svc.cluster.local wheezy_tcp@dns-test-service.dns-8140.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8140.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8140.svc.cluster.local jessie_udp@dns-test-service.dns-8140.svc.cluster.local jessie_tcp@dns-test-service.dns-8140.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8140.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8140.svc.cluster.local]

Dec  5 20:32:27.071: INFO: DNS probes using dns-8140/dns-test-5483d9d2-179e-11ea-a680-be755c6bedde succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:32:27.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8140" for this suite.
Dec  5 20:32:33.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:32:33.271: INFO: namespace dns-8140 deletion completed in 6.139368992s

• [SLOW TEST:15.481 seconds]
[sig-network] DNS
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:32:33.272: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-5dbb87e9-179e-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume secrets
Dec  5 20:32:33.319: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5dbc8c4c-179e-11ea-a680-be755c6bedde" in namespace "projected-364" to be "success or failure"
Dec  5 20:32:33.324: INFO: Pod "pod-projected-secrets-5dbc8c4c-179e-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.782185ms
Dec  5 20:32:35.330: INFO: Pod "pod-projected-secrets-5dbc8c4c-179e-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010125713s
Dec  5 20:32:37.336: INFO: Pod "pod-projected-secrets-5dbc8c4c-179e-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016613627s
STEP: Saw pod success
Dec  5 20:32:37.336: INFO: Pod "pod-projected-secrets-5dbc8c4c-179e-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:32:37.341: INFO: Trying to get logs from node crayfish pod pod-projected-secrets-5dbc8c4c-179e-11ea-a680-be755c6bedde container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  5 20:32:37.368: INFO: Waiting for pod pod-projected-secrets-5dbc8c4c-179e-11ea-a680-be755c6bedde to disappear
Dec  5 20:32:37.371: INFO: Pod pod-projected-secrets-5dbc8c4c-179e-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:32:37.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-364" for this suite.
Dec  5 20:32:43.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:32:43.515: INFO: namespace projected-364 deletion completed in 6.137814257s

• [SLOW TEST:10.243 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:32:43.516: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Dec  5 20:32:43.563: INFO: Waiting up to 5m0s for pod "downward-api-63d73f5d-179e-11ea-a680-be755c6bedde" in namespace "downward-api-9168" to be "success or failure"
Dec  5 20:32:43.567: INFO: Pod "downward-api-63d73f5d-179e-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.581262ms
Dec  5 20:32:45.573: INFO: Pod "downward-api-63d73f5d-179e-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009390891s
Dec  5 20:32:47.578: INFO: Pod "downward-api-63d73f5d-179e-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015019982s
STEP: Saw pod success
Dec  5 20:32:47.578: INFO: Pod "downward-api-63d73f5d-179e-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:32:47.583: INFO: Trying to get logs from node puma pod downward-api-63d73f5d-179e-11ea-a680-be755c6bedde container dapi-container: <nil>
STEP: delete the pod
Dec  5 20:32:47.613: INFO: Waiting for pod downward-api-63d73f5d-179e-11ea-a680-be755c6bedde to disappear
Dec  5 20:32:47.616: INFO: Pod downward-api-63d73f5d-179e-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:32:47.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9168" for this suite.
Dec  5 20:32:53.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:32:53.740: INFO: namespace downward-api-9168 deletion completed in 6.118214573s

• [SLOW TEST:10.224 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:32:53.740: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:33:19.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1353" for this suite.
Dec  5 20:33:25.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:33:26.035: INFO: namespace namespaces-1353 deletion completed in 6.145921904s
STEP: Destroying namespace "nsdeletetest-8926" for this suite.
Dec  5 20:33:26.038: INFO: Namespace nsdeletetest-8926 was already deleted
STEP: Destroying namespace "nsdeletetest-6010" for this suite.
Dec  5 20:33:32.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:33:32.171: INFO: namespace nsdeletetest-6010 deletion completed in 6.132550138s

• [SLOW TEST:38.431 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:33:32.171: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 20:33:32.214: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec  5 20:33:37.220: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  5 20:33:37.220: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec  5 20:33:39.225: INFO: Creating deployment "test-rollover-deployment"
Dec  5 20:33:39.235: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec  5 20:33:41.243: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec  5 20:33:41.252: INFO: Ensure that both replica sets have 1 created replica
Dec  5 20:33:41.261: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec  5 20:33:41.271: INFO: Updating deployment test-rollover-deployment
Dec  5 20:33:41.271: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec  5 20:33:43.280: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec  5 20:33:43.289: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec  5 20:33:43.297: INFO: all replica sets need to contain the pod-template-hash label
Dec  5 20:33:43.298: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174819, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174819, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174823, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174819, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 20:33:45.309: INFO: all replica sets need to contain the pod-template-hash label
Dec  5 20:33:45.309: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174819, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174819, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174823, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174819, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 20:33:47.309: INFO: all replica sets need to contain the pod-template-hash label
Dec  5 20:33:47.309: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174819, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174819, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174823, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174819, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 20:33:49.309: INFO: all replica sets need to contain the pod-template-hash label
Dec  5 20:33:49.309: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174819, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174819, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174823, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174819, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 20:33:51.309: INFO: all replica sets need to contain the pod-template-hash label
Dec  5 20:33:51.309: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174819, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174819, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174823, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174819, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 20:33:53.310: INFO: 
Dec  5 20:33:53.310: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174819, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174819, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174833, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711174819, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 20:33:55.309: INFO: 
Dec  5 20:33:55.309: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  5 20:33:55.323: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-3977,SelfLink:/apis/apps/v1/namespaces/deployment-3977/deployments/test-rollover-deployment,UID:8505d907-179e-11ea-b857-a68228aedf28,ResourceVersion:52359,Generation:2,CreationTimestamp:2019-12-05 20:33:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-05 20:33:39 +0000 UTC 2019-12-05 20:33:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-05 20:33:53 +0000 UTC 2019-12-05 20:33:39 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-659c699649" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  5 20:33:55.328: INFO: New ReplicaSet "test-rollover-deployment-659c699649" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-659c699649,GenerateName:,Namespace:deployment-3977,SelfLink:/apis/apps/v1/namespaces/deployment-3977/replicasets/test-rollover-deployment-659c699649,UID:863defb0-179e-11ea-9811-dae9897b3b7b,ResourceVersion:52348,Generation:2,CreationTimestamp:2019-12-05 20:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 8505d907-179e-11ea-b857-a68228aedf28 0xc002be3fc7 0xc002be3fc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  5 20:33:55.328: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec  5 20:33:55.328: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-3977,SelfLink:/apis/apps/v1/namespaces/deployment-3977/replicasets/test-rollover-controller,UID:80d658cb-179e-11ea-b857-a68228aedf28,ResourceVersion:52358,Generation:2,CreationTimestamp:2019-12-05 20:33:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 8505d907-179e-11ea-b857-a68228aedf28 0xc002be3ef7 0xc002be3ef8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 20:33:55.329: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-7b45b6464,GenerateName:,Namespace:deployment-3977,SelfLink:/apis/apps/v1/namespaces/deployment-3977/replicasets/test-rollover-deployment-7b45b6464,UID:85089af4-179e-11ea-9811-dae9897b3b7b,ResourceVersion:52315,Generation:2,CreationTimestamp:2019-12-05 20:33:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 8505d907-179e-11ea-b857-a68228aedf28 0xc002cf6090 0xc002cf6091}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 20:33:55.334: INFO: Pod "test-rollover-deployment-659c699649-t7hgq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-659c699649-t7hgq,GenerateName:test-rollover-deployment-659c699649-,Namespace:deployment-3977,SelfLink:/api/v1/namespaces/deployment-3977/pods/test-rollover-deployment-659c699649-t7hgq,UID:8641a52c-179e-11ea-9811-dae9897b3b7b,ResourceVersion:52327,Generation:0,CreationTimestamp:2019-12-05 20:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-659c699649 863defb0-179e-11ea-9811-dae9897b3b7b 0xc002cf6c67 0xc002cf6c68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n4g94 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n4g94,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-n4g94 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:puma,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002cf6ce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002cf6d00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:33:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:33:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:33:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:33:41 +0000 UTC  }],Message:,Reason:,HostIP:178.62.90.156,PodIP:25.0.1.75,StartTime:2019-12-05 20:33:41 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-05 20:33:42 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://c790bfe286c72f97c3b7f88ded2b6c15125b33d783c41bab1b215788cd994bfd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:33:55.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3977" for this suite.
Dec  5 20:34:01.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:34:01.476: INFO: namespace deployment-3977 deletion completed in 6.136684727s

• [SLOW TEST:29.305 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:34:01.477: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  5 20:34:01.522: INFO: Waiting up to 5m0s for pod "downwardapi-volume-924e9cf2-179e-11ea-a680-be755c6bedde" in namespace "projected-3862" to be "success or failure"
Dec  5 20:34:01.527: INFO: Pod "downwardapi-volume-924e9cf2-179e-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.694656ms
Dec  5 20:34:03.534: INFO: Pod "downwardapi-volume-924e9cf2-179e-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011688512s
STEP: Saw pod success
Dec  5 20:34:03.534: INFO: Pod "downwardapi-volume-924e9cf2-179e-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:34:03.538: INFO: Trying to get logs from node crayfish pod downwardapi-volume-924e9cf2-179e-11ea-a680-be755c6bedde container client-container: <nil>
STEP: delete the pod
Dec  5 20:34:03.565: INFO: Waiting for pod downwardapi-volume-924e9cf2-179e-11ea-a680-be755c6bedde to disappear
Dec  5 20:34:03.569: INFO: Pod downwardapi-volume-924e9cf2-179e-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:34:03.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3862" for this suite.
Dec  5 20:34:09.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:34:09.714: INFO: namespace projected-3862 deletion completed in 6.138626718s

• [SLOW TEST:8.237 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:34:09.714: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  5 20:34:09.759: INFO: Waiting up to 5m0s for pod "pod-9737b634-179e-11ea-a680-be755c6bedde" in namespace "emptydir-5257" to be "success or failure"
Dec  5 20:34:09.762: INFO: Pod "pod-9737b634-179e-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.054723ms
Dec  5 20:34:11.768: INFO: Pod "pod-9737b634-179e-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008541825s
STEP: Saw pod success
Dec  5 20:34:11.768: INFO: Pod "pod-9737b634-179e-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:34:11.772: INFO: Trying to get logs from node puma pod pod-9737b634-179e-11ea-a680-be755c6bedde container test-container: <nil>
STEP: delete the pod
Dec  5 20:34:11.800: INFO: Waiting for pod pod-9737b634-179e-11ea-a680-be755c6bedde to disappear
Dec  5 20:34:11.804: INFO: Pod pod-9737b634-179e-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:34:11.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5257" for this suite.
Dec  5 20:34:17.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:34:17.952: INFO: namespace emptydir-5257 deletion completed in 6.142744728s

• [SLOW TEST:8.238 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:34:17.954: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Dec  5 20:34:17.992: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:34:21.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9673" for this suite.
Dec  5 20:34:27.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:34:27.207: INFO: namespace init-container-9673 deletion completed in 6.136944501s

• [SLOW TEST:9.253 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:34:27.207: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  5 20:34:31.297: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  5 20:34:31.302: INFO: Pod pod-with-prestop-http-hook still exists
Dec  5 20:34:33.302: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  5 20:34:33.308: INFO: Pod pod-with-prestop-http-hook still exists
Dec  5 20:34:35.302: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  5 20:34:35.308: INFO: Pod pod-with-prestop-http-hook still exists
Dec  5 20:34:37.302: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  5 20:34:37.308: INFO: Pod pod-with-prestop-http-hook still exists
Dec  5 20:34:39.302: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  5 20:34:39.308: INFO: Pod pod-with-prestop-http-hook still exists
Dec  5 20:34:41.302: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  5 20:34:41.308: INFO: Pod pod-with-prestop-http-hook still exists
Dec  5 20:34:43.302: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  5 20:34:43.309: INFO: Pod pod-with-prestop-http-hook still exists
Dec  5 20:34:45.302: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  5 20:34:45.307: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:34:45.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3485" for this suite.
Dec  5 20:35:07.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:35:07.467: INFO: namespace container-lifecycle-hook-3485 deletion completed in 22.141173731s

• [SLOW TEST:40.260 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:35:07.467: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-b9a41f4a-179e-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume secrets
Dec  5 20:35:07.517: INFO: Waiting up to 5m0s for pod "pod-secrets-b9a528c3-179e-11ea-a680-be755c6bedde" in namespace "secrets-3625" to be "success or failure"
Dec  5 20:35:07.523: INFO: Pod "pod-secrets-b9a528c3-179e-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 5.798319ms
Dec  5 20:35:09.529: INFO: Pod "pod-secrets-b9a528c3-179e-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011385039s
Dec  5 20:35:11.535: INFO: Pod "pod-secrets-b9a528c3-179e-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01792529s
STEP: Saw pod success
Dec  5 20:35:11.535: INFO: Pod "pod-secrets-b9a528c3-179e-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:35:11.540: INFO: Trying to get logs from node crayfish pod pod-secrets-b9a528c3-179e-11ea-a680-be755c6bedde container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 20:35:11.567: INFO: Waiting for pod pod-secrets-b9a528c3-179e-11ea-a680-be755c6bedde to disappear
Dec  5 20:35:11.571: INFO: Pod pod-secrets-b9a528c3-179e-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:35:11.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3625" for this suite.
Dec  5 20:35:17.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:35:17.702: INFO: namespace secrets-3625 deletion completed in 6.125630283s

• [SLOW TEST:10.234 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:35:17.702: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Dec  5 20:35:18.188: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec  5 20:35:25.386: INFO: Waited 5.134402365s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:35:25.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2519" for this suite.
Dec  5 20:35:32.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:35:32.270: INFO: namespace aggregator-2519 deletion completed in 6.227082127s

• [SLOW TEST:14.568 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:35:32.270: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-c86eac4c-179e-11ea-a680-be755c6bedde
STEP: Creating configMap with name cm-test-opt-upd-c86eac91-179e-11ea-a680-be755c6bedde
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-c86eac4c-179e-11ea-a680-be755c6bedde
STEP: Updating configmap cm-test-opt-upd-c86eac91-179e-11ea-a680-be755c6bedde
STEP: Creating configMap with name cm-test-opt-create-c86eacad-179e-11ea-a680-be755c6bedde
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:36:44.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-564" for this suite.
Dec  5 20:37:06.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:37:07.082: INFO: namespace configmap-564 deletion completed in 22.134656799s

• [SLOW TEST:94.812 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:37:07.082: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec  5 20:37:11.155: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-00f0a95f-179f-11ea-a680-be755c6bedde,GenerateName:,Namespace:events-3060,SelfLink:/api/v1/namespaces/events-3060/pods/send-events-00f0a95f-179f-11ea-a680-be755c6bedde,UID:00f0fd87-179f-11ea-b857-a68228aedf28,ResourceVersion:53047,Generation:0,CreationTimestamp:2019-12-05 20:37:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 123239694,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zzg97 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zzg97,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-zzg97 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:crayfish,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001052260} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001052280}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:37:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:37:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:37:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 20:37:07 +0000 UTC  }],Message:,Reason:,HostIP:178.62.90.178,PodIP:25.0.2.80,StartTime:2019-12-05 20:37:07 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-12-05 20:37:08 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://8894299b4f9647a34c6efa8094f65fffa902692b1d1b1fa7a85ebc0a1c5c8414}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Dec  5 20:37:13.162: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec  5 20:37:15.168: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:37:15.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3060" for this suite.
Dec  5 20:37:55.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:37:55.332: INFO: namespace events-3060 deletion completed in 40.147771285s

• [SLOW TEST:48.250 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:37:55.334: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-1db32f3d-179f-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume secrets
Dec  5 20:37:55.389: INFO: Waiting up to 5m0s for pod "pod-secrets-1db4542d-179f-11ea-a680-be755c6bedde" in namespace "secrets-9907" to be "success or failure"
Dec  5 20:37:55.394: INFO: Pod "pod-secrets-1db4542d-179f-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.825578ms
Dec  5 20:37:57.400: INFO: Pod "pod-secrets-1db4542d-179f-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010421136s
STEP: Saw pod success
Dec  5 20:37:57.400: INFO: Pod "pod-secrets-1db4542d-179f-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:37:57.404: INFO: Trying to get logs from node puma pod pod-secrets-1db4542d-179f-11ea-a680-be755c6bedde container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 20:37:57.431: INFO: Waiting for pod pod-secrets-1db4542d-179f-11ea-a680-be755c6bedde to disappear
Dec  5 20:37:57.435: INFO: Pod pod-secrets-1db4542d-179f-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:37:57.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9907" for this suite.
Dec  5 20:38:03.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:38:03.576: INFO: namespace secrets-9907 deletion completed in 6.134892282s

• [SLOW TEST:8.242 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:38:03.577: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 20:38:03.608: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:38:05.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1521" for this suite.
Dec  5 20:38:43.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:38:43.805: INFO: namespace pods-1521 deletion completed in 38.147238681s

• [SLOW TEST:40.229 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:38:43.809: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  5 20:38:48.392: INFO: Successfully updated pod "pod-update-activedeadlineseconds-3a972625-179f-11ea-a680-be755c6bedde"
Dec  5 20:38:48.392: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3a972625-179f-11ea-a680-be755c6bedde" in namespace "pods-3220" to be "terminated due to deadline exceeded"
Dec  5 20:38:48.397: INFO: Pod "pod-update-activedeadlineseconds-3a972625-179f-11ea-a680-be755c6bedde": Phase="Running", Reason="", readiness=true. Elapsed: 4.996241ms
Dec  5 20:38:50.402: INFO: Pod "pod-update-activedeadlineseconds-3a972625-179f-11ea-a680-be755c6bedde": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.010710756s
Dec  5 20:38:50.403: INFO: Pod "pod-update-activedeadlineseconds-3a972625-179f-11ea-a680-be755c6bedde" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:38:50.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3220" for this suite.
Dec  5 20:38:56.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:38:56.532: INFO: namespace pods-3220 deletion completed in 6.124069635s

• [SLOW TEST:12.724 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:38:56.533: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  5 20:38:56.607: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:38:56.607: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:38:56.607: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:38:56.610: INFO: Number of nodes with available pods: 0
Dec  5 20:38:56.610: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:38:57.617: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:38:57.617: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:38:57.617: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:38:57.621: INFO: Number of nodes with available pods: 0
Dec  5 20:38:57.621: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:38:58.617: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:38:58.617: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:38:58.617: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:38:58.621: INFO: Number of nodes with available pods: 1
Dec  5 20:38:58.621: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:38:59.617: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:38:59.617: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:38:59.617: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:38:59.621: INFO: Number of nodes with available pods: 2
Dec  5 20:38:59.621: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec  5 20:38:59.642: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:38:59.642: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:38:59.643: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:38:59.650: INFO: Number of nodes with available pods: 1
Dec  5 20:38:59.650: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:39:00.656: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:39:00.656: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:39:00.656: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:39:00.666: INFO: Number of nodes with available pods: 1
Dec  5 20:39:00.666: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:39:01.657: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:39:01.657: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:39:01.657: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:39:01.662: INFO: Number of nodes with available pods: 1
Dec  5 20:39:01.662: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:39:02.657: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:39:02.657: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:39:02.657: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:39:02.662: INFO: Number of nodes with available pods: 2
Dec  5 20:39:02.662: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-251, will wait for the garbage collector to delete the pods
Dec  5 20:39:02.738: INFO: Deleting DaemonSet.extensions daemon-set took: 11.693212ms
Dec  5 20:39:03.039: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.472251ms
Dec  5 20:39:05.845: INFO: Number of nodes with available pods: 0
Dec  5 20:39:05.845: INFO: Number of running nodes: 0, number of available pods: 0
Dec  5 20:39:05.849: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-251/daemonsets","resourceVersion":"53424"},"items":null}

Dec  5 20:39:05.853: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-251/pods","resourceVersion":"53424"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:39:05.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-251" for this suite.
Dec  5 20:39:11.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:39:12.018: INFO: namespace daemonsets-251 deletion completed in 6.144462815s

• [SLOW TEST:15.486 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:39:12.019: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:39:16.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1523" for this suite.
Dec  5 20:39:22.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:39:22.269: INFO: namespace emptydir-wrapper-1523 deletion completed in 6.134606578s

• [SLOW TEST:10.251 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:39:22.271: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Dec  5 20:39:23.397: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 605
	[quantile=0.9] = 198685
	[quantile=0.99] = 447784
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 204610
	[quantile=0.9] = 547278
	[quantile=0.99] = 597523
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 5
	[quantile=0.9] = 12
	[quantile=0.99] = 42
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 18
	[quantile=0.9] = 31
	[quantile=0.99] = 69
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 14
	[quantile=0.9] = 42
	[quantile=0.99] = 69
For namespace_queue_latency_sum:
	[] = 10080
For namespace_queue_latency_count:
	[] = 551
For namespace_retries:
	[] = 564
For namespace_work_duration:
	[quantile=0.5] = 162704
	[quantile=0.9] = 240165
	[quantile=0.99] = 318182
For namespace_work_duration_sum:
	[] = 76584001
For namespace_work_duration_count:
	[] = 551
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:39:23.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7029" for this suite.
Dec  5 20:39:29.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:39:29.544: INFO: namespace gc-7029 deletion completed in 6.142603788s

• [SLOW TEST:7.274 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:39:29.546: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Dec  5 20:39:29.582: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  5 20:39:29.590: INFO: Waiting for terminating namespaces to be deleted...
Dec  5 20:39:29.594: INFO: 
Logging pods the kubelet thinks is on node crayfish before test
Dec  5 20:39:29.602: INFO: kube-proxy-xb74r from kube-system started at 2019-12-05 15:20:56 +0000 UTC (1 container statuses recorded)
Dec  5 20:39:29.602: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  5 20:39:29.602: INFO: sonobuoy-e2e-job-69046a85250743c3 from sonobuoy started at 2019-12-05 19:08:12 +0000 UTC (2 container statuses recorded)
Dec  5 20:39:29.602: INFO: 	Container e2e ready: true, restart count 0
Dec  5 20:39:29.602: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 20:39:29.602: INFO: sonobuoy-systemd-logs-daemon-set-892d09b6f39b465a-wx8xw from sonobuoy started at 2019-12-05 19:08:12 +0000 UTC (2 container statuses recorded)
Dec  5 20:39:29.602: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec  5 20:39:29.602: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  5 20:39:29.602: INFO: kube-flannel-ds-amd64-zqml7 from kube-system started at 2019-12-05 15:20:56 +0000 UTC (1 container statuses recorded)
Dec  5 20:39:29.602: INFO: 	Container kube-flannel ready: true, restart count 0
Dec  5 20:39:29.602: INFO: 
Logging pods the kubelet thinks is on node puma before test
Dec  5 20:39:29.609: INFO: kube-flannel-ds-amd64-5ql7s from kube-system started at 2019-12-05 15:20:44 +0000 UTC (1 container statuses recorded)
Dec  5 20:39:29.609: INFO: 	Container kube-flannel ready: true, restart count 0
Dec  5 20:39:29.609: INFO: sonobuoy-systemd-logs-daemon-set-892d09b6f39b465a-74vqz from sonobuoy started at 2019-12-05 19:08:12 +0000 UTC (2 container statuses recorded)
Dec  5 20:39:29.609: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec  5 20:39:29.609: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  5 20:39:29.609: INFO: kube-proxy-wwvd5 from kube-system started at 2019-12-05 15:20:44 +0000 UTC (1 container statuses recorded)
Dec  5 20:39:29.610: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  5 20:39:29.610: INFO: sonobuoy from sonobuoy started at 2019-12-05 19:08:11 +0000 UTC (1 container statuses recorded)
Dec  5 20:39:29.610: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15dd942dedc34651], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:39:30.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7543" for this suite.
Dec  5 20:39:36.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:39:36.800: INFO: namespace sched-pred-7543 deletion completed in 6.152660987s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.254 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:39:36.805: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Dec  5 20:39:36.852: INFO: Waiting up to 5m0s for pod "var-expansion-5a2e15ee-179f-11ea-a680-be755c6bedde" in namespace "var-expansion-6789" to be "success or failure"
Dec  5 20:39:36.855: INFO: Pod "var-expansion-5a2e15ee-179f-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.352073ms
Dec  5 20:39:38.861: INFO: Pod "var-expansion-5a2e15ee-179f-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009384372s
STEP: Saw pod success
Dec  5 20:39:38.861: INFO: Pod "var-expansion-5a2e15ee-179f-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:39:38.866: INFO: Trying to get logs from node puma pod var-expansion-5a2e15ee-179f-11ea-a680-be755c6bedde container dapi-container: <nil>
STEP: delete the pod
Dec  5 20:39:38.894: INFO: Waiting for pod var-expansion-5a2e15ee-179f-11ea-a680-be755c6bedde to disappear
Dec  5 20:39:38.898: INFO: Pod var-expansion-5a2e15ee-179f-11ea-a680-be755c6bedde no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:39:38.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6789" for this suite.
Dec  5 20:39:44.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:39:45.051: INFO: namespace var-expansion-6789 deletion completed in 6.147793975s

• [SLOW TEST:8.246 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:39:45.051: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-4947
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  5 20:39:45.087: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  5 20:40:11.192: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://25.0.1.85:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4947 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 20:40:11.192: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 20:40:11.375: INFO: Found all expected endpoints: [netserver-0]
Dec  5 20:40:11.380: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://25.0.2.85:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4947 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 20:40:11.380: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
Dec  5 20:40:11.509: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:40:11.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4947" for this suite.
Dec  5 20:40:33.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:40:33.659: INFO: namespace pod-network-test-4947 deletion completed in 22.143670712s

• [SLOW TEST:48.608 seconds]
[sig-network] Networking
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:40:33.660: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:40:33.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9798" for this suite.
Dec  5 20:40:55.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:40:55.869: INFO: namespace kubelet-test-9798 deletion completed in 22.140366677s

• [SLOW TEST:22.209 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:40:55.870: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-894e8f1d-179f-11ea-a680-be755c6bedde
STEP: Creating a pod to test consume secrets
Dec  5 20:40:55.924: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-894fba62-179f-11ea-a680-be755c6bedde" in namespace "projected-3338" to be "success or failure"
Dec  5 20:40:55.929: INFO: Pod "pod-projected-secrets-894fba62-179f-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.783459ms
Dec  5 20:40:57.933: INFO: Pod "pod-projected-secrets-894fba62-179f-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009269352s
STEP: Saw pod success
Dec  5 20:40:57.933: INFO: Pod "pod-projected-secrets-894fba62-179f-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:40:57.937: INFO: Trying to get logs from node puma pod pod-projected-secrets-894fba62-179f-11ea-a680-be755c6bedde container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 20:40:57.966: INFO: Waiting for pod pod-projected-secrets-894fba62-179f-11ea-a680-be755c6bedde to disappear
Dec  5 20:40:57.969: INFO: Pod pod-projected-secrets-894fba62-179f-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:40:57.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3338" for this suite.
Dec  5 20:41:03.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:41:04.114: INFO: namespace projected-3338 deletion completed in 6.14085104s

• [SLOW TEST:8.244 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:41:04.114: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  5 20:41:04.166: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec  5 20:41:04.179: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:04.179: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:04.179: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:04.182: INFO: Number of nodes with available pods: 0
Dec  5 20:41:04.182: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:41:05.188: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:05.188: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:05.188: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:05.194: INFO: Number of nodes with available pods: 0
Dec  5 20:41:05.194: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:41:06.188: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:06.188: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:06.188: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:06.193: INFO: Number of nodes with available pods: 2
Dec  5 20:41:06.193: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec  5 20:41:06.224: INFO: Wrong image for pod: daemon-set-klnj9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:06.224: INFO: Wrong image for pod: daemon-set-nhsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:06.232: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:06.232: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:06.232: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:07.238: INFO: Wrong image for pod: daemon-set-klnj9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:07.238: INFO: Wrong image for pod: daemon-set-nhsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:07.244: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:07.244: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:07.244: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:08.238: INFO: Wrong image for pod: daemon-set-klnj9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:08.238: INFO: Wrong image for pod: daemon-set-nhsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:08.244: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:08.244: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:08.244: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:09.238: INFO: Wrong image for pod: daemon-set-klnj9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:09.238: INFO: Wrong image for pod: daemon-set-nhsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:09.238: INFO: Pod daemon-set-nhsqs is not available
Dec  5 20:41:09.244: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:09.244: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:09.244: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:10.239: INFO: Wrong image for pod: daemon-set-klnj9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:10.239: INFO: Wrong image for pod: daemon-set-nhsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:10.239: INFO: Pod daemon-set-nhsqs is not available
Dec  5 20:41:10.246: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:10.247: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:10.247: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:11.238: INFO: Wrong image for pod: daemon-set-klnj9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:11.238: INFO: Wrong image for pod: daemon-set-nhsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:11.238: INFO: Pod daemon-set-nhsqs is not available
Dec  5 20:41:11.244: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:11.244: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:11.244: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:12.238: INFO: Wrong image for pod: daemon-set-klnj9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:12.238: INFO: Wrong image for pod: daemon-set-nhsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:12.238: INFO: Pod daemon-set-nhsqs is not available
Dec  5 20:41:12.244: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:12.244: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:12.244: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:13.238: INFO: Wrong image for pod: daemon-set-klnj9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:13.238: INFO: Wrong image for pod: daemon-set-nhsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:13.238: INFO: Pod daemon-set-nhsqs is not available
Dec  5 20:41:13.245: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:13.245: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:13.245: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:14.238: INFO: Wrong image for pod: daemon-set-klnj9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:14.238: INFO: Wrong image for pod: daemon-set-nhsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:14.238: INFO: Pod daemon-set-nhsqs is not available
Dec  5 20:41:14.244: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:14.244: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:14.244: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:15.238: INFO: Wrong image for pod: daemon-set-klnj9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:15.238: INFO: Pod daemon-set-x2lvs is not available
Dec  5 20:41:15.244: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:15.244: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:15.244: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:16.240: INFO: Wrong image for pod: daemon-set-klnj9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:16.240: INFO: Pod daemon-set-x2lvs is not available
Dec  5 20:41:16.247: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:16.247: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:16.247: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:17.237: INFO: Wrong image for pod: daemon-set-klnj9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:17.243: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:17.244: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:17.244: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:18.246: INFO: Wrong image for pod: daemon-set-klnj9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:18.246: INFO: Pod daemon-set-klnj9 is not available
Dec  5 20:41:18.253: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:18.253: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:18.253: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:19.237: INFO: Wrong image for pod: daemon-set-klnj9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:19.237: INFO: Pod daemon-set-klnj9 is not available
Dec  5 20:41:19.243: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:19.243: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:19.244: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:20.238: INFO: Wrong image for pod: daemon-set-klnj9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:20.238: INFO: Pod daemon-set-klnj9 is not available
Dec  5 20:41:20.245: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:20.245: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:20.245: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:21.237: INFO: Wrong image for pod: daemon-set-klnj9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:21.238: INFO: Pod daemon-set-klnj9 is not available
Dec  5 20:41:21.244: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:21.244: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:21.244: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:22.237: INFO: Wrong image for pod: daemon-set-klnj9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:22.237: INFO: Pod daemon-set-klnj9 is not available
Dec  5 20:41:22.243: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:22.244: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:22.244: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:23.239: INFO: Wrong image for pod: daemon-set-klnj9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 20:41:23.239: INFO: Pod daemon-set-klnj9 is not available
Dec  5 20:41:23.247: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:23.247: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:23.247: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:24.237: INFO: Pod daemon-set-kkqn4 is not available
Dec  5 20:41:24.242: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:24.242: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:24.242: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Dec  5 20:41:24.246: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:24.247: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:24.247: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:24.250: INFO: Number of nodes with available pods: 1
Dec  5 20:41:24.250: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:41:25.257: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:25.257: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:25.257: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:25.262: INFO: Number of nodes with available pods: 1
Dec  5 20:41:25.262: INFO: Node crayfish is running more than one daemon pod
Dec  5 20:41:26.257: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:26.258: INFO: DaemonSet pods can't tolerate node master-alligator with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:26.258: INFO: DaemonSet pods can't tolerate node master-porpoise with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 20:41:26.262: INFO: Number of nodes with available pods: 2
Dec  5 20:41:26.262: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5522, will wait for the garbage collector to delete the pods
Dec  5 20:41:26.351: INFO: Deleting DaemonSet.extensions daemon-set took: 9.921349ms
Dec  5 20:41:26.651: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.294586ms
Dec  5 20:41:29.856: INFO: Number of nodes with available pods: 0
Dec  5 20:41:29.856: INFO: Number of running nodes: 0, number of available pods: 0
Dec  5 20:41:29.860: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5522/daemonsets","resourceVersion":"54028"},"items":null}

Dec  5 20:41:29.865: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5522/pods","resourceVersion":"54028"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:41:29.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5522" for this suite.
Dec  5 20:41:35.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:41:36.027: INFO: namespace daemonsets-5522 deletion completed in 6.141021607s

• [SLOW TEST:31.913 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:41:36.027: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  5 20:41:36.077: INFO: Waiting up to 5m0s for pod "pod-a13e46b4-179f-11ea-a680-be755c6bedde" in namespace "emptydir-8081" to be "success or failure"
Dec  5 20:41:36.082: INFO: Pod "pod-a13e46b4-179f-11ea-a680-be755c6bedde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.625742ms
Dec  5 20:41:38.088: INFO: Pod "pod-a13e46b4-179f-11ea-a680-be755c6bedde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010654992s
STEP: Saw pod success
Dec  5 20:41:38.088: INFO: Pod "pod-a13e46b4-179f-11ea-a680-be755c6bedde" satisfied condition "success or failure"
Dec  5 20:41:38.093: INFO: Trying to get logs from node crayfish pod pod-a13e46b4-179f-11ea-a680-be755c6bedde container test-container: <nil>
STEP: delete the pod
Dec  5 20:41:38.119: INFO: Waiting for pod pod-a13e46b4-179f-11ea-a680-be755c6bedde to disappear
Dec  5 20:41:38.123: INFO: Pod pod-a13e46b4-179f-11ea-a680-be755c6bedde no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:41:38.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8081" for this suite.
Dec  5 20:41:44.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:41:44.271: INFO: namespace emptydir-8081 deletion completed in 6.142707523s

• [SLOW TEST:8.243 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:41:44.272: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1649
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 20:41:44.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-809'
Dec  5 20:41:44.416: INFO: stderr: ""
Dec  5 20:41:44.416: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1654
Dec  5 20:41:44.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-935805400 delete pods e2e-test-nginx-pod --namespace=kubectl-809'
Dec  5 20:41:54.250: INFO: stderr: ""
Dec  5 20:41:54.250: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:41:54.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-809" for this suite.
Dec  5 20:42:00.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:42:00.403: INFO: namespace kubectl-809 deletion completed in 6.146915353s

• [SLOW TEST:16.131 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  5 20:42:00.405: INFO: >>> kubeConfig: /tmp/kubeconfig-935805400
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-afc62482-179f-11ea-a680-be755c6bedde
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-afc62482-179f-11ea-a680-be755c6bedde
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  5 20:42:04.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6146" for this suite.
Dec  5 20:42:26.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 20:42:26.670: INFO: namespace projected-6146 deletion completed in 22.140437726s

• [SLOW TEST:26.265 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSDec  5 20:42:26.670: INFO: Running AfterSuite actions on all nodes
Dec  5 20:42:26.670: INFO: Running AfterSuite actions on node 1
Dec  5 20:42:26.670: INFO: Skipping dumping logs from cluster

Ran 204 of 3586 Specs in 5650.614 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3382 Skipped PASS

Ginkgo ran 1 suite in 1h34m11.745020649s
Test Suite Passed
