I0906 08:22:30.054080      16 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-369136095
I0906 08:22:30.054343      16 e2e.go:240] Starting e2e run "7703d0e2-d07f-11e9-b48b-b617478434fe" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1567758148 - Will randomize all specs
Will run 204 of 3585 specs

Sep  6 08:22:30.262: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 08:22:30.267: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep  6 08:22:30.299: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep  6 08:22:30.367: INFO: 34 / 34 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep  6 08:22:30.367: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
Sep  6 08:22:30.367: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep  6 08:22:30.382: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Sep  6 08:22:30.382: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-bridge-vlan-ds' (0 seconds elapsed)
Sep  6 08:22:30.382: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep  6 08:22:30.382: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-device-plugin-daemonset' (0 seconds elapsed)
Sep  6 08:22:30.382: INFO: e2e test version: v1.14.3
Sep  6 08:22:30.384: INFO: kube-apiserver version: v1.14.3
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:22:30.384: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename deployment
Sep  6 08:22:30.447: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Sep  6 08:22:30.469: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2251
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 08:22:30.604: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep  6 08:22:35.610: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  6 08:22:35.611: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep  6 08:22:35.642: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-2251,SelfLink:/apis/apps/v1/namespaces/deployment-2251/deployments/test-cleanup-deployment,UID:7b196290-d07f-11e9-b16e-525400186f34,ResourceVersion:848849,Generation:1,CreationTimestamp:2019-09-06 08:22:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Sep  6 08:22:35.646: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Sep  6 08:22:35.646: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep  6 08:22:35.646: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-2251,SelfLink:/apis/apps/v1/namespaces/deployment-2251/replicasets/test-cleanup-controller,UID:7819925a-d07f-11e9-b16e-525400186f34,ResourceVersion:848850,Generation:1,CreationTimestamp:2019-09-06 08:22:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 7b196290-d07f-11e9-b16e-525400186f34 0xc001f84c87 0xc001f84c88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep  6 08:22:35.651: INFO: Pod "test-cleanup-controller-zbqzk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-zbqzk,GenerateName:test-cleanup-controller-,Namespace:deployment-2251,SelfLink:/api/v1/namespaces/deployment-2251/pods/test-cleanup-controller-zbqzk,UID:781c413b-d07f-11e9-b16e-525400186f34,ResourceVersion:848845,Generation:0,CreationTimestamp:2019-09-06 08:22:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.67.209/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 7819925a-d07f-11e9-b16e-525400186f34 0xc001d7bef7 0xc001d7bef8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qhchg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qhchg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qhchg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-20-104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:22:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:22:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:22:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:22:30 +0000 UTC  }],Message:,Reason:,HostIP:192.168.20.104,PodIP:192.168.67.209,StartTime:2019-09-06 08:22:30 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 08:22:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:4cfccf7dd56a99a70f299c7370953c4823956609be8f37c036d64e63f220737e docker://a3822ef9b85016a9df3e262dd2841ca8db5b6e1f3e5ccd7e36acf00d661bca2e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:22:35.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2251" for this suite.
Sep  6 08:22:41.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:22:41.856: INFO: namespace deployment-2251 deletion completed in 6.190037479s

• [SLOW TEST:11.472 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:22:41.857: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8313
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 08:22:42.035: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ee9f9eb-d07f-11e9-b48b-b617478434fe" in namespace "projected-8313" to be "success or failure"
Sep  6 08:22:42.049: INFO: Pod "downwardapi-volume-7ee9f9eb-d07f-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 13.352642ms
Sep  6 08:22:44.055: INFO: Pod "downwardapi-volume-7ee9f9eb-d07f-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020276871s
Sep  6 08:22:46.063: INFO: Pod "downwardapi-volume-7ee9f9eb-d07f-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027808771s
STEP: Saw pod success
Sep  6 08:22:46.063: INFO: Pod "downwardapi-volume-7ee9f9eb-d07f-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:22:46.068: INFO: Trying to get logs from node kube-node-20-104 pod downwardapi-volume-7ee9f9eb-d07f-11e9-b48b-b617478434fe container client-container: <nil>
STEP: delete the pod
Sep  6 08:22:46.116: INFO: Waiting for pod downwardapi-volume-7ee9f9eb-d07f-11e9-b48b-b617478434fe to disappear
Sep  6 08:22:46.122: INFO: Pod downwardapi-volume-7ee9f9eb-d07f-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:22:46.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8313" for this suite.
Sep  6 08:22:52.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:22:52.304: INFO: namespace projected-8313 deletion completed in 6.174949931s

• [SLOW TEST:10.447 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:22:52.305: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9761
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9761.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9761.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9761.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9761.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9761.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9761.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 08:22:56.581: INFO: DNS probes using dns-9761/dns-test-852565a0-d07f-11e9-b48b-b617478434fe succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:22:56.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9761" for this suite.
Sep  6 08:23:02.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:23:02.846: INFO: namespace dns-9761 deletion completed in 6.230043163s

• [SLOW TEST:10.541 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:23:02.847: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6155
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep  6 08:23:05.064: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-8b6d8f3e-d07f-11e9-b48b-b617478434fe,GenerateName:,Namespace:events-6155,SelfLink:/api/v1/namespaces/events-6155/pods/send-events-8b6d8f3e-d07f-11e9-b48b-b617478434fe,UID:8b6e90ee-d07f-11e9-b16e-525400186f34,ResourceVersion:848989,Generation:0,CreationTimestamp:2019-09-06 08:23:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 14805004,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.67.213/32,kubernetes.io/psp: 00-privileged,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qg48d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qg48d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-qg48d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-20-104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:23:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:23:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:23:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:23:03 +0000 UTC  }],Message:,Reason:,HostIP:192.168.20.104,PodIP:192.168.67.213,StartTime:2019-09-06 08:23:03 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-09-06 08:23:04 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://8fbde63ddd003caa0f714d29f08dc169c29c6f2eb1eafd7f71b43f178f08cf3b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Sep  6 08:23:07.071: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep  6 08:23:09.077: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:23:09.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6155" for this suite.
Sep  6 08:23:51.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:23:51.281: INFO: namespace events-6155 deletion completed in 42.182725024s

• [SLOW TEST:48.435 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:23:51.282: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6049
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-a855f410-d07f-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume configMaps
Sep  6 08:23:51.534: INFO: Waiting up to 5m0s for pod "pod-configmaps-a8570661-d07f-11e9-b48b-b617478434fe" in namespace "configmap-6049" to be "success or failure"
Sep  6 08:23:51.540: INFO: Pod "pod-configmaps-a8570661-d07f-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.985478ms
Sep  6 08:23:53.547: INFO: Pod "pod-configmaps-a8570661-d07f-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012617593s
STEP: Saw pod success
Sep  6 08:23:53.547: INFO: Pod "pod-configmaps-a8570661-d07f-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:23:53.554: INFO: Trying to get logs from node kube-node-20-104 pod pod-configmaps-a8570661-d07f-11e9-b48b-b617478434fe container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 08:23:53.592: INFO: Waiting for pod pod-configmaps-a8570661-d07f-11e9-b48b-b617478434fe to disappear
Sep  6 08:23:53.597: INFO: Pod pod-configmaps-a8570661-d07f-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:23:53.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6049" for this suite.
Sep  6 08:23:59.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:23:59.775: INFO: namespace configmap-6049 deletion completed in 6.170962989s

• [SLOW TEST:8.494 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:23:59.776: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9015
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Sep  6 08:23:59.954: INFO: Waiting up to 5m0s for pod "pod-ad5b8e6b-d07f-11e9-b48b-b617478434fe" in namespace "emptydir-9015" to be "success or failure"
Sep  6 08:23:59.965: INFO: Pod "pod-ad5b8e6b-d07f-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.842319ms
Sep  6 08:24:01.972: INFO: Pod "pod-ad5b8e6b-d07f-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018097141s
STEP: Saw pod success
Sep  6 08:24:01.972: INFO: Pod "pod-ad5b8e6b-d07f-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:24:01.978: INFO: Trying to get logs from node kube-node-20-104 pod pod-ad5b8e6b-d07f-11e9-b48b-b617478434fe container test-container: <nil>
STEP: delete the pod
Sep  6 08:24:02.011: INFO: Waiting for pod pod-ad5b8e6b-d07f-11e9-b48b-b617478434fe to disappear
Sep  6 08:24:02.018: INFO: Pod pod-ad5b8e6b-d07f-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:24:02.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9015" for this suite.
Sep  6 08:24:08.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:24:08.208: INFO: namespace emptydir-9015 deletion completed in 6.183782216s

• [SLOW TEST:8.431 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:24:08.208: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3940
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-8317
STEP: Creating secret with name secret-test-b2652fed-d07f-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume secrets
Sep  6 08:24:08.586: INFO: Waiting up to 5m0s for pod "pod-secrets-b280bcce-d07f-11e9-b48b-b617478434fe" in namespace "secrets-3940" to be "success or failure"
Sep  6 08:24:08.597: INFO: Pod "pod-secrets-b280bcce-d07f-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.185136ms
Sep  6 08:24:10.605: INFO: Pod "pod-secrets-b280bcce-d07f-11e9-b48b-b617478434fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.01887843s
Sep  6 08:24:12.612: INFO: Pod "pod-secrets-b280bcce-d07f-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025576286s
STEP: Saw pod success
Sep  6 08:24:12.612: INFO: Pod "pod-secrets-b280bcce-d07f-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:24:12.617: INFO: Trying to get logs from node kube-node-20-104 pod pod-secrets-b280bcce-d07f-11e9-b48b-b617478434fe container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 08:24:12.657: INFO: Waiting for pod pod-secrets-b280bcce-d07f-11e9-b48b-b617478434fe to disappear
Sep  6 08:24:12.663: INFO: Pod pod-secrets-b280bcce-d07f-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:24:12.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3940" for this suite.
Sep  6 08:24:18.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:24:18.859: INFO: namespace secrets-3940 deletion completed in 6.177835643s
STEP: Destroying namespace "secret-namespace-8317" for this suite.
Sep  6 08:24:24.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:24:25.077: INFO: namespace secret-namespace-8317 deletion completed in 6.218321298s

• [SLOW TEST:16.869 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:24:25.077: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7060
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 08:24:25.277: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc735b87-d07f-11e9-b48b-b617478434fe" in namespace "downward-api-7060" to be "success or failure"
Sep  6 08:24:25.285: INFO: Pod "downwardapi-volume-bc735b87-d07f-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.50361ms
Sep  6 08:24:27.295: INFO: Pod "downwardapi-volume-bc735b87-d07f-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018594756s
STEP: Saw pod success
Sep  6 08:24:27.295: INFO: Pod "downwardapi-volume-bc735b87-d07f-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:24:27.300: INFO: Trying to get logs from node kube-node-20-104 pod downwardapi-volume-bc735b87-d07f-11e9-b48b-b617478434fe container client-container: <nil>
STEP: delete the pod
Sep  6 08:24:27.334: INFO: Waiting for pod downwardapi-volume-bc735b87-d07f-11e9-b48b-b617478434fe to disappear
Sep  6 08:24:27.338: INFO: Pod downwardapi-volume-bc735b87-d07f-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:24:27.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7060" for this suite.
Sep  6 08:24:33.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:24:33.559: INFO: namespace downward-api-7060 deletion completed in 6.215288062s

• [SLOW TEST:8.482 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:24:33.560: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7563
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 08:24:33.750: INFO: (0) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 15.71459ms)
Sep  6 08:24:33.757: INFO: (1) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 7.178583ms)
Sep  6 08:24:33.763: INFO: (2) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 6.29012ms)
Sep  6 08:24:33.770: INFO: (3) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 6.421923ms)
Sep  6 08:24:33.775: INFO: (4) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 5.205256ms)
Sep  6 08:24:33.781: INFO: (5) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 5.952914ms)
Sep  6 08:24:33.786: INFO: (6) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 5.546042ms)
Sep  6 08:24:33.792: INFO: (7) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 5.545697ms)
Sep  6 08:24:33.798: INFO: (8) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 5.593238ms)
Sep  6 08:24:33.803: INFO: (9) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 5.219336ms)
Sep  6 08:24:33.808: INFO: (10) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 5.161065ms)
Sep  6 08:24:33.813: INFO: (11) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 4.943344ms)
Sep  6 08:24:33.819: INFO: (12) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 5.369711ms)
Sep  6 08:24:33.825: INFO: (13) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 6.156067ms)
Sep  6 08:24:33.832: INFO: (14) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 7.127963ms)
Sep  6 08:24:33.839: INFO: (15) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 6.96701ms)
Sep  6 08:24:33.845: INFO: (16) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 6.049366ms)
Sep  6 08:24:33.853: INFO: (17) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 7.492464ms)
Sep  6 08:24:33.860: INFO: (18) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 7.823637ms)
Sep  6 08:24:33.868: INFO: (19) /api/v1/nodes/kube-master-20-101:10250/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 7.503697ms)
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:24:33.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7563" for this suite.
Sep  6 08:24:39.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:24:40.053: INFO: namespace proxy-7563 deletion completed in 6.179254958s

• [SLOW TEST:6.493 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:24:40.053: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6747
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 08:24:40.235: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c55e3762-d07f-11e9-b48b-b617478434fe" in namespace "projected-6747" to be "success or failure"
Sep  6 08:24:40.243: INFO: Pod "downwardapi-volume-c55e3762-d07f-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.695449ms
Sep  6 08:24:42.250: INFO: Pod "downwardapi-volume-c55e3762-d07f-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015026891s
Sep  6 08:24:44.256: INFO: Pod "downwardapi-volume-c55e3762-d07f-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021177579s
STEP: Saw pod success
Sep  6 08:24:44.256: INFO: Pod "downwardapi-volume-c55e3762-d07f-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:24:44.261: INFO: Trying to get logs from node kube-node-20-104 pod downwardapi-volume-c55e3762-d07f-11e9-b48b-b617478434fe container client-container: <nil>
STEP: delete the pod
Sep  6 08:24:44.295: INFO: Waiting for pod downwardapi-volume-c55e3762-d07f-11e9-b48b-b617478434fe to disappear
Sep  6 08:24:44.305: INFO: Pod downwardapi-volume-c55e3762-d07f-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:24:44.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6747" for this suite.
Sep  6 08:24:50.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:24:50.516: INFO: namespace projected-6747 deletion completed in 6.202508787s

• [SLOW TEST:10.463 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:24:50.519: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4231
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep  6 08:24:50.704: INFO: Waiting up to 5m0s for pod "downward-api-cb9bd827-d07f-11e9-b48b-b617478434fe" in namespace "downward-api-4231" to be "success or failure"
Sep  6 08:24:50.710: INFO: Pod "downward-api-cb9bd827-d07f-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.516897ms
Sep  6 08:24:52.717: INFO: Pod "downward-api-cb9bd827-d07f-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012818611s
STEP: Saw pod success
Sep  6 08:24:52.717: INFO: Pod "downward-api-cb9bd827-d07f-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:24:52.723: INFO: Trying to get logs from node kube-node-20-104 pod downward-api-cb9bd827-d07f-11e9-b48b-b617478434fe container dapi-container: <nil>
STEP: delete the pod
Sep  6 08:24:52.758: INFO: Waiting for pod downward-api-cb9bd827-d07f-11e9-b48b-b617478434fe to disappear
Sep  6 08:24:52.763: INFO: Pod downward-api-cb9bd827-d07f-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:24:52.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4231" for this suite.
Sep  6 08:24:58.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:24:58.945: INFO: namespace downward-api-4231 deletion completed in 6.175804973s

• [SLOW TEST:8.426 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:24:58.945: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7969
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Sep  6 08:24:59.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 create -f - --namespace=kubectl-7969'
Sep  6 08:24:59.813: INFO: stderr: ""
Sep  6 08:24:59.813: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 08:24:59.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7969'
Sep  6 08:24:59.946: INFO: stderr: ""
Sep  6 08:24:59.946: INFO: stdout: "update-demo-nautilus-5vvl4 update-demo-nautilus-jtfjb "
Sep  6 08:24:59.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-5vvl4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7969'
Sep  6 08:25:00.051: INFO: stderr: ""
Sep  6 08:25:00.052: INFO: stdout: ""
Sep  6 08:25:00.052: INFO: update-demo-nautilus-5vvl4 is created but not running
Sep  6 08:25:05.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7969'
Sep  6 08:25:05.205: INFO: stderr: ""
Sep  6 08:25:05.205: INFO: stdout: "update-demo-nautilus-5vvl4 update-demo-nautilus-jtfjb "
Sep  6 08:25:05.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-5vvl4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7969'
Sep  6 08:25:05.304: INFO: stderr: ""
Sep  6 08:25:05.304: INFO: stdout: "true"
Sep  6 08:25:05.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-5vvl4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7969'
Sep  6 08:25:05.407: INFO: stderr: ""
Sep  6 08:25:05.407: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 08:25:05.407: INFO: validating pod update-demo-nautilus-5vvl4
Sep  6 08:25:05.419: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 08:25:05.419: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 08:25:05.419: INFO: update-demo-nautilus-5vvl4 is verified up and running
Sep  6 08:25:05.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-jtfjb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7969'
Sep  6 08:25:05.533: INFO: stderr: ""
Sep  6 08:25:05.533: INFO: stdout: "true"
Sep  6 08:25:05.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-jtfjb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7969'
Sep  6 08:25:05.665: INFO: stderr: ""
Sep  6 08:25:05.665: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 08:25:05.665: INFO: validating pod update-demo-nautilus-jtfjb
Sep  6 08:25:05.675: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 08:25:05.675: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 08:25:05.675: INFO: update-demo-nautilus-jtfjb is verified up and running
STEP: using delete to clean up resources
Sep  6 08:25:05.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 delete --grace-period=0 --force -f - --namespace=kubectl-7969'
Sep  6 08:25:05.776: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 08:25:05.776: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  6 08:25:05.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7969'
Sep  6 08:25:05.896: INFO: stderr: "No resources found.\n"
Sep  6 08:25:05.897: INFO: stdout: ""
Sep  6 08:25:05.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods -l name=update-demo --namespace=kubectl-7969 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 08:25:06.094: INFO: stderr: ""
Sep  6 08:25:06.094: INFO: stdout: "update-demo-nautilus-5vvl4\nupdate-demo-nautilus-jtfjb\n"
Sep  6 08:25:06.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7969'
Sep  6 08:25:06.711: INFO: stderr: "No resources found.\n"
Sep  6 08:25:06.711: INFO: stdout: ""
Sep  6 08:25:06.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods -l name=update-demo --namespace=kubectl-7969 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 08:25:06.821: INFO: stderr: ""
Sep  6 08:25:06.821: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:25:06.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7969" for this suite.
Sep  6 08:25:28.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:25:29.010: INFO: namespace kubectl-7969 deletion completed in 22.172345442s

• [SLOW TEST:30.065 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:25:29.011: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3335
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Sep  6 08:25:29.192: INFO: Waiting up to 5m0s for pod "var-expansion-e28c5afd-d07f-11e9-b48b-b617478434fe" in namespace "var-expansion-3335" to be "success or failure"
Sep  6 08:25:29.198: INFO: Pod "var-expansion-e28c5afd-d07f-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.144109ms
Sep  6 08:25:31.206: INFO: Pod "var-expansion-e28c5afd-d07f-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014097507s
STEP: Saw pod success
Sep  6 08:25:31.206: INFO: Pod "var-expansion-e28c5afd-d07f-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:25:31.212: INFO: Trying to get logs from node kube-node-20-104 pod var-expansion-e28c5afd-d07f-11e9-b48b-b617478434fe container dapi-container: <nil>
STEP: delete the pod
Sep  6 08:25:31.251: INFO: Waiting for pod var-expansion-e28c5afd-d07f-11e9-b48b-b617478434fe to disappear
Sep  6 08:25:31.257: INFO: Pod var-expansion-e28c5afd-d07f-11e9-b48b-b617478434fe no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:25:31.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3335" for this suite.
Sep  6 08:25:37.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:25:37.451: INFO: namespace var-expansion-3335 deletion completed in 6.186548123s

• [SLOW TEST:8.440 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:25:37.453: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2548
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-e795d3e4-d07f-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume configMaps
Sep  6 08:25:37.654: INFO: Waiting up to 5m0s for pod "pod-configmaps-e7976eee-d07f-11e9-b48b-b617478434fe" in namespace "configmap-2548" to be "success or failure"
Sep  6 08:25:37.666: INFO: Pod "pod-configmaps-e7976eee-d07f-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.938057ms
Sep  6 08:25:39.673: INFO: Pod "pod-configmaps-e7976eee-d07f-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018531305s
STEP: Saw pod success
Sep  6 08:25:39.673: INFO: Pod "pod-configmaps-e7976eee-d07f-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:25:39.678: INFO: Trying to get logs from node kube-node-20-104 pod pod-configmaps-e7976eee-d07f-11e9-b48b-b617478434fe container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 08:25:39.717: INFO: Waiting for pod pod-configmaps-e7976eee-d07f-11e9-b48b-b617478434fe to disappear
Sep  6 08:25:39.722: INFO: Pod pod-configmaps-e7976eee-d07f-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:25:39.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2548" for this suite.
Sep  6 08:25:45.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:25:45.895: INFO: namespace configmap-2548 deletion completed in 6.165193261s

• [SLOW TEST:8.442 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:25:45.895: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7317
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-ec9cb45b-d07f-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume configMaps
Sep  6 08:25:46.087: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ec9e138f-d07f-11e9-b48b-b617478434fe" in namespace "projected-7317" to be "success or failure"
Sep  6 08:25:46.092: INFO: Pod "pod-projected-configmaps-ec9e138f-d07f-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.540453ms
Sep  6 08:25:48.100: INFO: Pod "pod-projected-configmaps-ec9e138f-d07f-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012743863s
STEP: Saw pod success
Sep  6 08:25:48.100: INFO: Pod "pod-projected-configmaps-ec9e138f-d07f-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:25:48.105: INFO: Trying to get logs from node kube-node-20-104 pod pod-projected-configmaps-ec9e138f-d07f-11e9-b48b-b617478434fe container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 08:25:48.142: INFO: Waiting for pod pod-projected-configmaps-ec9e138f-d07f-11e9-b48b-b617478434fe to disappear
Sep  6 08:25:48.147: INFO: Pod pod-projected-configmaps-ec9e138f-d07f-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:25:48.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7317" for this suite.
Sep  6 08:25:54.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:25:54.328: INFO: namespace projected-7317 deletion completed in 6.171506528s

• [SLOW TEST:8.433 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:25:54.328: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-365
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-f1a53150-d07f-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume secrets
Sep  6 08:25:54.539: INFO: Waiting up to 5m0s for pod "pod-secrets-f1a784d8-d07f-11e9-b48b-b617478434fe" in namespace "secrets-365" to be "success or failure"
Sep  6 08:25:54.544: INFO: Pod "pod-secrets-f1a784d8-d07f-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.158326ms
Sep  6 08:25:56.551: INFO: Pod "pod-secrets-f1a784d8-d07f-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011968082s
STEP: Saw pod success
Sep  6 08:25:56.551: INFO: Pod "pod-secrets-f1a784d8-d07f-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:25:56.571: INFO: Trying to get logs from node kube-node-20-104 pod pod-secrets-f1a784d8-d07f-11e9-b48b-b617478434fe container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 08:25:56.646: INFO: Waiting for pod pod-secrets-f1a784d8-d07f-11e9-b48b-b617478434fe to disappear
Sep  6 08:25:56.651: INFO: Pod pod-secrets-f1a784d8-d07f-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:25:56.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-365" for this suite.
Sep  6 08:26:02.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:26:02.844: INFO: namespace secrets-365 deletion completed in 6.184833802s

• [SLOW TEST:8.515 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:26:02.844: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-167
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-167
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 08:26:03.004: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  6 08:26:29.223: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.67.226:8080/dial?request=hostName&protocol=udp&host=192.168.65.125&port=8081&tries=1'] Namespace:pod-network-test-167 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 08:26:29.223: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 08:26:29.521: INFO: Waiting for endpoints: map[]
Sep  6 08:26:29.527: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.67.226:8080/dial?request=hostName&protocol=udp&host=192.168.67.225&port=8081&tries=1'] Namespace:pod-network-test-167 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 08:26:29.527: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 08:26:29.802: INFO: Waiting for endpoints: map[]
Sep  6 08:26:29.807: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.67.226:8080/dial?request=hostName&protocol=udp&host=192.168.66.194&port=8081&tries=1'] Namespace:pod-network-test-167 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 08:26:29.808: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 08:26:30.072: INFO: Waiting for endpoints: map[]
Sep  6 08:26:30.077: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.67.226:8080/dial?request=hostName&protocol=udp&host=192.168.64.138&port=8081&tries=1'] Namespace:pod-network-test-167 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 08:26:30.077: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 08:26:30.338: INFO: Waiting for endpoints: map[]
Sep  6 08:26:30.344: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.67.226:8080/dial?request=hostName&protocol=udp&host=192.168.68.11&port=8081&tries=1'] Namespace:pod-network-test-167 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 08:26:30.344: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 08:26:30.622: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:26:30.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-167" for this suite.
Sep  6 08:26:54.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:26:54.835: INFO: namespace pod-network-test-167 deletion completed in 24.204904917s

• [SLOW TEST:51.991 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:26:54.835: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-534
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-15b3f14c-d080-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume secrets
Sep  6 08:26:55.023: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-15b506bc-d080-11e9-b48b-b617478434fe" in namespace "projected-534" to be "success or failure"
Sep  6 08:26:55.028: INFO: Pod "pod-projected-secrets-15b506bc-d080-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.712952ms
Sep  6 08:26:57.035: INFO: Pod "pod-projected-secrets-15b506bc-d080-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011664748s
Sep  6 08:26:59.041: INFO: Pod "pod-projected-secrets-15b506bc-d080-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017909631s
STEP: Saw pod success
Sep  6 08:26:59.041: INFO: Pod "pod-projected-secrets-15b506bc-d080-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:26:59.046: INFO: Trying to get logs from node kube-node-20-104 pod pod-projected-secrets-15b506bc-d080-11e9-b48b-b617478434fe container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 08:26:59.081: INFO: Waiting for pod pod-projected-secrets-15b506bc-d080-11e9-b48b-b617478434fe to disappear
Sep  6 08:26:59.085: INFO: Pod pod-projected-secrets-15b506bc-d080-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:26:59.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-534" for this suite.
Sep  6 08:27:05.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:27:05.338: INFO: namespace projected-534 deletion completed in 6.247004145s

• [SLOW TEST:10.503 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:27:05.339: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6190
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep  6 08:27:08.072: INFO: Successfully updated pod "annotationupdate1bf5ec34-d080-11e9-b48b-b617478434fe"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:27:10.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6190" for this suite.
Sep  6 08:27:32.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:27:32.285: INFO: namespace downward-api-6190 deletion completed in 22.17602803s

• [SLOW TEST:26.946 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:27:32.286: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3402
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-2c080f87-d080-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume secrets
Sep  6 08:27:32.486: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2c09652d-d080-11e9-b48b-b617478434fe" in namespace "projected-3402" to be "success or failure"
Sep  6 08:27:32.501: INFO: Pod "pod-projected-secrets-2c09652d-d080-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 14.903199ms
Sep  6 08:27:34.508: INFO: Pod "pod-projected-secrets-2c09652d-d080-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021650619s
Sep  6 08:27:36.515: INFO: Pod "pod-projected-secrets-2c09652d-d080-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02863724s
STEP: Saw pod success
Sep  6 08:27:36.515: INFO: Pod "pod-projected-secrets-2c09652d-d080-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:27:36.523: INFO: Trying to get logs from node kube-node-20-104 pod pod-projected-secrets-2c09652d-d080-11e9-b48b-b617478434fe container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 08:27:36.557: INFO: Waiting for pod pod-projected-secrets-2c09652d-d080-11e9-b48b-b617478434fe to disappear
Sep  6 08:27:36.566: INFO: Pod pod-projected-secrets-2c09652d-d080-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:27:36.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3402" for this suite.
Sep  6 08:27:42.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:27:42.769: INFO: namespace projected-3402 deletion completed in 6.196490039s

• [SLOW TEST:10.483 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:27:42.770: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1654
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 08:27:42.974: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3249497a-d080-11e9-b48b-b617478434fe" in namespace "downward-api-1654" to be "success or failure"
Sep  6 08:27:42.980: INFO: Pod "downwardapi-volume-3249497a-d080-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.875431ms
Sep  6 08:27:44.987: INFO: Pod "downwardapi-volume-3249497a-d080-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012856607s
STEP: Saw pod success
Sep  6 08:27:44.987: INFO: Pod "downwardapi-volume-3249497a-d080-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:27:44.992: INFO: Trying to get logs from node kube-node-20-104 pod downwardapi-volume-3249497a-d080-11e9-b48b-b617478434fe container client-container: <nil>
STEP: delete the pod
Sep  6 08:27:45.029: INFO: Waiting for pod downwardapi-volume-3249497a-d080-11e9-b48b-b617478434fe to disappear
Sep  6 08:27:45.033: INFO: Pod downwardapi-volume-3249497a-d080-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:27:45.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1654" for this suite.
Sep  6 08:27:51.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:27:51.204: INFO: namespace downward-api-1654 deletion completed in 6.165016047s

• [SLOW TEST:8.434 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:27:51.205: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1692
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 08:27:51.380: INFO: Creating deployment "nginx-deployment"
Sep  6 08:27:51.389: INFO: Waiting for observed generation 1
Sep  6 08:27:53.408: INFO: Waiting for all required pods to come up
Sep  6 08:27:53.424: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep  6 08:27:55.449: INFO: Waiting for deployment "nginx-deployment" to complete
Sep  6 08:27:55.459: INFO: Updating deployment "nginx-deployment" with a non-existent image
Sep  6 08:27:55.473: INFO: Updating deployment nginx-deployment
Sep  6 08:27:55.473: INFO: Waiting for observed generation 2
Sep  6 08:27:57.487: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep  6 08:27:57.492: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep  6 08:27:57.495: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep  6 08:27:57.509: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep  6 08:27:57.509: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep  6 08:27:57.512: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep  6 08:27:57.519: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Sep  6 08:27:57.519: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Sep  6 08:27:57.531: INFO: Updating deployment nginx-deployment
Sep  6 08:27:57.531: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Sep  6 08:27:57.542: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep  6 08:27:57.549: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep  6 08:27:57.583: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-1692,SelfLink:/apis/apps/v1/namespaces/deployment-1692/deployments/nginx-deployment,UID:374ef660-d080-11e9-b16e-525400186f34,ResourceVersion:850115,Generation:3,CreationTimestamp:2019-09-06 08:27:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-09-06 08:27:55 +0000 UTC 2019-09-06 08:27:51 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.} {Available False 2019-09-06 08:27:57 +0000 UTC 2019-09-06 08:27:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Sep  6 08:27:57.598: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-1692,SelfLink:/apis/apps/v1/namespaces/deployment-1692/replicasets/nginx-deployment-5f9595f595,UID:39bf7386-d080-11e9-b16e-525400186f34,ResourceVersion:850113,Generation:3,CreationTimestamp:2019-09-06 08:27:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 374ef660-d080-11e9-b16e-525400186f34 0xc0026f21a7 0xc0026f21a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 08:27:57.598: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Sep  6 08:27:57.599: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-1692,SelfLink:/apis/apps/v1/namespaces/deployment-1692/replicasets/nginx-deployment-6f478d8d8,UID:37502ed9-d080-11e9-b16e-525400186f34,ResourceVersion:850112,Generation:3,CreationTimestamp:2019-09-06 08:27:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 374ef660-d080-11e9-b16e-525400186f34 0xc0026f2277 0xc0026f2278}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Sep  6 08:27:57.629: INFO: Pod "nginx-deployment-5f9595f595-7vqxf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-7vqxf,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-5f9595f595-7vqxf,UID:39d20fde-d080-11e9-b16e-525400186f34,ResourceVersion:850109,Generation:0,CreationTimestamp:2019-09-06 08:27:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.66.196/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 39bf7386-d080-11e9-b16e-525400186f34 0xc0026f2b57 0xc0026f2b58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-master-20-101,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.20.101,PodIP:,StartTime:2019-09-06 08:27:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.629: INFO: Pod "nginx-deployment-5f9595f595-c5vdn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-c5vdn,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-5f9595f595-c5vdn,UID:39c22c74-d080-11e9-b16e-525400186f34,ResourceVersion:850105,Generation:0,CreationTimestamp:2019-09-06 08:27:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.68.15/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 39bf7386-d080-11e9-b16e-525400186f34 0xc0026f2c80 0xc0026f2c81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-20-105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.20.105,PodIP:,StartTime:2019-09-06 08:27:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.629: INFO: Pod "nginx-deployment-5f9595f595-flq7m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-flq7m,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-5f9595f595-flq7m,UID:3b01f4f4-d080-11e9-b16e-525400186f34,ResourceVersion:850127,Generation:0,CreationTimestamp:2019-09-06 08:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 39bf7386-d080-11e9-b16e-525400186f34 0xc0026f2db0 0xc0026f2db1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.630: INFO: Pod "nginx-deployment-5f9595f595-nns82" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-nns82,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-5f9595f595-nns82,UID:3b01c015-d080-11e9-b16e-525400186f34,ResourceVersion:850126,Generation:0,CreationTimestamp:2019-09-06 08:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 39bf7386-d080-11e9-b16e-525400186f34 0xc0026f2e70 0xc0026f2e71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.630: INFO: Pod "nginx-deployment-5f9595f595-q6r75" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-q6r75,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-5f9595f595-q6r75,UID:39cef9fa-d080-11e9-b16e-525400186f34,ResourceVersion:850108,Generation:0,CreationTimestamp:2019-09-06 08:27:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.64.141/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 39bf7386-d080-11e9-b16e-525400186f34 0xc0026f2f30 0xc0026f2f31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-master-20-103,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.20.103,PodIP:,StartTime:2019-09-06 08:27:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.630: INFO: Pod "nginx-deployment-5f9595f595-sjtxz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-sjtxz,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-5f9595f595-sjtxz,UID:39c0ba36-d080-11e9-b16e-525400186f34,ResourceVersion:850106,Generation:0,CreationTimestamp:2019-09-06 08:27:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.67.234/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 39bf7386-d080-11e9-b16e-525400186f34 0xc0026f3050 0xc0026f3051}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-20-104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.20.104,PodIP:,StartTime:2019-09-06 08:27:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.631: INFO: Pod "nginx-deployment-5f9595f595-w7ls8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-w7ls8,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-5f9595f595-w7ls8,UID:3afce1c6-d080-11e9-b16e-525400186f34,ResourceVersion:850121,Generation:0,CreationTimestamp:2019-09-06 08:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 39bf7386-d080-11e9-b16e-525400186f34 0xc0026f3180 0xc0026f3181}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-20-104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.631: INFO: Pod "nginx-deployment-5f9595f595-xpj9h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-xpj9h,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-5f9595f595-xpj9h,UID:39c281e3-d080-11e9-b16e-525400186f34,ResourceVersion:850107,Generation:0,CreationTimestamp:2019-09-06 08:27:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.65.127/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 39bf7386-d080-11e9-b16e-525400186f34 0xc0026f3260 0xc0026f3261}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-master-20-102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.20.102,PodIP:,StartTime:2019-09-06 08:27:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.631: INFO: Pod "nginx-deployment-6f478d8d8-6vfr6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-6vfr6,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-6f478d8d8-6vfr6,UID:3afce463-d080-11e9-b16e-525400186f34,ResourceVersion:850123,Generation:0,CreationTimestamp:2019-09-06 08:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 37502ed9-d080-11e9-b16e-525400186f34 0xc0026f3380 0xc0026f3381}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-20-104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.631: INFO: Pod "nginx-deployment-6f478d8d8-7x9jz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7x9jz,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-6f478d8d8-7x9jz,UID:37566943-d080-11e9-b16e-525400186f34,ResourceVersion:850067,Generation:0,CreationTimestamp:2019-09-06 08:27:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.68.12/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 37502ed9-d080-11e9-b16e-525400186f34 0xc0026f3460 0xc0026f3461}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-20-105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.20.105,PodIP:192.168.68.12,StartTime:2019-09-06 08:27:51 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 08:27:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://1263d5409d62ede6b8c9afd2e272748f1dcd03f041f0b5c789024431765a22ba}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.632: INFO: Pod "nginx-deployment-6f478d8d8-f5jsk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-f5jsk,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-6f478d8d8-f5jsk,UID:37567cfe-d080-11e9-b16e-525400186f34,ResourceVersion:850047,Generation:0,CreationTimestamp:2019-09-06 08:27:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.67.232/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 37502ed9-d080-11e9-b16e-525400186f34 0xc0026f3587 0xc0026f3588}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-20-104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.20.104,PodIP:192.168.67.232,StartTime:2019-09-06 08:27:51 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 08:27:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:4cfccf7dd56a99a70f299c7370953c4823956609be8f37c036d64e63f220737e docker://124b6d6fe30de712db8001e131b651a8856827e54e10fb3f86e25f80272a2e32}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.632: INFO: Pod "nginx-deployment-6f478d8d8-gvxv5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-gvxv5,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-6f478d8d8-gvxv5,UID:3755fe06-d080-11e9-b16e-525400186f34,ResourceVersion:850048,Generation:0,CreationTimestamp:2019-09-06 08:27:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.64.140/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 37502ed9-d080-11e9-b16e-525400186f34 0xc0026f36b7 0xc0026f36b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-master-20-103,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.20.103,PodIP:192.168.64.140,StartTime:2019-09-06 08:27:51 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 08:27:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://f98bdaaa6dd6910b2e477e39327faa27f469caebd19c4e39fab350f01736c567}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.633: INFO: Pod "nginx-deployment-6f478d8d8-h4fgx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-h4fgx,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-6f478d8d8-h4fgx,UID:3b015779-d080-11e9-b16e-525400186f34,ResourceVersion:850124,Generation:0,CreationTimestamp:2019-09-06 08:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 37502ed9-d080-11e9-b16e-525400186f34 0xc0026f37d7 0xc0026f37d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.633: INFO: Pod "nginx-deployment-6f478d8d8-kklwn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-kklwn,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-6f478d8d8-kklwn,UID:375bb621-d080-11e9-b16e-525400186f34,ResourceVersion:850064,Generation:0,CreationTimestamp:2019-09-06 08:27:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.68.13/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 37502ed9-d080-11e9-b16e-525400186f34 0xc0026f38a0 0xc0026f38a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-20-105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.20.105,PodIP:192.168.68.13,StartTime:2019-09-06 08:27:51 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 08:27:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://71c619145c5930a70b03d2e98d130dd43cd465773450145bbe102cf4f1bb163f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.633: INFO: Pod "nginx-deployment-6f478d8d8-l2plj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-l2plj,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-6f478d8d8-l2plj,UID:3b01f316-d080-11e9-b16e-525400186f34,ResourceVersion:850128,Generation:0,CreationTimestamp:2019-09-06 08:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 37502ed9-d080-11e9-b16e-525400186f34 0xc0026f39c7 0xc0026f39c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.633: INFO: Pod "nginx-deployment-6f478d8d8-mls7j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-mls7j,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-6f478d8d8-mls7j,UID:3afa7dbf-d080-11e9-b16e-525400186f34,ResourceVersion:850116,Generation:0,CreationTimestamp:2019-09-06 08:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 37502ed9-d080-11e9-b16e-525400186f34 0xc0026f3a80 0xc0026f3a81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-master-20-101,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.634: INFO: Pod "nginx-deployment-6f478d8d8-mx7n4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-mx7n4,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-6f478d8d8-mx7n4,UID:3753a2c6-d080-11e9-b16e-525400186f34,ResourceVersion:850060,Generation:0,CreationTimestamp:2019-09-06 08:27:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.68.14/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 37502ed9-d080-11e9-b16e-525400186f34 0xc0026f3b50 0xc0026f3b51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-20-105,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.20.105,PodIP:192.168.68.14,StartTime:2019-09-06 08:27:51 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 08:27:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://12e69c6b1a699694bfdb93551212a04352f0c0c0dc3c7b5a4d1c3a62da6665bb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.634: INFO: Pod "nginx-deployment-6f478d8d8-npgc4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-npgc4,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-6f478d8d8-npgc4,UID:3afd1e1a-d080-11e9-b16e-525400186f34,ResourceVersion:850122,Generation:0,CreationTimestamp:2019-09-06 08:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 37502ed9-d080-11e9-b16e-525400186f34 0xc0026f3c77 0xc0026f3c78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-master-20-102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.634: INFO: Pod "nginx-deployment-6f478d8d8-qmg8g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-qmg8g,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-6f478d8d8-qmg8g,UID:3b014575-d080-11e9-b16e-525400186f34,ResourceVersion:850129,Generation:0,CreationTimestamp:2019-09-06 08:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 37502ed9-d080-11e9-b16e-525400186f34 0xc0026f3d50 0xc0026f3d51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.635: INFO: Pod "nginx-deployment-6f478d8d8-qmq9q" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-qmq9q,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-6f478d8d8-qmq9q,UID:375baafe-d080-11e9-b16e-525400186f34,ResourceVersion:850055,Generation:0,CreationTimestamp:2019-09-06 08:27:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.67.233/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 37502ed9-d080-11e9-b16e-525400186f34 0xc0026f3e00 0xc0026f3e01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-20-104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.20.104,PodIP:192.168.67.233,StartTime:2019-09-06 08:27:51 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 08:27:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:4cfccf7dd56a99a70f299c7370953c4823956609be8f37c036d64e63f220737e docker://3c4054468aa6791d3573f7ace46d98e9cea69efc9b5d904d1b7a8782cfd02f6e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.635: INFO: Pod "nginx-deployment-6f478d8d8-txxdv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-txxdv,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-6f478d8d8-txxdv,UID:3753db2a-d080-11e9-b16e-525400186f34,ResourceVersion:850041,Generation:0,CreationTimestamp:2019-09-06 08:27:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.65.126/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 37502ed9-d080-11e9-b16e-525400186f34 0xc0026f3f27 0xc0026f3f28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-master-20-102,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.20.102,PodIP:192.168.65.126,StartTime:2019-09-06 08:27:51 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 08:27:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://838fab459060b5dde49a0661711e96226b6a25bd99242d4c209c0f903e209da1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.635: INFO: Pod "nginx-deployment-6f478d8d8-v2g6t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-v2g6t,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-6f478d8d8-v2g6t,UID:3b01a1b4-d080-11e9-b16e-525400186f34,ResourceVersion:850125,Generation:0,CreationTimestamp:2019-09-06 08:27:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 37502ed9-d080-11e9-b16e-525400186f34 0xc0026d6047 0xc0026d6048}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 08:27:57.635: INFO: Pod "nginx-deployment-6f478d8d8-v852g" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-v852g,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1692,SelfLink:/api/v1/namespaces/deployment-1692/pods/nginx-deployment-6f478d8d8-v852g,UID:375bf4cb-d080-11e9-b16e-525400186f34,ResourceVersion:850052,Generation:0,CreationTimestamp:2019-09-06 08:27:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.64.139/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 37502ed9-d080-11e9-b16e-525400186f34 0xc0026d6100 0xc0026d6101}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kzcrs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kzcrs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kzcrs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-master-20-103,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:27:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.20.103,PodIP:192.168.64.139,StartTime:2019-09-06 08:27:51 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 08:27:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://7eb3aa53e4228cc9320ee9997f2662fcf8b0c3a4bad6a69648c0e980dfcea1d8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:27:57.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1692" for this suite.
Sep  6 08:28:05.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:28:06.110: INFO: namespace deployment-1692 deletion completed in 8.424074819s

• [SLOW TEST:14.905 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:28:06.111: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8414
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-8414/configmap-test-40350827-d080-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume configMaps
Sep  6 08:28:06.343: INFO: Waiting up to 5m0s for pod "pod-configmaps-40360202-d080-11e9-b48b-b617478434fe" in namespace "configmap-8414" to be "success or failure"
Sep  6 08:28:06.350: INFO: Pod "pod-configmaps-40360202-d080-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.423455ms
Sep  6 08:28:08.356: INFO: Pod "pod-configmaps-40360202-d080-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01312598s
STEP: Saw pod success
Sep  6 08:28:08.356: INFO: Pod "pod-configmaps-40360202-d080-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:28:08.362: INFO: Trying to get logs from node kube-node-20-104 pod pod-configmaps-40360202-d080-11e9-b48b-b617478434fe container env-test: <nil>
STEP: delete the pod
Sep  6 08:28:08.401: INFO: Waiting for pod pod-configmaps-40360202-d080-11e9-b48b-b617478434fe to disappear
Sep  6 08:28:08.406: INFO: Pod pod-configmaps-40360202-d080-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:28:08.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8414" for this suite.
Sep  6 08:28:14.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:28:14.613: INFO: namespace configmap-8414 deletion completed in 6.199477927s

• [SLOW TEST:8.502 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:28:14.614: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-680
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-4540618b-d080-11e9-b48b-b617478434fe
Sep  6 08:28:14.793: INFO: Pod name my-hostname-basic-4540618b-d080-11e9-b48b-b617478434fe: Found 0 pods out of 1
Sep  6 08:28:19.800: INFO: Pod name my-hostname-basic-4540618b-d080-11e9-b48b-b617478434fe: Found 1 pods out of 1
Sep  6 08:28:19.800: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-4540618b-d080-11e9-b48b-b617478434fe" are running
Sep  6 08:28:19.805: INFO: Pod "my-hostname-basic-4540618b-d080-11e9-b48b-b617478434fe-htgpk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 08:28:14 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 08:28:16 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 08:28:16 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 08:28:14 +0000 UTC Reason: Message:}])
Sep  6 08:28:19.805: INFO: Trying to dial the pod
Sep  6 08:28:24.834: INFO: Controller my-hostname-basic-4540618b-d080-11e9-b48b-b617478434fe: Got expected result from replica 1 [my-hostname-basic-4540618b-d080-11e9-b48b-b617478434fe-htgpk]: "my-hostname-basic-4540618b-d080-11e9-b48b-b617478434fe-htgpk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:28:24.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-680" for this suite.
Sep  6 08:28:30.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:28:31.039: INFO: namespace replication-controller-680 deletion completed in 6.195539367s

• [SLOW TEST:16.424 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:28:31.039: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9300
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0906 08:28:32.295545      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 08:28:32.295: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:28:32.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9300" for this suite.
Sep  6 08:28:38.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:28:38.526: INFO: namespace gc-9300 deletion completed in 6.223286337s

• [SLOW TEST:7.487 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:28:38.527: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9961
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:28:38.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9961" for this suite.
Sep  6 08:28:44.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:28:44.918: INFO: namespace kubelet-test-9961 deletion completed in 6.160309281s

• [SLOW TEST:6.391 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:28:44.919: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4491
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Sep  6 08:28:45.077: INFO: namespace kubectl-4491
Sep  6 08:28:45.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 create -f - --namespace=kubectl-4491'
Sep  6 08:28:45.425: INFO: stderr: ""
Sep  6 08:28:45.425: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep  6 08:28:46.438: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 08:28:46.438: INFO: Found 0 / 1
Sep  6 08:28:47.431: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 08:28:47.431: INFO: Found 1 / 1
Sep  6 08:28:47.431: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  6 08:28:47.435: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 08:28:47.435: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 08:28:47.435: INFO: wait on redis-master startup in kubectl-4491 
Sep  6 08:28:47.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 logs redis-master-7jdv2 redis-master --namespace=kubectl-4491'
Sep  6 08:28:47.600: INFO: stderr: ""
Sep  6 08:28:47.601: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Sep 08:28:46.939 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Sep 08:28:46.940 # Server started, Redis version 3.2.12\n1:M 06 Sep 08:28:46.941 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Sep 08:28:46.941 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Sep  6 08:28:47.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-4491'
Sep  6 08:28:47.755: INFO: stderr: ""
Sep  6 08:28:47.755: INFO: stdout: "service/rm2 exposed\n"
Sep  6 08:28:47.763: INFO: Service rm2 in namespace kubectl-4491 found.
STEP: exposing service
Sep  6 08:28:49.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-4491'
Sep  6 08:28:49.905: INFO: stderr: ""
Sep  6 08:28:49.905: INFO: stdout: "service/rm3 exposed\n"
Sep  6 08:28:49.911: INFO: Service rm3 in namespace kubectl-4491 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:28:51.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4491" for this suite.
Sep  6 08:29:13.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:29:14.104: INFO: namespace kubectl-4491 deletion completed in 22.177026398s

• [SLOW TEST:29.186 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:29:14.105: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1698
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-1698
Sep  6 08:29:18.298: INFO: Started pod liveness-http in namespace container-probe-1698
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 08:29:18.302: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:33:19.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1698" for this suite.
Sep  6 08:33:25.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:33:25.364: INFO: namespace container-probe-1698 deletion completed in 6.165189737s

• [SLOW TEST:251.259 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:33:25.365: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Sep  6 08:33:25.520: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-369136095 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:33:25.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1826" for this suite.
Sep  6 08:33:31.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:33:31.816: INFO: namespace kubectl-1826 deletion completed in 6.180674434s

• [SLOW TEST:6.451 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:33:31.816: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1905
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep  6 08:33:31.997: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1905,SelfLink:/api/v1/namespaces/watch-1905/configmaps/e2e-watch-test-watch-closed,UID:025224be-d081-11e9-b16e-525400186f34,ResourceVersion:851120,Generation:0,CreationTimestamp:2019-09-06 08:33:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 08:33:31.997: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1905,SelfLink:/api/v1/namespaces/watch-1905/configmaps/e2e-watch-test-watch-closed,UID:025224be-d081-11e9-b16e-525400186f34,ResourceVersion:851121,Generation:0,CreationTimestamp:2019-09-06 08:33:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep  6 08:33:32.019: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1905,SelfLink:/api/v1/namespaces/watch-1905/configmaps/e2e-watch-test-watch-closed,UID:025224be-d081-11e9-b16e-525400186f34,ResourceVersion:851122,Generation:0,CreationTimestamp:2019-09-06 08:33:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 08:33:32.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1905,SelfLink:/api/v1/namespaces/watch-1905/configmaps/e2e-watch-test-watch-closed,UID:025224be-d081-11e9-b16e-525400186f34,ResourceVersion:851123,Generation:0,CreationTimestamp:2019-09-06 08:33:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:33:32.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1905" for this suite.
Sep  6 08:33:38.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:33:38.235: INFO: namespace watch-1905 deletion completed in 6.207346659s

• [SLOW TEST:6.418 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:33:38.236: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-062955dd-d081-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume secrets
Sep  6 08:33:38.449: INFO: Waiting up to 5m0s for pod "pod-secrets-062aa37b-d081-11e9-b48b-b617478434fe" in namespace "secrets-2974" to be "success or failure"
Sep  6 08:33:38.460: INFO: Pod "pod-secrets-062aa37b-d081-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.685287ms
Sep  6 08:33:40.468: INFO: Pod "pod-secrets-062aa37b-d081-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019358804s
Sep  6 08:33:42.477: INFO: Pod "pod-secrets-062aa37b-d081-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027964733s
STEP: Saw pod success
Sep  6 08:33:42.477: INFO: Pod "pod-secrets-062aa37b-d081-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:33:42.481: INFO: Trying to get logs from node kube-node-20-104 pod pod-secrets-062aa37b-d081-11e9-b48b-b617478434fe container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 08:33:42.522: INFO: Waiting for pod pod-secrets-062aa37b-d081-11e9-b48b-b617478434fe to disappear
Sep  6 08:33:42.532: INFO: Pod pod-secrets-062aa37b-d081-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:33:42.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2974" for this suite.
Sep  6 08:33:48.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:33:48.741: INFO: namespace secrets-2974 deletion completed in 6.20084409s

• [SLOW TEST:10.505 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:33:48.742: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6287
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Sep  6 08:33:49.456: INFO: created pod pod-service-account-defaultsa
Sep  6 08:33:49.456: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep  6 08:33:49.471: INFO: created pod pod-service-account-mountsa
Sep  6 08:33:49.471: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep  6 08:33:49.485: INFO: created pod pod-service-account-nomountsa
Sep  6 08:33:49.485: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep  6 08:33:49.503: INFO: created pod pod-service-account-defaultsa-mountspec
Sep  6 08:33:49.503: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep  6 08:33:49.527: INFO: created pod pod-service-account-mountsa-mountspec
Sep  6 08:33:49.527: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep  6 08:33:49.542: INFO: created pod pod-service-account-nomountsa-mountspec
Sep  6 08:33:49.542: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep  6 08:33:49.565: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep  6 08:33:49.565: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep  6 08:33:49.587: INFO: created pod pod-service-account-mountsa-nomountspec
Sep  6 08:33:49.587: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep  6 08:33:49.618: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep  6 08:33:49.618: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:33:49.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6287" for this suite.
Sep  6 08:33:55.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:33:55.891: INFO: namespace svcaccounts-6287 deletion completed in 6.256989862s

• [SLOW TEST:7.149 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:33:55.892: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-3808
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-3808
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3808
STEP: Deleting pre-stop pod
Sep  6 08:34:05.211: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:34:05.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3808" for this suite.
Sep  6 08:34:45.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:34:45.474: INFO: namespace prestop-3808 deletion completed in 40.21316438s

• [SLOW TEST:49.582 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:34:45.474: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7649
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 08:34:45.660: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e3a7ddb-d081-11e9-b48b-b617478434fe" in namespace "downward-api-7649" to be "success or failure"
Sep  6 08:34:45.672: INFO: Pod "downwardapi-volume-2e3a7ddb-d081-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.547838ms
Sep  6 08:34:47.680: INFO: Pod "downwardapi-volume-2e3a7ddb-d081-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019764878s
Sep  6 08:34:49.688: INFO: Pod "downwardapi-volume-2e3a7ddb-d081-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028284837s
STEP: Saw pod success
Sep  6 08:34:49.689: INFO: Pod "downwardapi-volume-2e3a7ddb-d081-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:34:49.694: INFO: Trying to get logs from node kube-node-20-104 pod downwardapi-volume-2e3a7ddb-d081-11e9-b48b-b617478434fe container client-container: <nil>
STEP: delete the pod
Sep  6 08:34:49.731: INFO: Waiting for pod downwardapi-volume-2e3a7ddb-d081-11e9-b48b-b617478434fe to disappear
Sep  6 08:34:49.740: INFO: Pod downwardapi-volume-2e3a7ddb-d081-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:34:49.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7649" for this suite.
Sep  6 08:34:55.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:34:55.946: INFO: namespace downward-api-7649 deletion completed in 6.196889827s

• [SLOW TEST:10.472 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:34:55.946: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3548
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-3478d217-d081-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume configMaps
Sep  6 08:34:56.148: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3479d971-d081-11e9-b48b-b617478434fe" in namespace "projected-3548" to be "success or failure"
Sep  6 08:34:56.153: INFO: Pod "pod-projected-configmaps-3479d971-d081-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.219733ms
Sep  6 08:34:58.166: INFO: Pod "pod-projected-configmaps-3479d971-d081-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017514723s
STEP: Saw pod success
Sep  6 08:34:58.166: INFO: Pod "pod-projected-configmaps-3479d971-d081-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:34:58.175: INFO: Trying to get logs from node kube-node-20-104 pod pod-projected-configmaps-3479d971-d081-11e9-b48b-b617478434fe container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 08:34:58.228: INFO: Waiting for pod pod-projected-configmaps-3479d971-d081-11e9-b48b-b617478434fe to disappear
Sep  6 08:34:58.239: INFO: Pod pod-projected-configmaps-3479d971-d081-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:34:58.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3548" for this suite.
Sep  6 08:35:04.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:35:04.439: INFO: namespace projected-3548 deletion completed in 6.189956791s

• [SLOW TEST:8.493 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:35:04.439: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3743
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep  6 08:35:04.634: INFO: Waiting up to 5m0s for pod "downward-api-39890396-d081-11e9-b48b-b617478434fe" in namespace "downward-api-3743" to be "success or failure"
Sep  6 08:35:04.650: INFO: Pod "downward-api-39890396-d081-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 16.383123ms
Sep  6 08:35:06.657: INFO: Pod "downward-api-39890396-d081-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022893015s
Sep  6 08:35:08.765: INFO: Pod "downward-api-39890396-d081-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.131048189s
STEP: Saw pod success
Sep  6 08:35:08.765: INFO: Pod "downward-api-39890396-d081-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:35:08.775: INFO: Trying to get logs from node kube-node-20-104 pod downward-api-39890396-d081-11e9-b48b-b617478434fe container dapi-container: <nil>
STEP: delete the pod
Sep  6 08:35:08.827: INFO: Waiting for pod downward-api-39890396-d081-11e9-b48b-b617478434fe to disappear
Sep  6 08:35:08.835: INFO: Pod downward-api-39890396-d081-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:35:08.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3743" for this suite.
Sep  6 08:35:15.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:35:15.210: INFO: namespace downward-api-3743 deletion completed in 6.360975509s

• [SLOW TEST:10.771 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:35:15.210: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4992
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-4992/configmap-test-3ff35bb8-d081-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume configMaps
Sep  6 08:35:15.402: INFO: Waiting up to 5m0s for pod "pod-configmaps-3ff48805-d081-11e9-b48b-b617478434fe" in namespace "configmap-4992" to be "success or failure"
Sep  6 08:35:15.410: INFO: Pod "pod-configmaps-3ff48805-d081-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.625215ms
Sep  6 08:35:17.417: INFO: Pod "pod-configmaps-3ff48805-d081-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014699602s
Sep  6 08:35:19.424: INFO: Pod "pod-configmaps-3ff48805-d081-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022003518s
STEP: Saw pod success
Sep  6 08:35:19.424: INFO: Pod "pod-configmaps-3ff48805-d081-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:35:19.431: INFO: Trying to get logs from node kube-node-20-104 pod pod-configmaps-3ff48805-d081-11e9-b48b-b617478434fe container env-test: <nil>
STEP: delete the pod
Sep  6 08:35:19.467: INFO: Waiting for pod pod-configmaps-3ff48805-d081-11e9-b48b-b617478434fe to disappear
Sep  6 08:35:19.471: INFO: Pod pod-configmaps-3ff48805-d081-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:35:19.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4992" for this suite.
Sep  6 08:35:25.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:35:25.674: INFO: namespace configmap-4992 deletion completed in 6.196568896s

• [SLOW TEST:10.464 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:35:25.674: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2990
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:35:27.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2990" for this suite.
Sep  6 08:36:11.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:36:12.181: INFO: namespace kubelet-test-2990 deletion completed in 44.269855791s

• [SLOW TEST:46.507 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:36:12.181: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7857
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-61ec90de-d081-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume configMaps
Sep  6 08:36:12.399: INFO: Waiting up to 5m0s for pod "pod-configmaps-61ede3b7-d081-11e9-b48b-b617478434fe" in namespace "configmap-7857" to be "success or failure"
Sep  6 08:36:12.407: INFO: Pod "pod-configmaps-61ede3b7-d081-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.788621ms
Sep  6 08:36:14.419: INFO: Pod "pod-configmaps-61ede3b7-d081-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02015599s
Sep  6 08:36:16.428: INFO: Pod "pod-configmaps-61ede3b7-d081-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028767011s
STEP: Saw pod success
Sep  6 08:36:16.428: INFO: Pod "pod-configmaps-61ede3b7-d081-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:36:16.437: INFO: Trying to get logs from node kube-node-20-104 pod pod-configmaps-61ede3b7-d081-11e9-b48b-b617478434fe container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 08:36:16.485: INFO: Waiting for pod pod-configmaps-61ede3b7-d081-11e9-b48b-b617478434fe to disappear
Sep  6 08:36:16.489: INFO: Pod pod-configmaps-61ede3b7-d081-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:36:16.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7857" for this suite.
Sep  6 08:36:22.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:36:22.695: INFO: namespace configmap-7857 deletion completed in 6.198815464s

• [SLOW TEST:10.514 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:36:22.697: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9015
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 08:36:22.929: INFO: Create a RollingUpdate DaemonSet
Sep  6 08:36:22.938: INFO: Check that daemon pods launch on every node of the cluster
Sep  6 08:36:22.950: INFO: Number of nodes with available pods: 0
Sep  6 08:36:22.950: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 08:36:23.976: INFO: Number of nodes with available pods: 0
Sep  6 08:36:23.977: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 08:36:24.972: INFO: Number of nodes with available pods: 0
Sep  6 08:36:24.972: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 08:36:25.966: INFO: Number of nodes with available pods: 3
Sep  6 08:36:25.966: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 08:36:26.965: INFO: Number of nodes with available pods: 5
Sep  6 08:36:26.965: INFO: Number of running nodes: 5, number of available pods: 5
Sep  6 08:36:26.965: INFO: Update the DaemonSet to trigger a rollout
Sep  6 08:36:26.982: INFO: Updating DaemonSet daemon-set
Sep  6 08:36:31.007: INFO: Roll back the DaemonSet before rollout is complete
Sep  6 08:36:31.023: INFO: Updating DaemonSet daemon-set
Sep  6 08:36:31.023: INFO: Make sure DaemonSet rollback is complete
Sep  6 08:36:31.031: INFO: Wrong image for pod: daemon-set-m2962. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  6 08:36:31.031: INFO: Pod daemon-set-m2962 is not available
Sep  6 08:36:32.048: INFO: Wrong image for pod: daemon-set-m2962. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  6 08:36:32.048: INFO: Pod daemon-set-m2962 is not available
Sep  6 08:36:33.046: INFO: Wrong image for pod: daemon-set-m2962. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  6 08:36:33.046: INFO: Pod daemon-set-m2962 is not available
Sep  6 08:36:34.044: INFO: Wrong image for pod: daemon-set-m2962. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  6 08:36:34.044: INFO: Pod daemon-set-m2962 is not available
Sep  6 08:36:35.046: INFO: Pod daemon-set-w8vzd is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9015, will wait for the garbage collector to delete the pods
Sep  6 08:36:35.140: INFO: Deleting DaemonSet.extensions daemon-set took: 14.792352ms
Sep  6 08:36:35.641: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.341363ms
Sep  6 08:36:44.648: INFO: Number of nodes with available pods: 0
Sep  6 08:36:44.648: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 08:36:44.657: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9015/daemonsets","resourceVersion":"851857"},"items":null}

Sep  6 08:36:44.663: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9015/pods","resourceVersion":"851857"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:36:44.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9015" for this suite.
Sep  6 08:36:50.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:36:50.919: INFO: namespace daemonsets-9015 deletion completed in 6.217011069s

• [SLOW TEST:28.223 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:36:50.920: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2329
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-79012ee9-d081-11e9-b48b-b617478434fe
STEP: Creating configMap with name cm-test-opt-upd-79012fb3-d081-11e9-b48b-b617478434fe
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-79012ee9-d081-11e9-b48b-b617478434fe
STEP: Updating configmap cm-test-opt-upd-79012fb3-d081-11e9-b48b-b617478434fe
STEP: Creating configMap with name cm-test-opt-create-79012fd8-d081-11e9-b48b-b617478434fe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:38:16.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2329" for this suite.
Sep  6 08:38:34.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:38:34.260: INFO: namespace projected-2329 deletion completed in 18.239621775s

• [SLOW TEST:103.341 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:38:34.264: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5360
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep  6 08:38:34.461: INFO: PodSpec: initContainers in spec.initContainers
Sep  6 08:39:22.503: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b69cf901-d081-11e9-b48b-b617478434fe", GenerateName:"", Namespace:"init-container-5360", SelfLink:"/api/v1/namespaces/init-container-5360/pods/pod-init-b69cf901-d081-11e9-b48b-b617478434fe", UID:"b69ea3b1-d081-11e9-b16e-525400186f34", ResourceVersion:"852228", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63703355914, loc:(*time.Location)(0x8a1a0e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"461023818"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.68.22/32", "kubernetes.io/psp":"00-privileged"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-4qgpw", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002138000), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4qgpw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4qgpw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4qgpw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001bb01a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kube-node-20-105", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000920060), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration(nil), HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001bb0270)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703355914, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703355914, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703355914, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703355914, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.20.105", PodIP:"192.168.68.22", StartTime:(*v1.Time)(0xc001e16060), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00089c2a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00089c310)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://57e10c04b944e4575309a3b1051e8d75cc140109403af2e19eab35ce7f8a4f76"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001e160a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001e16080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:39:22.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5360" for this suite.
Sep  6 08:39:46.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:39:46.754: INFO: namespace init-container-5360 deletion completed in 24.238161632s

• [SLOW TEST:72.490 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:39:46.758: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-2639
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep  6 08:39:53.028: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 08:39:53.028: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 08:39:53.321: INFO: Exec stderr: ""
Sep  6 08:39:53.321: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 08:39:53.321: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 08:39:53.651: INFO: Exec stderr: ""
Sep  6 08:39:53.651: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 08:39:53.651: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 08:39:53.939: INFO: Exec stderr: ""
Sep  6 08:39:53.939: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 08:39:53.939: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 08:39:54.226: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep  6 08:39:54.226: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 08:39:54.226: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 08:39:54.589: INFO: Exec stderr: ""
Sep  6 08:39:54.589: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 08:39:54.589: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 08:39:54.837: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep  6 08:39:54.837: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 08:39:54.837: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 08:39:55.133: INFO: Exec stderr: ""
Sep  6 08:39:55.133: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 08:39:55.133: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 08:39:55.387: INFO: Exec stderr: ""
Sep  6 08:39:55.387: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 08:39:55.387: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 08:39:55.639: INFO: Exec stderr: ""
Sep  6 08:39:55.639: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 08:39:55.639: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 08:39:55.905: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:39:55.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2639" for this suite.
Sep  6 08:40:43.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:40:44.132: INFO: namespace e2e-kubelet-etc-hosts-2639 deletion completed in 48.217926168s

• [SLOW TEST:57.374 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:40:44.132: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4791
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-4791
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4791
STEP: Creating statefulset with conflicting port in namespace statefulset-4791
STEP: Waiting until pod test-pod will start running in namespace statefulset-4791
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4791
Sep  6 08:40:48.444: INFO: Observed stateful pod in namespace: statefulset-4791, name: ss-0, uid: 04336a6e-d082-11e9-b16e-525400186f34, status phase: Pending. Waiting for statefulset controller to delete.
Sep  6 08:40:54.438: INFO: Observed stateful pod in namespace: statefulset-4791, name: ss-0, uid: 04336a6e-d082-11e9-b16e-525400186f34, status phase: Failed. Waiting for statefulset controller to delete.
Sep  6 08:40:54.453: INFO: Observed stateful pod in namespace: statefulset-4791, name: ss-0, uid: 04336a6e-d082-11e9-b16e-525400186f34, status phase: Failed. Waiting for statefulset controller to delete.
Sep  6 08:40:54.474: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4791
STEP: Removing pod with conflicting port in namespace statefulset-4791
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4791 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep  6 08:40:58.564: INFO: Deleting all statefulset in ns statefulset-4791
Sep  6 08:40:58.570: INFO: Scaling statefulset ss to 0
Sep  6 08:41:08.608: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 08:41:08.616: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:41:08.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4791" for this suite.
Sep  6 08:41:14.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:41:14.844: INFO: namespace statefulset-4791 deletion completed in 6.196013902s

• [SLOW TEST:30.712 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:41:14.846: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6782
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-6782
Sep  6 08:41:19.059: INFO: Started pod liveness-exec in namespace container-probe-6782
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 08:41:19.064: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:45:20.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6782" for this suite.
Sep  6 08:45:26.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:45:26.279: INFO: namespace container-probe-6782 deletion completed in 6.2049867s

• [SLOW TEST:251.434 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:45:26.280: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6685
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 08:45:26.481: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ac2fd2d8-d082-11e9-b48b-b617478434fe" in namespace "projected-6685" to be "success or failure"
Sep  6 08:45:26.491: INFO: Pod "downwardapi-volume-ac2fd2d8-d082-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.096045ms
Sep  6 08:45:28.499: INFO: Pod "downwardapi-volume-ac2fd2d8-d082-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017507318s
Sep  6 08:45:30.506: INFO: Pod "downwardapi-volume-ac2fd2d8-d082-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025085714s
STEP: Saw pod success
Sep  6 08:45:30.506: INFO: Pod "downwardapi-volume-ac2fd2d8-d082-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:45:30.512: INFO: Trying to get logs from node kube-node-20-104 pod downwardapi-volume-ac2fd2d8-d082-11e9-b48b-b617478434fe container client-container: <nil>
STEP: delete the pod
Sep  6 08:45:30.550: INFO: Waiting for pod downwardapi-volume-ac2fd2d8-d082-11e9-b48b-b617478434fe to disappear
Sep  6 08:45:30.557: INFO: Pod downwardapi-volume-ac2fd2d8-d082-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:45:30.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6685" for this suite.
Sep  6 08:45:36.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:45:36.805: INFO: namespace projected-6685 deletion completed in 6.240157748s

• [SLOW TEST:10.526 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:45:36.806: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7699
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-b275684e-d082-11e9-b48b-b617478434fe
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:45:41.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7699" for this suite.
Sep  6 08:46:03.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:46:03.268: INFO: namespace configmap-7699 deletion completed in 22.19599756s

• [SLOW TEST:26.462 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:46:03.270: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3848
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep  6 08:46:03.449: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:46:09.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3848" for this suite.
Sep  6 08:46:31.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:46:31.219: INFO: namespace init-container-3848 deletion completed in 22.193491135s

• [SLOW TEST:27.950 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:46:31.220: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8396
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  6 08:46:39.500: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 08:46:39.507: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 08:46:41.507: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 08:46:41.516: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 08:46:43.508: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 08:46:43.516: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 08:46:45.507: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 08:46:45.516: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 08:46:47.508: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 08:46:47.522: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 08:46:49.508: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 08:46:49.514: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 08:46:51.507: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 08:46:51.513: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:46:51.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8396" for this suite.
Sep  6 08:47:15.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:47:15.810: INFO: namespace container-lifecycle-hook-8396 deletion completed in 24.277247295s

• [SLOW TEST:44.590 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:47:15.812: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2275
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Sep  6 08:47:20.045: INFO: Pod pod-hostip-ed7962a2-d082-11e9-b48b-b617478434fe has hostIP: 192.168.20.104
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:47:20.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2275" for this suite.
Sep  6 08:47:42.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:47:42.249: INFO: namespace pods-2275 deletion completed in 22.196307186s

• [SLOW TEST:26.437 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:47:42.249: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep  6 08:47:45.029: INFO: Successfully updated pod "annotationupdatefd39c084-d082-11e9-b48b-b617478434fe"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:47:47.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7391" for this suite.
Sep  6 08:48:09.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:48:09.281: INFO: namespace projected-7391 deletion completed in 22.211595836s

• [SLOW TEST:27.032 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:48:09.282: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3441
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-0d563e66-d083-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume secrets
Sep  6 08:48:09.484: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0d57a8c5-d083-11e9-b48b-b617478434fe" in namespace "projected-3441" to be "success or failure"
Sep  6 08:48:09.497: INFO: Pod "pod-projected-secrets-0d57a8c5-d083-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 12.911037ms
Sep  6 08:48:11.506: INFO: Pod "pod-projected-secrets-0d57a8c5-d083-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021726384s
Sep  6 08:48:13.515: INFO: Pod "pod-projected-secrets-0d57a8c5-d083-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030305302s
STEP: Saw pod success
Sep  6 08:48:13.515: INFO: Pod "pod-projected-secrets-0d57a8c5-d083-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:48:13.523: INFO: Trying to get logs from node kube-node-20-104 pod pod-projected-secrets-0d57a8c5-d083-11e9-b48b-b617478434fe container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 08:48:13.574: INFO: Waiting for pod pod-projected-secrets-0d57a8c5-d083-11e9-b48b-b617478434fe to disappear
Sep  6 08:48:13.583: INFO: Pod pod-projected-secrets-0d57a8c5-d083-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:48:13.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3441" for this suite.
Sep  6 08:48:19.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:48:19.826: INFO: namespace projected-3441 deletion completed in 6.231883837s

• [SLOW TEST:10.544 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:48:19.827: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-62
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  6 08:48:20.012: INFO: Waiting up to 5m0s for pod "pod-139ed9e0-d083-11e9-b48b-b617478434fe" in namespace "emptydir-62" to be "success or failure"
Sep  6 08:48:20.025: INFO: Pod "pod-139ed9e0-d083-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 13.441305ms
Sep  6 08:48:22.034: INFO: Pod "pod-139ed9e0-d083-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021708379s
Sep  6 08:48:24.042: INFO: Pod "pod-139ed9e0-d083-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029562125s
STEP: Saw pod success
Sep  6 08:48:24.042: INFO: Pod "pod-139ed9e0-d083-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:48:24.046: INFO: Trying to get logs from node kube-node-20-104 pod pod-139ed9e0-d083-11e9-b48b-b617478434fe container test-container: <nil>
STEP: delete the pod
Sep  6 08:48:24.076: INFO: Waiting for pod pod-139ed9e0-d083-11e9-b48b-b617478434fe to disappear
Sep  6 08:48:24.080: INFO: Pod pod-139ed9e0-d083-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:48:24.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-62" for this suite.
Sep  6 08:48:30.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:48:30.301: INFO: namespace emptydir-62 deletion completed in 6.2142029s

• [SLOW TEST:10.474 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:48:30.301: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2609
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  6 08:48:34.583: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 08:48:34.590: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 08:48:36.590: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 08:48:36.596: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 08:48:38.590: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 08:48:38.597: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 08:48:40.590: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 08:48:40.601: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 08:48:42.590: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 08:48:42.598: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 08:48:44.590: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 08:48:44.598: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 08:48:46.590: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 08:48:46.598: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 08:48:48.590: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 08:48:48.601: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 08:48:50.590: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 08:48:50.598: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 08:48:52.590: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 08:48:52.596: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 08:48:54.590: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 08:48:54.597: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 08:48:56.590: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 08:48:56.599: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 08:48:58.590: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 08:48:58.596: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 08:49:00.590: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 08:49:00.598: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:49:00.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2609" for this suite.
Sep  6 08:49:24.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:49:24.819: INFO: namespace container-lifecycle-hook-2609 deletion completed in 24.214296373s

• [SLOW TEST:54.518 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:49:24.820: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8542
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-zjk2m in namespace proxy-8542
I0906 08:49:25.039465      16 runners.go:184] Created replication controller with name: proxy-service-zjk2m, namespace: proxy-8542, replica count: 1
I0906 08:49:26.090450      16 runners.go:184] proxy-service-zjk2m Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 08:49:27.090866      16 runners.go:184] proxy-service-zjk2m Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 08:49:28.091163      16 runners.go:184] proxy-service-zjk2m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 08:49:29.091376      16 runners.go:184] proxy-service-zjk2m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 08:49:30.091645      16 runners.go:184] proxy-service-zjk2m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 08:49:31.091885      16 runners.go:184] proxy-service-zjk2m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 08:49:32.092339      16 runners.go:184] proxy-service-zjk2m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 08:49:33.092587      16 runners.go:184] proxy-service-zjk2m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 08:49:34.093074      16 runners.go:184] proxy-service-zjk2m Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 08:49:34.100: INFO: setup took 9.100909271s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep  6 08:49:34.124: INFO: (0) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 22.191893ms)
Sep  6 08:49:34.124: INFO: (0) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 22.480572ms)
Sep  6 08:49:34.124: INFO: (0) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 22.386329ms)
Sep  6 08:49:34.125: INFO: (0) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 24.623199ms)
Sep  6 08:49:34.125: INFO: (0) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 22.895376ms)
Sep  6 08:49:34.125: INFO: (0) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 23.932603ms)
Sep  6 08:49:34.125: INFO: (0) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 24.786423ms)
Sep  6 08:49:34.125: INFO: (0) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 25.418327ms)
Sep  6 08:49:34.126: INFO: (0) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 24.038548ms)
Sep  6 08:49:34.126: INFO: (0) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 24.390218ms)
Sep  6 08:49:34.126: INFO: (0) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 24.355053ms)
Sep  6 08:49:34.130: INFO: (0) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 28.995137ms)
Sep  6 08:49:34.130: INFO: (0) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 29.524573ms)
Sep  6 08:49:34.131: INFO: (0) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 29.638655ms)
Sep  6 08:49:34.131: INFO: (0) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 31.003968ms)
Sep  6 08:49:34.132: INFO: (0) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 30.560448ms)
Sep  6 08:49:34.141: INFO: (1) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 9.32486ms)
Sep  6 08:49:34.141: INFO: (1) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 9.069713ms)
Sep  6 08:49:34.141: INFO: (1) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 9.463482ms)
Sep  6 08:49:34.143: INFO: (1) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 10.416377ms)
Sep  6 08:49:34.143: INFO: (1) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 9.418484ms)
Sep  6 08:49:34.145: INFO: (1) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 13.684145ms)
Sep  6 08:49:34.146: INFO: (1) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 13.34686ms)
Sep  6 08:49:34.146: INFO: (1) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 12.510709ms)
Sep  6 08:49:34.146: INFO: (1) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 12.874048ms)
Sep  6 08:49:34.146: INFO: (1) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 13.024779ms)
Sep  6 08:49:34.146: INFO: (1) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 13.203853ms)
Sep  6 08:49:34.147: INFO: (1) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 13.563757ms)
Sep  6 08:49:34.147: INFO: (1) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 13.534606ms)
Sep  6 08:49:34.147: INFO: (1) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 14.06458ms)
Sep  6 08:49:34.147: INFO: (1) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 13.930596ms)
Sep  6 08:49:34.148: INFO: (1) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 15.407447ms)
Sep  6 08:49:34.158: INFO: (2) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 10.425058ms)
Sep  6 08:49:34.160: INFO: (2) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 12.314255ms)
Sep  6 08:49:34.161: INFO: (2) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 12.980227ms)
Sep  6 08:49:34.162: INFO: (2) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 13.737105ms)
Sep  6 08:49:34.162: INFO: (2) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 13.80489ms)
Sep  6 08:49:34.162: INFO: (2) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 14.530223ms)
Sep  6 08:49:34.163: INFO: (2) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 15.121624ms)
Sep  6 08:49:34.164: INFO: (2) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 15.700861ms)
Sep  6 08:49:34.164: INFO: (2) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 15.902065ms)
Sep  6 08:49:34.164: INFO: (2) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 16.314609ms)
Sep  6 08:49:34.165: INFO: (2) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 16.973409ms)
Sep  6 08:49:34.165: INFO: (2) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 17.27794ms)
Sep  6 08:49:34.165: INFO: (2) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 17.374242ms)
Sep  6 08:49:34.165: INFO: (2) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 17.347495ms)
Sep  6 08:49:34.165: INFO: (2) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 17.25849ms)
Sep  6 08:49:34.165: INFO: (2) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 17.158736ms)
Sep  6 08:49:34.180: INFO: (3) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 13.564035ms)
Sep  6 08:49:34.180: INFO: (3) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 14.553238ms)
Sep  6 08:49:34.180: INFO: (3) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 14.338193ms)
Sep  6 08:49:34.180: INFO: (3) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 13.489776ms)
Sep  6 08:49:34.180: INFO: (3) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 12.715355ms)
Sep  6 08:49:34.180: INFO: (3) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 12.492475ms)
Sep  6 08:49:34.180: INFO: (3) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 12.481143ms)
Sep  6 08:49:34.180: INFO: (3) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 12.496326ms)
Sep  6 08:49:34.180: INFO: (3) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 14.234556ms)
Sep  6 08:49:34.180: INFO: (3) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 14.139128ms)
Sep  6 08:49:34.180: INFO: (3) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 13.606171ms)
Sep  6 08:49:34.180: INFO: (3) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 12.852217ms)
Sep  6 08:49:34.180: INFO: (3) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 14.646025ms)
Sep  6 08:49:34.182: INFO: (3) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 14.918501ms)
Sep  6 08:49:34.182: INFO: (3) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 15.089539ms)
Sep  6 08:49:34.182: INFO: (3) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 15.017151ms)
Sep  6 08:49:34.192: INFO: (4) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 9.930273ms)
Sep  6 08:49:34.193: INFO: (4) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 9.81946ms)
Sep  6 08:49:34.193: INFO: (4) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 10.318365ms)
Sep  6 08:49:34.193: INFO: (4) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 10.602289ms)
Sep  6 08:49:34.193: INFO: (4) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 10.901186ms)
Sep  6 08:49:34.194: INFO: (4) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 11.506284ms)
Sep  6 08:49:34.194: INFO: (4) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 11.58185ms)
Sep  6 08:49:34.195: INFO: (4) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 11.541747ms)
Sep  6 08:49:34.195: INFO: (4) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 12.438968ms)
Sep  6 08:49:34.195: INFO: (4) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 12.475933ms)
Sep  6 08:49:34.196: INFO: (4) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 12.43443ms)
Sep  6 08:49:34.196: INFO: (4) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 12.755228ms)
Sep  6 08:49:34.196: INFO: (4) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 13.696841ms)
Sep  6 08:49:34.196: INFO: (4) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 13.636457ms)
Sep  6 08:49:34.196: INFO: (4) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 13.468317ms)
Sep  6 08:49:34.198: INFO: (4) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 15.38845ms)
Sep  6 08:49:34.211: INFO: (5) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 12.593186ms)
Sep  6 08:49:34.212: INFO: (5) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 12.41265ms)
Sep  6 08:49:34.212: INFO: (5) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 13.082681ms)
Sep  6 08:49:34.212: INFO: (5) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 13.402505ms)
Sep  6 08:49:34.213: INFO: (5) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 14.489414ms)
Sep  6 08:49:34.213: INFO: (5) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 15.298887ms)
Sep  6 08:49:34.214: INFO: (5) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 14.892747ms)
Sep  6 08:49:34.214: INFO: (5) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 14.685924ms)
Sep  6 08:49:34.214: INFO: (5) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 15.374247ms)
Sep  6 08:49:34.214: INFO: (5) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 15.334888ms)
Sep  6 08:49:34.215: INFO: (5) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 16.082239ms)
Sep  6 08:49:34.215: INFO: (5) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 16.971164ms)
Sep  6 08:49:34.217: INFO: (5) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 18.949718ms)
Sep  6 08:49:34.219: INFO: (5) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 19.827272ms)
Sep  6 08:49:34.219: INFO: (5) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 20.474169ms)
Sep  6 08:49:34.219: INFO: (5) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 19.702688ms)
Sep  6 08:49:34.230: INFO: (6) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 10.151049ms)
Sep  6 08:49:34.234: INFO: (6) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 14.869651ms)
Sep  6 08:49:34.234: INFO: (6) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 14.620124ms)
Sep  6 08:49:34.234: INFO: (6) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 14.790929ms)
Sep  6 08:49:34.234: INFO: (6) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 14.783686ms)
Sep  6 08:49:34.234: INFO: (6) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 14.812737ms)
Sep  6 08:49:34.235: INFO: (6) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 14.999628ms)
Sep  6 08:49:34.235: INFO: (6) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 15.437243ms)
Sep  6 08:49:34.235: INFO: (6) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 15.417533ms)
Sep  6 08:49:34.235: INFO: (6) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 15.027329ms)
Sep  6 08:49:34.236: INFO: (6) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 16.815639ms)
Sep  6 08:49:34.237: INFO: (6) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 17.810887ms)
Sep  6 08:49:34.237: INFO: (6) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 17.822481ms)
Sep  6 08:49:34.237: INFO: (6) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 17.741647ms)
Sep  6 08:49:34.238: INFO: (6) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 17.879607ms)
Sep  6 08:49:34.238: INFO: (6) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 18.172023ms)
Sep  6 08:49:34.246: INFO: (7) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 8.022603ms)
Sep  6 08:49:34.246: INFO: (7) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 8.273213ms)
Sep  6 08:49:34.250: INFO: (7) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 10.573506ms)
Sep  6 08:49:34.250: INFO: (7) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 10.916707ms)
Sep  6 08:49:34.250: INFO: (7) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 10.883478ms)
Sep  6 08:49:34.250: INFO: (7) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 12.71392ms)
Sep  6 08:49:34.251: INFO: (7) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 11.607418ms)
Sep  6 08:49:34.251: INFO: (7) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 11.291056ms)
Sep  6 08:49:34.251: INFO: (7) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 12.919233ms)
Sep  6 08:49:34.251: INFO: (7) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 12.34734ms)
Sep  6 08:49:34.251: INFO: (7) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 12.344628ms)
Sep  6 08:49:34.251: INFO: (7) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 12.81207ms)
Sep  6 08:49:34.253: INFO: (7) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 13.289893ms)
Sep  6 08:49:34.253: INFO: (7) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 13.517247ms)
Sep  6 08:49:34.254: INFO: (7) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 13.863156ms)
Sep  6 08:49:34.254: INFO: (7) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 13.659677ms)
Sep  6 08:49:34.261: INFO: (8) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 7.139276ms)
Sep  6 08:49:34.262: INFO: (8) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 7.61119ms)
Sep  6 08:49:34.263: INFO: (8) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 8.264445ms)
Sep  6 08:49:34.268: INFO: (8) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 12.677194ms)
Sep  6 08:49:34.268: INFO: (8) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 13.880268ms)
Sep  6 08:49:34.269: INFO: (8) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 14.353191ms)
Sep  6 08:49:34.269: INFO: (8) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 15.148331ms)
Sep  6 08:49:34.269: INFO: (8) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 14.662893ms)
Sep  6 08:49:34.270: INFO: (8) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 14.870428ms)
Sep  6 08:49:34.270: INFO: (8) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 14.941177ms)
Sep  6 08:49:34.271: INFO: (8) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 15.734457ms)
Sep  6 08:49:34.271: INFO: (8) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 16.389855ms)
Sep  6 08:49:34.272: INFO: (8) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 17.416656ms)
Sep  6 08:49:34.273: INFO: (8) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 18.050139ms)
Sep  6 08:49:34.273: INFO: (8) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 17.916648ms)
Sep  6 08:49:34.273: INFO: (8) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 18.425585ms)
Sep  6 08:49:34.282: INFO: (9) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 8.478027ms)
Sep  6 08:49:34.286: INFO: (9) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 12.286557ms)
Sep  6 08:49:34.286: INFO: (9) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 12.954773ms)
Sep  6 08:49:34.287: INFO: (9) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 13.108511ms)
Sep  6 08:49:34.287: INFO: (9) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 13.82321ms)
Sep  6 08:49:34.288: INFO: (9) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 13.546668ms)
Sep  6 08:49:34.288: INFO: (9) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 12.976731ms)
Sep  6 08:49:34.288: INFO: (9) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 13.330981ms)
Sep  6 08:49:34.295: INFO: (9) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 20.632307ms)
Sep  6 08:49:34.295: INFO: (9) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 20.777492ms)
Sep  6 08:49:34.295: INFO: (9) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 21.532541ms)
Sep  6 08:49:34.296: INFO: (9) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 21.153058ms)
Sep  6 08:49:34.296: INFO: (9) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 21.131111ms)
Sep  6 08:49:34.296: INFO: (9) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 21.055277ms)
Sep  6 08:49:34.296: INFO: (9) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 21.225332ms)
Sep  6 08:49:34.296: INFO: (9) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 21.768878ms)
Sep  6 08:49:34.307: INFO: (10) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 10.499858ms)
Sep  6 08:49:34.310: INFO: (10) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 14.197791ms)
Sep  6 08:49:34.310: INFO: (10) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 14.109641ms)
Sep  6 08:49:34.310: INFO: (10) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 14.330293ms)
Sep  6 08:49:34.311: INFO: (10) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 14.597679ms)
Sep  6 08:49:34.311: INFO: (10) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 14.529012ms)
Sep  6 08:49:34.311: INFO: (10) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 14.542704ms)
Sep  6 08:49:34.311: INFO: (10) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 14.897236ms)
Sep  6 08:49:34.311: INFO: (10) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 15.388235ms)
Sep  6 08:49:34.311: INFO: (10) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 15.120294ms)
Sep  6 08:49:34.312: INFO: (10) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 15.389133ms)
Sep  6 08:49:34.312: INFO: (10) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 15.787767ms)
Sep  6 08:49:34.312: INFO: (10) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 15.700547ms)
Sep  6 08:49:34.312: INFO: (10) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 15.979372ms)
Sep  6 08:49:34.312: INFO: (10) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 16.301474ms)
Sep  6 08:49:34.313: INFO: (10) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 16.241836ms)
Sep  6 08:49:34.321: INFO: (11) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 8.337202ms)
Sep  6 08:49:34.325: INFO: (11) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 10.835149ms)
Sep  6 08:49:34.325: INFO: (11) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 12.363304ms)
Sep  6 08:49:34.325: INFO: (11) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 11.567721ms)
Sep  6 08:49:34.326: INFO: (11) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 13.117021ms)
Sep  6 08:49:34.326: INFO: (11) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 11.944291ms)
Sep  6 08:49:34.326: INFO: (11) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 13.240371ms)
Sep  6 08:49:34.326: INFO: (11) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 12.603732ms)
Sep  6 08:49:34.326: INFO: (11) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 13.20228ms)
Sep  6 08:49:34.327: INFO: (11) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 12.541867ms)
Sep  6 08:49:34.327: INFO: (11) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 13.165116ms)
Sep  6 08:49:34.328: INFO: (11) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 13.648385ms)
Sep  6 08:49:34.328: INFO: (11) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 15.267296ms)
Sep  6 08:49:34.328: INFO: (11) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 15.02855ms)
Sep  6 08:49:34.329: INFO: (11) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 15.957341ms)
Sep  6 08:49:34.329: INFO: (11) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 14.813178ms)
Sep  6 08:49:34.335: INFO: (12) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 6.308765ms)
Sep  6 08:49:34.337: INFO: (12) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 7.552815ms)
Sep  6 08:49:34.337: INFO: (12) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 7.444543ms)
Sep  6 08:49:34.337: INFO: (12) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 7.73297ms)
Sep  6 08:49:34.339: INFO: (12) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 9.224388ms)
Sep  6 08:49:34.339: INFO: (12) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 9.432243ms)
Sep  6 08:49:34.339: INFO: (12) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 9.431697ms)
Sep  6 08:49:34.340: INFO: (12) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 10.104964ms)
Sep  6 08:49:34.342: INFO: (12) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 12.938208ms)
Sep  6 08:49:34.343: INFO: (12) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 12.984728ms)
Sep  6 08:49:34.343: INFO: (12) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 13.626701ms)
Sep  6 08:49:34.344: INFO: (12) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 14.182328ms)
Sep  6 08:49:34.344: INFO: (12) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 14.217946ms)
Sep  6 08:49:34.344: INFO: (12) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 14.208321ms)
Sep  6 08:49:34.344: INFO: (12) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 15.415111ms)
Sep  6 08:49:34.344: INFO: (12) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 14.672086ms)
Sep  6 08:49:34.353: INFO: (13) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 7.692851ms)
Sep  6 08:49:34.355: INFO: (13) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 9.317574ms)
Sep  6 08:49:34.355: INFO: (13) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 9.373406ms)
Sep  6 08:49:34.355: INFO: (13) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 10.967302ms)
Sep  6 08:49:34.356: INFO: (13) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 10.897385ms)
Sep  6 08:49:34.356: INFO: (13) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 11.11606ms)
Sep  6 08:49:34.356: INFO: (13) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 10.88221ms)
Sep  6 08:49:34.357: INFO: (13) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 11.709071ms)
Sep  6 08:49:34.357: INFO: (13) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 11.424338ms)
Sep  6 08:49:34.357: INFO: (13) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 11.834149ms)
Sep  6 08:49:34.358: INFO: (13) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 13.153833ms)
Sep  6 08:49:34.358: INFO: (13) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 13.5314ms)
Sep  6 08:49:34.358: INFO: (13) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 12.166986ms)
Sep  6 08:49:34.359: INFO: (13) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 13.054088ms)
Sep  6 08:49:34.360: INFO: (13) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 14.271388ms)
Sep  6 08:49:34.360: INFO: (13) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 14.196378ms)
Sep  6 08:49:34.368: INFO: (14) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 7.569404ms)
Sep  6 08:49:34.370: INFO: (14) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 9.421222ms)
Sep  6 08:49:34.371: INFO: (14) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 9.600323ms)
Sep  6 08:49:34.371: INFO: (14) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 9.415584ms)
Sep  6 08:49:34.371: INFO: (14) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 9.681415ms)
Sep  6 08:49:34.372: INFO: (14) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 11.00645ms)
Sep  6 08:49:34.372: INFO: (14) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 12.258465ms)
Sep  6 08:49:34.373: INFO: (14) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 12.229848ms)
Sep  6 08:49:34.373: INFO: (14) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 12.214048ms)
Sep  6 08:49:34.373: INFO: (14) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 12.188231ms)
Sep  6 08:49:34.373: INFO: (14) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 11.426184ms)
Sep  6 08:49:34.373: INFO: (14) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 12.883981ms)
Sep  6 08:49:34.375: INFO: (14) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 14.280733ms)
Sep  6 08:49:34.378: INFO: (14) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 16.988272ms)
Sep  6 08:49:34.378: INFO: (14) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 16.294315ms)
Sep  6 08:49:34.378: INFO: (14) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 16.640739ms)
Sep  6 08:49:34.387: INFO: (15) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 8.934985ms)
Sep  6 08:49:34.392: INFO: (15) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 12.652869ms)
Sep  6 08:49:34.395: INFO: (15) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 16.462044ms)
Sep  6 08:49:34.395: INFO: (15) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 16.231897ms)
Sep  6 08:49:34.395: INFO: (15) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 16.426973ms)
Sep  6 08:49:34.395: INFO: (15) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 15.835901ms)
Sep  6 08:49:34.395: INFO: (15) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 15.751388ms)
Sep  6 08:49:34.395: INFO: (15) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 15.541639ms)
Sep  6 08:49:34.395: INFO: (15) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 16.441125ms)
Sep  6 08:49:34.395: INFO: (15) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 15.9516ms)
Sep  6 08:49:34.395: INFO: (15) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 15.815857ms)
Sep  6 08:49:34.395: INFO: (15) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 15.692422ms)
Sep  6 08:49:34.395: INFO: (15) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 16.385923ms)
Sep  6 08:49:34.395: INFO: (15) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 16.329473ms)
Sep  6 08:49:34.395: INFO: (15) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 16.564321ms)
Sep  6 08:49:34.395: INFO: (15) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 16.247749ms)
Sep  6 08:49:34.405: INFO: (16) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 8.703074ms)
Sep  6 08:49:34.406: INFO: (16) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 9.597021ms)
Sep  6 08:49:34.407: INFO: (16) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 9.904248ms)
Sep  6 08:49:34.407: INFO: (16) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 10.188777ms)
Sep  6 08:49:34.407: INFO: (16) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 11.427318ms)
Sep  6 08:49:34.408: INFO: (16) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 11.526592ms)
Sep  6 08:49:34.408: INFO: (16) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 10.705305ms)
Sep  6 08:49:34.408: INFO: (16) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 11.094799ms)
Sep  6 08:49:34.408: INFO: (16) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 10.887528ms)
Sep  6 08:49:34.408: INFO: (16) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 12.777457ms)
Sep  6 08:49:34.408: INFO: (16) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 12.840213ms)
Sep  6 08:49:34.409: INFO: (16) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 11.807149ms)
Sep  6 08:49:34.410: INFO: (16) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 14.068862ms)
Sep  6 08:49:34.410: INFO: (16) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 13.604308ms)
Sep  6 08:49:34.411: INFO: (16) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 14.353413ms)
Sep  6 08:49:34.411: INFO: (16) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 14.290493ms)
Sep  6 08:49:34.418: INFO: (17) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 6.495673ms)
Sep  6 08:49:34.418: INFO: (17) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 7.489495ms)
Sep  6 08:49:34.421: INFO: (17) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 9.291134ms)
Sep  6 08:49:34.421: INFO: (17) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 9.602464ms)
Sep  6 08:49:34.421: INFO: (17) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 9.542302ms)
Sep  6 08:49:34.421: INFO: (17) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 9.844905ms)
Sep  6 08:49:34.422: INFO: (17) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 10.346661ms)
Sep  6 08:49:34.423: INFO: (17) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 12.334698ms)
Sep  6 08:49:34.424: INFO: (17) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 11.932898ms)
Sep  6 08:49:34.424: INFO: (17) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 12.384763ms)
Sep  6 08:49:34.424: INFO: (17) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 12.329121ms)
Sep  6 08:49:34.425: INFO: (17) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 13.569959ms)
Sep  6 08:49:34.426: INFO: (17) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 14.580925ms)
Sep  6 08:49:34.426: INFO: (17) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 14.758358ms)
Sep  6 08:49:34.427: INFO: (17) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 15.062323ms)
Sep  6 08:49:34.428: INFO: (17) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 16.654107ms)
Sep  6 08:49:34.437: INFO: (18) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 8.88004ms)
Sep  6 08:49:34.440: INFO: (18) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 10.314193ms)
Sep  6 08:49:34.440: INFO: (18) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 10.25853ms)
Sep  6 08:49:34.441: INFO: (18) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 11.166621ms)
Sep  6 08:49:34.441: INFO: (18) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 10.996241ms)
Sep  6 08:49:34.441: INFO: (18) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 11.137588ms)
Sep  6 08:49:34.442: INFO: (18) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 13.266925ms)
Sep  6 08:49:34.442: INFO: (18) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 12.312241ms)
Sep  6 08:49:34.443: INFO: (18) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 13.595497ms)
Sep  6 08:49:34.443: INFO: (18) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 13.029174ms)
Sep  6 08:49:34.443: INFO: (18) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 13.151449ms)
Sep  6 08:49:34.447: INFO: (18) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 18.452112ms)
Sep  6 08:49:34.461: INFO: (18) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 31.479047ms)
Sep  6 08:49:34.461: INFO: (18) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 31.871798ms)
Sep  6 08:49:34.461: INFO: (18) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 31.684854ms)
Sep  6 08:49:34.461: INFO: (18) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 32.001208ms)
Sep  6 08:49:34.471: INFO: (19) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:462/proxy/: tls qux (200; 9.746831ms)
Sep  6 08:49:34.478: INFO: (19) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname1/proxy/: foo (200; 16.998743ms)
Sep  6 08:49:34.479: INFO: (19) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname2/proxy/: tls qux (200; 16.916179ms)
Sep  6 08:49:34.479: INFO: (19) /api/v1/namespaces/proxy-8542/services/proxy-service-zjk2m:portname2/proxy/: bar (200; 17.097485ms)
Sep  6 08:49:34.479: INFO: (19) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname2/proxy/: bar (200; 16.748899ms)
Sep  6 08:49:34.479: INFO: (19) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:460/proxy/: tls baz (200; 17.29265ms)
Sep  6 08:49:34.479: INFO: (19) /api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/https:proxy-service-zjk2m-67x89:443/proxy/tlsrewritem... (200; 17.499238ms)
Sep  6 08:49:34.479: INFO: (19) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:162/proxy/: bar (200; 16.857167ms)
Sep  6 08:49:34.479: INFO: (19) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:162/proxy/: bar (200; 17.131052ms)
Sep  6 08:49:34.479: INFO: (19) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:1080/proxy/rewriteme">test<... (200; 17.330318ms)
Sep  6 08:49:34.479: INFO: (19) /api/v1/namespaces/proxy-8542/services/https:proxy-service-zjk2m:tlsportname1/proxy/: tls baz (200; 16.969763ms)
Sep  6 08:49:34.479: INFO: (19) /api/v1/namespaces/proxy-8542/services/http:proxy-service-zjk2m:portname1/proxy/: foo (200; 17.278942ms)
Sep  6 08:49:34.480: INFO: (19) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89/proxy/rewriteme">test</a> (200; 17.854729ms)
Sep  6 08:49:34.480: INFO: (19) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:160/proxy/: foo (200; 18.108168ms)
Sep  6 08:49:34.480: INFO: (19) /api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/: <a href="/api/v1/namespaces/proxy-8542/pods/http:proxy-service-zjk2m-67x89:1080/proxy/rewriteme">... (200; 17.974327ms)
Sep  6 08:49:34.481: INFO: (19) /api/v1/namespaces/proxy-8542/pods/proxy-service-zjk2m-67x89:160/proxy/: foo (200; 19.112976ms)
STEP: deleting ReplicationController proxy-service-zjk2m in namespace proxy-8542, will wait for the garbage collector to delete the pods
Sep  6 08:49:34.582: INFO: Deleting ReplicationController proxy-service-zjk2m took: 45.540284ms
Sep  6 08:49:35.082: INFO: Terminating ReplicationController proxy-service-zjk2m pods took: 500.215461ms
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:49:40.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8542" for this suite.
Sep  6 08:49:46.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:49:46.436: INFO: namespace proxy-8542 deletion completed in 6.345079556s

• [SLOW TEST:21.616 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:49:46.439: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7488
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 08:49:46.687: INFO: Waiting up to 5m0s for pod "downwardapi-volume-474631c5-d083-11e9-b48b-b617478434fe" in namespace "projected-7488" to be "success or failure"
Sep  6 08:49:46.694: INFO: Pod "downwardapi-volume-474631c5-d083-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.176017ms
Sep  6 08:49:48.703: INFO: Pod "downwardapi-volume-474631c5-d083-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01566345s
Sep  6 08:49:50.709: INFO: Pod "downwardapi-volume-474631c5-d083-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021569527s
STEP: Saw pod success
Sep  6 08:49:50.709: INFO: Pod "downwardapi-volume-474631c5-d083-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:49:50.714: INFO: Trying to get logs from node kube-node-20-104 pod downwardapi-volume-474631c5-d083-11e9-b48b-b617478434fe container client-container: <nil>
STEP: delete the pod
Sep  6 08:49:50.760: INFO: Waiting for pod downwardapi-volume-474631c5-d083-11e9-b48b-b617478434fe to disappear
Sep  6 08:49:50.766: INFO: Pod downwardapi-volume-474631c5-d083-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:49:50.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7488" for this suite.
Sep  6 08:49:56.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:49:56.980: INFO: namespace projected-7488 deletion completed in 6.205847379s

• [SLOW TEST:10.541 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:49:56.980: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5528
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep  6 08:49:57.172: INFO: Waiting up to 5m0s for pod "downward-api-4d8854bb-d083-11e9-b48b-b617478434fe" in namespace "downward-api-5528" to be "success or failure"
Sep  6 08:49:57.189: INFO: Pod "downward-api-4d8854bb-d083-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 17.162441ms
Sep  6 08:49:59.196: INFO: Pod "downward-api-4d8854bb-d083-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023438854s
STEP: Saw pod success
Sep  6 08:49:59.196: INFO: Pod "downward-api-4d8854bb-d083-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:49:59.201: INFO: Trying to get logs from node kube-node-20-104 pod downward-api-4d8854bb-d083-11e9-b48b-b617478434fe container dapi-container: <nil>
STEP: delete the pod
Sep  6 08:49:59.237: INFO: Waiting for pod downward-api-4d8854bb-d083-11e9-b48b-b617478434fe to disappear
Sep  6 08:49:59.241: INFO: Pod downward-api-4d8854bb-d083-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:49:59.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5528" for this suite.
Sep  6 08:50:07.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:50:07.433: INFO: namespace downward-api-5528 deletion completed in 8.185366242s

• [SLOW TEST:10.453 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:50:07.433: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3714
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep  6 08:50:12.716: INFO: Successfully updated pod "labelsupdate53f847ce-d083-11e9-b48b-b617478434fe"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:50:14.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3714" for this suite.
Sep  6 08:50:36.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:50:37.001: INFO: namespace projected-3714 deletion completed in 22.238019006s

• [SLOW TEST:29.568 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:50:37.002: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7014
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-65665fee-d083-11e9-b48b-b617478434fe
STEP: Creating secret with name s-test-opt-upd-656660f8-d083-11e9-b48b-b617478434fe
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-65665fee-d083-11e9-b48b-b617478434fe
STEP: Updating secret s-test-opt-upd-656660f8-d083-11e9-b48b-b617478434fe
STEP: Creating secret with name s-test-opt-create-656661db-d083-11e9-b48b-b617478434fe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:50:45.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7014" for this suite.
Sep  6 08:51:07.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:51:07.585: INFO: namespace projected-7014 deletion completed in 22.167255951s

• [SLOW TEST:30.583 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:51:07.586: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7579
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7579
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Sep  6 08:51:07.782: INFO: Found 0 stateful pods, waiting for 3
Sep  6 08:51:17.791: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 08:51:17.791: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 08:51:17.791: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 08:51:17.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-7579 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 08:51:18.312: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 08:51:18.312: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 08:51:18.312: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep  6 08:51:28.378: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep  6 08:51:38.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-7579 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 08:51:38.783: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 08:51:38.783: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 08:51:38.783: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 08:51:48.820: INFO: Waiting for StatefulSet statefulset-7579/ss2 to complete update
Sep  6 08:51:48.820: INFO: Waiting for Pod statefulset-7579/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Sep  6 08:51:48.820: INFO: Waiting for Pod statefulset-7579/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Sep  6 08:51:58.842: INFO: Waiting for StatefulSet statefulset-7579/ss2 to complete update
Sep  6 08:51:58.842: INFO: Waiting for Pod statefulset-7579/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Sep  6 08:51:58.842: INFO: Waiting for Pod statefulset-7579/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Sep  6 08:52:08.833: INFO: Waiting for StatefulSet statefulset-7579/ss2 to complete update
Sep  6 08:52:08.833: INFO: Waiting for Pod statefulset-7579/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Sep  6 08:52:18.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-7579 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 08:52:19.305: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 08:52:19.305: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 08:52:19.305: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 08:52:29.365: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep  6 08:52:39.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-7579 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 08:52:39.797: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 08:52:39.797: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 08:52:39.797: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 08:52:59.836: INFO: Waiting for StatefulSet statefulset-7579/ss2 to complete update
Sep  6 08:52:59.836: INFO: Waiting for Pod statefulset-7579/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep  6 08:53:09.850: INFO: Deleting all statefulset in ns statefulset-7579
Sep  6 08:53:09.855: INFO: Scaling statefulset ss2 to 0
Sep  6 08:53:39.887: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 08:53:39.900: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:53:39.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7579" for this suite.
Sep  6 08:53:47.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:53:48.284: INFO: namespace statefulset-7579 deletion completed in 8.325883175s

• [SLOW TEST:160.699 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:53:48.285: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3468
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-d7690bf8-d083-11e9-b48b-b617478434fe
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-d7690bf8-d083-11e9-b48b-b617478434fe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:55:23.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3468" for this suite.
Sep  6 08:55:45.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:55:45.527: INFO: namespace projected-3468 deletion completed in 22.167314312s

• [SLOW TEST:117.242 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:55:45.529: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6393
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-6393
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6393 to expose endpoints map[]
Sep  6 08:55:45.719: INFO: Get endpoints failed (5.721085ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Sep  6 08:55:46.726: INFO: successfully validated that service endpoint-test2 in namespace services-6393 exposes endpoints map[] (1.012948113s elapsed)
STEP: Creating pod pod1 in namespace services-6393
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6393 to expose endpoints map[pod1:[80]]
Sep  6 08:55:49.792: INFO: successfully validated that service endpoint-test2 in namespace services-6393 exposes endpoints map[pod1:[80]] (3.049343974s elapsed)
STEP: Creating pod pod2 in namespace services-6393
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6393 to expose endpoints map[pod1:[80] pod2:[80]]
Sep  6 08:55:51.852: INFO: successfully validated that service endpoint-test2 in namespace services-6393 exposes endpoints map[pod1:[80] pod2:[80]] (2.0507997s elapsed)
STEP: Deleting pod pod1 in namespace services-6393
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6393 to expose endpoints map[pod2:[80]]
Sep  6 08:55:52.884: INFO: successfully validated that service endpoint-test2 in namespace services-6393 exposes endpoints map[pod2:[80]] (1.023900421s elapsed)
STEP: Deleting pod pod2 in namespace services-6393
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6393 to expose endpoints map[]
Sep  6 08:55:52.901: INFO: successfully validated that service endpoint-test2 in namespace services-6393 exposes endpoints map[] (5.38832ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:55:52.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6393" for this suite.
Sep  6 08:56:14.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:56:15.139: INFO: namespace services-6393 deletion completed in 22.190454811s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:29.610 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:56:15.139: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8582
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 08:56:15.337: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2eee8e47-d084-11e9-b48b-b617478434fe" in namespace "downward-api-8582" to be "success or failure"
Sep  6 08:56:15.344: INFO: Pod "downwardapi-volume-2eee8e47-d084-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.457316ms
Sep  6 08:56:17.352: INFO: Pod "downwardapi-volume-2eee8e47-d084-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014918953s
Sep  6 08:56:19.362: INFO: Pod "downwardapi-volume-2eee8e47-d084-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024626147s
STEP: Saw pod success
Sep  6 08:56:19.362: INFO: Pod "downwardapi-volume-2eee8e47-d084-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:56:19.368: INFO: Trying to get logs from node kube-node-20-104 pod downwardapi-volume-2eee8e47-d084-11e9-b48b-b617478434fe container client-container: <nil>
STEP: delete the pod
Sep  6 08:56:19.406: INFO: Waiting for pod downwardapi-volume-2eee8e47-d084-11e9-b48b-b617478434fe to disappear
Sep  6 08:56:19.411: INFO: Pod downwardapi-volume-2eee8e47-d084-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:56:19.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8582" for this suite.
Sep  6 08:56:25.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:56:25.597: INFO: namespace downward-api-8582 deletion completed in 6.177717162s

• [SLOW TEST:10.458 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:56:25.598: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2800
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Sep  6 08:56:25.763: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Sep  6 08:56:25.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 create -f - --namespace=kubectl-2800'
Sep  6 08:56:26.521: INFO: stderr: ""
Sep  6 08:56:26.522: INFO: stdout: "service/redis-slave created\n"
Sep  6 08:56:26.522: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Sep  6 08:56:26.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 create -f - --namespace=kubectl-2800'
Sep  6 08:56:26.870: INFO: stderr: ""
Sep  6 08:56:26.870: INFO: stdout: "service/redis-master created\n"
Sep  6 08:56:26.870: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep  6 08:56:26.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 create -f - --namespace=kubectl-2800'
Sep  6 08:56:27.183: INFO: stderr: ""
Sep  6 08:56:27.183: INFO: stdout: "service/frontend created\n"
Sep  6 08:56:27.184: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Sep  6 08:56:27.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 create -f - --namespace=kubectl-2800'
Sep  6 08:56:27.502: INFO: stderr: ""
Sep  6 08:56:27.502: INFO: stdout: "deployment.apps/frontend created\n"
Sep  6 08:56:27.502: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  6 08:56:27.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 create -f - --namespace=kubectl-2800'
Sep  6 08:56:27.838: INFO: stderr: ""
Sep  6 08:56:27.838: INFO: stdout: "deployment.apps/redis-master created\n"
Sep  6 08:56:27.838: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Sep  6 08:56:27.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 create -f - --namespace=kubectl-2800'
Sep  6 08:56:28.283: INFO: stderr: ""
Sep  6 08:56:28.283: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Sep  6 08:56:28.283: INFO: Waiting for all frontend pods to be Running.
Sep  6 08:56:33.335: INFO: Waiting for frontend to serve content.
Sep  6 08:56:33.365: INFO: Trying to add a new entry to the guestbook.
Sep  6 08:56:33.390: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Sep  6 08:56:33.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 delete --grace-period=0 --force -f - --namespace=kubectl-2800'
Sep  6 08:56:33.575: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 08:56:33.575: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 08:56:33.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 delete --grace-period=0 --force -f - --namespace=kubectl-2800'
Sep  6 08:56:33.723: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 08:56:33.723: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 08:56:33.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 delete --grace-period=0 --force -f - --namespace=kubectl-2800'
Sep  6 08:56:33.861: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 08:56:33.861: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 08:56:33.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 delete --grace-period=0 --force -f - --namespace=kubectl-2800'
Sep  6 08:56:33.978: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 08:56:33.978: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 08:56:33.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 delete --grace-period=0 --force -f - --namespace=kubectl-2800'
Sep  6 08:56:34.094: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 08:56:34.094: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 08:56:34.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 delete --grace-period=0 --force -f - --namespace=kubectl-2800'
Sep  6 08:56:34.195: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 08:56:34.195: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:56:34.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2800" for this suite.
Sep  6 08:57:14.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:57:14.423: INFO: namespace kubectl-2800 deletion completed in 40.219656797s

• [SLOW TEST:48.825 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:57:14.424: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3770
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  6 08:57:14.616: INFO: Waiting up to 5m0s for pod "pod-5244b122-d084-11e9-b48b-b617478434fe" in namespace "emptydir-3770" to be "success or failure"
Sep  6 08:57:14.622: INFO: Pod "pod-5244b122-d084-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.122982ms
Sep  6 08:57:16.628: INFO: Pod "pod-5244b122-d084-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012466762s
STEP: Saw pod success
Sep  6 08:57:16.628: INFO: Pod "pod-5244b122-d084-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:57:16.634: INFO: Trying to get logs from node kube-node-20-104 pod pod-5244b122-d084-11e9-b48b-b617478434fe container test-container: <nil>
STEP: delete the pod
Sep  6 08:57:16.674: INFO: Waiting for pod pod-5244b122-d084-11e9-b48b-b617478434fe to disappear
Sep  6 08:57:16.681: INFO: Pod pod-5244b122-d084-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:57:16.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3770" for this suite.
Sep  6 08:57:22.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:57:22.885: INFO: namespace emptydir-3770 deletion completed in 6.194916273s

• [SLOW TEST:8.462 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:57:22.886: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1792
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:57:25.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1792" for this suite.
Sep  6 08:57:31.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:57:31.335: INFO: namespace emptydir-wrapper-1792 deletion completed in 6.178474582s

• [SLOW TEST:8.449 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:57:31.335: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1858
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep  6 08:57:31.519: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:57:35.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1858" for this suite.
Sep  6 08:57:41.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:57:41.688: INFO: namespace init-container-1858 deletion completed in 6.16658514s

• [SLOW TEST:10.353 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:57:41.689: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3141
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:57:43.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3141" for this suite.
Sep  6 08:58:35.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:58:36.090: INFO: namespace kubelet-test-3141 deletion completed in 52.181457045s

• [SLOW TEST:54.402 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:58:36.092: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5505
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 08:58:36.277: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep  6 08:58:41.284: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  6 08:58:41.284: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep  6 08:58:43.291: INFO: Creating deployment "test-rollover-deployment"
Sep  6 08:58:43.305: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep  6 08:58:45.316: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep  6 08:58:45.325: INFO: Ensure that both replica sets have 1 created replica
Sep  6 08:58:45.332: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep  6 08:58:45.344: INFO: Updating deployment test-rollover-deployment
Sep  6 08:58:45.344: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep  6 08:58:47.359: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep  6 08:58:47.371: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep  6 08:58:47.380: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 08:58:47.380: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357123, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357123, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357125, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357123, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 08:58:49.392: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 08:58:49.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357123, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357123, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357127, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357123, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 08:58:51.403: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 08:58:51.403: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357123, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357123, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357127, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357123, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 08:58:53.392: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 08:58:53.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357123, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357123, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357127, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357123, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 08:58:55.391: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 08:58:55.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357123, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357123, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357127, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357123, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 08:58:57.393: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 08:58:57.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357123, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357123, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357127, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357123, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 08:58:59.399: INFO: 
Sep  6 08:58:59.399: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep  6 08:58:59.415: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-5505,SelfLink:/apis/apps/v1/namespaces/deployment-5505/deployments/test-rollover-deployment,UID:872216c4-d084-11e9-b16e-525400186f34,ResourceVersion:855575,Generation:2,CreationTimestamp:2019-09-06 08:58:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-06 08:58:43 +0000 UTC 2019-09-06 08:58:43 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-06 08:58:57 +0000 UTC 2019-09-06 08:58:43 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 08:58:59.420: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-5505,SelfLink:/apis/apps/v1/namespaces/deployment-5505/replicasets/test-rollover-deployment-766b4d6c9d,UID:885b5e5b-d084-11e9-b16e-525400186f34,ResourceVersion:855568,Generation:2,CreationTimestamp:2019-09-06 08:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 872216c4-d084-11e9-b16e-525400186f34 0xc00294c527 0xc00294c528}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep  6 08:58:59.420: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep  6 08:58:59.420: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-5505,SelfLink:/apis/apps/v1/namespaces/deployment-5505/replicasets/test-rollover-controller,UID:82f1f88e-d084-11e9-b16e-525400186f34,ResourceVersion:855574,Generation:2,CreationTimestamp:2019-09-06 08:58:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 872216c4-d084-11e9-b16e-525400186f34 0xc00294c377 0xc00294c378}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 08:58:59.421: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-5505,SelfLink:/apis/apps/v1/namespaces/deployment-5505/replicasets/test-rollover-deployment-6455657675,UID:8725c785-d084-11e9-b16e-525400186f34,ResourceVersion:855536,Generation:2,CreationTimestamp:2019-09-06 08:58:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 872216c4-d084-11e9-b16e-525400186f34 0xc00294c447 0xc00294c448}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 08:58:59.426: INFO: Pod "test-rollover-deployment-766b4d6c9d-smg6c" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-smg6c,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-5505,SelfLink:/api/v1/namespaces/deployment-5505/pods/test-rollover-deployment-766b4d6c9d-smg6c,UID:88611c75-d084-11e9-b16e-525400186f34,ResourceVersion:855543,Generation:0,CreationTimestamp:2019-09-06 08:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.67.39/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d 885b5e5b-d084-11e9-b16e-525400186f34 0xc00294d0a7 0xc00294d0a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-265vk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-265vk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-265vk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-20-104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:58:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:58:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:58:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 08:58:45 +0000 UTC  }],Message:,Reason:,HostIP:192.168.20.104,PodIP:192.168.67.39,StartTime:2019-09-06 08:58:45 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-06 08:58:46 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://0d2530d2a0d1e5c25681ede188fa0d51d9608d0851c79b69f448e385a4835bde}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:58:59.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5505" for this suite.
Sep  6 08:59:05.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:59:05.626: INFO: namespace deployment-5505 deletion completed in 6.192704169s

• [SLOW TEST:29.534 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:59:05.627: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8409
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-948acd7c-d084-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume configMaps
Sep  6 08:59:05.807: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-948bbcf3-d084-11e9-b48b-b617478434fe" in namespace "projected-8409" to be "success or failure"
Sep  6 08:59:05.812: INFO: Pod "pod-projected-configmaps-948bbcf3-d084-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.281027ms
Sep  6 08:59:07.822: INFO: Pod "pod-projected-configmaps-948bbcf3-d084-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015655295s
STEP: Saw pod success
Sep  6 08:59:07.823: INFO: Pod "pod-projected-configmaps-948bbcf3-d084-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:59:07.828: INFO: Trying to get logs from node kube-node-20-104 pod pod-projected-configmaps-948bbcf3-d084-11e9-b48b-b617478434fe container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 08:59:07.865: INFO: Waiting for pod pod-projected-configmaps-948bbcf3-d084-11e9-b48b-b617478434fe to disappear
Sep  6 08:59:07.869: INFO: Pod pod-projected-configmaps-948bbcf3-d084-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:59:07.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8409" for this suite.
Sep  6 08:59:13.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:59:14.060: INFO: namespace projected-8409 deletion completed in 6.184754542s

• [SLOW TEST:8.434 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:59:14.061: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3095
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-999295bc-d084-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume secrets
Sep  6 08:59:14.251: INFO: Waiting up to 5m0s for pod "pod-secrets-9993d33d-d084-11e9-b48b-b617478434fe" in namespace "secrets-3095" to be "success or failure"
Sep  6 08:59:14.255: INFO: Pod "pod-secrets-9993d33d-d084-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.68459ms
Sep  6 08:59:16.264: INFO: Pod "pod-secrets-9993d33d-d084-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013191773s
STEP: Saw pod success
Sep  6 08:59:16.264: INFO: Pod "pod-secrets-9993d33d-d084-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:59:16.269: INFO: Trying to get logs from node kube-node-20-104 pod pod-secrets-9993d33d-d084-11e9-b48b-b617478434fe container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 08:59:16.301: INFO: Waiting for pod pod-secrets-9993d33d-d084-11e9-b48b-b617478434fe to disappear
Sep  6 08:59:16.306: INFO: Pod pod-secrets-9993d33d-d084-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:59:16.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3095" for this suite.
Sep  6 08:59:22.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:59:22.525: INFO: namespace secrets-3095 deletion completed in 6.195430916s

• [SLOW TEST:8.464 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:59:22.525: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4753
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  6 08:59:22.707: INFO: Waiting up to 5m0s for pod "pod-9e9e1a88-d084-11e9-b48b-b617478434fe" in namespace "emptydir-4753" to be "success or failure"
Sep  6 08:59:22.716: INFO: Pod "pod-9e9e1a88-d084-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 9.239356ms
Sep  6 08:59:24.723: INFO: Pod "pod-9e9e1a88-d084-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01570005s
STEP: Saw pod success
Sep  6 08:59:24.723: INFO: Pod "pod-9e9e1a88-d084-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 08:59:24.729: INFO: Trying to get logs from node kube-node-20-104 pod pod-9e9e1a88-d084-11e9-b48b-b617478434fe container test-container: <nil>
STEP: delete the pod
Sep  6 08:59:24.763: INFO: Waiting for pod pod-9e9e1a88-d084-11e9-b48b-b617478434fe to disappear
Sep  6 08:59:24.768: INFO: Pod pod-9e9e1a88-d084-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:59:24.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4753" for this suite.
Sep  6 08:59:30.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:59:30.962: INFO: namespace emptydir-4753 deletion completed in 6.185989892s

• [SLOW TEST:8.437 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:59:30.962: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7356
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Sep  6 08:59:31.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 api-versions'
Sep  6 08:59:31.340: INFO: stderr: ""
Sep  6 08:59:31.340: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nloadbalance.caicloud.io/v1alpha2\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nresource.caicloud.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 08:59:31.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7356" for this suite.
Sep  6 08:59:37.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 08:59:37.518: INFO: namespace kubectl-7356 deletion completed in 6.168974446s

• [SLOW TEST:6.555 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 08:59:37.519: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1997
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:00:02.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1997" for this suite.
Sep  6 09:00:10.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:00:10.276: INFO: namespace container-runtime-1997 deletion completed in 8.212349803s

• [SLOW TEST:32.757 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:00:10.276: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7154
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:00:10.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7154" for this suite.
Sep  6 09:00:32.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:00:32.690: INFO: namespace pods-7154 deletion completed in 22.182043794s

• [SLOW TEST:22.414 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:00:32.691: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8135
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 09:00:32.855: INFO: Creating ReplicaSet my-hostname-basic-c87012b6-d084-11e9-b48b-b617478434fe
Sep  6 09:00:32.871: INFO: Pod name my-hostname-basic-c87012b6-d084-11e9-b48b-b617478434fe: Found 0 pods out of 1
Sep  6 09:00:37.880: INFO: Pod name my-hostname-basic-c87012b6-d084-11e9-b48b-b617478434fe: Found 1 pods out of 1
Sep  6 09:00:37.880: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-c87012b6-d084-11e9-b48b-b617478434fe" is running
Sep  6 09:00:37.885: INFO: Pod "my-hostname-basic-c87012b6-d084-11e9-b48b-b617478434fe-gcfsf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 09:00:32 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 09:00:35 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 09:00:35 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 09:00:32 +0000 UTC Reason: Message:}])
Sep  6 09:00:37.885: INFO: Trying to dial the pod
Sep  6 09:00:42.907: INFO: Controller my-hostname-basic-c87012b6-d084-11e9-b48b-b617478434fe: Got expected result from replica 1 [my-hostname-basic-c87012b6-d084-11e9-b48b-b617478434fe-gcfsf]: "my-hostname-basic-c87012b6-d084-11e9-b48b-b617478434fe-gcfsf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:00:42.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8135" for this suite.
Sep  6 09:00:48.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:00:49.102: INFO: namespace replicaset-8135 deletion completed in 6.188126876s

• [SLOW TEST:16.411 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:00:49.103: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5857
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  6 09:00:53.881: INFO: Successfully updated pod "pod-update-d23eb458-d084-11e9-b48b-b617478434fe"
STEP: verifying the updated pod is in kubernetes
Sep  6 09:00:53.897: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:00:53.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5857" for this suite.
Sep  6 09:01:15.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:01:16.100: INFO: namespace pods-5857 deletion completed in 22.193369745s

• [SLOW TEST:26.997 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:01:16.101: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3226
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-e2513403-d084-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume secrets
Sep  6 09:01:16.301: INFO: Waiting up to 5m0s for pod "pod-secrets-e2525f10-d084-11e9-b48b-b617478434fe" in namespace "secrets-3226" to be "success or failure"
Sep  6 09:01:16.307: INFO: Pod "pod-secrets-e2525f10-d084-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.839308ms
Sep  6 09:01:18.312: INFO: Pod "pod-secrets-e2525f10-d084-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011145678s
STEP: Saw pod success
Sep  6 09:01:18.312: INFO: Pod "pod-secrets-e2525f10-d084-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:01:18.317: INFO: Trying to get logs from node kube-node-20-104 pod pod-secrets-e2525f10-d084-11e9-b48b-b617478434fe container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 09:01:18.349: INFO: Waiting for pod pod-secrets-e2525f10-d084-11e9-b48b-b617478434fe to disappear
Sep  6 09:01:18.354: INFO: Pod pod-secrets-e2525f10-d084-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:01:18.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3226" for this suite.
Sep  6 09:01:24.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:01:24.541: INFO: namespace secrets-3226 deletion completed in 6.176214683s

• [SLOW TEST:8.440 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:01:24.541: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3748
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3748.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3748.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3748.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3748.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3748.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3748.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3748.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3748.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3748.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3748.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3748.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3748.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3748.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 98.79.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.79.98_udp@PTR;check="$$(dig +tcp +noall +answer +search 98.79.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.79.98_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3748.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3748.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3748.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3748.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3748.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3748.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3748.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3748.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3748.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3748.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3748.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3748.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3748.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 98.79.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.79.98_udp@PTR;check="$$(dig +tcp +noall +answer +search 98.79.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.79.98_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 09:01:28.790: INFO: Unable to read wheezy_udp@dns-test-service.dns-3748.svc.cluster.local from pod dns-3748/dns-test-e75d43b4-d084-11e9-b48b-b617478434fe: the server could not find the requested resource (get pods dns-test-e75d43b4-d084-11e9-b48b-b617478434fe)
Sep  6 09:01:28.797: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3748.svc.cluster.local from pod dns-3748/dns-test-e75d43b4-d084-11e9-b48b-b617478434fe: the server could not find the requested resource (get pods dns-test-e75d43b4-d084-11e9-b48b-b617478434fe)
Sep  6 09:01:28.804: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3748.svc.cluster.local from pod dns-3748/dns-test-e75d43b4-d084-11e9-b48b-b617478434fe: the server could not find the requested resource (get pods dns-test-e75d43b4-d084-11e9-b48b-b617478434fe)
Sep  6 09:01:28.813: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3748.svc.cluster.local from pod dns-3748/dns-test-e75d43b4-d084-11e9-b48b-b617478434fe: the server could not find the requested resource (get pods dns-test-e75d43b4-d084-11e9-b48b-b617478434fe)
Sep  6 09:01:28.874: INFO: Unable to read jessie_udp@dns-test-service.dns-3748.svc.cluster.local from pod dns-3748/dns-test-e75d43b4-d084-11e9-b48b-b617478434fe: the server could not find the requested resource (get pods dns-test-e75d43b4-d084-11e9-b48b-b617478434fe)
Sep  6 09:01:28.881: INFO: Unable to read jessie_tcp@dns-test-service.dns-3748.svc.cluster.local from pod dns-3748/dns-test-e75d43b4-d084-11e9-b48b-b617478434fe: the server could not find the requested resource (get pods dns-test-e75d43b4-d084-11e9-b48b-b617478434fe)
Sep  6 09:01:28.888: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3748.svc.cluster.local from pod dns-3748/dns-test-e75d43b4-d084-11e9-b48b-b617478434fe: the server could not find the requested resource (get pods dns-test-e75d43b4-d084-11e9-b48b-b617478434fe)
Sep  6 09:01:28.896: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3748.svc.cluster.local from pod dns-3748/dns-test-e75d43b4-d084-11e9-b48b-b617478434fe: the server could not find the requested resource (get pods dns-test-e75d43b4-d084-11e9-b48b-b617478434fe)
Sep  6 09:01:28.930: INFO: Lookups using dns-3748/dns-test-e75d43b4-d084-11e9-b48b-b617478434fe failed for: [wheezy_udp@dns-test-service.dns-3748.svc.cluster.local wheezy_tcp@dns-test-service.dns-3748.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3748.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3748.svc.cluster.local jessie_udp@dns-test-service.dns-3748.svc.cluster.local jessie_tcp@dns-test-service.dns-3748.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3748.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3748.svc.cluster.local]

Sep  6 09:01:34.041: INFO: DNS probes using dns-3748/dns-test-e75d43b4-d084-11e9-b48b-b617478434fe succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:01:34.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3748" for this suite.
Sep  6 09:01:40.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:01:40.383: INFO: namespace dns-3748 deletion completed in 6.208908894s

• [SLOW TEST:15.842 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:01:40.383: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-3555
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep  6 09:01:40.895: INFO: Pod name wrapped-volume-race-f0f95afd-d084-11e9-b48b-b617478434fe: Found 0 pods out of 5
Sep  6 09:01:45.906: INFO: Pod name wrapped-volume-race-f0f95afd-d084-11e9-b48b-b617478434fe: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f0f95afd-d084-11e9-b48b-b617478434fe in namespace emptydir-wrapper-3555, will wait for the garbage collector to delete the pods
Sep  6 09:01:58.036: INFO: Deleting ReplicationController wrapped-volume-race-f0f95afd-d084-11e9-b48b-b617478434fe took: 18.448474ms
Sep  6 09:01:58.536: INFO: Terminating ReplicationController wrapped-volume-race-f0f95afd-d084-11e9-b48b-b617478434fe pods took: 500.405162ms
STEP: Creating RC which spawns configmap-volume pods
Sep  6 09:02:35.070: INFO: Pod name wrapped-volume-race-11436a85-d085-11e9-b48b-b617478434fe: Found 0 pods out of 5
Sep  6 09:02:40.082: INFO: Pod name wrapped-volume-race-11436a85-d085-11e9-b48b-b617478434fe: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-11436a85-d085-11e9-b48b-b617478434fe in namespace emptydir-wrapper-3555, will wait for the garbage collector to delete the pods
Sep  6 09:02:52.210: INFO: Deleting ReplicationController wrapped-volume-race-11436a85-d085-11e9-b48b-b617478434fe took: 14.955287ms
Sep  6 09:02:52.711: INFO: Terminating ReplicationController wrapped-volume-race-11436a85-d085-11e9-b48b-b617478434fe pods took: 500.50348ms
STEP: Creating RC which spawns configmap-volume pods
Sep  6 09:03:34.756: INFO: Pod name wrapped-volume-race-34d50bd8-d085-11e9-b48b-b617478434fe: Found 0 pods out of 5
Sep  6 09:03:39.768: INFO: Pod name wrapped-volume-race-34d50bd8-d085-11e9-b48b-b617478434fe: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-34d50bd8-d085-11e9-b48b-b617478434fe in namespace emptydir-wrapper-3555, will wait for the garbage collector to delete the pods
Sep  6 09:03:51.887: INFO: Deleting ReplicationController wrapped-volume-race-34d50bd8-d085-11e9-b48b-b617478434fe took: 19.541199ms
Sep  6 09:03:52.388: INFO: Terminating ReplicationController wrapped-volume-race-34d50bd8-d085-11e9-b48b-b617478434fe pods took: 500.301745ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:04:35.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3555" for this suite.
Sep  6 09:04:43.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:04:43.563: INFO: namespace emptydir-wrapper-3555 deletion completed in 8.211368445s

• [SLOW TEST:183.180 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:04:43.564: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-316
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-qm7h
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 09:04:43.768: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-qm7h" in namespace "subpath-316" to be "success or failure"
Sep  6 09:04:43.773: INFO: Pod "pod-subpath-test-projected-qm7h": Phase="Pending", Reason="", readiness=false. Elapsed: 5.169336ms
Sep  6 09:04:45.781: INFO: Pod "pod-subpath-test-projected-qm7h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013165969s
Sep  6 09:04:47.790: INFO: Pod "pod-subpath-test-projected-qm7h": Phase="Running", Reason="", readiness=true. Elapsed: 4.021543785s
Sep  6 09:04:49.796: INFO: Pod "pod-subpath-test-projected-qm7h": Phase="Running", Reason="", readiness=true. Elapsed: 6.028417747s
Sep  6 09:04:51.802: INFO: Pod "pod-subpath-test-projected-qm7h": Phase="Running", Reason="", readiness=true. Elapsed: 8.034102977s
Sep  6 09:04:53.808: INFO: Pod "pod-subpath-test-projected-qm7h": Phase="Running", Reason="", readiness=true. Elapsed: 10.039709864s
Sep  6 09:04:55.814: INFO: Pod "pod-subpath-test-projected-qm7h": Phase="Running", Reason="", readiness=true. Elapsed: 12.046459441s
Sep  6 09:04:57.821: INFO: Pod "pod-subpath-test-projected-qm7h": Phase="Running", Reason="", readiness=true. Elapsed: 14.052711575s
Sep  6 09:04:59.828: INFO: Pod "pod-subpath-test-projected-qm7h": Phase="Running", Reason="", readiness=true. Elapsed: 16.059776389s
Sep  6 09:05:01.836: INFO: Pod "pod-subpath-test-projected-qm7h": Phase="Running", Reason="", readiness=true. Elapsed: 18.068390891s
Sep  6 09:05:03.843: INFO: Pod "pod-subpath-test-projected-qm7h": Phase="Running", Reason="", readiness=true. Elapsed: 20.074512335s
Sep  6 09:05:05.851: INFO: Pod "pod-subpath-test-projected-qm7h": Phase="Running", Reason="", readiness=true. Elapsed: 22.082956512s
Sep  6 09:05:07.859: INFO: Pod "pod-subpath-test-projected-qm7h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.091063224s
STEP: Saw pod success
Sep  6 09:05:07.859: INFO: Pod "pod-subpath-test-projected-qm7h" satisfied condition "success or failure"
Sep  6 09:05:07.866: INFO: Trying to get logs from node kube-node-20-104 pod pod-subpath-test-projected-qm7h container test-container-subpath-projected-qm7h: <nil>
STEP: delete the pod
Sep  6 09:05:08.048: INFO: Waiting for pod pod-subpath-test-projected-qm7h to disappear
Sep  6 09:05:08.054: INFO: Pod pod-subpath-test-projected-qm7h no longer exists
STEP: Deleting pod pod-subpath-test-projected-qm7h
Sep  6 09:05:08.054: INFO: Deleting pod "pod-subpath-test-projected-qm7h" in namespace "subpath-316"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:05:08.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-316" for this suite.
Sep  6 09:05:14.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:05:14.241: INFO: namespace subpath-316 deletion completed in 6.172421925s

• [SLOW TEST:30.678 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:05:14.242: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8797
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 09:05:14.405: INFO: Creating deployment "test-recreate-deployment"
Sep  6 09:05:14.414: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep  6 09:05:14.425: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep  6 09:05:16.444: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep  6 09:05:16.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357514, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357514, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357514, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357514, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 09:05:18.457: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep  6 09:05:18.470: INFO: Updating deployment test-recreate-deployment
Sep  6 09:05:18.470: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep  6 09:05:18.589: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-8797,SelfLink:/apis/apps/v1/namespaces/deployment-8797/deployments/test-recreate-deployment,UID:704161cc-d085-11e9-b16e-525400186f34,ResourceVersion:856914,Generation:2,CreationTimestamp:2019-09-06 09:05:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-09-06 09:05:18 +0000 UTC 2019-09-06 09:05:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-09-06 09:05:18 +0000 UTC 2019-09-06 09:05:14 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Sep  6 09:05:18.596: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-8797,SelfLink:/apis/apps/v1/namespaces/deployment-8797/replicasets/test-recreate-deployment-c9cbd8684,UID:72b69532-d085-11e9-b16e-525400186f34,ResourceVersion:856913,Generation:1,CreationTimestamp:2019-09-06 09:05:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 704161cc-d085-11e9-b16e-525400186f34 0xc002e68300 0xc002e68301}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 09:05:18.596: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep  6 09:05:18.596: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-8797,SelfLink:/apis/apps/v1/namespaces/deployment-8797/replicasets/test-recreate-deployment-7d57d5ff7c,UID:7042be96-d085-11e9-b16e-525400186f34,ResourceVersion:856907,Generation:2,CreationTimestamp:2019-09-06 09:05:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 704161cc-d085-11e9-b16e-525400186f34 0xc002e68237 0xc002e68238}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 09:05:18.602: INFO: Pod "test-recreate-deployment-c9cbd8684-qz842" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-qz842,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-8797,SelfLink:/api/v1/namespaces/deployment-8797/pods/test-recreate-deployment-c9cbd8684-qz842,UID:72b7ecf4-d085-11e9-b16e-525400186f34,ResourceVersion:856915,Generation:0,CreationTimestamp:2019-09-06 09:05:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 72b69532-d085-11e9-b16e-525400186f34 0xc002e68b40 0xc002e68b41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-n9v7t {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-n9v7t,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-n9v7t true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-20-104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:05:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:05:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:05:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:05:18 +0000 UTC  }],Message:,Reason:,HostIP:192.168.20.104,PodIP:,StartTime:2019-09-06 09:05:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:05:18.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8797" for this suite.
Sep  6 09:05:24.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:05:24.776: INFO: namespace deployment-8797 deletion completed in 6.166747258s

• [SLOW TEST:10.534 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:05:24.778: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7453
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-7453/secret-test-7687b584-d085-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume secrets
Sep  6 09:05:24.953: INFO: Waiting up to 5m0s for pod "pod-configmaps-7688f32e-d085-11e9-b48b-b617478434fe" in namespace "secrets-7453" to be "success or failure"
Sep  6 09:05:24.962: INFO: Pod "pod-configmaps-7688f32e-d085-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 9.148796ms
Sep  6 09:05:26.970: INFO: Pod "pod-configmaps-7688f32e-d085-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017374267s
STEP: Saw pod success
Sep  6 09:05:26.970: INFO: Pod "pod-configmaps-7688f32e-d085-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:05:26.976: INFO: Trying to get logs from node kube-node-20-104 pod pod-configmaps-7688f32e-d085-11e9-b48b-b617478434fe container env-test: <nil>
STEP: delete the pod
Sep  6 09:05:27.010: INFO: Waiting for pod pod-configmaps-7688f32e-d085-11e9-b48b-b617478434fe to disappear
Sep  6 09:05:27.014: INFO: Pod pod-configmaps-7688f32e-d085-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:05:27.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7453" for this suite.
Sep  6 09:05:33.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:05:33.200: INFO: namespace secrets-7453 deletion completed in 6.178258876s

• [SLOW TEST:8.422 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:05:33.201: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3866
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep  6 09:05:33.379: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:05:50.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3866" for this suite.
Sep  6 09:05:56.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:05:56.222: INFO: namespace pods-3866 deletion completed in 6.164681583s

• [SLOW TEST:23.021 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:05:56.223: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4239
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Sep  6 09:05:56.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 create -f - --namespace=kubectl-4239'
Sep  6 09:05:56.799: INFO: stderr: ""
Sep  6 09:05:56.800: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep  6 09:05:57.807: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 09:05:57.807: INFO: Found 0 / 1
Sep  6 09:05:58.807: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 09:05:58.807: INFO: Found 1 / 1
Sep  6 09:05:58.807: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep  6 09:05:58.814: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 09:05:58.814: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 09:05:58.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 patch pod redis-master-wr972 --namespace=kubectl-4239 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep  6 09:05:58.938: INFO: stderr: ""
Sep  6 09:05:58.938: INFO: stdout: "pod/redis-master-wr972 patched\n"
STEP: checking annotations
Sep  6 09:05:58.943: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 09:05:58.943: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:05:58.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4239" for this suite.
Sep  6 09:06:20.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:06:21.112: INFO: namespace kubectl-4239 deletion completed in 22.162926395s

• [SLOW TEST:24.889 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:06:21.113: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4032
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-982087e3-d085-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume configMaps
Sep  6 09:06:21.319: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9821723b-d085-11e9-b48b-b617478434fe" in namespace "projected-4032" to be "success or failure"
Sep  6 09:06:21.323: INFO: Pod "pod-projected-configmaps-9821723b-d085-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.348752ms
Sep  6 09:06:23.331: INFO: Pod "pod-projected-configmaps-9821723b-d085-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011812388s
STEP: Saw pod success
Sep  6 09:06:23.331: INFO: Pod "pod-projected-configmaps-9821723b-d085-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:06:23.338: INFO: Trying to get logs from node kube-node-20-104 pod pod-projected-configmaps-9821723b-d085-11e9-b48b-b617478434fe container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 09:06:23.383: INFO: Waiting for pod pod-projected-configmaps-9821723b-d085-11e9-b48b-b617478434fe to disappear
Sep  6 09:06:23.390: INFO: Pod pod-projected-configmaps-9821723b-d085-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:06:23.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4032" for this suite.
Sep  6 09:06:29.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:06:29.593: INFO: namespace projected-4032 deletion completed in 6.194186783s

• [SLOW TEST:8.481 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:06:29.594: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5241
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-lgw5
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 09:06:29.792: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lgw5" in namespace "subpath-5241" to be "success or failure"
Sep  6 09:06:29.800: INFO: Pod "pod-subpath-test-configmap-lgw5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.13736ms
Sep  6 09:06:31.807: INFO: Pod "pod-subpath-test-configmap-lgw5": Phase="Running", Reason="", readiness=true. Elapsed: 2.014759247s
Sep  6 09:06:33.814: INFO: Pod "pod-subpath-test-configmap-lgw5": Phase="Running", Reason="", readiness=true. Elapsed: 4.021916122s
Sep  6 09:06:35.820: INFO: Pod "pod-subpath-test-configmap-lgw5": Phase="Running", Reason="", readiness=true. Elapsed: 6.027666833s
Sep  6 09:06:37.829: INFO: Pod "pod-subpath-test-configmap-lgw5": Phase="Running", Reason="", readiness=true. Elapsed: 8.036868925s
Sep  6 09:06:39.838: INFO: Pod "pod-subpath-test-configmap-lgw5": Phase="Running", Reason="", readiness=true. Elapsed: 10.046303914s
Sep  6 09:06:41.848: INFO: Pod "pod-subpath-test-configmap-lgw5": Phase="Running", Reason="", readiness=true. Elapsed: 12.055868134s
Sep  6 09:06:43.857: INFO: Pod "pod-subpath-test-configmap-lgw5": Phase="Running", Reason="", readiness=true. Elapsed: 14.064567444s
Sep  6 09:06:45.866: INFO: Pod "pod-subpath-test-configmap-lgw5": Phase="Running", Reason="", readiness=true. Elapsed: 16.074178942s
Sep  6 09:06:47.879: INFO: Pod "pod-subpath-test-configmap-lgw5": Phase="Running", Reason="", readiness=true. Elapsed: 18.086629744s
Sep  6 09:06:49.888: INFO: Pod "pod-subpath-test-configmap-lgw5": Phase="Running", Reason="", readiness=true. Elapsed: 20.095828681s
Sep  6 09:06:51.903: INFO: Pod "pod-subpath-test-configmap-lgw5": Phase="Running", Reason="", readiness=true. Elapsed: 22.110850353s
Sep  6 09:06:53.915: INFO: Pod "pod-subpath-test-configmap-lgw5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.122974432s
STEP: Saw pod success
Sep  6 09:06:53.915: INFO: Pod "pod-subpath-test-configmap-lgw5" satisfied condition "success or failure"
Sep  6 09:06:53.926: INFO: Trying to get logs from node kube-node-20-104 pod pod-subpath-test-configmap-lgw5 container test-container-subpath-configmap-lgw5: <nil>
STEP: delete the pod
Sep  6 09:06:53.984: INFO: Waiting for pod pod-subpath-test-configmap-lgw5 to disappear
Sep  6 09:06:53.999: INFO: Pod pod-subpath-test-configmap-lgw5 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lgw5
Sep  6 09:06:53.999: INFO: Deleting pod "pod-subpath-test-configmap-lgw5" in namespace "subpath-5241"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:06:54.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5241" for this suite.
Sep  6 09:07:00.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:07:00.345: INFO: namespace subpath-5241 deletion completed in 6.318764438s

• [SLOW TEST:30.751 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:07:00.347: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3545
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep  6 09:07:00.599: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:07:06.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3545" for this suite.
Sep  6 09:07:12.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:07:13.257: INFO: namespace init-container-3545 deletion completed in 6.372764356s

• [SLOW TEST:12.910 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:07:13.257: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-181
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 09:07:13.510: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b73a15d4-d085-11e9-b48b-b617478434fe" in namespace "projected-181" to be "success or failure"
Sep  6 09:07:13.529: INFO: Pod "downwardapi-volume-b73a15d4-d085-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 18.945391ms
Sep  6 09:07:15.538: INFO: Pod "downwardapi-volume-b73a15d4-d085-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028230138s
Sep  6 09:07:17.550: INFO: Pod "downwardapi-volume-b73a15d4-d085-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03980495s
STEP: Saw pod success
Sep  6 09:07:17.550: INFO: Pod "downwardapi-volume-b73a15d4-d085-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:07:17.559: INFO: Trying to get logs from node kube-node-20-104 pod downwardapi-volume-b73a15d4-d085-11e9-b48b-b617478434fe container client-container: <nil>
STEP: delete the pod
Sep  6 09:07:17.617: INFO: Waiting for pod downwardapi-volume-b73a15d4-d085-11e9-b48b-b617478434fe to disappear
Sep  6 09:07:17.635: INFO: Pod downwardapi-volume-b73a15d4-d085-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:07:17.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-181" for this suite.
Sep  6 09:07:23.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:07:23.979: INFO: namespace projected-181 deletion completed in 6.328016952s

• [SLOW TEST:10.722 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:07:23.980: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9182
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-bd9cbe36-d085-11e9-b48b-b617478434fe
STEP: Creating secret with name secret-projected-all-test-volume-bd9cbdb6-d085-11e9-b48b-b617478434fe
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep  6 09:07:24.238: INFO: Waiting up to 5m0s for pod "projected-volume-bd9cbcac-d085-11e9-b48b-b617478434fe" in namespace "projected-9182" to be "success or failure"
Sep  6 09:07:24.256: INFO: Pod "projected-volume-bd9cbcac-d085-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 17.048382ms
Sep  6 09:07:26.267: INFO: Pod "projected-volume-bd9cbcac-d085-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028518117s
Sep  6 09:07:28.280: INFO: Pod "projected-volume-bd9cbcac-d085-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04171932s
STEP: Saw pod success
Sep  6 09:07:28.280: INFO: Pod "projected-volume-bd9cbcac-d085-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:07:28.289: INFO: Trying to get logs from node kube-node-20-104 pod projected-volume-bd9cbcac-d085-11e9-b48b-b617478434fe container projected-all-volume-test: <nil>
STEP: delete the pod
Sep  6 09:07:28.374: INFO: Waiting for pod projected-volume-bd9cbcac-d085-11e9-b48b-b617478434fe to disappear
Sep  6 09:07:28.387: INFO: Pod projected-volume-bd9cbcac-d085-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:07:28.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9182" for this suite.
Sep  6 09:07:34.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:07:34.743: INFO: namespace projected-9182 deletion completed in 6.343250544s

• [SLOW TEST:10.763 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:07:34.743: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9339
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-c410b5f9-d085-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume secrets
Sep  6 09:07:35.067: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c4130861-d085-11e9-b48b-b617478434fe" in namespace "projected-9339" to be "success or failure"
Sep  6 09:07:35.081: INFO: Pod "pod-projected-secrets-c4130861-d085-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 14.462844ms
Sep  6 09:07:37.093: INFO: Pod "pod-projected-secrets-c4130861-d085-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025769253s
Sep  6 09:07:39.105: INFO: Pod "pod-projected-secrets-c4130861-d085-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038479463s
STEP: Saw pod success
Sep  6 09:07:39.105: INFO: Pod "pod-projected-secrets-c4130861-d085-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:07:39.120: INFO: Trying to get logs from node kube-node-20-104 pod pod-projected-secrets-c4130861-d085-11e9-b48b-b617478434fe container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 09:07:39.186: INFO: Waiting for pod pod-projected-secrets-c4130861-d085-11e9-b48b-b617478434fe to disappear
Sep  6 09:07:39.200: INFO: Pod pod-projected-secrets-c4130861-d085-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:07:39.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9339" for this suite.
Sep  6 09:07:45.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:07:45.531: INFO: namespace projected-9339 deletion completed in 6.319511469s

• [SLOW TEST:10.788 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:07:45.532: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1492
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-1492
Sep  6 09:07:49.796: INFO: Started pod liveness-http in namespace container-probe-1492
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 09:07:49.805: INFO: Initial restart count of pod liveness-http is 0
Sep  6 09:08:07.914: INFO: Restart count of pod container-probe-1492/liveness-http is now 1 (18.109021996s elapsed)
Sep  6 09:08:28.018: INFO: Restart count of pod container-probe-1492/liveness-http is now 2 (38.212572238s elapsed)
Sep  6 09:08:48.110: INFO: Restart count of pod container-probe-1492/liveness-http is now 3 (58.304937655s elapsed)
Sep  6 09:09:08.227: INFO: Restart count of pod container-probe-1492/liveness-http is now 4 (1m18.422097295s elapsed)
Sep  6 09:10:16.544: INFO: Restart count of pod container-probe-1492/liveness-http is now 5 (2m26.738480743s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:10:16.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1492" for this suite.
Sep  6 09:10:22.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:10:22.742: INFO: namespace container-probe-1492 deletion completed in 6.16933552s

• [SLOW TEST:157.209 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:10:22.742: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1208
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-2824989f-d086-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume secrets
Sep  6 09:10:22.941: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2825ed2c-d086-11e9-b48b-b617478434fe" in namespace "projected-1208" to be "success or failure"
Sep  6 09:10:22.952: INFO: Pod "pod-projected-secrets-2825ed2c-d086-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.615356ms
Sep  6 09:10:24.960: INFO: Pod "pod-projected-secrets-2825ed2c-d086-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018964913s
Sep  6 09:10:26.970: INFO: Pod "pod-projected-secrets-2825ed2c-d086-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028516394s
STEP: Saw pod success
Sep  6 09:10:26.970: INFO: Pod "pod-projected-secrets-2825ed2c-d086-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:10:26.975: INFO: Trying to get logs from node kube-node-20-104 pod pod-projected-secrets-2825ed2c-d086-11e9-b48b-b617478434fe container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 09:10:27.016: INFO: Waiting for pod pod-projected-secrets-2825ed2c-d086-11e9-b48b-b617478434fe to disappear
Sep  6 09:10:27.021: INFO: Pod pod-projected-secrets-2825ed2c-d086-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:10:27.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1208" for this suite.
Sep  6 09:10:33.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:10:33.218: INFO: namespace projected-1208 deletion completed in 6.190024827s

• [SLOW TEST:10.476 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:10:33.218: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1732
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 09:10:33.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 version --client'
Sep  6 09:10:33.499: INFO: stderr: ""
Sep  6 09:10:33.499: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:44:30Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Sep  6 09:10:33.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 create -f - --namespace=kubectl-1732'
Sep  6 09:10:34.166: INFO: stderr: ""
Sep  6 09:10:34.166: INFO: stdout: "replicationcontroller/redis-master created\n"
Sep  6 09:10:34.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 create -f - --namespace=kubectl-1732'
Sep  6 09:10:34.377: INFO: stderr: ""
Sep  6 09:10:34.377: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep  6 09:10:35.385: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 09:10:35.385: INFO: Found 0 / 1
Sep  6 09:10:36.387: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 09:10:36.387: INFO: Found 1 / 1
Sep  6 09:10:36.387: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  6 09:10:36.395: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 09:10:36.395: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 09:10:36.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 describe pod redis-master-ptghz --namespace=kubectl-1732'
Sep  6 09:10:36.541: INFO: stderr: ""
Sep  6 09:10:36.541: INFO: stdout: "Name:           redis-master-ptghz\nNamespace:      kubectl-1732\nNode:           kube-node-20-104/192.168.20.104\nStart Time:     Fri, 06 Sep 2019 09:10:34 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 192.168.67.65/32\n                kubernetes.io/psp: e2e-test-privileged-psp\nStatus:         Running\nIP:             192.168.67.65\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://bf3227e68580131920d6deebb42e75215ca221212e97d1ca6efc3d20aa00b7ef\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 06 Sep 2019 09:10:35 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-xqhwx (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-xqhwx:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-xqhwx\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     <none>\nEvents:\n  Type    Reason     Age   From                       Message\n  ----    ------     ----  ----                       -------\n  Normal  Scheduled  2s    default-scheduler          Successfully assigned kubectl-1732/redis-master-ptghz to kube-node-20-104\n  Normal  Pulled     1s    kubelet, kube-node-20-104  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, kube-node-20-104  Created container redis-master\n  Normal  Started    1s    kubelet, kube-node-20-104  Started container redis-master\n"
Sep  6 09:10:36.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 describe rc redis-master --namespace=kubectl-1732'
Sep  6 09:10:36.678: INFO: stderr: ""
Sep  6 09:10:36.678: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-1732\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-ptghz\n"
Sep  6 09:10:36.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 describe service redis-master --namespace=kubectl-1732'
Sep  6 09:10:36.806: INFO: stderr: ""
Sep  6 09:10:36.806: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-1732\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.254.131.58\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         192.168.67.65:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep  6 09:10:36.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 describe node kube-master-20-101'
Sep  6 09:10:36.993: INFO: stderr: ""
Sep  6 09:10:36.993: INFO: stdout: "Name:               kube-master-20-101\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    caicloud.io/hostname=kube-master-20-101\n                    caicloud.io/nvidia-gpu=false\n                    caicloud.io/role=master\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=kube-master-20-101\n                    kubernetes.io/os=linux\n                    loadbalance.caicloud.io/kube-system.apiserver=true\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"9a:4f:bf:7a:98:6d\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.20.101\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 02 Sep 2019 05:17:36 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 06 Sep 2019 09:09:51 +0000   Mon, 02 Sep 2019 05:17:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 06 Sep 2019 09:09:51 +0000   Mon, 02 Sep 2019 05:17:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 06 Sep 2019 09:09:51 +0000   Mon, 02 Sep 2019 05:17:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 06 Sep 2019 09:09:51 +0000   Mon, 02 Sep 2019 05:18:56 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.20.101\n  Hostname:    kube-master-20-101\nCapacity:\n cpu:                4\n ephemeral-storage:  81880Mi\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             8009532Ki\n pods:               110\nAllocatable:\n cpu:                3890m\n ephemeral-storage:  84998828871\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             7765820Ki\n pods:               110\nSystem Info:\n Machine ID:                 5987f19838834521b333182fa6c6b6fb\n System UUID:                B41134BF-5904-4EB6-8FEA-186F96E457F9\n Boot ID:                    4ab24f4c-669c-4cd7-9ca1-975e507525cb\n Kernel Version:             3.10.0-862.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.5\n Kubelet Version:            v1.14.3\n Kube-Proxy Version:         v1.14.3\nPodCIDR:                     192.168.66.0/24\nNon-terminated Pods:         (10 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-167311b706124356-jpwmn    0 (0%)        0 (0%)      0 (0%)           0 (0%)         48m\n  kube-system                apiserver-provider-ipvsdr-preset-559b6b5667-pj6j9          200m (5%)     200m (5%)   100Mi (1%)       100Mi (1%)     4d3h\n  kube-system                apiserver-proxy-nginx-preset-86dd5d987-npl2l               200m (5%)     200m (5%)   100Mi (1%)       100Mi (1%)     4d3h\n  kube-system                canal-lqc46                                                250m (6%)     0 (0%)      100Mi (1%)       0 (0%)         4d3h\n  kube-system                default-http-backend-67946497c6-n6vbx                      100m (2%)     100m (2%)   100Mi (1%)       100Mi (1%)     4d3h\n  kube-system                kube-apiserver-kube-master-20-101                          250m (6%)     0 (0%)      10Mi (0%)        0 (0%)         62m\n  kube-system                kube-bridge-vlan-ds-t5ztc                                  50m (1%)      100m (2%)   30Mi (0%)        50Mi (0%)      4d3h\n  kube-system                kube-controller-manager-kube-master-20-101                 200m (5%)     0 (0%)      10Mi (0%)        0 (0%)         4d3h\n  kube-system                kube-proxy-kube-master-20-101                              100m (2%)     0 (0%)      10Mi (0%)        0 (0%)         4d3h\n  kube-system                kube-scheduler-kube-master-20-101                          100m (2%)     0 (0%)      10Mi (0%)        0 (0%)         4d3h\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1450m (37%)  600m (15%)\n  memory             470Mi (6%)   350Mi (4%)\n  ephemeral-storage  0 (0%)       0 (0%)\nEvents:              <none>\n"
Sep  6 09:10:36.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 describe namespace kubectl-1732'
Sep  6 09:10:37.123: INFO: stderr: ""
Sep  6 09:10:37.123: INFO: stdout: "Name:         kubectl-1732\nLabels:       e2e-framework=kubectl\n              e2e-run=7703d0e2-d07f-11e9-b48b-b617478434fe\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:10:37.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1732" for this suite.
Sep  6 09:10:59.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:10:59.306: INFO: namespace kubectl-1732 deletion completed in 22.173303247s

• [SLOW TEST:26.088 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:10:59.307: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-568
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-568.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-568.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 09:11:03.554: INFO: DNS probes using dns-568/dns-test-3dee3fed-d086-11e9-b48b-b617478434fe succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:11:03.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-568" for this suite.
Sep  6 09:11:09.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:11:09.797: INFO: namespace dns-568 deletion completed in 6.205874429s

• [SLOW TEST:10.490 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:11:09.797: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1667
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Sep  6 09:11:09.971: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 09:11:09.984: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 09:11:09.992: INFO: 
Logging pods the kubelet thinks is on node kube-master-20-101 before test
Sep  6 09:11:10.006: INFO: kube-proxy-kube-master-20-101 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:11:10.006: INFO: canal-lqc46 from kube-system started at 2019-09-02 05:18:15 +0000 UTC (3 container statuses recorded)
Sep  6 09:11:10.006: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 09:11:10.006: INFO: 	Container install-cni ready: true, restart count 0
Sep  6 09:11:10.006: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 09:11:10.006: INFO: kube-bridge-vlan-ds-t5ztc from kube-system started at 2019-09-02 05:18:18 +0000 UTC (1 container statuses recorded)
Sep  6 09:11:10.006: INFO: 	Container kube-bridge-vlan ready: true, restart count 0
Sep  6 09:11:10.006: INFO: apiserver-provider-ipvsdr-preset-559b6b5667-pj6j9 from kube-system started at 2019-09-02 05:18:56 +0000 UTC (1 container statuses recorded)
Sep  6 09:11:10.006: INFO: 	Container ipvsdr ready: true, restart count 0
Sep  6 09:11:10.006: INFO: apiserver-proxy-nginx-preset-86dd5d987-npl2l from kube-system started at 2019-09-02 05:18:56 +0000 UTC (2 container statuses recorded)
Sep  6 09:11:10.006: INFO: 	Container proxy ready: true, restart count 0
Sep  6 09:11:10.006: INFO: 	Container sidecar ready: true, restart count 0
Sep  6 09:11:10.006: INFO: default-http-backend-67946497c6-n6vbx from kube-system started at 2019-09-02 05:18:56 +0000 UTC (1 container statuses recorded)
Sep  6 09:11:10.006: INFO: 	Container default-http-backend ready: true, restart count 0
Sep  6 09:11:10.006: INFO: kube-apiserver-kube-master-20-101 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:11:10.006: INFO: sonobuoy-systemd-logs-daemon-set-167311b706124356-jpwmn from heptio-sonobuoy started at 2019-09-06 08:22:17 +0000 UTC (2 container statuses recorded)
Sep  6 09:11:10.006: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 09:11:10.006: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 09:11:10.006: INFO: kube-scheduler-kube-master-20-101 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:11:10.006: INFO: kube-controller-manager-kube-master-20-101 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:11:10.006: INFO: 
Logging pods the kubelet thinks is on node kube-master-20-102 before test
Sep  6 09:11:10.019: INFO: kube-controller-manager-kube-master-20-102 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:11:10.019: INFO: canal-9rsk5 from kube-system started at 2019-09-02 05:18:15 +0000 UTC (3 container statuses recorded)
Sep  6 09:11:10.019: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 09:11:10.019: INFO: 	Container install-cni ready: true, restart count 0
Sep  6 09:11:10.019: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 09:11:10.019: INFO: kube-bridge-vlan-ds-fsb6t from kube-system started at 2019-09-02 05:18:18 +0000 UTC (1 container statuses recorded)
Sep  6 09:11:10.019: INFO: 	Container kube-bridge-vlan ready: true, restart count 0
Sep  6 09:11:10.019: INFO: kube-apiserver-kube-master-20-102 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:11:10.019: INFO: sonobuoy-systemd-logs-daemon-set-167311b706124356-s5nt7 from heptio-sonobuoy started at 2019-09-06 08:22:17 +0000 UTC (2 container statuses recorded)
Sep  6 09:11:10.019: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 09:11:10.019: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 09:11:10.019: INFO: kube-proxy-kube-master-20-102 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:11:10.019: INFO: kube-scheduler-kube-master-20-102 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:11:10.019: INFO: apiserver-provider-ipvsdr-preset-559b6b5667-rfgxb from kube-system started at 2019-09-02 05:19:04 +0000 UTC (1 container statuses recorded)
Sep  6 09:11:10.019: INFO: 	Container ipvsdr ready: true, restart count 0
Sep  6 09:11:10.019: INFO: coredns-5b7d754486-fbnd4 from kube-system started at 2019-09-02 05:19:43 +0000 UTC (1 container statuses recorded)
Sep  6 09:11:10.019: INFO: 	Container coredns ready: true, restart count 0
Sep  6 09:11:10.019: INFO: apiserver-proxy-nginx-preset-86dd5d987-b4gcj from kube-system started at 2019-09-02 05:19:06 +0000 UTC (2 container statuses recorded)
Sep  6 09:11:10.019: INFO: 	Container proxy ready: true, restart count 0
Sep  6 09:11:10.019: INFO: 	Container sidecar ready: true, restart count 0
Sep  6 09:11:10.019: INFO: 
Logging pods the kubelet thinks is on node kube-master-20-103 before test
Sep  6 09:11:10.034: INFO: coredns-autoscaler-78bb695c57-s4ll9 from kube-system started at 2019-09-02 05:18:58 +0000 UTC (1 container statuses recorded)
Sep  6 09:11:10.034: INFO: 	Container autoscaler ready: true, restart count 0
Sep  6 09:11:10.034: INFO: coredns-5b7d754486-s6sxk from kube-system started at 2019-09-02 05:18:58 +0000 UTC (1 container statuses recorded)
Sep  6 09:11:10.034: INFO: 	Container coredns ready: true, restart count 0
Sep  6 09:11:10.034: INFO: apiserver-proxy-nginx-preset-86dd5d987-l8sx9 from kube-system started at 2019-09-02 05:18:58 +0000 UTC (2 container statuses recorded)
Sep  6 09:11:10.034: INFO: 	Container proxy ready: true, restart count 0
Sep  6 09:11:10.034: INFO: 	Container sidecar ready: true, restart count 0
Sep  6 09:11:10.034: INFO: kube-apiserver-kube-master-20-103 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:11:10.034: INFO: kube-controller-manager-kube-master-20-103 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:11:10.034: INFO: kube-scheduler-kube-master-20-103 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:11:10.035: INFO: canal-kl5cq from kube-system started at 2019-09-02 05:18:15 +0000 UTC (3 container statuses recorded)
Sep  6 09:11:10.035: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 09:11:10.035: INFO: 	Container install-cni ready: true, restart count 0
Sep  6 09:11:10.035: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 09:11:10.035: INFO: kube-bridge-vlan-ds-s92sg from kube-system started at 2019-09-02 05:18:18 +0000 UTC (1 container statuses recorded)
Sep  6 09:11:10.035: INFO: 	Container kube-bridge-vlan ready: true, restart count 0
Sep  6 09:11:10.035: INFO: heketi-74c4948bbc-t94zf from default started at 2019-09-02 05:21:42 +0000 UTC (1 container statuses recorded)
Sep  6 09:11:10.035: INFO: 	Container heketi ready: true, restart count 0
Sep  6 09:11:10.035: INFO: sonobuoy-systemd-logs-daemon-set-167311b706124356-cf92l from heptio-sonobuoy started at 2019-09-06 08:22:17 +0000 UTC (2 container statuses recorded)
Sep  6 09:11:10.035: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 09:11:10.035: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 09:11:10.035: INFO: kube-proxy-kube-master-20-103 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:11:10.035: INFO: apiserver-provider-ipvsdr-preset-559b6b5667-h5cxj from kube-system started at 2019-09-02 05:18:58 +0000 UTC (1 container statuses recorded)
Sep  6 09:11:10.035: INFO: 	Container ipvsdr ready: true, restart count 0
Sep  6 09:11:10.035: INFO: 
Logging pods the kubelet thinks is on node kube-node-20-104 before test
Sep  6 09:11:10.043: INFO: kube-proxy-d54r6 from kube-system started at 2019-09-02 05:20:50 +0000 UTC (1 container statuses recorded)
Sep  6 09:11:10.043: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 09:11:10.043: INFO: pod-configmaps-925c98e3-ce6a-11e9-803c-a60c2349fa0a from configmap-7786 started at 2019-09-03 16:47:52 +0000 UTC (2 container statuses recorded)
Sep  6 09:11:10.043: INFO: 	Container configmap-volume-binary-test ready: false, restart count 0
Sep  6 09:11:10.043: INFO: 	Container configmap-volume-data-test ready: false, restart count 0
Sep  6 09:11:10.043: INFO: sonobuoy-systemd-logs-daemon-set-167311b706124356-s6f6v from heptio-sonobuoy started at 2019-09-06 08:22:17 +0000 UTC (2 container statuses recorded)
Sep  6 09:11:10.043: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 09:11:10.043: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 09:11:10.043: INFO: kube-bridge-vlan-ds-c5thz from kube-system started at 2019-09-02 05:20:50 +0000 UTC (1 container statuses recorded)
Sep  6 09:11:10.043: INFO: 	Container kube-bridge-vlan ready: true, restart count 0
Sep  6 09:11:10.043: INFO: canal-dvgsp from kube-system started at 2019-09-02 05:20:50 +0000 UTC (3 container statuses recorded)
Sep  6 09:11:10.043: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 09:11:10.043: INFO: 	Container install-cni ready: true, restart count 0
Sep  6 09:11:10.043: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 09:11:10.043: INFO: 
Logging pods the kubelet thinks is on node kube-node-20-105 before test
Sep  6 09:11:10.054: INFO: canal-x798k from kube-system started at 2019-09-02 05:20:51 +0000 UTC (3 container statuses recorded)
Sep  6 09:11:10.054: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 09:11:10.054: INFO: 	Container install-cni ready: true, restart count 0
Sep  6 09:11:10.054: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 09:11:10.054: INFO: sonobuoy-systemd-logs-daemon-set-167311b706124356-lhlls from heptio-sonobuoy started at 2019-09-06 08:22:17 +0000 UTC (2 container statuses recorded)
Sep  6 09:11:10.054: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 09:11:10.054: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 09:11:10.054: INFO: kube-proxy-584q7 from kube-system started at 2019-09-02 05:20:51 +0000 UTC (1 container statuses recorded)
Sep  6 09:11:10.054: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 09:11:10.054: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-06 08:22:15 +0000 UTC (1 container statuses recorded)
Sep  6 09:11:10.054: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 09:11:10.054: INFO: sonobuoy-e2e-job-0a81706125044ba9 from heptio-sonobuoy started at 2019-09-06 08:22:17 +0000 UTC (2 container statuses recorded)
Sep  6 09:11:10.054: INFO: 	Container e2e ready: true, restart count 0
Sep  6 09:11:10.054: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 09:11:10.054: INFO: kube-bridge-vlan-ds-gssmn from kube-system started at 2019-09-02 05:20:51 +0000 UTC (1 container statuses recorded)
Sep  6 09:11:10.054: INFO: 	Container kube-bridge-vlan ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node kube-master-20-101
STEP: verifying the node has the label node kube-master-20-102
STEP: verifying the node has the label node kube-master-20-103
STEP: verifying the node has the label node kube-node-20-104
STEP: verifying the node has the label node kube-node-20-105
Sep  6 09:11:10.176: INFO: Pod heketi-74c4948bbc-t94zf requesting resource cpu=0m on Node kube-master-20-103
Sep  6 09:11:10.176: INFO: Pod sonobuoy requesting resource cpu=0m on Node kube-node-20-105
Sep  6 09:11:10.176: INFO: Pod sonobuoy-e2e-job-0a81706125044ba9 requesting resource cpu=0m on Node kube-node-20-105
Sep  6 09:11:10.176: INFO: Pod sonobuoy-systemd-logs-daemon-set-167311b706124356-cf92l requesting resource cpu=0m on Node kube-master-20-103
Sep  6 09:11:10.176: INFO: Pod sonobuoy-systemd-logs-daemon-set-167311b706124356-jpwmn requesting resource cpu=0m on Node kube-master-20-101
Sep  6 09:11:10.176: INFO: Pod sonobuoy-systemd-logs-daemon-set-167311b706124356-lhlls requesting resource cpu=0m on Node kube-node-20-105
Sep  6 09:11:10.176: INFO: Pod sonobuoy-systemd-logs-daemon-set-167311b706124356-s5nt7 requesting resource cpu=0m on Node kube-master-20-102
Sep  6 09:11:10.176: INFO: Pod sonobuoy-systemd-logs-daemon-set-167311b706124356-s6f6v requesting resource cpu=0m on Node kube-node-20-104
Sep  6 09:11:10.176: INFO: Pod apiserver-provider-ipvsdr-preset-559b6b5667-h5cxj requesting resource cpu=200m on Node kube-master-20-103
Sep  6 09:11:10.176: INFO: Pod apiserver-provider-ipvsdr-preset-559b6b5667-pj6j9 requesting resource cpu=200m on Node kube-master-20-101
Sep  6 09:11:10.176: INFO: Pod apiserver-provider-ipvsdr-preset-559b6b5667-rfgxb requesting resource cpu=200m on Node kube-master-20-102
Sep  6 09:11:10.176: INFO: Pod apiserver-proxy-nginx-preset-86dd5d987-b4gcj requesting resource cpu=200m on Node kube-master-20-102
Sep  6 09:11:10.177: INFO: Pod apiserver-proxy-nginx-preset-86dd5d987-l8sx9 requesting resource cpu=200m on Node kube-master-20-103
Sep  6 09:11:10.177: INFO: Pod apiserver-proxy-nginx-preset-86dd5d987-npl2l requesting resource cpu=200m on Node kube-master-20-101
Sep  6 09:11:10.177: INFO: Pod canal-9rsk5 requesting resource cpu=250m on Node kube-master-20-102
Sep  6 09:11:10.177: INFO: Pod canal-dvgsp requesting resource cpu=250m on Node kube-node-20-104
Sep  6 09:11:10.177: INFO: Pod canal-kl5cq requesting resource cpu=250m on Node kube-master-20-103
Sep  6 09:11:10.177: INFO: Pod canal-lqc46 requesting resource cpu=250m on Node kube-master-20-101
Sep  6 09:11:10.177: INFO: Pod canal-x798k requesting resource cpu=250m on Node kube-node-20-105
Sep  6 09:11:10.177: INFO: Pod coredns-5b7d754486-fbnd4 requesting resource cpu=100m on Node kube-master-20-102
Sep  6 09:11:10.177: INFO: Pod coredns-5b7d754486-s6sxk requesting resource cpu=100m on Node kube-master-20-103
Sep  6 09:11:10.177: INFO: Pod coredns-autoscaler-78bb695c57-s4ll9 requesting resource cpu=20m on Node kube-master-20-103
Sep  6 09:11:10.177: INFO: Pod default-http-backend-67946497c6-n6vbx requesting resource cpu=100m on Node kube-master-20-101
Sep  6 09:11:10.177: INFO: Pod kube-apiserver-kube-master-20-101 requesting resource cpu=250m on Node kube-master-20-101
Sep  6 09:11:10.177: INFO: Pod kube-apiserver-kube-master-20-102 requesting resource cpu=250m on Node kube-master-20-102
Sep  6 09:11:10.177: INFO: Pod kube-apiserver-kube-master-20-103 requesting resource cpu=250m on Node kube-master-20-103
Sep  6 09:11:10.177: INFO: Pod kube-bridge-vlan-ds-c5thz requesting resource cpu=50m on Node kube-node-20-104
Sep  6 09:11:10.177: INFO: Pod kube-bridge-vlan-ds-fsb6t requesting resource cpu=50m on Node kube-master-20-102
Sep  6 09:11:10.177: INFO: Pod kube-bridge-vlan-ds-gssmn requesting resource cpu=50m on Node kube-node-20-105
Sep  6 09:11:10.177: INFO: Pod kube-bridge-vlan-ds-s92sg requesting resource cpu=50m on Node kube-master-20-103
Sep  6 09:11:10.177: INFO: Pod kube-bridge-vlan-ds-t5ztc requesting resource cpu=50m on Node kube-master-20-101
Sep  6 09:11:10.177: INFO: Pod kube-controller-manager-kube-master-20-101 requesting resource cpu=200m on Node kube-master-20-101
Sep  6 09:11:10.177: INFO: Pod kube-controller-manager-kube-master-20-102 requesting resource cpu=200m on Node kube-master-20-102
Sep  6 09:11:10.177: INFO: Pod kube-controller-manager-kube-master-20-103 requesting resource cpu=200m on Node kube-master-20-103
Sep  6 09:11:10.177: INFO: Pod kube-proxy-584q7 requesting resource cpu=100m on Node kube-node-20-105
Sep  6 09:11:10.177: INFO: Pod kube-proxy-d54r6 requesting resource cpu=100m on Node kube-node-20-104
Sep  6 09:11:10.177: INFO: Pod kube-proxy-kube-master-20-101 requesting resource cpu=100m on Node kube-master-20-101
Sep  6 09:11:10.177: INFO: Pod kube-proxy-kube-master-20-102 requesting resource cpu=100m on Node kube-master-20-102
Sep  6 09:11:10.177: INFO: Pod kube-proxy-kube-master-20-103 requesting resource cpu=100m on Node kube-master-20-103
Sep  6 09:11:10.177: INFO: Pod kube-scheduler-kube-master-20-101 requesting resource cpu=100m on Node kube-master-20-101
Sep  6 09:11:10.177: INFO: Pod kube-scheduler-kube-master-20-102 requesting resource cpu=100m on Node kube-master-20-102
Sep  6 09:11:10.177: INFO: Pod kube-scheduler-kube-master-20-103 requesting resource cpu=100m on Node kube-master-20-103
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-444f946f-d086-11e9-b48b-b617478434fe.15c1ce6311ea1c73], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1667/filler-pod-444f946f-d086-11e9-b48b-b617478434fe to kube-master-20-101]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-444f946f-d086-11e9-b48b-b617478434fe.15c1ce63685b26a0], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-444f946f-d086-11e9-b48b-b617478434fe.15c1ce636d12af77], Reason = [Created], Message = [Created container filler-pod-444f946f-d086-11e9-b48b-b617478434fe]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-444f946f-d086-11e9-b48b-b617478434fe.15c1ce637c6d075d], Reason = [Started], Message = [Started container filler-pod-444f946f-d086-11e9-b48b-b617478434fe]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4451a4bc-d086-11e9-b48b-b617478434fe.15c1ce63137b3410], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1667/filler-pod-4451a4bc-d086-11e9-b48b-b617478434fe to kube-master-20-102]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4451a4bc-d086-11e9-b48b-b617478434fe.15c1ce63680a8963], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4451a4bc-d086-11e9-b48b-b617478434fe.15c1ce636da3e8a7], Reason = [Created], Message = [Created container filler-pod-4451a4bc-d086-11e9-b48b-b617478434fe]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4451a4bc-d086-11e9-b48b-b617478434fe.15c1ce637fa5cb77], Reason = [Started], Message = [Started container filler-pod-4451a4bc-d086-11e9-b48b-b617478434fe]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-44533ea2-d086-11e9-b48b-b617478434fe.15c1ce6313747c26], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1667/filler-pod-44533ea2-d086-11e9-b48b-b617478434fe to kube-master-20-103]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-44533ea2-d086-11e9-b48b-b617478434fe.15c1ce6368ed62db], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-44533ea2-d086-11e9-b48b-b617478434fe.15c1ce636d9925ae], Reason = [Created], Message = [Created container filler-pod-44533ea2-d086-11e9-b48b-b617478434fe]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-44533ea2-d086-11e9-b48b-b617478434fe.15c1ce637d95da0c], Reason = [Started], Message = [Started container filler-pod-44533ea2-d086-11e9-b48b-b617478434fe]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4454aa31-d086-11e9-b48b-b617478434fe.15c1ce631514348e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1667/filler-pod-4454aa31-d086-11e9-b48b-b617478434fe to kube-node-20-104]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4454aa31-d086-11e9-b48b-b617478434fe.15c1ce635b496a73], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4454aa31-d086-11e9-b48b-b617478434fe.15c1ce635ec19800], Reason = [Created], Message = [Created container filler-pod-4454aa31-d086-11e9-b48b-b617478434fe]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4454aa31-d086-11e9-b48b-b617478434fe.15c1ce636fa2b29b], Reason = [Started], Message = [Started container filler-pod-4454aa31-d086-11e9-b48b-b617478434fe]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4456f801-d086-11e9-b48b-b617478434fe.15c1ce6316e20ea8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1667/filler-pod-4456f801-d086-11e9-b48b-b617478434fe to kube-node-20-105]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4456f801-d086-11e9-b48b-b617478434fe.15c1ce6364317375], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4456f801-d086-11e9-b48b-b617478434fe.15c1ce636a254561], Reason = [Created], Message = [Created container filler-pod-4456f801-d086-11e9-b48b-b617478434fe]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4456f801-d086-11e9-b48b-b617478434fe.15c1ce6378ba8063], Reason = [Started], Message = [Started container filler-pod-4456f801-d086-11e9-b48b-b617478434fe]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15c1ce640836a3e4], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu.]
STEP: removing the label node off the node kube-master-20-101
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node kube-master-20-102
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node kube-master-20-103
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node kube-node-20-104
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node kube-node-20-105
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:11:15.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1667" for this suite.
Sep  6 09:11:21.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:11:21.851: INFO: namespace sched-pred-1667 deletion completed in 6.384783732s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:12.054 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:11:21.851: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8571
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep  6 09:11:22.042: INFO: Waiting up to 5m0s for pod "pod-4b60002b-d086-11e9-b48b-b617478434fe" in namespace "emptydir-8571" to be "success or failure"
Sep  6 09:11:22.050: INFO: Pod "pod-4b60002b-d086-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.342521ms
Sep  6 09:11:24.056: INFO: Pod "pod-4b60002b-d086-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014011344s
STEP: Saw pod success
Sep  6 09:11:24.057: INFO: Pod "pod-4b60002b-d086-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:11:24.061: INFO: Trying to get logs from node kube-node-20-104 pod pod-4b60002b-d086-11e9-b48b-b617478434fe container test-container: <nil>
STEP: delete the pod
Sep  6 09:11:24.092: INFO: Waiting for pod pod-4b60002b-d086-11e9-b48b-b617478434fe to disappear
Sep  6 09:11:24.097: INFO: Pod pod-4b60002b-d086-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:11:24.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8571" for this suite.
Sep  6 09:11:30.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:11:30.450: INFO: namespace emptydir-8571 deletion completed in 6.342878208s

• [SLOW TEST:8.599 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:11:30.451: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5638
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Sep  6 09:11:30.691: INFO: Waiting up to 5m0s for pod "var-expansion-5085d501-d086-11e9-b48b-b617478434fe" in namespace "var-expansion-5638" to be "success or failure"
Sep  6 09:11:30.705: INFO: Pod "var-expansion-5085d501-d086-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 13.579689ms
Sep  6 09:11:32.721: INFO: Pod "var-expansion-5085d501-d086-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029572442s
Sep  6 09:11:34.740: INFO: Pod "var-expansion-5085d501-d086-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048927392s
STEP: Saw pod success
Sep  6 09:11:34.741: INFO: Pod "var-expansion-5085d501-d086-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:11:34.747: INFO: Trying to get logs from node kube-node-20-104 pod var-expansion-5085d501-d086-11e9-b48b-b617478434fe container dapi-container: <nil>
STEP: delete the pod
Sep  6 09:11:34.817: INFO: Waiting for pod var-expansion-5085d501-d086-11e9-b48b-b617478434fe to disappear
Sep  6 09:11:34.826: INFO: Pod var-expansion-5085d501-d086-11e9-b48b-b617478434fe no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:11:34.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5638" for this suite.
Sep  6 09:11:40.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:11:41.213: INFO: namespace var-expansion-5638 deletion completed in 6.376625139s

• [SLOW TEST:10.761 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:11:41.214: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6429
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 09:11:41.446: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep  6 09:11:41.475: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  6 09:11:46.487: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  6 09:11:46.488: INFO: Creating deployment "test-rolling-update-deployment"
Sep  6 09:11:46.505: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep  6 09:11:46.521: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep  6 09:11:48.551: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep  6 09:11:48.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357906, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357906, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357906, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357906, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 09:11:50.570: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep  6 09:11:50.594: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-6429,SelfLink:/apis/apps/v1/namespaces/deployment-6429/deployments/test-rolling-update-deployment,UID:59f4a47b-d086-11e9-b16e-525400186f34,ResourceVersion:858142,Generation:1,CreationTimestamp:2019-09-06 09:11:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-06 09:11:46 +0000 UTC 2019-09-06 09:11:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-06 09:11:49 +0000 UTC 2019-09-06 09:11:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 09:11:50.603: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-6429,SelfLink:/apis/apps/v1/namespaces/deployment-6429/replicasets/test-rolling-update-deployment-67599b4d9,UID:59fa4c5a-d086-11e9-b16e-525400186f34,ResourceVersion:858135,Generation:1,CreationTimestamp:2019-09-06 09:11:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 59f4a47b-d086-11e9-b16e-525400186f34 0xc0029a4350 0xc0029a4351}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep  6 09:11:50.603: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep  6 09:11:50.603: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-6429,SelfLink:/apis/apps/v1/namespaces/deployment-6429/replicasets/test-rolling-update-controller,UID:56f36204-d086-11e9-b16e-525400186f34,ResourceVersion:858141,Generation:2,CreationTimestamp:2019-09-06 09:11:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 59f4a47b-d086-11e9-b16e-525400186f34 0xc0029a4287 0xc0029a4288}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 09:11:50.613: INFO: Pod "test-rolling-update-deployment-67599b4d9-866k7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-866k7,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-6429,SelfLink:/api/v1/namespaces/deployment-6429/pods/test-rolling-update-deployment-67599b4d9-866k7,UID:59fcf55e-d086-11e9-b16e-525400186f34,ResourceVersion:858134,Generation:0,CreationTimestamp:2019-09-06 09:11:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.67.71/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 59fa4c5a-d086-11e9-b16e-525400186f34 0xc0029a4bf0 0xc0029a4bf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bl5v5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bl5v5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-bl5v5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node-20-104,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:11:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:11:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:11:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:11:46 +0000 UTC  }],Message:,Reason:,HostIP:192.168.20.104,PodIP:192.168.67.71,StartTime:2019-09-06 09:11:46 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-06 09:11:48 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://f3f2b03607f5f0b1da8ee7e0dd9b8459d6becafc473285546b42df9c19f276f4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:11:50.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6429" for this suite.
Sep  6 09:11:58.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:11:58.991: INFO: namespace deployment-6429 deletion completed in 8.365133886s

• [SLOW TEST:17.778 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:11:58.993: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9954
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 09:11:59.209: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61868372-d086-11e9-b48b-b617478434fe" in namespace "downward-api-9954" to be "success or failure"
Sep  6 09:11:59.222: INFO: Pod "downwardapi-volume-61868372-d086-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 12.763623ms
Sep  6 09:12:01.240: INFO: Pod "downwardapi-volume-61868372-d086-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03106495s
Sep  6 09:12:03.250: INFO: Pod "downwardapi-volume-61868372-d086-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040869426s
STEP: Saw pod success
Sep  6 09:12:03.250: INFO: Pod "downwardapi-volume-61868372-d086-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:12:03.259: INFO: Trying to get logs from node kube-node-20-104 pod downwardapi-volume-61868372-d086-11e9-b48b-b617478434fe container client-container: <nil>
STEP: delete the pod
Sep  6 09:12:03.306: INFO: Waiting for pod downwardapi-volume-61868372-d086-11e9-b48b-b617478434fe to disappear
Sep  6 09:12:03.314: INFO: Pod downwardapi-volume-61868372-d086-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:12:03.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9954" for this suite.
Sep  6 09:12:09.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:12:09.667: INFO: namespace downward-api-9954 deletion completed in 6.343628724s

• [SLOW TEST:10.675 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:12:09.668: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-3581
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Sep  6 09:12:11.385: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep  6 09:12:13.590: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357931, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357931, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357931, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357931, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 09:12:15.609: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357931, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357931, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357931, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703357931, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 09:12:19.362: INFO: Waited 1.745165647s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:12:20.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3581" for this suite.
Sep  6 09:12:26.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:12:26.420: INFO: namespace aggregator-3581 deletion completed in 6.38097993s

• [SLOW TEST:16.751 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:12:26.420: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-157
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 09:12:52.696: INFO: Container started at 2019-09-06 09:12:28 +0000 UTC, pod became ready at 2019-09-06 09:12:51 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:12:52.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-157" for this suite.
Sep  6 09:13:16.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:13:17.051: INFO: namespace container-probe-157 deletion completed in 24.341833452s

• [SLOW TEST:50.631 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:13:17.053: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1663
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:13:21.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1663" for this suite.
Sep  6 09:13:27.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:13:27.642: INFO: namespace kubelet-test-1663 deletion completed in 6.315642361s

• [SLOW TEST:10.589 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:13:27.644: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4809
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 09:13:32.007: INFO: Waiting up to 5m0s for pod "client-envvars-98d35b86-d086-11e9-b48b-b617478434fe" in namespace "pods-4809" to be "success or failure"
Sep  6 09:13:32.032: INFO: Pod "client-envvars-98d35b86-d086-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 25.117515ms
Sep  6 09:13:34.052: INFO: Pod "client-envvars-98d35b86-d086-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044761642s
Sep  6 09:13:36.063: INFO: Pod "client-envvars-98d35b86-d086-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055870862s
Sep  6 09:13:38.073: INFO: Pod "client-envvars-98d35b86-d086-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066133406s
STEP: Saw pod success
Sep  6 09:13:38.073: INFO: Pod "client-envvars-98d35b86-d086-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:13:38.081: INFO: Trying to get logs from node kube-node-20-104 pod client-envvars-98d35b86-d086-11e9-b48b-b617478434fe container env3cont: <nil>
STEP: delete the pod
Sep  6 09:13:38.138: INFO: Waiting for pod client-envvars-98d35b86-d086-11e9-b48b-b617478434fe to disappear
Sep  6 09:13:38.148: INFO: Pod client-envvars-98d35b86-d086-11e9-b48b-b617478434fe no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:13:38.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4809" for this suite.
Sep  6 09:14:22.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:14:22.466: INFO: namespace pods-4809 deletion completed in 44.298326522s

• [SLOW TEST:54.823 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:14:22.468: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-558
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 09:14:22.703: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:14:28.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-558" for this suite.
Sep  6 09:14:34.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:14:35.099: INFO: namespace custom-resource-definition-558 deletion completed in 6.242385885s

• [SLOW TEST:12.632 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:14:35.100: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3390
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7213
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6643
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:14:41.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3390" for this suite.
Sep  6 09:14:47.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:14:48.189: INFO: namespace namespaces-3390 deletion completed in 6.352968476s
STEP: Destroying namespace "nsdeletetest-7213" for this suite.
Sep  6 09:14:48.204: INFO: Namespace nsdeletetest-7213 was already deleted
STEP: Destroying namespace "nsdeletetest-6643" for this suite.
Sep  6 09:14:54.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:14:54.569: INFO: namespace nsdeletetest-6643 deletion completed in 6.364682773s

• [SLOW TEST:19.469 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:14:54.569: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4363
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-ca33bc11-d086-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume configMaps
Sep  6 09:14:54.847: INFO: Waiting up to 5m0s for pod "pod-configmaps-ca355fb7-d086-11e9-b48b-b617478434fe" in namespace "configmap-4363" to be "success or failure"
Sep  6 09:14:54.857: INFO: Pod "pod-configmaps-ca355fb7-d086-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 9.642887ms
Sep  6 09:14:56.873: INFO: Pod "pod-configmaps-ca355fb7-d086-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025648269s
Sep  6 09:14:58.884: INFO: Pod "pod-configmaps-ca355fb7-d086-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036863332s
STEP: Saw pod success
Sep  6 09:14:58.884: INFO: Pod "pod-configmaps-ca355fb7-d086-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:14:58.901: INFO: Trying to get logs from node kube-node-20-104 pod pod-configmaps-ca355fb7-d086-11e9-b48b-b617478434fe container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 09:14:58.977: INFO: Waiting for pod pod-configmaps-ca355fb7-d086-11e9-b48b-b617478434fe to disappear
Sep  6 09:14:58.998: INFO: Pod pod-configmaps-ca355fb7-d086-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:14:58.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4363" for this suite.
Sep  6 09:15:07.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:15:07.384: INFO: namespace configmap-4363 deletion completed in 8.366842808s

• [SLOW TEST:12.815 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:15:07.385: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1696
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1696
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1696
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1696
Sep  6 09:15:07.921: INFO: Found 0 stateful pods, waiting for 1
Sep  6 09:15:17.933: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep  6 09:15:17.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 09:15:18.710: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 09:15:18.710: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 09:15:18.710: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 09:15:18.740: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 09:15:18.740: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 09:15:18.801: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999406s
Sep  6 09:15:19.816: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.990653382s
Sep  6 09:15:20.825: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.975715715s
Sep  6 09:15:21.833: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.967356939s
Sep  6 09:15:22.848: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.958710393s
Sep  6 09:15:23.859: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.943696502s
Sep  6 09:15:24.869: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.932772621s
Sep  6 09:15:25.878: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.923274842s
Sep  6 09:15:26.885: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.91427909s
Sep  6 09:15:27.895: INFO: Verifying statefulset ss doesn't scale past 1 for another 906.889096ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1696
Sep  6 09:15:28.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:15:29.439: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 09:15:29.439: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 09:15:29.439: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 09:15:29.450: INFO: Found 1 stateful pods, waiting for 3
Sep  6 09:15:39.463: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 09:15:39.463: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 09:15:39.463: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep  6 09:15:39.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 09:15:40.154: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 09:15:40.154: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 09:15:40.154: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 09:15:40.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 09:15:40.895: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 09:15:40.895: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 09:15:40.895: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 09:15:40.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 09:15:41.666: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 09:15:41.666: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 09:15:41.666: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 09:15:41.666: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 09:15:41.678: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Sep  6 09:15:51.705: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 09:15:51.706: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 09:15:51.706: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 09:15:51.745: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999888s
Sep  6 09:15:52.757: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.983466573s
Sep  6 09:15:53.768: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.97400779s
Sep  6 09:15:54.778: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.963166949s
Sep  6 09:15:55.791: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.952676202s
Sep  6 09:15:56.807: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.940121796s
Sep  6 09:15:57.819: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.924453339s
Sep  6 09:15:58.837: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.911664406s
Sep  6 09:15:59.845: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.89443728s
Sep  6 09:16:00.857: INFO: Verifying statefulset ss doesn't scale past 3 for another 885.650987ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1696
Sep  6 09:16:01.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:16:02.575: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 09:16:02.576: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 09:16:02.576: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 09:16:02.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:16:03.229: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 09:16:03.229: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 09:16:03.229: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 09:16:03.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:16:03.647: INFO: rc: 126
Sep  6 09:16:03.647: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil> cannot exec in a stopped state: unknown
 command terminated with exit code 126
 [] <nil> 0xc002281d70 exit status 126 <nil> <nil> true [0xc001a53b48 0xc001a53b60 0xc001a53b78] [0xc001a53b48 0xc001a53b60 0xc001a53b78] [0xc001a53b58 0xc001a53b70] [0x9c00a0 0x9c00a0] 0xc0033adaa0 <nil>}:
Command stdout:
cannot exec in a stopped state: unknown

stderr:
command terminated with exit code 126

error:
exit status 126

Sep  6 09:16:13.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:16:13.915: INFO: rc: 1
Sep  6 09:16:13.916: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc00231c300 exit status 1 <nil> <nil> true [0xc003050028 0xc003050058 0xc003050070] [0xc003050028 0xc003050058 0xc003050070] [0xc003050050 0xc003050068] [0x9c00a0 0x9c00a0] 0xc00213c900 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Sep  6 09:16:23.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:16:24.092: INFO: rc: 1
Sep  6 09:16:24.093: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002826330 exit status 1 <nil> <nil> true [0xc00200c000 0xc00200c018 0xc00200c030] [0xc00200c000 0xc00200c018 0xc00200c030] [0xc00200c010 0xc00200c028] [0x9c00a0 0x9c00a0] 0xc002666480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:16:34.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:16:34.261: INFO: rc: 1
Sep  6 09:16:34.262: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002826930 exit status 1 <nil> <nil> true [0xc00200c038 0xc00200c050 0xc00200c068] [0xc00200c038 0xc00200c050 0xc00200c068] [0xc00200c048 0xc00200c060] [0x9c00a0 0x9c00a0] 0xc002666900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:16:44.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:16:44.417: INFO: rc: 1
Sep  6 09:16:44.417: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002826e40 exit status 1 <nil> <nil> true [0xc00200c070 0xc00200c088 0xc00200c0a0] [0xc00200c070 0xc00200c088 0xc00200c0a0] [0xc00200c080 0xc00200c098] [0x9c00a0 0x9c00a0] 0xc002666cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:16:54.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:16:54.587: INFO: rc: 1
Sep  6 09:16:54.587: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001bc4480 exit status 1 <nil> <nil> true [0xc002416020 0xc002416058 0xc0024160a0] [0xc002416020 0xc002416058 0xc0024160a0] [0xc002416048 0xc002416088] [0x9c00a0 0x9c00a0] 0xc002d945a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:17:04.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:17:04.779: INFO: rc: 1
Sep  6 09:17:04.779: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0028271a0 exit status 1 <nil> <nil> true [0xc00200c0a8 0xc00200c0c0 0xc00200c0e8] [0xc00200c0a8 0xc00200c0c0 0xc00200c0e8] [0xc00200c0b8 0xc00200c0e0] [0x9c00a0 0x9c00a0] 0xc002667020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:17:14.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:17:14.986: INFO: rc: 1
Sep  6 09:17:14.987: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00231c6f0 exit status 1 <nil> <nil> true [0xc003050078 0xc003050090 0xc0030500b8] [0xc003050078 0xc003050090 0xc0030500b8] [0xc003050088 0xc0030500a8] [0x9c00a0 0x9c00a0] 0xc00213d200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:17:24.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:17:25.148: INFO: rc: 1
Sep  6 09:17:25.148: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002827500 exit status 1 <nil> <nil> true [0xc00200c0f0 0xc00200c108 0xc00200c128] [0xc00200c0f0 0xc00200c108 0xc00200c128] [0xc00200c100 0xc00200c120] [0x9c00a0 0x9c00a0] 0xc002667380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:17:35.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:17:35.312: INFO: rc: 1
Sep  6 09:17:35.312: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00231ca80 exit status 1 <nil> <nil> true [0xc0030500c8 0xc0030500f8 0xc003050128] [0xc0030500c8 0xc0030500f8 0xc003050128] [0xc0030500e8 0xc003050118] [0x9c00a0 0x9c00a0] 0xc00213d980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:17:45.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:17:45.466: INFO: rc: 1
Sep  6 09:17:45.466: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002827830 exit status 1 <nil> <nil> true [0xc00200c130 0xc00200c148 0xc00200c160] [0xc00200c130 0xc00200c148 0xc00200c160] [0xc00200c140 0xc00200c158] [0x9c00a0 0x9c00a0] 0xc0026676e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:17:55.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:17:55.643: INFO: rc: 1
Sep  6 09:17:55.643: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00278c300 exit status 1 <nil> <nil> true [0xc001aee038 0xc001aee098 0xc001aee0f8] [0xc001aee038 0xc001aee098 0xc001aee0f8] [0xc001aee068 0xc001aee0d8] [0x9c00a0 0x9c00a0] 0xc002abc2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:18:05.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:18:05.779: INFO: rc: 1
Sep  6 09:18:05.779: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00231ce10 exit status 1 <nil> <nil> true [0xc003050138 0xc003050170 0xc0030501a8] [0xc003050138 0xc003050170 0xc0030501a8] [0xc003050160 0xc003050198] [0x9c00a0 0x9c00a0] 0xc00213df80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:18:15.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:18:15.955: INFO: rc: 1
Sep  6 09:18:15.955: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00231c330 exit status 1 <nil> <nil> true [0xc003050028 0xc003050058 0xc003050070] [0xc003050028 0xc003050058 0xc003050070] [0xc003050050 0xc003050068] [0x9c00a0 0x9c00a0] 0xc00213c900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:18:25.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:18:26.108: INFO: rc: 1
Sep  6 09:18:26.108: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00231c720 exit status 1 <nil> <nil> true [0xc003050078 0xc003050090 0xc0030500b8] [0xc003050078 0xc003050090 0xc0030500b8] [0xc003050088 0xc0030500a8] [0x9c00a0 0x9c00a0] 0xc00213d200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:18:36.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:18:36.289: INFO: rc: 1
Sep  6 09:18:36.290: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00231ca50 exit status 1 <nil> <nil> true [0xc0030500c8 0xc0030500f8 0xc003050128] [0xc0030500c8 0xc0030500f8 0xc003050128] [0xc0030500e8 0xc003050118] [0x9c00a0 0x9c00a0] 0xc00213d980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:18:46.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:18:46.514: INFO: rc: 1
Sep  6 09:18:46.515: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00231cde0 exit status 1 <nil> <nil> true [0xc003050138 0xc003050170 0xc0030501a8] [0xc003050138 0xc003050170 0xc0030501a8] [0xc003050160 0xc003050198] [0x9c00a0 0x9c00a0] 0xc00213df80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:18:56.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:18:56.723: INFO: rc: 1
Sep  6 09:18:56.723: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00278c360 exit status 1 <nil> <nil> true [0xc00200c000 0xc00200c018 0xc00200c030] [0xc00200c000 0xc00200c018 0xc00200c030] [0xc00200c010 0xc00200c028] [0x9c00a0 0x9c00a0] 0xc002666480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:19:06.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:19:06.898: INFO: rc: 1
Sep  6 09:19:06.898: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002826390 exit status 1 <nil> <nil> true [0xc001aee038 0xc001aee098 0xc001aee0f8] [0xc001aee038 0xc001aee098 0xc001aee0f8] [0xc001aee068 0xc001aee0d8] [0x9c00a0 0x9c00a0] 0xc002abc2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:19:16.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:19:17.098: INFO: rc: 1
Sep  6 09:19:17.098: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00231d1d0 exit status 1 <nil> <nil> true [0xc0030501c0 0xc0030501f0 0xc003050230] [0xc0030501c0 0xc0030501f0 0xc003050230] [0xc0030501e0 0xc003050220] [0x9c00a0 0x9c00a0] 0xc0033a8cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:19:27.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:19:27.293: INFO: rc: 1
Sep  6 09:19:27.294: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001bc4300 exit status 1 <nil> <nil> true [0xc002416020 0xc002416058 0xc0024160a0] [0xc002416020 0xc002416058 0xc0024160a0] [0xc002416048 0xc002416088] [0x9c00a0 0x9c00a0] 0xc002d945a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:19:37.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:19:37.447: INFO: rc: 1
Sep  6 09:19:37.448: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001bc47b0 exit status 1 <nil> <nil> true [0xc0024160a8 0xc0024160f0 0xc002416118] [0xc0024160a8 0xc0024160f0 0xc002416118] [0xc0024160b8 0xc002416110] [0x9c00a0 0x9c00a0] 0xc002d94ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:19:47.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:19:47.585: INFO: rc: 1
Sep  6 09:19:47.585: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00231d500 exit status 1 <nil> <nil> true [0xc003050240 0xc003050280 0xc0030502b0] [0xc003050240 0xc003050280 0xc0030502b0] [0xc003050260 0xc0030502a0] [0x9c00a0 0x9c00a0] 0xc0033a9680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:19:57.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:19:57.686: INFO: rc: 1
Sep  6 09:19:57.686: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001bc4b40 exit status 1 <nil> <nil> true [0xc002416120 0xc002416170 0xc0024161d0] [0xc002416120 0xc002416170 0xc0024161d0] [0xc002416158 0xc0024161c8] [0x9c00a0 0x9c00a0] 0xc002d94f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:20:07.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:20:07.879: INFO: rc: 1
Sep  6 09:20:07.879: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001bc4ea0 exit status 1 <nil> <nil> true [0xc0024161f0 0xc002416230 0xc002416278] [0xc0024161f0 0xc002416230 0xc002416278] [0xc002416220 0xc002416270] [0x9c00a0 0x9c00a0] 0xc002d952c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:20:17.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:20:18.000: INFO: rc: 1
Sep  6 09:20:18.001: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00278c300 exit status 1 <nil> <nil> true [0xc00200c008 0xc00200c020 0xc00200c038] [0xc00200c008 0xc00200c020 0xc00200c038] [0xc00200c018 0xc00200c030] [0x9c00a0 0x9c00a0] 0xc00213c900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:20:28.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:20:28.184: INFO: rc: 1
Sep  6 09:20:28.184: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001bc4480 exit status 1 <nil> <nil> true [0xc002416020 0xc002416058 0xc0024160a0] [0xc002416020 0xc002416058 0xc0024160a0] [0xc002416048 0xc002416088] [0x9c00a0 0x9c00a0] 0xc002666480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:20:38.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:20:38.320: INFO: rc: 1
Sep  6 09:20:38.320: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002826330 exit status 1 <nil> <nil> true [0xc001aee038 0xc001aee098 0xc001aee0f8] [0xc001aee038 0xc001aee098 0xc001aee0f8] [0xc001aee068 0xc001aee0d8] [0x9c00a0 0x9c00a0] 0xc002d945a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:20:48.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:20:48.497: INFO: rc: 1
Sep  6 09:20:48.498: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00278c690 exit status 1 <nil> <nil> true [0xc00200c040 0xc00200c058 0xc00200c070] [0xc00200c040 0xc00200c058 0xc00200c070] [0xc00200c050 0xc00200c068] [0x9c00a0 0x9c00a0] 0xc00213d200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:20:58.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:20:58.710: INFO: rc: 1
Sep  6 09:20:58.711: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00278ca20 exit status 1 <nil> <nil> true [0xc00200c078 0xc00200c090 0xc00200c0a8] [0xc00200c078 0xc00200c090 0xc00200c0a8] [0xc00200c088 0xc00200c0a0] [0x9c00a0 0x9c00a0] 0xc00213d980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Sep  6 09:21:08.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-1696 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:21:08.851: INFO: rc: 1
Sep  6 09:21:08.851: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Sep  6 09:21:08.851: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep  6 09:21:08.884: INFO: Deleting all statefulset in ns statefulset-1696
Sep  6 09:21:08.891: INFO: Scaling statefulset ss to 0
Sep  6 09:21:08.920: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 09:21:08.930: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:21:08.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1696" for this suite.
Sep  6 09:21:17.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:21:17.340: INFO: namespace statefulset-1696 deletion completed in 8.364619883s

• [SLOW TEST:369.956 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:21:17.341: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5087
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0906 09:21:27.839441      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 09:21:27.839: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:21:27.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5087" for this suite.
Sep  6 09:21:35.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:21:36.188: INFO: namespace gc-5087 deletion completed in 8.337995876s

• [SLOW TEST:18.847 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:21:36.189: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1688
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1688
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Sep  6 09:21:36.526: INFO: Found 0 stateful pods, waiting for 3
Sep  6 09:21:46.543: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 09:21:46.543: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 09:21:46.543: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep  6 09:21:46.608: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep  6 09:21:56.682: INFO: Updating stateful set ss2
Sep  6 09:21:56.709: INFO: Waiting for Pod statefulset-1688/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Sep  6 09:22:06.950: INFO: Found 1 stateful pods, waiting for 3
Sep  6 09:22:16.961: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 09:22:16.961: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 09:22:16.961: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep  6 09:22:17.017: INFO: Updating stateful set ss2
Sep  6 09:22:17.041: INFO: Waiting for Pod statefulset-1688/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Sep  6 09:22:27.122: INFO: Updating stateful set ss2
Sep  6 09:22:27.157: INFO: Waiting for StatefulSet statefulset-1688/ss2 to complete update
Sep  6 09:22:27.157: INFO: Waiting for Pod statefulset-1688/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Sep  6 09:22:37.190: INFO: Waiting for StatefulSet statefulset-1688/ss2 to complete update
Sep  6 09:22:37.190: INFO: Waiting for Pod statefulset-1688/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep  6 09:22:47.179: INFO: Deleting all statefulset in ns statefulset-1688
Sep  6 09:22:47.187: INFO: Scaling statefulset ss2 to 0
Sep  6 09:23:07.227: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 09:23:07.235: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:23:07.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1688" for this suite.
Sep  6 09:23:15.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:23:15.688: INFO: namespace statefulset-1688 deletion completed in 8.411371939s

• [SLOW TEST:99.499 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:23:15.691: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep  6 09:23:15.907: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4158,SelfLink:/api/v1/namespaces/watch-4158/configmaps/e2e-watch-test-configmap-a,UID:f4e02e78-d087-11e9-b16e-525400186f34,ResourceVersion:860160,Generation:0,CreationTimestamp:2019-09-06 09:23:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 09:23:15.908: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4158,SelfLink:/api/v1/namespaces/watch-4158/configmaps/e2e-watch-test-configmap-a,UID:f4e02e78-d087-11e9-b16e-525400186f34,ResourceVersion:860160,Generation:0,CreationTimestamp:2019-09-06 09:23:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep  6 09:23:25.929: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4158,SelfLink:/api/v1/namespaces/watch-4158/configmaps/e2e-watch-test-configmap-a,UID:f4e02e78-d087-11e9-b16e-525400186f34,ResourceVersion:860183,Generation:0,CreationTimestamp:2019-09-06 09:23:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep  6 09:23:25.930: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4158,SelfLink:/api/v1/namespaces/watch-4158/configmaps/e2e-watch-test-configmap-a,UID:f4e02e78-d087-11e9-b16e-525400186f34,ResourceVersion:860183,Generation:0,CreationTimestamp:2019-09-06 09:23:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep  6 09:23:35.952: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4158,SelfLink:/api/v1/namespaces/watch-4158/configmaps/e2e-watch-test-configmap-a,UID:f4e02e78-d087-11e9-b16e-525400186f34,ResourceVersion:860204,Generation:0,CreationTimestamp:2019-09-06 09:23:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 09:23:35.952: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4158,SelfLink:/api/v1/namespaces/watch-4158/configmaps/e2e-watch-test-configmap-a,UID:f4e02e78-d087-11e9-b16e-525400186f34,ResourceVersion:860204,Generation:0,CreationTimestamp:2019-09-06 09:23:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep  6 09:23:45.973: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4158,SelfLink:/api/v1/namespaces/watch-4158/configmaps/e2e-watch-test-configmap-a,UID:f4e02e78-d087-11e9-b16e-525400186f34,ResourceVersion:860225,Generation:0,CreationTimestamp:2019-09-06 09:23:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 09:23:45.973: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4158,SelfLink:/api/v1/namespaces/watch-4158/configmaps/e2e-watch-test-configmap-a,UID:f4e02e78-d087-11e9-b16e-525400186f34,ResourceVersion:860225,Generation:0,CreationTimestamp:2019-09-06 09:23:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep  6 09:23:55.997: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4158,SelfLink:/api/v1/namespaces/watch-4158/configmaps/e2e-watch-test-configmap-b,UID:0cc3c44d-d088-11e9-b16e-525400186f34,ResourceVersion:860247,Generation:0,CreationTimestamp:2019-09-06 09:23:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 09:23:55.997: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4158,SelfLink:/api/v1/namespaces/watch-4158/configmaps/e2e-watch-test-configmap-b,UID:0cc3c44d-d088-11e9-b16e-525400186f34,ResourceVersion:860247,Generation:0,CreationTimestamp:2019-09-06 09:23:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep  6 09:24:06.017: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4158,SelfLink:/api/v1/namespaces/watch-4158/configmaps/e2e-watch-test-configmap-b,UID:0cc3c44d-d088-11e9-b16e-525400186f34,ResourceVersion:860267,Generation:0,CreationTimestamp:2019-09-06 09:23:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 09:24:06.017: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4158,SelfLink:/api/v1/namespaces/watch-4158/configmaps/e2e-watch-test-configmap-b,UID:0cc3c44d-d088-11e9-b16e-525400186f34,ResourceVersion:860267,Generation:0,CreationTimestamp:2019-09-06 09:23:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:24:16.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4158" for this suite.
Sep  6 09:24:22.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:24:22.366: INFO: namespace watch-4158 deletion completed in 6.33470023s

• [SLOW TEST:66.675 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:24:22.366: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-1224
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Sep  6 09:24:22.591: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-1224" to be "success or failure"
Sep  6 09:24:22.600: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.887992ms
Sep  6 09:24:24.609: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017536381s
Sep  6 09:24:26.620: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028861877s
Sep  6 09:24:28.630: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038840678s
STEP: Saw pod success
Sep  6 09:24:28.630: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Sep  6 09:24:28.641: INFO: Trying to get logs from node kube-node-20-104 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Sep  6 09:24:28.694: INFO: Waiting for pod pod-host-path-test to disappear
Sep  6 09:24:28.700: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:24:28.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-1224" for this suite.
Sep  6 09:24:34.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:24:35.061: INFO: namespace hostpath-1224 deletion completed in 6.352090972s

• [SLOW TEST:12.695 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:24:35.063: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1108
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Sep  6 09:24:35.279: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 09:24:35.301: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 09:24:35.309: INFO: 
Logging pods the kubelet thinks is on node kube-master-20-101 before test
Sep  6 09:24:35.330: INFO: kube-controller-manager-kube-master-20-101 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:24:35.330: INFO: kube-scheduler-kube-master-20-101 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:24:35.330: INFO: canal-lqc46 from kube-system started at 2019-09-02 05:18:15 +0000 UTC (3 container statuses recorded)
Sep  6 09:24:35.330: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 09:24:35.330: INFO: 	Container install-cni ready: true, restart count 0
Sep  6 09:24:35.330: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 09:24:35.330: INFO: kube-bridge-vlan-ds-t5ztc from kube-system started at 2019-09-02 05:18:18 +0000 UTC (1 container statuses recorded)
Sep  6 09:24:35.330: INFO: 	Container kube-bridge-vlan ready: true, restart count 0
Sep  6 09:24:35.330: INFO: apiserver-provider-ipvsdr-preset-559b6b5667-pj6j9 from kube-system started at 2019-09-02 05:18:56 +0000 UTC (1 container statuses recorded)
Sep  6 09:24:35.330: INFO: 	Container ipvsdr ready: true, restart count 0
Sep  6 09:24:35.330: INFO: kube-proxy-kube-master-20-101 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:24:35.330: INFO: default-http-backend-67946497c6-n6vbx from kube-system started at 2019-09-02 05:18:56 +0000 UTC (1 container statuses recorded)
Sep  6 09:24:35.330: INFO: 	Container default-http-backend ready: true, restart count 0
Sep  6 09:24:35.330: INFO: kube-apiserver-kube-master-20-101 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:24:35.330: INFO: sonobuoy-systemd-logs-daemon-set-167311b706124356-jpwmn from heptio-sonobuoy started at 2019-09-06 08:22:17 +0000 UTC (2 container statuses recorded)
Sep  6 09:24:35.330: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  6 09:24:35.330: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  6 09:24:35.330: INFO: apiserver-proxy-nginx-preset-86dd5d987-npl2l from kube-system started at 2019-09-02 05:18:56 +0000 UTC (2 container statuses recorded)
Sep  6 09:24:35.330: INFO: 	Container proxy ready: true, restart count 0
Sep  6 09:24:35.330: INFO: 	Container sidecar ready: true, restart count 0
Sep  6 09:24:35.330: INFO: 
Logging pods the kubelet thinks is on node kube-master-20-102 before test
Sep  6 09:24:35.351: INFO: kube-bridge-vlan-ds-fsb6t from kube-system started at 2019-09-02 05:18:18 +0000 UTC (1 container statuses recorded)
Sep  6 09:24:35.351: INFO: 	Container kube-bridge-vlan ready: true, restart count 0
Sep  6 09:24:35.351: INFO: kube-apiserver-kube-master-20-102 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:24:35.351: INFO: sonobuoy-systemd-logs-daemon-set-167311b706124356-s5nt7 from heptio-sonobuoy started at 2019-09-06 08:22:17 +0000 UTC (2 container statuses recorded)
Sep  6 09:24:35.351: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  6 09:24:35.351: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  6 09:24:35.351: INFO: kube-controller-manager-kube-master-20-102 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:24:35.351: INFO: canal-9rsk5 from kube-system started at 2019-09-02 05:18:15 +0000 UTC (3 container statuses recorded)
Sep  6 09:24:35.351: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 09:24:35.351: INFO: 	Container install-cni ready: true, restart count 0
Sep  6 09:24:35.351: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 09:24:35.351: INFO: apiserver-provider-ipvsdr-preset-559b6b5667-rfgxb from kube-system started at 2019-09-02 05:19:04 +0000 UTC (1 container statuses recorded)
Sep  6 09:24:35.351: INFO: 	Container ipvsdr ready: true, restart count 0
Sep  6 09:24:35.351: INFO: coredns-5b7d754486-fbnd4 from kube-system started at 2019-09-02 05:19:43 +0000 UTC (1 container statuses recorded)
Sep  6 09:24:35.351: INFO: 	Container coredns ready: true, restart count 0
Sep  6 09:24:35.351: INFO: kube-proxy-kube-master-20-102 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:24:35.351: INFO: kube-scheduler-kube-master-20-102 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:24:35.351: INFO: apiserver-proxy-nginx-preset-86dd5d987-b4gcj from kube-system started at 2019-09-02 05:19:06 +0000 UTC (2 container statuses recorded)
Sep  6 09:24:35.351: INFO: 	Container proxy ready: true, restart count 0
Sep  6 09:24:35.351: INFO: 	Container sidecar ready: true, restart count 0
Sep  6 09:24:35.351: INFO: 
Logging pods the kubelet thinks is on node kube-master-20-103 before test
Sep  6 09:24:35.375: INFO: kube-proxy-kube-master-20-103 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:24:35.375: INFO: kube-scheduler-kube-master-20-103 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:24:35.375: INFO: canal-kl5cq from kube-system started at 2019-09-02 05:18:15 +0000 UTC (3 container statuses recorded)
Sep  6 09:24:35.375: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 09:24:35.375: INFO: 	Container install-cni ready: true, restart count 0
Sep  6 09:24:35.375: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 09:24:35.375: INFO: kube-bridge-vlan-ds-s92sg from kube-system started at 2019-09-02 05:18:18 +0000 UTC (1 container statuses recorded)
Sep  6 09:24:35.375: INFO: 	Container kube-bridge-vlan ready: true, restart count 0
Sep  6 09:24:35.375: INFO: heketi-74c4948bbc-t94zf from default started at 2019-09-02 05:21:42 +0000 UTC (1 container statuses recorded)
Sep  6 09:24:35.375: INFO: 	Container heketi ready: true, restart count 0
Sep  6 09:24:35.375: INFO: sonobuoy-systemd-logs-daemon-set-167311b706124356-cf92l from heptio-sonobuoy started at 2019-09-06 08:22:17 +0000 UTC (2 container statuses recorded)
Sep  6 09:24:35.375: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  6 09:24:35.375: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  6 09:24:35.375: INFO: apiserver-provider-ipvsdr-preset-559b6b5667-h5cxj from kube-system started at 2019-09-02 05:18:58 +0000 UTC (1 container statuses recorded)
Sep  6 09:24:35.375: INFO: 	Container ipvsdr ready: true, restart count 0
Sep  6 09:24:35.375: INFO: coredns-autoscaler-78bb695c57-s4ll9 from kube-system started at 2019-09-02 05:18:58 +0000 UTC (1 container statuses recorded)
Sep  6 09:24:35.375: INFO: 	Container autoscaler ready: true, restart count 0
Sep  6 09:24:35.375: INFO: kube-controller-manager-kube-master-20-103 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:24:35.375: INFO: coredns-5b7d754486-s6sxk from kube-system started at 2019-09-02 05:18:58 +0000 UTC (1 container statuses recorded)
Sep  6 09:24:35.375: INFO: 	Container coredns ready: true, restart count 0
Sep  6 09:24:35.375: INFO: apiserver-proxy-nginx-preset-86dd5d987-l8sx9 from kube-system started at 2019-09-02 05:18:58 +0000 UTC (2 container statuses recorded)
Sep  6 09:24:35.375: INFO: 	Container proxy ready: true, restart count 0
Sep  6 09:24:35.375: INFO: 	Container sidecar ready: true, restart count 0
Sep  6 09:24:35.375: INFO: kube-apiserver-kube-master-20-103 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:24:35.375: INFO: 
Logging pods the kubelet thinks is on node kube-node-20-104 before test
Sep  6 09:24:35.393: INFO: canal-dvgsp from kube-system started at 2019-09-02 05:20:50 +0000 UTC (3 container statuses recorded)
Sep  6 09:24:35.393: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 09:24:35.393: INFO: 	Container install-cni ready: true, restart count 0
Sep  6 09:24:35.393: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 09:24:35.393: INFO: pod-configmaps-925c98e3-ce6a-11e9-803c-a60c2349fa0a from configmap-7786 started at 2019-09-03 16:47:52 +0000 UTC (2 container statuses recorded)
Sep  6 09:24:35.393: INFO: 	Container configmap-volume-binary-test ready: false, restart count 0
Sep  6 09:24:35.393: INFO: 	Container configmap-volume-data-test ready: false, restart count 0
Sep  6 09:24:35.393: INFO: sonobuoy-systemd-logs-daemon-set-167311b706124356-s6f6v from heptio-sonobuoy started at 2019-09-06 08:22:17 +0000 UTC (2 container statuses recorded)
Sep  6 09:24:35.393: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  6 09:24:35.393: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  6 09:24:35.393: INFO: kube-proxy-d54r6 from kube-system started at 2019-09-02 05:20:50 +0000 UTC (1 container statuses recorded)
Sep  6 09:24:35.393: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 09:24:35.393: INFO: kube-bridge-vlan-ds-c5thz from kube-system started at 2019-09-02 05:20:50 +0000 UTC (1 container statuses recorded)
Sep  6 09:24:35.393: INFO: 	Container kube-bridge-vlan ready: true, restart count 0
Sep  6 09:24:35.393: INFO: 
Logging pods the kubelet thinks is on node kube-node-20-105 before test
Sep  6 09:24:35.413: INFO: canal-x798k from kube-system started at 2019-09-02 05:20:51 +0000 UTC (3 container statuses recorded)
Sep  6 09:24:35.413: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 09:24:35.413: INFO: 	Container install-cni ready: true, restart count 0
Sep  6 09:24:35.413: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 09:24:35.413: INFO: kube-proxy-584q7 from kube-system started at 2019-09-02 05:20:51 +0000 UTC (1 container statuses recorded)
Sep  6 09:24:35.413: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 09:24:35.413: INFO: sonobuoy-systemd-logs-daemon-set-167311b706124356-lhlls from heptio-sonobuoy started at 2019-09-06 08:22:17 +0000 UTC (2 container statuses recorded)
Sep  6 09:24:35.413: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  6 09:24:35.413: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  6 09:24:35.413: INFO: sonobuoy-e2e-job-0a81706125044ba9 from heptio-sonobuoy started at 2019-09-06 08:22:17 +0000 UTC (2 container statuses recorded)
Sep  6 09:24:35.413: INFO: 	Container e2e ready: true, restart count 0
Sep  6 09:24:35.413: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 09:24:35.413: INFO: kube-bridge-vlan-ds-gssmn from kube-system started at 2019-09-02 05:20:51 +0000 UTC (1 container statuses recorded)
Sep  6 09:24:35.413: INFO: 	Container kube-bridge-vlan ready: true, restart count 0
Sep  6 09:24:35.413: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-06 08:22:15 +0000 UTC (1 container statuses recorded)
Sep  6 09:24:35.413: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15c1cf1e8fefaa18], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:24:36.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1108" for this suite.
Sep  6 09:24:42.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:24:42.866: INFO: namespace sched-pred-1108 deletion completed in 6.374563639s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.804 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:24:42.867: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6061
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-7bkc
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 09:24:43.161: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-7bkc" in namespace "subpath-6061" to be "success or failure"
Sep  6 09:24:43.171: INFO: Pod "pod-subpath-test-downwardapi-7bkc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.071159ms
Sep  6 09:24:45.181: INFO: Pod "pod-subpath-test-downwardapi-7bkc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019662402s
Sep  6 09:24:47.191: INFO: Pod "pod-subpath-test-downwardapi-7bkc": Phase="Running", Reason="", readiness=true. Elapsed: 4.029901901s
Sep  6 09:24:49.201: INFO: Pod "pod-subpath-test-downwardapi-7bkc": Phase="Running", Reason="", readiness=true. Elapsed: 6.039754529s
Sep  6 09:24:51.212: INFO: Pod "pod-subpath-test-downwardapi-7bkc": Phase="Running", Reason="", readiness=true. Elapsed: 8.05045746s
Sep  6 09:24:53.229: INFO: Pod "pod-subpath-test-downwardapi-7bkc": Phase="Running", Reason="", readiness=true. Elapsed: 10.067083905s
Sep  6 09:24:55.237: INFO: Pod "pod-subpath-test-downwardapi-7bkc": Phase="Running", Reason="", readiness=true. Elapsed: 12.075212965s
Sep  6 09:24:57.253: INFO: Pod "pod-subpath-test-downwardapi-7bkc": Phase="Running", Reason="", readiness=true. Elapsed: 14.091344164s
Sep  6 09:24:59.268: INFO: Pod "pod-subpath-test-downwardapi-7bkc": Phase="Running", Reason="", readiness=true. Elapsed: 16.10594648s
Sep  6 09:25:01.279: INFO: Pod "pod-subpath-test-downwardapi-7bkc": Phase="Running", Reason="", readiness=true. Elapsed: 18.117170743s
Sep  6 09:25:03.289: INFO: Pod "pod-subpath-test-downwardapi-7bkc": Phase="Running", Reason="", readiness=true. Elapsed: 20.127057649s
Sep  6 09:25:05.301: INFO: Pod "pod-subpath-test-downwardapi-7bkc": Phase="Running", Reason="", readiness=true. Elapsed: 22.138993557s
Sep  6 09:25:07.311: INFO: Pod "pod-subpath-test-downwardapi-7bkc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.149498483s
STEP: Saw pod success
Sep  6 09:25:07.311: INFO: Pod "pod-subpath-test-downwardapi-7bkc" satisfied condition "success or failure"
Sep  6 09:25:07.321: INFO: Trying to get logs from node kube-node-20-104 pod pod-subpath-test-downwardapi-7bkc container test-container-subpath-downwardapi-7bkc: <nil>
STEP: delete the pod
Sep  6 09:25:07.381: INFO: Waiting for pod pod-subpath-test-downwardapi-7bkc to disappear
Sep  6 09:25:07.390: INFO: Pod pod-subpath-test-downwardapi-7bkc no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-7bkc
Sep  6 09:25:07.390: INFO: Deleting pod "pod-subpath-test-downwardapi-7bkc" in namespace "subpath-6061"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:25:07.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6061" for this suite.
Sep  6 09:25:13.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:25:13.811: INFO: namespace subpath-6061 deletion completed in 6.394033131s

• [SLOW TEST:30.944 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:25:13.811: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9223
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-3b4e09ab-d088-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume configMaps
Sep  6 09:25:14.114: INFO: Waiting up to 5m0s for pod "pod-configmaps-3b4ffe9a-d088-11e9-b48b-b617478434fe" in namespace "configmap-9223" to be "success or failure"
Sep  6 09:25:14.201: INFO: Pod "pod-configmaps-3b4ffe9a-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 86.61279ms
Sep  6 09:25:16.212: INFO: Pod "pod-configmaps-3b4ffe9a-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.097852117s
Sep  6 09:25:18.225: INFO: Pod "pod-configmaps-3b4ffe9a-d088-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.110460558s
STEP: Saw pod success
Sep  6 09:25:18.225: INFO: Pod "pod-configmaps-3b4ffe9a-d088-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:25:18.232: INFO: Trying to get logs from node kube-node-20-104 pod pod-configmaps-3b4ffe9a-d088-11e9-b48b-b617478434fe container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 09:25:18.286: INFO: Waiting for pod pod-configmaps-3b4ffe9a-d088-11e9-b48b-b617478434fe to disappear
Sep  6 09:25:18.294: INFO: Pod pod-configmaps-3b4ffe9a-d088-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:25:18.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9223" for this suite.
Sep  6 09:25:24.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:25:24.736: INFO: namespace configmap-9223 deletion completed in 6.425714939s

• [SLOW TEST:10.925 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:25:24.738: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9442
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-41d1feeb-d088-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume configMaps
Sep  6 09:25:25.029: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-41d3b7f7-d088-11e9-b48b-b617478434fe" in namespace "projected-9442" to be "success or failure"
Sep  6 09:25:25.039: INFO: Pod "pod-projected-configmaps-41d3b7f7-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.203547ms
Sep  6 09:25:27.064: INFO: Pod "pod-projected-configmaps-41d3b7f7-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035342133s
Sep  6 09:25:29.074: INFO: Pod "pod-projected-configmaps-41d3b7f7-d088-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045225293s
STEP: Saw pod success
Sep  6 09:25:29.074: INFO: Pod "pod-projected-configmaps-41d3b7f7-d088-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:25:29.083: INFO: Trying to get logs from node kube-node-20-104 pod pod-projected-configmaps-41d3b7f7-d088-11e9-b48b-b617478434fe container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 09:25:29.154: INFO: Waiting for pod pod-projected-configmaps-41d3b7f7-d088-11e9-b48b-b617478434fe to disappear
Sep  6 09:25:29.165: INFO: Pod pod-projected-configmaps-41d3b7f7-d088-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:25:29.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9442" for this suite.
Sep  6 09:25:35.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:25:35.491: INFO: namespace projected-9442 deletion completed in 6.308226271s

• [SLOW TEST:10.754 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:25:35.492: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6383
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 09:25:35.722: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4833e45a-d088-11e9-b48b-b617478434fe" in namespace "projected-6383" to be "success or failure"
Sep  6 09:25:35.731: INFO: Pod "downwardapi-volume-4833e45a-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.332831ms
Sep  6 09:25:37.744: INFO: Pod "downwardapi-volume-4833e45a-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021783294s
Sep  6 09:25:39.756: INFO: Pod "downwardapi-volume-4833e45a-d088-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033440268s
STEP: Saw pod success
Sep  6 09:25:39.756: INFO: Pod "downwardapi-volume-4833e45a-d088-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:25:39.764: INFO: Trying to get logs from node kube-node-20-104 pod downwardapi-volume-4833e45a-d088-11e9-b48b-b617478434fe container client-container: <nil>
STEP: delete the pod
Sep  6 09:25:39.819: INFO: Waiting for pod downwardapi-volume-4833e45a-d088-11e9-b48b-b617478434fe to disappear
Sep  6 09:25:39.826: INFO: Pod downwardapi-volume-4833e45a-d088-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:25:39.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6383" for this suite.
Sep  6 09:25:45.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:25:46.190: INFO: namespace projected-6383 deletion completed in 6.348169609s

• [SLOW TEST:10.698 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:25:46.191: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-676
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Sep  6 09:25:46.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 create -f - --namespace=kubectl-676'
Sep  6 09:25:47.427: INFO: stderr: ""
Sep  6 09:25:47.427: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 09:25:47.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-676'
Sep  6 09:25:47.651: INFO: stderr: ""
Sep  6 09:25:47.651: INFO: stdout: "update-demo-nautilus-6gssq update-demo-nautilus-nrhkz "
Sep  6 09:25:47.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-6gssq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-676'
Sep  6 09:25:47.847: INFO: stderr: ""
Sep  6 09:25:47.847: INFO: stdout: ""
Sep  6 09:25:47.847: INFO: update-demo-nautilus-6gssq is created but not running
Sep  6 09:25:52.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-676'
Sep  6 09:25:52.977: INFO: stderr: ""
Sep  6 09:25:52.977: INFO: stdout: "update-demo-nautilus-6gssq update-demo-nautilus-nrhkz "
Sep  6 09:25:52.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-6gssq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-676'
Sep  6 09:25:53.169: INFO: stderr: ""
Sep  6 09:25:53.169: INFO: stdout: "true"
Sep  6 09:25:53.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-6gssq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-676'
Sep  6 09:25:53.320: INFO: stderr: ""
Sep  6 09:25:53.320: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 09:25:53.320: INFO: validating pod update-demo-nautilus-6gssq
Sep  6 09:25:53.334: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 09:25:53.335: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 09:25:53.335: INFO: update-demo-nautilus-6gssq is verified up and running
Sep  6 09:25:53.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-nrhkz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-676'
Sep  6 09:25:53.495: INFO: stderr: ""
Sep  6 09:25:53.495: INFO: stdout: "true"
Sep  6 09:25:53.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-nrhkz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-676'
Sep  6 09:25:53.655: INFO: stderr: ""
Sep  6 09:25:53.655: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 09:25:53.655: INFO: validating pod update-demo-nautilus-nrhkz
Sep  6 09:25:53.668: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 09:25:53.668: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 09:25:53.668: INFO: update-demo-nautilus-nrhkz is verified up and running
STEP: scaling down the replication controller
Sep  6 09:25:53.671: INFO: scanned /root for discovery docs: <nil>
Sep  6 09:25:53.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-676'
Sep  6 09:25:54.934: INFO: stderr: ""
Sep  6 09:25:54.934: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 09:25:54.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-676'
Sep  6 09:25:55.104: INFO: stderr: ""
Sep  6 09:25:55.104: INFO: stdout: "update-demo-nautilus-6gssq update-demo-nautilus-nrhkz "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep  6 09:26:00.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-676'
Sep  6 09:26:00.326: INFO: stderr: ""
Sep  6 09:26:00.326: INFO: stdout: "update-demo-nautilus-6gssq "
Sep  6 09:26:00.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-6gssq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-676'
Sep  6 09:26:00.524: INFO: stderr: ""
Sep  6 09:26:00.524: INFO: stdout: "true"
Sep  6 09:26:00.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-6gssq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-676'
Sep  6 09:26:00.675: INFO: stderr: ""
Sep  6 09:26:00.675: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 09:26:00.675: INFO: validating pod update-demo-nautilus-6gssq
Sep  6 09:26:00.688: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 09:26:00.688: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 09:26:00.688: INFO: update-demo-nautilus-6gssq is verified up and running
STEP: scaling up the replication controller
Sep  6 09:26:00.692: INFO: scanned /root for discovery docs: <nil>
Sep  6 09:26:00.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-676'
Sep  6 09:26:01.892: INFO: stderr: ""
Sep  6 09:26:01.892: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 09:26:01.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-676'
Sep  6 09:26:02.074: INFO: stderr: ""
Sep  6 09:26:02.074: INFO: stdout: "update-demo-nautilus-2hqsp update-demo-nautilus-6gssq "
Sep  6 09:26:02.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-2hqsp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-676'
Sep  6 09:26:02.346: INFO: stderr: ""
Sep  6 09:26:02.346: INFO: stdout: ""
Sep  6 09:26:02.346: INFO: update-demo-nautilus-2hqsp is created but not running
Sep  6 09:26:07.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-676'
Sep  6 09:26:07.500: INFO: stderr: ""
Sep  6 09:26:07.501: INFO: stdout: "update-demo-nautilus-2hqsp update-demo-nautilus-6gssq "
Sep  6 09:26:07.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-2hqsp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-676'
Sep  6 09:26:07.718: INFO: stderr: ""
Sep  6 09:26:07.718: INFO: stdout: "true"
Sep  6 09:26:07.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-2hqsp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-676'
Sep  6 09:26:07.900: INFO: stderr: ""
Sep  6 09:26:07.900: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 09:26:07.900: INFO: validating pod update-demo-nautilus-2hqsp
Sep  6 09:26:07.917: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 09:26:07.918: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 09:26:07.918: INFO: update-demo-nautilus-2hqsp is verified up and running
Sep  6 09:26:07.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-6gssq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-676'
Sep  6 09:26:08.101: INFO: stderr: ""
Sep  6 09:26:08.101: INFO: stdout: "true"
Sep  6 09:26:08.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-6gssq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-676'
Sep  6 09:26:08.294: INFO: stderr: ""
Sep  6 09:26:08.294: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 09:26:08.294: INFO: validating pod update-demo-nautilus-6gssq
Sep  6 09:26:08.307: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 09:26:08.307: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 09:26:08.307: INFO: update-demo-nautilus-6gssq is verified up and running
STEP: using delete to clean up resources
Sep  6 09:26:08.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 delete --grace-period=0 --force -f - --namespace=kubectl-676'
Sep  6 09:26:08.495: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 09:26:08.495: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  6 09:26:08.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-676'
Sep  6 09:26:08.700: INFO: stderr: "No resources found.\n"
Sep  6 09:26:08.700: INFO: stdout: ""
Sep  6 09:26:08.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods -l name=update-demo --namespace=kubectl-676 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 09:26:08.841: INFO: stderr: ""
Sep  6 09:26:08.841: INFO: stdout: "update-demo-nautilus-2hqsp\nupdate-demo-nautilus-6gssq\n"
Sep  6 09:26:09.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-676'
Sep  6 09:26:09.546: INFO: stderr: "No resources found.\n"
Sep  6 09:26:09.546: INFO: stdout: ""
Sep  6 09:26:09.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods -l name=update-demo --namespace=kubectl-676 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 09:26:09.742: INFO: stderr: ""
Sep  6 09:26:09.742: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:26:09.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-676" for this suite.
Sep  6 09:26:33.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:26:34.078: INFO: namespace kubectl-676 deletion completed in 24.319943608s

• [SLOW TEST:47.888 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:26:34.080: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2097
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  6 09:26:34.331: INFO: Waiting up to 5m0s for pod "pod-6b2283d2-d088-11e9-b48b-b617478434fe" in namespace "emptydir-2097" to be "success or failure"
Sep  6 09:26:34.348: INFO: Pod "pod-6b2283d2-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 16.170599ms
Sep  6 09:26:36.362: INFO: Pod "pod-6b2283d2-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030098202s
Sep  6 09:26:38.371: INFO: Pod "pod-6b2283d2-d088-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039027963s
STEP: Saw pod success
Sep  6 09:26:38.371: INFO: Pod "pod-6b2283d2-d088-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:26:38.381: INFO: Trying to get logs from node kube-node-20-104 pod pod-6b2283d2-d088-11e9-b48b-b617478434fe container test-container: <nil>
STEP: delete the pod
Sep  6 09:26:38.435: INFO: Waiting for pod pod-6b2283d2-d088-11e9-b48b-b617478434fe to disappear
Sep  6 09:26:38.451: INFO: Pod pod-6b2283d2-d088-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:26:38.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2097" for this suite.
Sep  6 09:26:44.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:26:44.837: INFO: namespace emptydir-2097 deletion completed in 6.37406922s

• [SLOW TEST:10.758 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:26:44.838: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6913
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0906 09:26:51.154781      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 09:26:51.155: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:26:51.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6913" for this suite.
Sep  6 09:26:59.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:26:59.517: INFO: namespace gc-6913 deletion completed in 8.350026241s

• [SLOW TEST:14.679 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:26:59.519: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4328
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3827
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4230
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:27:27.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4328" for this suite.
Sep  6 09:27:33.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:27:33.539: INFO: namespace namespaces-4328 deletion completed in 6.322320294s
STEP: Destroying namespace "nsdeletetest-3827" for this suite.
Sep  6 09:27:33.557: INFO: Namespace nsdeletetest-3827 was already deleted
STEP: Destroying namespace "nsdeletetest-4230" for this suite.
Sep  6 09:27:39.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:27:39.903: INFO: namespace nsdeletetest-4230 deletion completed in 6.346674853s

• [SLOW TEST:40.385 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:27:39.905: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-173
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-9260665b-d088-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume configMaps
Sep  6 09:27:40.176: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9261e509-d088-11e9-b48b-b617478434fe" in namespace "projected-173" to be "success or failure"
Sep  6 09:27:40.192: INFO: Pod "pod-projected-configmaps-9261e509-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 16.522104ms
Sep  6 09:27:42.201: INFO: Pod "pod-projected-configmaps-9261e509-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025179764s
Sep  6 09:27:44.212: INFO: Pod "pod-projected-configmaps-9261e509-d088-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036262236s
STEP: Saw pod success
Sep  6 09:27:44.212: INFO: Pod "pod-projected-configmaps-9261e509-d088-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:27:44.222: INFO: Trying to get logs from node kube-node-20-104 pod pod-projected-configmaps-9261e509-d088-11e9-b48b-b617478434fe container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 09:27:44.303: INFO: Waiting for pod pod-projected-configmaps-9261e509-d088-11e9-b48b-b617478434fe to disappear
Sep  6 09:27:44.318: INFO: Pod pod-projected-configmaps-9261e509-d088-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:27:44.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-173" for this suite.
Sep  6 09:27:50.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:27:50.690: INFO: namespace projected-173 deletion completed in 6.357275165s

• [SLOW TEST:10.786 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:27:50.692: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-1906
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1906
I0906 09:27:50.927008      16 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1906, replica count: 1
I0906 09:27:51.978014      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 09:27:52.978418      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 09:27:53.978702      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 09:27:54.979005      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 09:27:55.979473      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 09:27:56.102: INFO: Created: latency-svc-j2t5r
Sep  6 09:27:56.115: INFO: Got endpoints: latency-svc-j2t5r [34.921736ms]
Sep  6 09:27:56.153: INFO: Created: latency-svc-qpt8n
Sep  6 09:27:56.174: INFO: Got endpoints: latency-svc-qpt8n [58.951185ms]
Sep  6 09:27:56.176: INFO: Created: latency-svc-hpmt5
Sep  6 09:27:56.197: INFO: Got endpoints: latency-svc-hpmt5 [81.522669ms]
Sep  6 09:27:56.220: INFO: Created: latency-svc-k4kmh
Sep  6 09:27:56.257: INFO: Got endpoints: latency-svc-k4kmh [141.705112ms]
Sep  6 09:27:56.274: INFO: Created: latency-svc-7rnvr
Sep  6 09:27:56.285: INFO: Got endpoints: latency-svc-7rnvr [168.843877ms]
Sep  6 09:27:56.308: INFO: Created: latency-svc-thjhd
Sep  6 09:27:56.329: INFO: Got endpoints: latency-svc-thjhd [213.094222ms]
Sep  6 09:27:56.338: INFO: Created: latency-svc-zcmtd
Sep  6 09:27:56.351: INFO: Got endpoints: latency-svc-zcmtd [235.269714ms]
Sep  6 09:27:56.355: INFO: Created: latency-svc-lnd54
Sep  6 09:27:56.373: INFO: Got endpoints: latency-svc-lnd54 [256.422735ms]
Sep  6 09:27:56.380: INFO: Created: latency-svc-bgjmv
Sep  6 09:27:56.392: INFO: Got endpoints: latency-svc-bgjmv [275.25833ms]
Sep  6 09:27:56.405: INFO: Created: latency-svc-fk72l
Sep  6 09:27:56.421: INFO: Got endpoints: latency-svc-fk72l [305.237977ms]
Sep  6 09:27:56.434: INFO: Created: latency-svc-794fg
Sep  6 09:27:56.457: INFO: Got endpoints: latency-svc-794fg [340.296144ms]
Sep  6 09:27:56.463: INFO: Created: latency-svc-7lj6x
Sep  6 09:27:56.486: INFO: Got endpoints: latency-svc-7lj6x [369.80535ms]
Sep  6 09:27:56.504: INFO: Created: latency-svc-l74pq
Sep  6 09:27:56.513: INFO: Got endpoints: latency-svc-l74pq [396.256519ms]
Sep  6 09:27:56.551: INFO: Created: latency-svc-9lnfc
Sep  6 09:27:56.568: INFO: Got endpoints: latency-svc-9lnfc [451.561632ms]
Sep  6 09:27:56.585: INFO: Created: latency-svc-4pwcr
Sep  6 09:27:56.603: INFO: Got endpoints: latency-svc-4pwcr [486.718213ms]
Sep  6 09:27:56.621: INFO: Created: latency-svc-mhrcx
Sep  6 09:27:56.640: INFO: Got endpoints: latency-svc-mhrcx [523.445697ms]
Sep  6 09:27:56.652: INFO: Created: latency-svc-8jvvs
Sep  6 09:27:56.667: INFO: Got endpoints: latency-svc-8jvvs [493.180848ms]
Sep  6 09:27:56.672: INFO: Created: latency-svc-4zbb6
Sep  6 09:27:56.684: INFO: Got endpoints: latency-svc-4zbb6 [487.586168ms]
Sep  6 09:27:56.705: INFO: Created: latency-svc-jxd6z
Sep  6 09:27:56.716: INFO: Got endpoints: latency-svc-jxd6z [458.82119ms]
Sep  6 09:27:56.733: INFO: Created: latency-svc-47594
Sep  6 09:27:56.752: INFO: Got endpoints: latency-svc-47594 [467.015841ms]
Sep  6 09:27:56.785: INFO: Created: latency-svc-rctrj
Sep  6 09:27:56.797: INFO: Created: latency-svc-zdpkd
Sep  6 09:27:56.797: INFO: Got endpoints: latency-svc-rctrj [468.439968ms]
Sep  6 09:27:56.804: INFO: Got endpoints: latency-svc-zdpkd [452.682779ms]
Sep  6 09:27:56.831: INFO: Created: latency-svc-n6js2
Sep  6 09:27:56.841: INFO: Got endpoints: latency-svc-n6js2 [467.878227ms]
Sep  6 09:27:56.851: INFO: Created: latency-svc-v5gr8
Sep  6 09:27:56.872: INFO: Created: latency-svc-28fl6
Sep  6 09:27:56.873: INFO: Got endpoints: latency-svc-v5gr8 [480.858553ms]
Sep  6 09:27:56.901: INFO: Got endpoints: latency-svc-28fl6 [479.421091ms]
Sep  6 09:27:56.902: INFO: Created: latency-svc-6bhhz
Sep  6 09:27:56.910: INFO: Got endpoints: latency-svc-6bhhz [453.281694ms]
Sep  6 09:27:56.938: INFO: Created: latency-svc-28k46
Sep  6 09:27:56.970: INFO: Got endpoints: latency-svc-28k46 [481.392784ms]
Sep  6 09:27:56.988: INFO: Created: latency-svc-7nbq5
Sep  6 09:27:57.023: INFO: Got endpoints: latency-svc-7nbq5 [509.705866ms]
Sep  6 09:27:57.026: INFO: Created: latency-svc-5bcxv
Sep  6 09:27:57.049: INFO: Got endpoints: latency-svc-5bcxv [480.213477ms]
Sep  6 09:27:57.058: INFO: Created: latency-svc-fq9vb
Sep  6 09:27:57.079: INFO: Got endpoints: latency-svc-fq9vb [475.700269ms]
Sep  6 09:27:57.089: INFO: Created: latency-svc-68lsn
Sep  6 09:27:57.114: INFO: Created: latency-svc-4tfbn
Sep  6 09:27:57.117: INFO: Got endpoints: latency-svc-68lsn [476.586813ms]
Sep  6 09:27:57.140: INFO: Got endpoints: latency-svc-4tfbn [472.932777ms]
Sep  6 09:27:57.145: INFO: Created: latency-svc-24lfc
Sep  6 09:27:57.168: INFO: Got endpoints: latency-svc-24lfc [483.947001ms]
Sep  6 09:27:57.186: INFO: Created: latency-svc-wdjf5
Sep  6 09:27:57.210: INFO: Got endpoints: latency-svc-wdjf5 [494.133355ms]
Sep  6 09:27:57.214: INFO: Created: latency-svc-hmq4g
Sep  6 09:27:57.226: INFO: Got endpoints: latency-svc-hmq4g [474.00806ms]
Sep  6 09:27:57.232: INFO: Created: latency-svc-snxm6
Sep  6 09:27:57.253: INFO: Got endpoints: latency-svc-snxm6 [455.504058ms]
Sep  6 09:27:57.255: INFO: Created: latency-svc-826lr
Sep  6 09:27:57.272: INFO: Got endpoints: latency-svc-826lr [468.001794ms]
Sep  6 09:27:57.287: INFO: Created: latency-svc-xsf9z
Sep  6 09:27:57.317: INFO: Got endpoints: latency-svc-xsf9z [475.894754ms]
Sep  6 09:27:57.323: INFO: Created: latency-svc-z5zlj
Sep  6 09:27:57.339: INFO: Got endpoints: latency-svc-z5zlj [466.246634ms]
Sep  6 09:27:57.356: INFO: Created: latency-svc-9f2k5
Sep  6 09:27:57.375: INFO: Got endpoints: latency-svc-9f2k5 [474.34814ms]
Sep  6 09:27:57.389: INFO: Created: latency-svc-fdhwt
Sep  6 09:27:57.402: INFO: Got endpoints: latency-svc-fdhwt [491.157797ms]
Sep  6 09:27:57.435: INFO: Created: latency-svc-5kkc2
Sep  6 09:27:57.456: INFO: Got endpoints: latency-svc-5kkc2 [485.669442ms]
Sep  6 09:27:57.468: INFO: Created: latency-svc-drn5g
Sep  6 09:27:57.489: INFO: Got endpoints: latency-svc-drn5g [465.930189ms]
Sep  6 09:27:57.491: INFO: Created: latency-svc-g8bnp
Sep  6 09:27:57.502: INFO: Got endpoints: latency-svc-g8bnp [452.491772ms]
Sep  6 09:27:57.545: INFO: Created: latency-svc-vf9ln
Sep  6 09:27:57.561: INFO: Got endpoints: latency-svc-vf9ln [481.763412ms]
Sep  6 09:27:57.568: INFO: Created: latency-svc-czr62
Sep  6 09:27:57.588: INFO: Got endpoints: latency-svc-czr62 [470.783889ms]
Sep  6 09:27:57.603: INFO: Created: latency-svc-bw646
Sep  6 09:27:57.625: INFO: Got endpoints: latency-svc-bw646 [484.984697ms]
Sep  6 09:27:57.637: INFO: Created: latency-svc-xwjpz
Sep  6 09:27:57.664: INFO: Got endpoints: latency-svc-xwjpz [495.203436ms]
Sep  6 09:27:57.677: INFO: Created: latency-svc-nckvp
Sep  6 09:27:57.696: INFO: Got endpoints: latency-svc-nckvp [485.68169ms]
Sep  6 09:27:57.712: INFO: Created: latency-svc-lx5vr
Sep  6 09:27:57.733: INFO: Got endpoints: latency-svc-lx5vr [506.949093ms]
Sep  6 09:27:57.771: INFO: Created: latency-svc-cnnzl
Sep  6 09:27:57.789: INFO: Got endpoints: latency-svc-cnnzl [536.225675ms]
Sep  6 09:27:57.802: INFO: Created: latency-svc-5f76p
Sep  6 09:27:57.828: INFO: Got endpoints: latency-svc-5f76p [555.700286ms]
Sep  6 09:27:57.829: INFO: Created: latency-svc-tmsgg
Sep  6 09:27:57.857: INFO: Created: latency-svc-ml4hx
Sep  6 09:27:57.857: INFO: Got endpoints: latency-svc-tmsgg [539.904046ms]
Sep  6 09:27:57.906: INFO: Created: latency-svc-cqhrp
Sep  6 09:27:57.909: INFO: Got endpoints: latency-svc-ml4hx [570.124342ms]
Sep  6 09:27:57.921: INFO: Got endpoints: latency-svc-cqhrp [545.731365ms]
Sep  6 09:27:57.944: INFO: Created: latency-svc-lxfwn
Sep  6 09:27:57.985: INFO: Created: latency-svc-g5kf7
Sep  6 09:27:57.990: INFO: Got endpoints: latency-svc-lxfwn [588.191383ms]
Sep  6 09:27:58.015: INFO: Got endpoints: latency-svc-g5kf7 [93.505577ms]
Sep  6 09:27:58.034: INFO: Created: latency-svc-7k2dj
Sep  6 09:27:58.063: INFO: Created: latency-svc-qq7xj
Sep  6 09:27:58.068: INFO: Got endpoints: latency-svc-7k2dj [611.858232ms]
Sep  6 09:27:58.085: INFO: Got endpoints: latency-svc-qq7xj [596.102821ms]
Sep  6 09:27:58.108: INFO: Created: latency-svc-kwwgh
Sep  6 09:27:58.120: INFO: Got endpoints: latency-svc-kwwgh [618.309806ms]
Sep  6 09:27:58.135: INFO: Created: latency-svc-kdxzv
Sep  6 09:27:58.158: INFO: Created: latency-svc-mqcsp
Sep  6 09:27:58.161: INFO: Got endpoints: latency-svc-kdxzv [599.410651ms]
Sep  6 09:27:58.170: INFO: Got endpoints: latency-svc-mqcsp [582.221478ms]
Sep  6 09:27:58.191: INFO: Created: latency-svc-j9pfm
Sep  6 09:27:58.222: INFO: Got endpoints: latency-svc-j9pfm [597.211754ms]
Sep  6 09:27:58.242: INFO: Created: latency-svc-v9btn
Sep  6 09:27:58.299: INFO: Got endpoints: latency-svc-v9btn [634.817595ms]
Sep  6 09:27:58.299: INFO: Created: latency-svc-5jwrf
Sep  6 09:27:58.315: INFO: Got endpoints: latency-svc-5jwrf [619.499833ms]
Sep  6 09:27:58.328: INFO: Created: latency-svc-8mdtd
Sep  6 09:27:58.366: INFO: Got endpoints: latency-svc-8mdtd [632.974852ms]
Sep  6 09:27:58.390: INFO: Created: latency-svc-8b2g6
Sep  6 09:27:58.417: INFO: Created: latency-svc-vskvj
Sep  6 09:27:58.422: INFO: Got endpoints: latency-svc-8b2g6 [633.172002ms]
Sep  6 09:27:58.443: INFO: Got endpoints: latency-svc-vskvj [614.472152ms]
Sep  6 09:27:58.466: INFO: Created: latency-svc-vfvmg
Sep  6 09:27:58.485: INFO: Got endpoints: latency-svc-vfvmg [627.786225ms]
Sep  6 09:27:58.518: INFO: Created: latency-svc-zdh7n
Sep  6 09:27:58.539: INFO: Got endpoints: latency-svc-zdh7n [629.686732ms]
Sep  6 09:27:58.545: INFO: Created: latency-svc-v4pqx
Sep  6 09:27:58.571: INFO: Created: latency-svc-vhd2v
Sep  6 09:27:58.577: INFO: Got endpoints: latency-svc-v4pqx [586.736649ms]
Sep  6 09:27:58.612: INFO: Got endpoints: latency-svc-vhd2v [597.02807ms]
Sep  6 09:27:58.623: INFO: Created: latency-svc-kczzk
Sep  6 09:27:58.658: INFO: Got endpoints: latency-svc-kczzk [590.441583ms]
Sep  6 09:27:58.660: INFO: Created: latency-svc-zc68t
Sep  6 09:27:58.698: INFO: Created: latency-svc-m6zkd
Sep  6 09:27:58.698: INFO: Got endpoints: latency-svc-zc68t [613.480571ms]
Sep  6 09:27:58.721: INFO: Got endpoints: latency-svc-m6zkd [600.947423ms]
Sep  6 09:27:58.734: INFO: Created: latency-svc-mnnlj
Sep  6 09:27:58.756: INFO: Got endpoints: latency-svc-mnnlj [595.307158ms]
Sep  6 09:27:58.774: INFO: Created: latency-svc-6rfqj
Sep  6 09:27:58.809: INFO: Got endpoints: latency-svc-6rfqj [638.148457ms]
Sep  6 09:27:58.812: INFO: Created: latency-svc-5w4sv
Sep  6 09:27:58.831: INFO: Got endpoints: latency-svc-5w4sv [608.809457ms]
Sep  6 09:27:58.854: INFO: Created: latency-svc-kxcqb
Sep  6 09:27:58.877: INFO: Got endpoints: latency-svc-kxcqb [578.273991ms]
Sep  6 09:27:58.889: INFO: Created: latency-svc-ml6wm
Sep  6 09:27:58.930: INFO: Got endpoints: latency-svc-ml6wm [614.538886ms]
Sep  6 09:27:58.937: INFO: Created: latency-svc-jvs2s
Sep  6 09:27:58.959: INFO: Got endpoints: latency-svc-jvs2s [592.521319ms]
Sep  6 09:27:58.988: INFO: Created: latency-svc-j5cxd
Sep  6 09:27:59.036: INFO: Created: latency-svc-dbcxs
Sep  6 09:27:59.043: INFO: Got endpoints: latency-svc-j5cxd [621.172618ms]
Sep  6 09:27:59.059: INFO: Created: latency-svc-w9mqt
Sep  6 09:27:59.059: INFO: Got endpoints: latency-svc-dbcxs [615.797035ms]
Sep  6 09:27:59.107: INFO: Got endpoints: latency-svc-w9mqt [621.792098ms]
Sep  6 09:27:59.125: INFO: Created: latency-svc-xsb4g
Sep  6 09:27:59.137: INFO: Got endpoints: latency-svc-xsb4g [598.286398ms]
Sep  6 09:27:59.145: INFO: Created: latency-svc-bzkb5
Sep  6 09:27:59.193: INFO: Got endpoints: latency-svc-bzkb5 [616.075745ms]
Sep  6 09:27:59.201: INFO: Created: latency-svc-hx2bt
Sep  6 09:27:59.226: INFO: Got endpoints: latency-svc-hx2bt [613.900433ms]
Sep  6 09:27:59.262: INFO: Created: latency-svc-7ccbt
Sep  6 09:27:59.277: INFO: Got endpoints: latency-svc-7ccbt [618.621533ms]
Sep  6 09:27:59.287: INFO: Created: latency-svc-4k4lh
Sep  6 09:27:59.312: INFO: Created: latency-svc-wm5zd
Sep  6 09:27:59.323: INFO: Got endpoints: latency-svc-4k4lh [624.382833ms]
Sep  6 09:27:59.342: INFO: Got endpoints: latency-svc-wm5zd [620.720773ms]
Sep  6 09:27:59.360: INFO: Created: latency-svc-k7h8p
Sep  6 09:27:59.392: INFO: Got endpoints: latency-svc-k7h8p [636.016786ms]
Sep  6 09:27:59.414: INFO: Created: latency-svc-kql2g
Sep  6 09:27:59.445: INFO: Got endpoints: latency-svc-kql2g [636.54664ms]
Sep  6 09:27:59.465: INFO: Created: latency-svc-xcvtt
Sep  6 09:27:59.485: INFO: Got endpoints: latency-svc-xcvtt [654.070361ms]
Sep  6 09:27:59.491: INFO: Created: latency-svc-xjb64
Sep  6 09:27:59.505: INFO: Got endpoints: latency-svc-xjb64 [628.180644ms]
Sep  6 09:27:59.542: INFO: Created: latency-svc-jnml4
Sep  6 09:27:59.576: INFO: Got endpoints: latency-svc-jnml4 [646.28186ms]
Sep  6 09:27:59.577: INFO: Created: latency-svc-wjkkp
Sep  6 09:27:59.606: INFO: Got endpoints: latency-svc-wjkkp [647.553701ms]
Sep  6 09:27:59.614: INFO: Created: latency-svc-vq4h2
Sep  6 09:27:59.626: INFO: Got endpoints: latency-svc-vq4h2 [582.230031ms]
Sep  6 09:27:59.641: INFO: Created: latency-svc-4jdsw
Sep  6 09:27:59.661: INFO: Got endpoints: latency-svc-4jdsw [602.4059ms]
Sep  6 09:27:59.681: INFO: Created: latency-svc-p6wk8
Sep  6 09:27:59.697: INFO: Got endpoints: latency-svc-p6wk8 [590.711254ms]
Sep  6 09:27:59.699: INFO: Created: latency-svc-6gnlv
Sep  6 09:27:59.715: INFO: Got endpoints: latency-svc-6gnlv [577.659731ms]
Sep  6 09:27:59.727: INFO: Created: latency-svc-lhpxz
Sep  6 09:27:59.744: INFO: Got endpoints: latency-svc-lhpxz [551.513951ms]
Sep  6 09:27:59.755: INFO: Created: latency-svc-fq79k
Sep  6 09:27:59.765: INFO: Got endpoints: latency-svc-fq79k [538.47409ms]
Sep  6 09:27:59.783: INFO: Created: latency-svc-kbjhd
Sep  6 09:27:59.823: INFO: Got endpoints: latency-svc-kbjhd [546.181458ms]
Sep  6 09:27:59.839: INFO: Created: latency-svc-nm5ts
Sep  6 09:27:59.857: INFO: Got endpoints: latency-svc-nm5ts [534.025032ms]
Sep  6 09:27:59.887: INFO: Created: latency-svc-dwj95
Sep  6 09:27:59.913: INFO: Got endpoints: latency-svc-dwj95 [571.247737ms]
Sep  6 09:27:59.923: INFO: Created: latency-svc-7gh8j
Sep  6 09:27:59.943: INFO: Got endpoints: latency-svc-7gh8j [550.519312ms]
Sep  6 09:27:59.973: INFO: Created: latency-svc-5qlrz
Sep  6 09:28:00.004: INFO: Got endpoints: latency-svc-5qlrz [558.640828ms]
Sep  6 09:28:00.027: INFO: Created: latency-svc-b9pbd
Sep  6 09:28:00.044: INFO: Created: latency-svc-fl9m4
Sep  6 09:28:00.052: INFO: Got endpoints: latency-svc-b9pbd [566.857593ms]
Sep  6 09:28:00.077: INFO: Created: latency-svc-t4449
Sep  6 09:28:00.082: INFO: Got endpoints: latency-svc-fl9m4 [576.863272ms]
Sep  6 09:28:00.111: INFO: Created: latency-svc-2jhtf
Sep  6 09:28:00.165: INFO: Got endpoints: latency-svc-t4449 [588.35971ms]
Sep  6 09:28:00.166: INFO: Created: latency-svc-lxv6q
Sep  6 09:28:00.195: INFO: Got endpoints: latency-svc-2jhtf [588.159627ms]
Sep  6 09:28:00.265: INFO: Created: latency-svc-ggp6p
Sep  6 09:28:00.272: INFO: Got endpoints: latency-svc-lxv6q [646.407919ms]
Sep  6 09:28:00.316: INFO: Got endpoints: latency-svc-ggp6p [654.705984ms]
Sep  6 09:28:00.359: INFO: Created: latency-svc-p6qbs
Sep  6 09:28:00.392: INFO: Got endpoints: latency-svc-p6qbs [694.703048ms]
Sep  6 09:28:00.404: INFO: Created: latency-svc-fbkjj
Sep  6 09:28:00.436: INFO: Got endpoints: latency-svc-fbkjj [720.830668ms]
Sep  6 09:28:00.493: INFO: Created: latency-svc-2qq4k
Sep  6 09:28:00.551: INFO: Got endpoints: latency-svc-2qq4k [806.438191ms]
Sep  6 09:28:00.576: INFO: Created: latency-svc-f45qk
Sep  6 09:28:00.612: INFO: Got endpoints: latency-svc-f45qk [846.81364ms]
Sep  6 09:28:00.621: INFO: Created: latency-svc-gl6kq
Sep  6 09:28:00.664: INFO: Got endpoints: latency-svc-gl6kq [840.640722ms]
Sep  6 09:28:00.698: INFO: Created: latency-svc-tmmdg
Sep  6 09:28:00.722: INFO: Got endpoints: latency-svc-tmmdg [864.589062ms]
Sep  6 09:28:00.754: INFO: Created: latency-svc-wtw4l
Sep  6 09:28:00.770: INFO: Got endpoints: latency-svc-wtw4l [856.866543ms]
Sep  6 09:28:00.793: INFO: Created: latency-svc-ns64m
Sep  6 09:28:00.803: INFO: Got endpoints: latency-svc-ns64m [860.177131ms]
Sep  6 09:28:00.816: INFO: Created: latency-svc-hpdnj
Sep  6 09:28:00.837: INFO: Got endpoints: latency-svc-hpdnj [832.571456ms]
Sep  6 09:28:00.875: INFO: Created: latency-svc-5pbwp
Sep  6 09:28:00.898: INFO: Got endpoints: latency-svc-5pbwp [844.969886ms]
Sep  6 09:28:00.911: INFO: Created: latency-svc-tpdkj
Sep  6 09:28:00.934: INFO: Got endpoints: latency-svc-tpdkj [851.118314ms]
Sep  6 09:28:00.950: INFO: Created: latency-svc-625mj
Sep  6 09:28:00.972: INFO: Got endpoints: latency-svc-625mj [806.968386ms]
Sep  6 09:28:01.001: INFO: Created: latency-svc-bcmnw
Sep  6 09:28:01.028: INFO: Got endpoints: latency-svc-bcmnw [832.921131ms]
Sep  6 09:28:01.046: INFO: Created: latency-svc-2wjgz
Sep  6 09:28:01.069: INFO: Got endpoints: latency-svc-2wjgz [796.389443ms]
Sep  6 09:28:01.088: INFO: Created: latency-svc-hwtv7
Sep  6 09:28:01.116: INFO: Got endpoints: latency-svc-hwtv7 [799.878588ms]
Sep  6 09:28:01.148: INFO: Created: latency-svc-89mjf
Sep  6 09:28:01.187: INFO: Got endpoints: latency-svc-89mjf [794.863102ms]
Sep  6 09:28:01.210: INFO: Created: latency-svc-gb7hn
Sep  6 09:28:01.256: INFO: Created: latency-svc-pbtnm
Sep  6 09:28:01.262: INFO: Got endpoints: latency-svc-gb7hn [825.807249ms]
Sep  6 09:28:01.300: INFO: Got endpoints: latency-svc-pbtnm [748.868693ms]
Sep  6 09:28:01.301: INFO: Created: latency-svc-nmvhl
Sep  6 09:28:01.326: INFO: Got endpoints: latency-svc-nmvhl [714.497517ms]
Sep  6 09:28:01.350: INFO: Created: latency-svc-b89n8
Sep  6 09:28:01.400: INFO: Got endpoints: latency-svc-b89n8 [735.863228ms]
Sep  6 09:28:01.414: INFO: Created: latency-svc-875tm
Sep  6 09:28:01.446: INFO: Got endpoints: latency-svc-875tm [724.580348ms]
Sep  6 09:28:01.460: INFO: Created: latency-svc-fqtx9
Sep  6 09:28:01.498: INFO: Got endpoints: latency-svc-fqtx9 [727.376518ms]
Sep  6 09:28:01.507: INFO: Created: latency-svc-wqp4s
Sep  6 09:28:01.560: INFO: Created: latency-svc-z8z89
Sep  6 09:28:01.560: INFO: Got endpoints: latency-svc-wqp4s [756.483931ms]
Sep  6 09:28:01.590: INFO: Got endpoints: latency-svc-z8z89 [753.546261ms]
Sep  6 09:28:01.609: INFO: Created: latency-svc-vplxt
Sep  6 09:28:01.629: INFO: Got endpoints: latency-svc-vplxt [731.077644ms]
Sep  6 09:28:01.666: INFO: Created: latency-svc-gpngl
Sep  6 09:28:01.682: INFO: Got endpoints: latency-svc-gpngl [748.637395ms]
Sep  6 09:28:01.696: INFO: Created: latency-svc-47t8p
Sep  6 09:28:01.742: INFO: Created: latency-svc-nj52z
Sep  6 09:28:01.742: INFO: Got endpoints: latency-svc-47t8p [769.733594ms]
Sep  6 09:28:01.785: INFO: Got endpoints: latency-svc-nj52z [756.869611ms]
Sep  6 09:28:01.788: INFO: Created: latency-svc-grx29
Sep  6 09:28:01.833: INFO: Got endpoints: latency-svc-grx29 [763.730011ms]
Sep  6 09:28:01.846: INFO: Created: latency-svc-2f5gg
Sep  6 09:28:01.890: INFO: Created: latency-svc-vmqgf
Sep  6 09:28:01.897: INFO: Got endpoints: latency-svc-2f5gg [780.945326ms]
Sep  6 09:28:01.914: INFO: Got endpoints: latency-svc-vmqgf [726.698117ms]
Sep  6 09:28:01.928: INFO: Created: latency-svc-798pg
Sep  6 09:28:01.986: INFO: Got endpoints: latency-svc-798pg [724.02519ms]
Sep  6 09:28:01.996: INFO: Created: latency-svc-zx82d
Sep  6 09:28:02.032: INFO: Got endpoints: latency-svc-zx82d [731.633928ms]
Sep  6 09:28:02.049: INFO: Created: latency-svc-jrzb4
Sep  6 09:28:02.075: INFO: Got endpoints: latency-svc-jrzb4 [748.42414ms]
Sep  6 09:28:02.095: INFO: Created: latency-svc-x4277
Sep  6 09:28:02.127: INFO: Got endpoints: latency-svc-x4277 [726.481789ms]
Sep  6 09:28:02.147: INFO: Created: latency-svc-jvttb
Sep  6 09:28:02.185: INFO: Got endpoints: latency-svc-jvttb [738.264972ms]
Sep  6 09:28:02.193: INFO: Created: latency-svc-d7rv8
Sep  6 09:28:02.222: INFO: Got endpoints: latency-svc-d7rv8 [723.944031ms]
Sep  6 09:28:02.240: INFO: Created: latency-svc-hzklj
Sep  6 09:28:02.265: INFO: Got endpoints: latency-svc-hzklj [705.526449ms]
Sep  6 09:28:02.277: INFO: Created: latency-svc-j58sl
Sep  6 09:28:02.300: INFO: Got endpoints: latency-svc-j58sl [709.325693ms]
Sep  6 09:28:02.339: INFO: Created: latency-svc-wwfks
Sep  6 09:28:02.362: INFO: Got endpoints: latency-svc-wwfks [733.672053ms]
Sep  6 09:28:02.373: INFO: Created: latency-svc-9vd94
Sep  6 09:28:02.398: INFO: Got endpoints: latency-svc-9vd94 [715.170046ms]
Sep  6 09:28:02.400: INFO: Created: latency-svc-hv9zz
Sep  6 09:28:02.421: INFO: Got endpoints: latency-svc-hv9zz [678.601207ms]
Sep  6 09:28:02.441: INFO: Created: latency-svc-mp5zz
Sep  6 09:28:02.475: INFO: Got endpoints: latency-svc-mp5zz [689.418378ms]
Sep  6 09:28:02.495: INFO: Created: latency-svc-69ddd
Sep  6 09:28:02.538: INFO: Got endpoints: latency-svc-69ddd [705.679771ms]
Sep  6 09:28:02.579: INFO: Created: latency-svc-ktchj
Sep  6 09:28:02.638: INFO: Got endpoints: latency-svc-ktchj [740.442121ms]
Sep  6 09:28:02.639: INFO: Created: latency-svc-hjpph
Sep  6 09:28:02.672: INFO: Got endpoints: latency-svc-hjpph [757.449701ms]
Sep  6 09:28:02.698: INFO: Created: latency-svc-vpvj8
Sep  6 09:28:02.743: INFO: Got endpoints: latency-svc-vpvj8 [756.66021ms]
Sep  6 09:28:02.756: INFO: Created: latency-svc-jf8kg
Sep  6 09:28:02.785: INFO: Got endpoints: latency-svc-jf8kg [753.384995ms]
Sep  6 09:28:02.821: INFO: Created: latency-svc-ktk4r
Sep  6 09:28:02.850: INFO: Got endpoints: latency-svc-ktk4r [775.075306ms]
Sep  6 09:28:02.862: INFO: Created: latency-svc-cdht6
Sep  6 09:28:02.897: INFO: Got endpoints: latency-svc-cdht6 [770.239918ms]
Sep  6 09:28:02.929: INFO: Created: latency-svc-gdtjx
Sep  6 09:28:02.962: INFO: Got endpoints: latency-svc-gdtjx [777.394243ms]
Sep  6 09:28:02.961: INFO: Created: latency-svc-xzhpq
Sep  6 09:28:02.990: INFO: Got endpoints: latency-svc-xzhpq [767.78261ms]
Sep  6 09:28:03.004: INFO: Created: latency-svc-fbptg
Sep  6 09:28:03.045: INFO: Got endpoints: latency-svc-fbptg [779.794226ms]
Sep  6 09:28:03.057: INFO: Created: latency-svc-zwxqf
Sep  6 09:28:03.098: INFO: Got endpoints: latency-svc-zwxqf [798.099755ms]
Sep  6 09:28:03.133: INFO: Created: latency-svc-w8rgw
Sep  6 09:28:03.182: INFO: Got endpoints: latency-svc-w8rgw [819.039481ms]
Sep  6 09:28:03.189: INFO: Created: latency-svc-8kczg
Sep  6 09:28:03.211: INFO: Got endpoints: latency-svc-8kczg [813.306643ms]
Sep  6 09:28:03.224: INFO: Created: latency-svc-2cjx8
Sep  6 09:28:03.249: INFO: Got endpoints: latency-svc-2cjx8 [828.295774ms]
Sep  6 09:28:03.263: INFO: Created: latency-svc-sj5wr
Sep  6 09:28:03.301: INFO: Got endpoints: latency-svc-sj5wr [825.99369ms]
Sep  6 09:28:03.310: INFO: Created: latency-svc-vwp9k
Sep  6 09:28:03.331: INFO: Got endpoints: latency-svc-vwp9k [792.404099ms]
Sep  6 09:28:03.337: INFO: Created: latency-svc-4htkx
Sep  6 09:28:03.376: INFO: Got endpoints: latency-svc-4htkx [737.73797ms]
Sep  6 09:28:03.392: INFO: Created: latency-svc-phz5s
Sep  6 09:28:03.414: INFO: Got endpoints: latency-svc-phz5s [742.094858ms]
Sep  6 09:28:03.462: INFO: Created: latency-svc-jw97l
Sep  6 09:28:03.538: INFO: Created: latency-svc-b8skq
Sep  6 09:28:03.546: INFO: Got endpoints: latency-svc-jw97l [803.482728ms]
Sep  6 09:28:03.572: INFO: Got endpoints: latency-svc-b8skq [786.509222ms]
Sep  6 09:28:03.603: INFO: Created: latency-svc-ts44h
Sep  6 09:28:03.617: INFO: Got endpoints: latency-svc-ts44h [767.143428ms]
Sep  6 09:28:03.647: INFO: Created: latency-svc-j5ldj
Sep  6 09:28:03.667: INFO: Got endpoints: latency-svc-j5ldj [769.652146ms]
Sep  6 09:28:03.686: INFO: Created: latency-svc-29j75
Sep  6 09:28:03.701: INFO: Got endpoints: latency-svc-29j75 [738.194874ms]
Sep  6 09:28:03.720: INFO: Created: latency-svc-prmtk
Sep  6 09:28:03.741: INFO: Got endpoints: latency-svc-prmtk [751.354176ms]
Sep  6 09:28:03.758: INFO: Created: latency-svc-d8s67
Sep  6 09:28:03.778: INFO: Got endpoints: latency-svc-d8s67 [732.56274ms]
Sep  6 09:28:03.790: INFO: Created: latency-svc-g5hnr
Sep  6 09:28:03.812: INFO: Got endpoints: latency-svc-g5hnr [713.726431ms]
Sep  6 09:28:03.817: INFO: Created: latency-svc-wm7x2
Sep  6 09:28:03.846: INFO: Got endpoints: latency-svc-wm7x2 [664.120197ms]
Sep  6 09:28:03.849: INFO: Created: latency-svc-28cvh
Sep  6 09:28:03.874: INFO: Created: latency-svc-bcss4
Sep  6 09:28:03.874: INFO: Got endpoints: latency-svc-28cvh [663.240769ms]
Sep  6 09:28:03.888: INFO: Got endpoints: latency-svc-bcss4 [639.385838ms]
Sep  6 09:28:03.915: INFO: Created: latency-svc-nvcps
Sep  6 09:28:03.940: INFO: Got endpoints: latency-svc-nvcps [638.818335ms]
Sep  6 09:28:03.964: INFO: Created: latency-svc-t9brb
Sep  6 09:28:03.994: INFO: Got endpoints: latency-svc-t9brb [662.950754ms]
Sep  6 09:28:04.008: INFO: Created: latency-svc-d8ws8
Sep  6 09:28:04.027: INFO: Got endpoints: latency-svc-d8ws8 [651.003846ms]
Sep  6 09:28:04.036: INFO: Created: latency-svc-dcm7j
Sep  6 09:28:04.087: INFO: Got endpoints: latency-svc-dcm7j [672.801975ms]
Sep  6 09:28:04.096: INFO: Created: latency-svc-l2g9d
Sep  6 09:28:04.137: INFO: Got endpoints: latency-svc-l2g9d [590.000831ms]
Sep  6 09:28:04.143: INFO: Created: latency-svc-2x8lr
Sep  6 09:28:04.162: INFO: Created: latency-svc-f7xhd
Sep  6 09:28:04.181: INFO: Got endpoints: latency-svc-2x8lr [608.912245ms]
Sep  6 09:28:04.194: INFO: Created: latency-svc-gwv6f
Sep  6 09:28:04.219: INFO: Got endpoints: latency-svc-f7xhd [601.750182ms]
Sep  6 09:28:04.220: INFO: Created: latency-svc-j46z6
Sep  6 09:28:04.265: INFO: Created: latency-svc-8xnjc
Sep  6 09:28:04.281: INFO: Got endpoints: latency-svc-gwv6f [613.801804ms]
Sep  6 09:28:04.301: INFO: Created: latency-svc-2gq6n
Sep  6 09:28:04.320: INFO: Got endpoints: latency-svc-j46z6 [618.581023ms]
Sep  6 09:28:04.321: INFO: Created: latency-svc-rpxmf
Sep  6 09:28:04.360: INFO: Created: latency-svc-ws84h
Sep  6 09:28:04.375: INFO: Got endpoints: latency-svc-8xnjc [633.568971ms]
Sep  6 09:28:04.385: INFO: Created: latency-svc-sm9sn
Sep  6 09:28:04.423: INFO: Created: latency-svc-c8gkp
Sep  6 09:28:04.423: INFO: Got endpoints: latency-svc-2gq6n [645.017687ms]
Sep  6 09:28:04.451: INFO: Created: latency-svc-fdfch
Sep  6 09:28:04.473: INFO: Got endpoints: latency-svc-rpxmf [661.220674ms]
Sep  6 09:28:04.523: INFO: Got endpoints: latency-svc-ws84h [677.128463ms]
Sep  6 09:28:04.569: INFO: Got endpoints: latency-svc-sm9sn [694.252496ms]
Sep  6 09:28:04.624: INFO: Got endpoints: latency-svc-c8gkp [735.684539ms]
Sep  6 09:28:04.666: INFO: Got endpoints: latency-svc-fdfch [726.455487ms]
Sep  6 09:28:04.666: INFO: Latencies: [58.951185ms 81.522669ms 93.505577ms 141.705112ms 168.843877ms 213.094222ms 235.269714ms 256.422735ms 275.25833ms 305.237977ms 340.296144ms 369.80535ms 396.256519ms 451.561632ms 452.491772ms 452.682779ms 453.281694ms 455.504058ms 458.82119ms 465.930189ms 466.246634ms 467.015841ms 467.878227ms 468.001794ms 468.439968ms 470.783889ms 472.932777ms 474.00806ms 474.34814ms 475.700269ms 475.894754ms 476.586813ms 479.421091ms 480.213477ms 480.858553ms 481.392784ms 481.763412ms 483.947001ms 484.984697ms 485.669442ms 485.68169ms 486.718213ms 487.586168ms 491.157797ms 493.180848ms 494.133355ms 495.203436ms 506.949093ms 509.705866ms 523.445697ms 534.025032ms 536.225675ms 538.47409ms 539.904046ms 545.731365ms 546.181458ms 550.519312ms 551.513951ms 555.700286ms 558.640828ms 566.857593ms 570.124342ms 571.247737ms 576.863272ms 577.659731ms 578.273991ms 582.221478ms 582.230031ms 586.736649ms 588.159627ms 588.191383ms 588.35971ms 590.000831ms 590.441583ms 590.711254ms 592.521319ms 595.307158ms 596.102821ms 597.02807ms 597.211754ms 598.286398ms 599.410651ms 600.947423ms 601.750182ms 602.4059ms 608.809457ms 608.912245ms 611.858232ms 613.480571ms 613.801804ms 613.900433ms 614.472152ms 614.538886ms 615.797035ms 616.075745ms 618.309806ms 618.581023ms 618.621533ms 619.499833ms 620.720773ms 621.172618ms 621.792098ms 624.382833ms 627.786225ms 628.180644ms 629.686732ms 632.974852ms 633.172002ms 633.568971ms 634.817595ms 636.016786ms 636.54664ms 638.148457ms 638.818335ms 639.385838ms 645.017687ms 646.28186ms 646.407919ms 647.553701ms 651.003846ms 654.070361ms 654.705984ms 661.220674ms 662.950754ms 663.240769ms 664.120197ms 672.801975ms 677.128463ms 678.601207ms 689.418378ms 694.252496ms 694.703048ms 705.526449ms 705.679771ms 709.325693ms 713.726431ms 714.497517ms 715.170046ms 720.830668ms 723.944031ms 724.02519ms 724.580348ms 726.455487ms 726.481789ms 726.698117ms 727.376518ms 731.077644ms 731.633928ms 732.56274ms 733.672053ms 735.684539ms 735.863228ms 737.73797ms 738.194874ms 738.264972ms 740.442121ms 742.094858ms 748.42414ms 748.637395ms 748.868693ms 751.354176ms 753.384995ms 753.546261ms 756.483931ms 756.66021ms 756.869611ms 757.449701ms 763.730011ms 767.143428ms 767.78261ms 769.652146ms 769.733594ms 770.239918ms 775.075306ms 777.394243ms 779.794226ms 780.945326ms 786.509222ms 792.404099ms 794.863102ms 796.389443ms 798.099755ms 799.878588ms 803.482728ms 806.438191ms 806.968386ms 813.306643ms 819.039481ms 825.807249ms 825.99369ms 828.295774ms 832.571456ms 832.921131ms 840.640722ms 844.969886ms 846.81364ms 851.118314ms 856.866543ms 860.177131ms 864.589062ms]
Sep  6 09:28:04.667: INFO: 50 %ile: 621.172618ms
Sep  6 09:28:04.667: INFO: 90 %ile: 796.389443ms
Sep  6 09:28:04.667: INFO: 99 %ile: 860.177131ms
Sep  6 09:28:04.667: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:28:04.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1906" for this suite.
Sep  6 09:28:34.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:28:35.044: INFO: namespace svc-latency-1906 deletion completed in 30.363297811s

• [SLOW TEST:44.353 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:28:35.046: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-119
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Sep  6 09:28:35.283: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-369136095 proxy --unix-socket=/tmp/kubectl-proxy-unix969497350/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:28:35.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-119" for this suite.
Sep  6 09:28:41.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:28:41.683: INFO: namespace kubectl-119 deletion completed in 6.216091899s

• [SLOW TEST:6.637 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:28:41.683: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6829
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-b72b1106-d088-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume configMaps
Sep  6 09:28:41.917: INFO: Waiting up to 5m0s for pod "pod-configmaps-b72d064a-d088-11e9-b48b-b617478434fe" in namespace "configmap-6829" to be "success or failure"
Sep  6 09:28:41.927: INFO: Pod "pod-configmaps-b72d064a-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.165349ms
Sep  6 09:28:43.937: INFO: Pod "pod-configmaps-b72d064a-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020158836s
Sep  6 09:28:45.946: INFO: Pod "pod-configmaps-b72d064a-d088-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029599474s
STEP: Saw pod success
Sep  6 09:28:45.947: INFO: Pod "pod-configmaps-b72d064a-d088-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:28:45.953: INFO: Trying to get logs from node kube-node-20-104 pod pod-configmaps-b72d064a-d088-11e9-b48b-b617478434fe container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 09:28:45.999: INFO: Waiting for pod pod-configmaps-b72d064a-d088-11e9-b48b-b617478434fe to disappear
Sep  6 09:28:46.006: INFO: Pod pod-configmaps-b72d064a-d088-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:28:46.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6829" for this suite.
Sep  6 09:28:52.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:28:52.338: INFO: namespace configmap-6829 deletion completed in 6.321827892s

• [SLOW TEST:10.655 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:28:52.339: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2661
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Sep  6 09:28:52.558: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 09:28:52.580: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 09:28:52.588: INFO: 
Logging pods the kubelet thinks is on node kube-master-20-101 before test
Sep  6 09:28:52.608: INFO: kube-controller-manager-kube-master-20-101 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:28:52.608: INFO: kube-scheduler-kube-master-20-101 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:28:52.608: INFO: canal-lqc46 from kube-system started at 2019-09-02 05:18:15 +0000 UTC (3 container statuses recorded)
Sep  6 09:28:52.608: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 09:28:52.608: INFO: 	Container install-cni ready: true, restart count 0
Sep  6 09:28:52.608: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 09:28:52.608: INFO: kube-bridge-vlan-ds-t5ztc from kube-system started at 2019-09-02 05:18:18 +0000 UTC (1 container statuses recorded)
Sep  6 09:28:52.608: INFO: 	Container kube-bridge-vlan ready: true, restart count 0
Sep  6 09:28:52.608: INFO: apiserver-provider-ipvsdr-preset-559b6b5667-pj6j9 from kube-system started at 2019-09-02 05:18:56 +0000 UTC (1 container statuses recorded)
Sep  6 09:28:52.608: INFO: 	Container ipvsdr ready: true, restart count 0
Sep  6 09:28:52.608: INFO: kube-proxy-kube-master-20-101 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:28:52.608: INFO: default-http-backend-67946497c6-n6vbx from kube-system started at 2019-09-02 05:18:56 +0000 UTC (1 container statuses recorded)
Sep  6 09:28:52.608: INFO: 	Container default-http-backend ready: true, restart count 0
Sep  6 09:28:52.608: INFO: kube-apiserver-kube-master-20-101 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:28:52.608: INFO: sonobuoy-systemd-logs-daemon-set-167311b706124356-jpwmn from heptio-sonobuoy started at 2019-09-06 08:22:17 +0000 UTC (2 container statuses recorded)
Sep  6 09:28:52.608: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  6 09:28:52.608: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  6 09:28:52.608: INFO: apiserver-proxy-nginx-preset-86dd5d987-npl2l from kube-system started at 2019-09-02 05:18:56 +0000 UTC (2 container statuses recorded)
Sep  6 09:28:52.608: INFO: 	Container proxy ready: true, restart count 0
Sep  6 09:28:52.608: INFO: 	Container sidecar ready: true, restart count 0
Sep  6 09:28:52.608: INFO: 
Logging pods the kubelet thinks is on node kube-master-20-102 before test
Sep  6 09:28:52.628: INFO: canal-9rsk5 from kube-system started at 2019-09-02 05:18:15 +0000 UTC (3 container statuses recorded)
Sep  6 09:28:52.628: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 09:28:52.628: INFO: 	Container install-cni ready: true, restart count 0
Sep  6 09:28:52.628: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 09:28:52.628: INFO: kube-bridge-vlan-ds-fsb6t from kube-system started at 2019-09-02 05:18:18 +0000 UTC (1 container statuses recorded)
Sep  6 09:28:52.628: INFO: 	Container kube-bridge-vlan ready: true, restart count 0
Sep  6 09:28:52.628: INFO: kube-apiserver-kube-master-20-102 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:28:52.628: INFO: sonobuoy-systemd-logs-daemon-set-167311b706124356-s5nt7 from heptio-sonobuoy started at 2019-09-06 08:22:17 +0000 UTC (2 container statuses recorded)
Sep  6 09:28:52.628: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  6 09:28:52.628: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  6 09:28:52.628: INFO: kube-controller-manager-kube-master-20-102 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:28:52.628: INFO: kube-scheduler-kube-master-20-102 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:28:52.628: INFO: apiserver-provider-ipvsdr-preset-559b6b5667-rfgxb from kube-system started at 2019-09-02 05:19:04 +0000 UTC (1 container statuses recorded)
Sep  6 09:28:52.628: INFO: 	Container ipvsdr ready: true, restart count 0
Sep  6 09:28:52.628: INFO: coredns-5b7d754486-fbnd4 from kube-system started at 2019-09-02 05:19:43 +0000 UTC (1 container statuses recorded)
Sep  6 09:28:52.628: INFO: 	Container coredns ready: true, restart count 0
Sep  6 09:28:52.628: INFO: kube-proxy-kube-master-20-102 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:28:52.628: INFO: apiserver-proxy-nginx-preset-86dd5d987-b4gcj from kube-system started at 2019-09-02 05:19:06 +0000 UTC (2 container statuses recorded)
Sep  6 09:28:52.628: INFO: 	Container proxy ready: true, restart count 0
Sep  6 09:28:52.628: INFO: 	Container sidecar ready: true, restart count 0
Sep  6 09:28:52.628: INFO: 
Logging pods the kubelet thinks is on node kube-master-20-103 before test
Sep  6 09:28:52.645: INFO: coredns-autoscaler-78bb695c57-s4ll9 from kube-system started at 2019-09-02 05:18:58 +0000 UTC (1 container statuses recorded)
Sep  6 09:28:52.645: INFO: 	Container autoscaler ready: true, restart count 0
Sep  6 09:28:52.645: INFO: kube-controller-manager-kube-master-20-103 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:28:52.645: INFO: coredns-5b7d754486-s6sxk from kube-system started at 2019-09-02 05:18:58 +0000 UTC (1 container statuses recorded)
Sep  6 09:28:52.645: INFO: 	Container coredns ready: true, restart count 0
Sep  6 09:28:52.645: INFO: apiserver-proxy-nginx-preset-86dd5d987-l8sx9 from kube-system started at 2019-09-02 05:18:58 +0000 UTC (2 container statuses recorded)
Sep  6 09:28:52.645: INFO: 	Container proxy ready: true, restart count 0
Sep  6 09:28:52.645: INFO: 	Container sidecar ready: true, restart count 0
Sep  6 09:28:52.645: INFO: kube-apiserver-kube-master-20-103 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:28:52.646: INFO: kube-proxy-kube-master-20-103 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:28:52.646: INFO: kube-scheduler-kube-master-20-103 from kube-system started at <nil> (0 container statuses recorded)
Sep  6 09:28:52.646: INFO: canal-kl5cq from kube-system started at 2019-09-02 05:18:15 +0000 UTC (3 container statuses recorded)
Sep  6 09:28:52.646: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 09:28:52.646: INFO: 	Container install-cni ready: true, restart count 0
Sep  6 09:28:52.646: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 09:28:52.646: INFO: kube-bridge-vlan-ds-s92sg from kube-system started at 2019-09-02 05:18:18 +0000 UTC (1 container statuses recorded)
Sep  6 09:28:52.646: INFO: 	Container kube-bridge-vlan ready: true, restart count 0
Sep  6 09:28:52.646: INFO: heketi-74c4948bbc-t94zf from default started at 2019-09-02 05:21:42 +0000 UTC (1 container statuses recorded)
Sep  6 09:28:52.646: INFO: 	Container heketi ready: true, restart count 0
Sep  6 09:28:52.646: INFO: sonobuoy-systemd-logs-daemon-set-167311b706124356-cf92l from heptio-sonobuoy started at 2019-09-06 08:22:17 +0000 UTC (2 container statuses recorded)
Sep  6 09:28:52.646: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  6 09:28:52.646: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  6 09:28:52.646: INFO: apiserver-provider-ipvsdr-preset-559b6b5667-h5cxj from kube-system started at 2019-09-02 05:18:58 +0000 UTC (1 container statuses recorded)
Sep  6 09:28:52.646: INFO: 	Container ipvsdr ready: true, restart count 0
Sep  6 09:28:52.646: INFO: 
Logging pods the kubelet thinks is on node kube-node-20-104 before test
Sep  6 09:28:52.661: INFO: canal-dvgsp from kube-system started at 2019-09-02 05:20:50 +0000 UTC (3 container statuses recorded)
Sep  6 09:28:52.661: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 09:28:52.661: INFO: 	Container install-cni ready: true, restart count 0
Sep  6 09:28:52.661: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 09:28:52.661: INFO: pod-configmaps-925c98e3-ce6a-11e9-803c-a60c2349fa0a from configmap-7786 started at 2019-09-03 16:47:52 +0000 UTC (2 container statuses recorded)
Sep  6 09:28:52.661: INFO: 	Container configmap-volume-binary-test ready: false, restart count 0
Sep  6 09:28:52.661: INFO: 	Container configmap-volume-data-test ready: false, restart count 0
Sep  6 09:28:52.661: INFO: sonobuoy-systemd-logs-daemon-set-167311b706124356-s6f6v from heptio-sonobuoy started at 2019-09-06 08:22:17 +0000 UTC (2 container statuses recorded)
Sep  6 09:28:52.661: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  6 09:28:52.661: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  6 09:28:52.661: INFO: kube-proxy-d54r6 from kube-system started at 2019-09-02 05:20:50 +0000 UTC (1 container statuses recorded)
Sep  6 09:28:52.661: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 09:28:52.661: INFO: kube-bridge-vlan-ds-c5thz from kube-system started at 2019-09-02 05:20:50 +0000 UTC (1 container statuses recorded)
Sep  6 09:28:52.661: INFO: 	Container kube-bridge-vlan ready: true, restart count 0
Sep  6 09:28:52.661: INFO: 
Logging pods the kubelet thinks is on node kube-node-20-105 before test
Sep  6 09:28:52.675: INFO: canal-x798k from kube-system started at 2019-09-02 05:20:51 +0000 UTC (3 container statuses recorded)
Sep  6 09:28:52.675: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 09:28:52.675: INFO: 	Container install-cni ready: true, restart count 0
Sep  6 09:28:52.675: INFO: 	Container kube-flannel ready: true, restart count 0
Sep  6 09:28:52.675: INFO: kube-proxy-584q7 from kube-system started at 2019-09-02 05:20:51 +0000 UTC (1 container statuses recorded)
Sep  6 09:28:52.675: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 09:28:52.675: INFO: sonobuoy-systemd-logs-daemon-set-167311b706124356-lhlls from heptio-sonobuoy started at 2019-09-06 08:22:17 +0000 UTC (2 container statuses recorded)
Sep  6 09:28:52.675: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  6 09:28:52.675: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  6 09:28:52.675: INFO: kube-bridge-vlan-ds-gssmn from kube-system started at 2019-09-02 05:20:51 +0000 UTC (1 container statuses recorded)
Sep  6 09:28:52.675: INFO: 	Container kube-bridge-vlan ready: true, restart count 0
Sep  6 09:28:52.675: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-06 08:22:15 +0000 UTC (1 container statuses recorded)
Sep  6 09:28:52.675: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 09:28:52.675: INFO: sonobuoy-e2e-job-0a81706125044ba9 from heptio-sonobuoy started at 2019-09-06 08:22:17 +0000 UTC (2 container statuses recorded)
Sep  6 09:28:52.675: INFO: 	Container e2e ready: true, restart count 0
Sep  6 09:28:52.675: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c00b3f4b-d088-11e9-b48b-b617478434fe 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-c00b3f4b-d088-11e9-b48b-b617478434fe off the node kube-node-20-104
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c00b3f4b-d088-11e9-b48b-b617478434fe
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:29:00.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2661" for this suite.
Sep  6 09:29:12.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:29:13.267: INFO: namespace sched-pred-2661 deletion completed in 12.358190543s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:20.928 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:29:13.268: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2135
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 09:29:13.510: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca02cafe-d088-11e9-b48b-b617478434fe" in namespace "downward-api-2135" to be "success or failure"
Sep  6 09:29:13.520: INFO: Pod "downwardapi-volume-ca02cafe-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 9.67896ms
Sep  6 09:29:15.529: INFO: Pod "downwardapi-volume-ca02cafe-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018656624s
Sep  6 09:29:17.539: INFO: Pod "downwardapi-volume-ca02cafe-d088-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028294167s
STEP: Saw pod success
Sep  6 09:29:17.539: INFO: Pod "downwardapi-volume-ca02cafe-d088-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:29:17.547: INFO: Trying to get logs from node kube-node-20-104 pod downwardapi-volume-ca02cafe-d088-11e9-b48b-b617478434fe container client-container: <nil>
STEP: delete the pod
Sep  6 09:29:17.606: INFO: Waiting for pod downwardapi-volume-ca02cafe-d088-11e9-b48b-b617478434fe to disappear
Sep  6 09:29:17.613: INFO: Pod downwardapi-volume-ca02cafe-d088-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:29:17.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2135" for this suite.
Sep  6 09:29:23.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:29:23.950: INFO: namespace downward-api-2135 deletion completed in 6.317835814s

• [SLOW TEST:10.683 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:29:23.952: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-67
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 09:29:24.176: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d05fa617-d088-11e9-b48b-b617478434fe" in namespace "downward-api-67" to be "success or failure"
Sep  6 09:29:24.184: INFO: Pod "downwardapi-volume-d05fa617-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.410989ms
Sep  6 09:29:26.195: INFO: Pod "downwardapi-volume-d05fa617-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018649886s
Sep  6 09:29:28.207: INFO: Pod "downwardapi-volume-d05fa617-d088-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03055663s
STEP: Saw pod success
Sep  6 09:29:28.207: INFO: Pod "downwardapi-volume-d05fa617-d088-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:29:28.216: INFO: Trying to get logs from node kube-node-20-104 pod downwardapi-volume-d05fa617-d088-11e9-b48b-b617478434fe container client-container: <nil>
STEP: delete the pod
Sep  6 09:29:28.299: INFO: Waiting for pod downwardapi-volume-d05fa617-d088-11e9-b48b-b617478434fe to disappear
Sep  6 09:29:28.308: INFO: Pod downwardapi-volume-d05fa617-d088-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:29:28.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-67" for this suite.
Sep  6 09:29:34.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:29:34.651: INFO: namespace downward-api-67 deletion completed in 6.328292438s

• [SLOW TEST:10.699 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:29:34.651: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3835
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  6 09:29:39.453: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d6c29ff9-d088-11e9-b48b-b617478434fe"
Sep  6 09:29:39.454: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d6c29ff9-d088-11e9-b48b-b617478434fe" in namespace "pods-3835" to be "terminated due to deadline exceeded"
Sep  6 09:29:39.460: INFO: Pod "pod-update-activedeadlineseconds-d6c29ff9-d088-11e9-b48b-b617478434fe": Phase="Running", Reason="", readiness=true. Elapsed: 5.640048ms
Sep  6 09:29:41.471: INFO: Pod "pod-update-activedeadlineseconds-d6c29ff9-d088-11e9-b48b-b617478434fe": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.01640472s
Sep  6 09:29:41.471: INFO: Pod "pod-update-activedeadlineseconds-d6c29ff9-d088-11e9-b48b-b617478434fe" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:29:41.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3835" for this suite.
Sep  6 09:29:47.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:29:47.855: INFO: namespace pods-3835 deletion completed in 6.363959179s

• [SLOW TEST:13.204 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:29:47.856: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-821
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 09:29:48.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-821'
Sep  6 09:29:48.312: INFO: stderr: ""
Sep  6 09:29:48.312: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Sep  6 09:29:53.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pod e2e-test-nginx-pod --namespace=kubectl-821 -o json'
Sep  6 09:29:53.509: INFO: stderr: ""
Sep  6 09:29:53.509: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.67.106/32\",\n            \"kubernetes.io/psp\": \"00-privileged\"\n        },\n        \"creationTimestamp\": \"2019-09-06T09:29:48Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-821\",\n        \"resourceVersion\": \"862783\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-821/pods/e2e-test-nginx-pod\",\n        \"uid\": \"debd8a60-d088-11e9-a540-525400ae4cb2\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-jsf7l\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"kube-node-20-104\",\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"volumes\": [\n            {\n                \"name\": \"default-token-jsf7l\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-jsf7l\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-06T09:29:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-06T09:29:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-06T09:29:50Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-06T09:29:48Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://fed59f636153702cd62c7062b752772dce81a29d0798d958c0c274fb9d3ed2bd\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:4cfccf7dd56a99a70f299c7370953c4823956609be8f37c036d64e63f220737e\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-09-06T09:29:50Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.20.104\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.67.106\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-09-06T09:29:48Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep  6 09:29:53.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 replace -f - --namespace=kubectl-821'
Sep  6 09:29:53.995: INFO: stderr: ""
Sep  6 09:29:53.995: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Sep  6 09:29:54.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 delete pods e2e-test-nginx-pod --namespace=kubectl-821'
Sep  6 09:29:56.949: INFO: stderr: ""
Sep  6 09:29:56.949: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:29:56.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-821" for this suite.
Sep  6 09:30:05.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:30:05.423: INFO: namespace kubectl-821 deletion completed in 8.461337891s

• [SLOW TEST:17.567 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:30:05.424: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9141
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 09:30:05.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-9141'
Sep  6 09:30:06.193: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 09:30:06.193: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Sep  6 09:30:08.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 delete deployment e2e-test-nginx-deployment --namespace=kubectl-9141'
Sep  6 09:30:08.392: INFO: stderr: ""
Sep  6 09:30:08.392: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:30:08.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9141" for this suite.
Sep  6 09:30:14.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:30:14.746: INFO: namespace kubectl-9141 deletion completed in 6.343306135s

• [SLOW TEST:9.322 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:30:14.748: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4188
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep  6 09:30:14.982: INFO: Waiting up to 5m0s for pod "downward-api-eea7c592-d088-11e9-b48b-b617478434fe" in namespace "downward-api-4188" to be "success or failure"
Sep  6 09:30:14.993: INFO: Pod "downward-api-eea7c592-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.325636ms
Sep  6 09:30:17.007: INFO: Pod "downward-api-eea7c592-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025075575s
Sep  6 09:30:19.033: INFO: Pod "downward-api-eea7c592-d088-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051128263s
STEP: Saw pod success
Sep  6 09:30:19.033: INFO: Pod "downward-api-eea7c592-d088-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:30:19.047: INFO: Trying to get logs from node kube-node-20-104 pod downward-api-eea7c592-d088-11e9-b48b-b617478434fe container dapi-container: <nil>
STEP: delete the pod
Sep  6 09:30:19.129: INFO: Waiting for pod downward-api-eea7c592-d088-11e9-b48b-b617478434fe to disappear
Sep  6 09:30:19.138: INFO: Pod downward-api-eea7c592-d088-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:30:19.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4188" for this suite.
Sep  6 09:30:25.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:30:25.584: INFO: namespace downward-api-4188 deletion completed in 6.434808127s

• [SLOW TEST:10.836 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:30:25.585: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-390
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Sep  6 09:30:25.840: INFO: Waiting up to 5m0s for pod "client-containers-f51ff3cd-d088-11e9-b48b-b617478434fe" in namespace "containers-390" to be "success or failure"
Sep  6 09:30:25.865: INFO: Pod "client-containers-f51ff3cd-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 25.262038ms
Sep  6 09:30:27.876: INFO: Pod "client-containers-f51ff3cd-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035775334s
Sep  6 09:30:29.885: INFO: Pod "client-containers-f51ff3cd-d088-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044497076s
STEP: Saw pod success
Sep  6 09:30:29.885: INFO: Pod "client-containers-f51ff3cd-d088-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:30:29.894: INFO: Trying to get logs from node kube-node-20-104 pod client-containers-f51ff3cd-d088-11e9-b48b-b617478434fe container test-container: <nil>
STEP: delete the pod
Sep  6 09:30:29.945: INFO: Waiting for pod client-containers-f51ff3cd-d088-11e9-b48b-b617478434fe to disappear
Sep  6 09:30:29.955: INFO: Pod client-containers-f51ff3cd-d088-11e9-b48b-b617478434fe no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:30:29.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-390" for this suite.
Sep  6 09:30:35.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:30:36.296: INFO: namespace containers-390 deletion completed in 6.329343267s

• [SLOW TEST:10.711 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:30:36.297: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7516
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Sep  6 09:30:36.563: INFO: Waiting up to 5m0s for pod "client-containers-fb83b0e4-d088-11e9-b48b-b617478434fe" in namespace "containers-7516" to be "success or failure"
Sep  6 09:30:36.579: INFO: Pod "client-containers-fb83b0e4-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 16.234237ms
Sep  6 09:30:38.591: INFO: Pod "client-containers-fb83b0e4-d088-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02810169s
Sep  6 09:30:40.613: INFO: Pod "client-containers-fb83b0e4-d088-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049751737s
STEP: Saw pod success
Sep  6 09:30:40.613: INFO: Pod "client-containers-fb83b0e4-d088-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:30:40.623: INFO: Trying to get logs from node kube-node-20-104 pod client-containers-fb83b0e4-d088-11e9-b48b-b617478434fe container test-container: <nil>
STEP: delete the pod
Sep  6 09:30:40.682: INFO: Waiting for pod client-containers-fb83b0e4-d088-11e9-b48b-b617478434fe to disappear
Sep  6 09:30:40.696: INFO: Pod client-containers-fb83b0e4-d088-11e9-b48b-b617478434fe no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:30:40.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7516" for this suite.
Sep  6 09:30:46.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:30:47.114: INFO: namespace containers-7516 deletion completed in 6.39736073s

• [SLOW TEST:10.817 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:30:47.114: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1823
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  6 09:30:47.453: INFO: Number of nodes with available pods: 0
Sep  6 09:30:47.453: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:30:48.476: INFO: Number of nodes with available pods: 0
Sep  6 09:30:48.476: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:30:49.490: INFO: Number of nodes with available pods: 0
Sep  6 09:30:49.490: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:30:50.487: INFO: Number of nodes with available pods: 0
Sep  6 09:30:50.487: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:30:51.486: INFO: Number of nodes with available pods: 4
Sep  6 09:30:51.486: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:30:52.476: INFO: Number of nodes with available pods: 5
Sep  6 09:30:52.476: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep  6 09:30:52.558: INFO: Number of nodes with available pods: 4
Sep  6 09:30:52.558: INFO: Node kube-master-20-103 is running more than one daemon pod
Sep  6 09:30:53.588: INFO: Number of nodes with available pods: 4
Sep  6 09:30:53.588: INFO: Node kube-master-20-103 is running more than one daemon pod
Sep  6 09:30:54.584: INFO: Number of nodes with available pods: 4
Sep  6 09:30:54.584: INFO: Node kube-master-20-103 is running more than one daemon pod
Sep  6 09:30:55.588: INFO: Number of nodes with available pods: 5
Sep  6 09:30:55.588: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1823, will wait for the garbage collector to delete the pods
Sep  6 09:30:55.690: INFO: Deleting DaemonSet.extensions daemon-set took: 26.091922ms
Sep  6 09:30:56.190: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.404707ms
Sep  6 09:31:10.105: INFO: Number of nodes with available pods: 0
Sep  6 09:31:10.106: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 09:31:10.116: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1823/daemonsets","resourceVersion":"863115"},"items":null}

Sep  6 09:31:10.124: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1823/pods","resourceVersion":"863115"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:31:10.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1823" for this suite.
Sep  6 09:31:18.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:31:18.616: INFO: namespace daemonsets-1823 deletion completed in 8.413300963s

• [SLOW TEST:31.502 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:31:18.617: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-327
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  6 09:31:18.866: INFO: Waiting up to 5m0s for pod "pod-14baee36-d089-11e9-b48b-b617478434fe" in namespace "emptydir-327" to be "success or failure"
Sep  6 09:31:18.875: INFO: Pod "pod-14baee36-d089-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 9.35611ms
Sep  6 09:31:20.886: INFO: Pod "pod-14baee36-d089-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019914601s
Sep  6 09:31:22.895: INFO: Pod "pod-14baee36-d089-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028619673s
STEP: Saw pod success
Sep  6 09:31:22.895: INFO: Pod "pod-14baee36-d089-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:31:22.903: INFO: Trying to get logs from node kube-node-20-104 pod pod-14baee36-d089-11e9-b48b-b617478434fe container test-container: <nil>
STEP: delete the pod
Sep  6 09:31:22.974: INFO: Waiting for pod pod-14baee36-d089-11e9-b48b-b617478434fe to disappear
Sep  6 09:31:22.982: INFO: Pod pod-14baee36-d089-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:31:22.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-327" for this suite.
Sep  6 09:31:29.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:31:29.468: INFO: namespace emptydir-327 deletion completed in 6.475508148s

• [SLOW TEST:10.851 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:31:29.469: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4592
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:31:34.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4592" for this suite.
Sep  6 09:31:58.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:31:59.104: INFO: namespace replication-controller-4592 deletion completed in 24.293611654s

• [SLOW TEST:29.636 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:31:59.105: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1192
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-1192
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 09:31:59.328: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  6 09:32:23.674: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.64.156:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1192 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 09:32:23.675: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 09:32:24.090: INFO: Found all expected endpoints: [netserver-0]
Sep  6 09:32:24.102: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.66.223:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1192 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 09:32:24.102: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 09:32:24.563: INFO: Found all expected endpoints: [netserver-1]
Sep  6 09:32:24.575: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.65.138:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1192 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 09:32:24.575: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 09:32:25.013: INFO: Found all expected endpoints: [netserver-2]
Sep  6 09:32:25.027: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.68.42:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1192 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 09:32:25.028: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 09:32:25.545: INFO: Found all expected endpoints: [netserver-3]
Sep  6 09:32:25.558: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.67.114:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1192 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 09:32:25.558: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 09:32:25.864: INFO: Found all expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:32:25.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1192" for this suite.
Sep  6 09:32:49.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:32:50.188: INFO: namespace pod-network-test-1192 deletion completed in 24.311563062s

• [SLOW TEST:51.083 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:32:50.189: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4494
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep  6 09:32:50.530: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-4494,SelfLink:/api/v1/namespaces/watch-4494/configmaps/e2e-watch-test-resource-version,UID:4b528b9d-d089-11e9-b16e-525400186f34,ResourceVersion:863437,Generation:0,CreationTimestamp:2019-09-06 09:32:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 09:32:50.530: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-4494,SelfLink:/api/v1/namespaces/watch-4494/configmaps/e2e-watch-test-resource-version,UID:4b528b9d-d089-11e9-b16e-525400186f34,ResourceVersion:863438,Generation:0,CreationTimestamp:2019-09-06 09:32:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:32:50.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4494" for this suite.
Sep  6 09:32:56.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:32:56.922: INFO: namespace watch-4494 deletion completed in 6.360524544s

• [SLOW TEST:6.732 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:32:56.922: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8191
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  6 09:32:57.184: INFO: Waiting up to 5m0s for pod "pod-4f5352cb-d089-11e9-b48b-b617478434fe" in namespace "emptydir-8191" to be "success or failure"
Sep  6 09:32:57.210: INFO: Pod "pod-4f5352cb-d089-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 26.066894ms
Sep  6 09:32:59.221: INFO: Pod "pod-4f5352cb-d089-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037012883s
Sep  6 09:33:01.232: INFO: Pod "pod-4f5352cb-d089-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047567655s
STEP: Saw pod success
Sep  6 09:33:01.232: INFO: Pod "pod-4f5352cb-d089-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:33:01.249: INFO: Trying to get logs from node kube-node-20-104 pod pod-4f5352cb-d089-11e9-b48b-b617478434fe container test-container: <nil>
STEP: delete the pod
Sep  6 09:33:01.317: INFO: Waiting for pod pod-4f5352cb-d089-11e9-b48b-b617478434fe to disappear
Sep  6 09:33:01.325: INFO: Pod pod-4f5352cb-d089-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:33:01.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8191" for this suite.
Sep  6 09:33:07.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:33:07.678: INFO: namespace emptydir-8191 deletion completed in 6.337038334s

• [SLOW TEST:10.756 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:33:07.681: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1003
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0906 09:33:48.021176      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 09:33:48.021: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:33:48.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1003" for this suite.
Sep  6 09:33:56.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:33:56.450: INFO: namespace gc-1003 deletion completed in 8.415569202s

• [SLOW TEST:48.769 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:33:56.451: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3084
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:34:56.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3084" for this suite.
Sep  6 09:35:20.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:35:20.946: INFO: namespace container-probe-3084 deletion completed in 24.178145159s

• [SLOW TEST:84.495 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:35:20.947: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1210
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  6 09:35:21.194: INFO: Number of nodes with available pods: 0
Sep  6 09:35:21.194: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:35:22.217: INFO: Number of nodes with available pods: 0
Sep  6 09:35:22.217: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:35:23.216: INFO: Number of nodes with available pods: 0
Sep  6 09:35:23.216: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:35:24.211: INFO: Number of nodes with available pods: 4
Sep  6 09:35:24.211: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:35:25.208: INFO: Number of nodes with available pods: 5
Sep  6 09:35:25.208: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep  6 09:35:25.248: INFO: Number of nodes with available pods: 4
Sep  6 09:35:25.248: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:35:26.263: INFO: Number of nodes with available pods: 4
Sep  6 09:35:26.263: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:35:27.263: INFO: Number of nodes with available pods: 4
Sep  6 09:35:27.263: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:35:28.265: INFO: Number of nodes with available pods: 4
Sep  6 09:35:28.265: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:35:29.261: INFO: Number of nodes with available pods: 4
Sep  6 09:35:29.261: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:35:30.267: INFO: Number of nodes with available pods: 4
Sep  6 09:35:30.267: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:35:31.266: INFO: Number of nodes with available pods: 4
Sep  6 09:35:31.266: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:35:32.261: INFO: Number of nodes with available pods: 4
Sep  6 09:35:32.261: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:35:33.265: INFO: Number of nodes with available pods: 4
Sep  6 09:35:33.265: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:35:34.262: INFO: Number of nodes with available pods: 4
Sep  6 09:35:34.262: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:35:35.264: INFO: Number of nodes with available pods: 4
Sep  6 09:35:35.264: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:35:36.269: INFO: Number of nodes with available pods: 4
Sep  6 09:35:36.269: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:35:37.265: INFO: Number of nodes with available pods: 5
Sep  6 09:35:37.265: INFO: Number of running nodes: 5, number of available pods: 5
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1210, will wait for the garbage collector to delete the pods
Sep  6 09:35:37.341: INFO: Deleting DaemonSet.extensions daemon-set took: 15.325005ms
Sep  6 09:35:37.842: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.420292ms
Sep  6 09:35:50.850: INFO: Number of nodes with available pods: 0
Sep  6 09:35:50.850: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 09:35:50.855: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1210/daemonsets","resourceVersion":"864031"},"items":null}

Sep  6 09:35:50.860: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1210/pods","resourceVersion":"864031"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:35:50.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1210" for this suite.
Sep  6 09:35:56.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:35:57.093: INFO: namespace daemonsets-1210 deletion completed in 6.178539416s

• [SLOW TEST:36.146 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:35:57.093: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 09:35:57.294: INFO: (0) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 16.829708ms)
Sep  6 09:35:57.302: INFO: (1) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 7.94436ms)
Sep  6 09:35:57.309: INFO: (2) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 7.369745ms)
Sep  6 09:35:57.316: INFO: (3) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 6.883218ms)
Sep  6 09:35:57.323: INFO: (4) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 7.104393ms)
Sep  6 09:35:57.330: INFO: (5) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 7.290558ms)
Sep  6 09:35:57.338: INFO: (6) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 7.237041ms)
Sep  6 09:35:57.343: INFO: (7) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 5.71731ms)
Sep  6 09:35:57.350: INFO: (8) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 6.46411ms)
Sep  6 09:35:57.358: INFO: (9) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 8.076304ms)
Sep  6 09:35:57.366: INFO: (10) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 7.369447ms)
Sep  6 09:35:57.372: INFO: (11) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 6.342618ms)
Sep  6 09:35:57.378: INFO: (12) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 6.190332ms)
Sep  6 09:35:57.385: INFO: (13) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 6.619485ms)
Sep  6 09:35:57.391: INFO: (14) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 5.876775ms)
Sep  6 09:35:57.397: INFO: (15) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 5.90509ms)
Sep  6 09:35:57.403: INFO: (16) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 6.147177ms)
Sep  6 09:35:57.409: INFO: (17) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 6.365194ms)
Sep  6 09:35:57.417: INFO: (18) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 7.332017ms)
Sep  6 09:35:57.424: INFO: (19) /api/v1/nodes/kube-master-20-101/proxy/logs/: <pre>
<a href="agent.log">agent.log</a>
<a href="anaconda/">anaconda/</a>
<a href="apiserver/">ap... (200; 7.377747ms)
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:35:57.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6635" for this suite.
Sep  6 09:36:03.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:36:03.650: INFO: namespace proxy-6635 deletion completed in 6.218260282s

• [SLOW TEST:6.557 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:36:03.651: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6253
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Sep  6 09:36:03.841: INFO: Waiting up to 5m0s for pod "client-containers-be97bb74-d089-11e9-b48b-b617478434fe" in namespace "containers-6253" to be "success or failure"
Sep  6 09:36:03.847: INFO: Pod "client-containers-be97bb74-d089-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.893217ms
Sep  6 09:36:05.853: INFO: Pod "client-containers-be97bb74-d089-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012535399s
Sep  6 09:36:07.863: INFO: Pod "client-containers-be97bb74-d089-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022251132s
STEP: Saw pod success
Sep  6 09:36:07.863: INFO: Pod "client-containers-be97bb74-d089-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:36:07.869: INFO: Trying to get logs from node kube-node-20-104 pod client-containers-be97bb74-d089-11e9-b48b-b617478434fe container test-container: <nil>
STEP: delete the pod
Sep  6 09:36:07.912: INFO: Waiting for pod client-containers-be97bb74-d089-11e9-b48b-b617478434fe to disappear
Sep  6 09:36:07.918: INFO: Pod client-containers-be97bb74-d089-11e9-b48b-b617478434fe no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:36:07.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6253" for this suite.
Sep  6 09:36:13.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:36:14.100: INFO: namespace containers-6253 deletion completed in 6.174557948s

• [SLOW TEST:10.449 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:36:14.101: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7154
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 09:36:14.288: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c4d27749-d089-11e9-b48b-b617478434fe" in namespace "downward-api-7154" to be "success or failure"
Sep  6 09:36:14.293: INFO: Pod "downwardapi-volume-c4d27749-d089-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.291603ms
Sep  6 09:36:16.302: INFO: Pod "downwardapi-volume-c4d27749-d089-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013706685s
Sep  6 09:36:18.309: INFO: Pod "downwardapi-volume-c4d27749-d089-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021483384s
STEP: Saw pod success
Sep  6 09:36:18.310: INFO: Pod "downwardapi-volume-c4d27749-d089-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:36:18.315: INFO: Trying to get logs from node kube-node-20-104 pod downwardapi-volume-c4d27749-d089-11e9-b48b-b617478434fe container client-container: <nil>
STEP: delete the pod
Sep  6 09:36:18.349: INFO: Waiting for pod downwardapi-volume-c4d27749-d089-11e9-b48b-b617478434fe to disappear
Sep  6 09:36:18.353: INFO: Pod downwardapi-volume-c4d27749-d089-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:36:18.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7154" for this suite.
Sep  6 09:36:24.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:36:24.557: INFO: namespace downward-api-7154 deletion completed in 6.196714525s

• [SLOW TEST:10.457 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:36:24.557: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0906 09:36:34.792496      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 09:36:34.792: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:36:34.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3553" for this suite.
Sep  6 09:36:40.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:36:41.014: INFO: namespace gc-3553 deletion completed in 6.21285747s

• [SLOW TEST:16.457 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:36:41.016: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5594
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-d4dd5d50-d089-11e9-b48b-b617478434fe
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:36:41.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5594" for this suite.
Sep  6 09:36:47.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:36:47.381: INFO: namespace configmap-5594 deletion completed in 6.181384429s

• [SLOW TEST:6.365 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:36:47.382: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-662
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 09:36:47.594: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d8ac6244-d089-11e9-b48b-b617478434fe" in namespace "projected-662" to be "success or failure"
Sep  6 09:36:47.600: INFO: Pod "downwardapi-volume-d8ac6244-d089-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.558001ms
Sep  6 09:36:49.607: INFO: Pod "downwardapi-volume-d8ac6244-d089-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012650164s
Sep  6 09:36:51.614: INFO: Pod "downwardapi-volume-d8ac6244-d089-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019930764s
STEP: Saw pod success
Sep  6 09:36:51.614: INFO: Pod "downwardapi-volume-d8ac6244-d089-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:36:51.620: INFO: Trying to get logs from node kube-node-20-104 pod downwardapi-volume-d8ac6244-d089-11e9-b48b-b617478434fe container client-container: <nil>
STEP: delete the pod
Sep  6 09:36:51.654: INFO: Waiting for pod downwardapi-volume-d8ac6244-d089-11e9-b48b-b617478434fe to disappear
Sep  6 09:36:51.659: INFO: Pod downwardapi-volume-d8ac6244-d089-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:36:51.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-662" for this suite.
Sep  6 09:36:57.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:36:57.831: INFO: namespace projected-662 deletion completed in 6.16557863s

• [SLOW TEST:10.450 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:36:57.832: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2743
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep  6 09:36:58.012: INFO: Pod name pod-release: Found 0 pods out of 1
Sep  6 09:37:03.019: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:37:04.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2743" for this suite.
Sep  6 09:37:10.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:37:10.294: INFO: namespace replication-controller-2743 deletion completed in 6.23784092s

• [SLOW TEST:12.462 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:37:10.294: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2899
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Sep  6 09:37:10.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 --namespace=kubectl-2899 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Sep  6 09:37:13.406: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Sep  6 09:37:13.406: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:37:15.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2899" for this suite.
Sep  6 09:37:21.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:37:21.634: INFO: namespace kubectl-2899 deletion completed in 6.205054923s

• [SLOW TEST:11.340 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:37:21.635: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8582
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-8582
Sep  6 09:37:25.835: INFO: Started pod liveness-http in namespace container-probe-8582
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 09:37:25.839: INFO: Initial restart count of pod liveness-http is 0
Sep  6 09:37:39.896: INFO: Restart count of pod container-probe-8582/liveness-http is now 1 (14.056920845s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:37:39.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8582" for this suite.
Sep  6 09:37:45.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:37:46.133: INFO: namespace container-probe-8582 deletion completed in 6.203916818s

• [SLOW TEST:24.498 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:37:46.133: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5119
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 09:37:46.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5119'
Sep  6 09:37:46.449: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 09:37:46.449: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Sep  6 09:37:48.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 delete deployment e2e-test-nginx-deployment --namespace=kubectl-5119'
Sep  6 09:37:48.598: INFO: stderr: ""
Sep  6 09:37:48.598: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:37:48.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5119" for this suite.
Sep  6 09:38:10.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:38:10.797: INFO: namespace kubectl-5119 deletion completed in 22.190299108s

• [SLOW TEST:24.664 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:38:10.798: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-338
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep  6 09:38:15.537: INFO: Successfully updated pod "labelsupdate0a610722-d08a-11e9-b48b-b617478434fe"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:38:17.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-338" for this suite.
Sep  6 09:38:33.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:38:33.807: INFO: namespace downward-api-338 deletion completed in 16.225230959s

• [SLOW TEST:23.009 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:38:33.809: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4268
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 09:38:34.059: INFO: Waiting up to 5m0s for pod "downwardapi-volume-18207caf-d08a-11e9-b48b-b617478434fe" in namespace "projected-4268" to be "success or failure"
Sep  6 09:38:34.077: INFO: Pod "downwardapi-volume-18207caf-d08a-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 17.97508ms
Sep  6 09:38:36.084: INFO: Pod "downwardapi-volume-18207caf-d08a-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025581307s
STEP: Saw pod success
Sep  6 09:38:36.084: INFO: Pod "downwardapi-volume-18207caf-d08a-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:38:36.092: INFO: Trying to get logs from node kube-node-20-104 pod downwardapi-volume-18207caf-d08a-11e9-b48b-b617478434fe container client-container: <nil>
STEP: delete the pod
Sep  6 09:38:36.130: INFO: Waiting for pod downwardapi-volume-18207caf-d08a-11e9-b48b-b617478434fe to disappear
Sep  6 09:38:36.135: INFO: Pod downwardapi-volume-18207caf-d08a-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:38:36.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4268" for this suite.
Sep  6 09:38:42.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:38:42.332: INFO: namespace projected-4268 deletion completed in 6.189567983s

• [SLOW TEST:8.523 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:38:42.334: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5652
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  6 09:38:42.522: INFO: Waiting up to 5m0s for pod "pod-1d2d3d43-d08a-11e9-b48b-b617478434fe" in namespace "emptydir-5652" to be "success or failure"
Sep  6 09:38:42.527: INFO: Pod "pod-1d2d3d43-d08a-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.19019ms
Sep  6 09:38:44.535: INFO: Pod "pod-1d2d3d43-d08a-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012709606s
STEP: Saw pod success
Sep  6 09:38:44.535: INFO: Pod "pod-1d2d3d43-d08a-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:38:44.543: INFO: Trying to get logs from node kube-node-20-104 pod pod-1d2d3d43-d08a-11e9-b48b-b617478434fe container test-container: <nil>
STEP: delete the pod
Sep  6 09:38:44.578: INFO: Waiting for pod pod-1d2d3d43-d08a-11e9-b48b-b617478434fe to disappear
Sep  6 09:38:44.582: INFO: Pod pod-1d2d3d43-d08a-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:38:44.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5652" for this suite.
Sep  6 09:38:50.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:38:50.753: INFO: namespace emptydir-5652 deletion completed in 6.165009317s

• [SLOW TEST:8.420 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:38:50.754: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7841
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-22303f8f-d08a-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume configMaps
Sep  6 09:38:50.940: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2231365a-d08a-11e9-b48b-b617478434fe" in namespace "projected-7841" to be "success or failure"
Sep  6 09:38:50.946: INFO: Pod "pod-projected-configmaps-2231365a-d08a-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.253649ms
Sep  6 09:38:52.952: INFO: Pod "pod-projected-configmaps-2231365a-d08a-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011544142s
STEP: Saw pod success
Sep  6 09:38:52.952: INFO: Pod "pod-projected-configmaps-2231365a-d08a-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:38:52.958: INFO: Trying to get logs from node kube-node-20-104 pod pod-projected-configmaps-2231365a-d08a-11e9-b48b-b617478434fe container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 09:38:53.002: INFO: Waiting for pod pod-projected-configmaps-2231365a-d08a-11e9-b48b-b617478434fe to disappear
Sep  6 09:38:53.008: INFO: Pod pod-projected-configmaps-2231365a-d08a-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:38:53.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7841" for this suite.
Sep  6 09:38:59.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:38:59.226: INFO: namespace projected-7841 deletion completed in 6.209551689s

• [SLOW TEST:8.472 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:38:59.227: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8355
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 09:38:59.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8355'
Sep  6 09:38:59.537: INFO: stderr: ""
Sep  6 09:38:59.537: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Sep  6 09:38:59.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 delete pods e2e-test-nginx-pod --namespace=kubectl-8355'
Sep  6 09:39:10.047: INFO: stderr: ""
Sep  6 09:39:10.047: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:39:10.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8355" for this suite.
Sep  6 09:39:16.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:39:16.296: INFO: namespace kubectl-8355 deletion completed in 6.237920334s

• [SLOW TEST:17.070 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:39:16.297: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2263
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  6 09:39:16.500: INFO: Waiting up to 5m0s for pod "pod-316e2621-d08a-11e9-b48b-b617478434fe" in namespace "emptydir-2263" to be "success or failure"
Sep  6 09:39:16.506: INFO: Pod "pod-316e2621-d08a-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.389192ms
Sep  6 09:39:18.512: INFO: Pod "pod-316e2621-d08a-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012402646s
STEP: Saw pod success
Sep  6 09:39:18.512: INFO: Pod "pod-316e2621-d08a-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:39:18.517: INFO: Trying to get logs from node kube-node-20-104 pod pod-316e2621-d08a-11e9-b48b-b617478434fe container test-container: <nil>
STEP: delete the pod
Sep  6 09:39:18.553: INFO: Waiting for pod pod-316e2621-d08a-11e9-b48b-b617478434fe to disappear
Sep  6 09:39:18.557: INFO: Pod pod-316e2621-d08a-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:39:18.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2263" for this suite.
Sep  6 09:39:24.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:39:24.753: INFO: namespace emptydir-2263 deletion completed in 6.186416238s

• [SLOW TEST:8.456 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:39:24.753: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5094
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5094
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 09:39:24.920: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  6 09:39:47.163: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.68.48 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5094 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 09:39:47.163: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 09:39:48.426: INFO: Found all expected endpoints: [netserver-0]
Sep  6 09:39:48.433: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.66.227 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5094 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 09:39:48.433: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 09:39:49.730: INFO: Found all expected endpoints: [netserver-1]
Sep  6 09:39:49.736: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.64.159 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5094 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 09:39:49.736: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 09:39:51.004: INFO: Found all expected endpoints: [netserver-2]
Sep  6 09:39:51.011: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.65.142 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5094 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 09:39:51.011: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 09:39:52.284: INFO: Found all expected endpoints: [netserver-3]
Sep  6 09:39:52.290: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.67.137 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5094 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 09:39:52.290: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 09:39:53.579: INFO: Found all expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:39:53.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5094" for this suite.
Sep  6 09:40:17.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:40:17.788: INFO: namespace pod-network-test-5094 deletion completed in 24.200260462s

• [SLOW TEST:53.035 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:40:17.789: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:40:18.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5189" for this suite.
Sep  6 09:40:24.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:40:24.500: INFO: namespace services-5189 deletion completed in 6.29678647s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.711 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:40:24.501: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7513
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-5a1327d8-d08a-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume secrets
Sep  6 09:40:24.704: INFO: Waiting up to 5m0s for pod "pod-secrets-5a14c806-d08a-11e9-b48b-b617478434fe" in namespace "secrets-7513" to be "success or failure"
Sep  6 09:40:24.711: INFO: Pod "pod-secrets-5a14c806-d08a-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.57822ms
Sep  6 09:40:26.718: INFO: Pod "pod-secrets-5a14c806-d08a-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013812807s
Sep  6 09:40:28.724: INFO: Pod "pod-secrets-5a14c806-d08a-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019494949s
STEP: Saw pod success
Sep  6 09:40:28.724: INFO: Pod "pod-secrets-5a14c806-d08a-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:40:28.730: INFO: Trying to get logs from node kube-node-20-104 pod pod-secrets-5a14c806-d08a-11e9-b48b-b617478434fe container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 09:40:28.761: INFO: Waiting for pod pod-secrets-5a14c806-d08a-11e9-b48b-b617478434fe to disappear
Sep  6 09:40:28.766: INFO: Pod pod-secrets-5a14c806-d08a-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:40:28.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7513" for this suite.
Sep  6 09:40:34.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:40:34.939: INFO: namespace secrets-7513 deletion completed in 6.164577927s

• [SLOW TEST:10.437 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:40:34.939: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9522
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-9522
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-9522
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9522
Sep  6 09:40:35.141: INFO: Found 0 stateful pods, waiting for 1
Sep  6 09:40:45.151: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep  6 09:40:45.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 09:40:45.571: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 09:40:45.571: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 09:40:45.571: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 09:40:45.578: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  6 09:40:55.586: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 09:40:55.586: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 09:40:55.613: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Sep  6 09:40:55.613: INFO: ss-0  kube-node-20-104  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:35 +0000 UTC  }]
Sep  6 09:40:55.613: INFO: 
Sep  6 09:40:55.613: INFO: StatefulSet ss has not reached scale 3, at 1
Sep  6 09:40:56.623: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993519653s
Sep  6 09:40:57.631: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983519547s
Sep  6 09:40:58.638: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.976175718s
Sep  6 09:40:59.645: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.968582014s
Sep  6 09:41:00.653: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.961603s
Sep  6 09:41:01.660: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.953880823s
Sep  6 09:41:02.667: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.946984355s
Sep  6 09:41:03.674: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.939423072s
Sep  6 09:41:04.681: INFO: Verifying statefulset ss doesn't scale past 3 for another 933.212012ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9522
Sep  6 09:41:05.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:41:06.087: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 09:41:06.087: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 09:41:06.087: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 09:41:06.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:41:06.505: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  6 09:41:06.505: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 09:41:06.505: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 09:41:06.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:41:06.890: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  6 09:41:06.890: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 09:41:06.890: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 09:41:06.897: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 09:41:06.897: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 09:41:06.897: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep  6 09:41:06.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 09:41:07.255: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 09:41:07.255: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 09:41:07.255: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 09:41:07.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 09:41:07.675: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 09:41:07.675: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 09:41:07.675: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 09:41:07.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 09:41:08.033: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 09:41:08.033: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 09:41:08.033: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 09:41:08.033: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 09:41:08.039: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep  6 09:41:18.057: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 09:41:18.057: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 09:41:18.057: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 09:41:18.080: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Sep  6 09:41:18.080: INFO: ss-0  kube-node-20-104    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:35 +0000 UTC  }]
Sep  6 09:41:18.080: INFO: ss-1  kube-node-20-105    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  }]
Sep  6 09:41:18.080: INFO: ss-2  kube-master-20-103  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  }]
Sep  6 09:41:18.080: INFO: 
Sep  6 09:41:18.080: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 09:41:19.088: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Sep  6 09:41:19.089: INFO: ss-0  kube-node-20-104    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:35 +0000 UTC  }]
Sep  6 09:41:19.089: INFO: ss-1  kube-node-20-105    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  }]
Sep  6 09:41:19.089: INFO: ss-2  kube-master-20-103  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  }]
Sep  6 09:41:19.089: INFO: 
Sep  6 09:41:19.089: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 09:41:20.096: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Sep  6 09:41:20.096: INFO: ss-0  kube-node-20-104    Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:35 +0000 UTC  }]
Sep  6 09:41:20.096: INFO: ss-1  kube-node-20-105    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  }]
Sep  6 09:41:20.096: INFO: ss-2  kube-master-20-103  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  }]
Sep  6 09:41:20.096: INFO: 
Sep  6 09:41:20.096: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 09:41:21.103: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Sep  6 09:41:21.103: INFO: ss-1  kube-node-20-105    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  }]
Sep  6 09:41:21.103: INFO: ss-2  kube-master-20-103  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  }]
Sep  6 09:41:21.103: INFO: 
Sep  6 09:41:21.103: INFO: StatefulSet ss has not reached scale 0, at 2
Sep  6 09:41:22.114: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Sep  6 09:41:22.114: INFO: ss-1  kube-node-20-105    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  }]
Sep  6 09:41:22.114: INFO: ss-2  kube-master-20-103  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  }]
Sep  6 09:41:22.115: INFO: 
Sep  6 09:41:22.115: INFO: StatefulSet ss has not reached scale 0, at 2
Sep  6 09:41:23.122: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Sep  6 09:41:23.122: INFO: ss-1  kube-node-20-105    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  }]
Sep  6 09:41:23.122: INFO: ss-2  kube-master-20-103  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  }]
Sep  6 09:41:23.122: INFO: 
Sep  6 09:41:23.122: INFO: StatefulSet ss has not reached scale 0, at 2
Sep  6 09:41:24.130: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Sep  6 09:41:24.130: INFO: ss-1  kube-node-20-105    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  }]
Sep  6 09:41:24.130: INFO: ss-2  kube-master-20-103  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  }]
Sep  6 09:41:24.130: INFO: 
Sep  6 09:41:24.130: INFO: StatefulSet ss has not reached scale 0, at 2
Sep  6 09:41:25.138: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Sep  6 09:41:25.138: INFO: ss-1  kube-node-20-105  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  }]
Sep  6 09:41:25.138: INFO: 
Sep  6 09:41:25.138: INFO: StatefulSet ss has not reached scale 0, at 1
Sep  6 09:41:26.146: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Sep  6 09:41:26.146: INFO: ss-1  kube-node-20-105  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  }]
Sep  6 09:41:26.147: INFO: 
Sep  6 09:41:26.147: INFO: StatefulSet ss has not reached scale 0, at 1
Sep  6 09:41:27.154: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Sep  6 09:41:27.154: INFO: ss-1  kube-node-20-105  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:41:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 09:40:55 +0000 UTC  }]
Sep  6 09:41:27.154: INFO: 
Sep  6 09:41:27.154: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9522
Sep  6 09:41:28.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:41:28.378: INFO: rc: 1
Sep  6 09:41:28.379: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc001873590 exit status 1 <nil> <nil> true [0xc001a52368 0xc001a52380 0xc001a523b0] [0xc001a52368 0xc001a52380 0xc001a523b0] [0xc001a52378 0xc001a523a8] [0x9c00a0 0x9c00a0] 0xc0022920c0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Sep  6 09:41:38.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:41:38.485: INFO: rc: 1
Sep  6 09:41:38.486: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0031f19e0 exit status 1 <nil> <nil> true [0xc0028de758 0xc0028de770 0xc0028de788] [0xc0028de758 0xc0028de770 0xc0028de788] [0xc0028de768 0xc0028de780] [0x9c00a0 0x9c00a0] 0xc0029730e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:41:48.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:41:48.605: INFO: rc: 1
Sep  6 09:41:48.606: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002e96990 exit status 1 <nil> <nil> true [0xc002182f10 0xc002182f28 0xc002182f40] [0xc002182f10 0xc002182f28 0xc002182f40] [0xc002182f20 0xc002182f38] [0x9c00a0 0x9c00a0] 0xc0025999e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:41:58.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:41:58.710: INFO: rc: 1
Sep  6 09:41:58.710: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0031f1d10 exit status 1 <nil> <nil> true [0xc0028de790 0xc0028de7a8 0xc0028de7c0] [0xc0028de790 0xc0028de7a8 0xc0028de7c0] [0xc0028de7a0 0xc0028de7b8] [0x9c00a0 0x9c00a0] 0xc002973440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:42:08.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:42:08.807: INFO: rc: 1
Sep  6 09:42:08.807: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001afa120 exit status 1 <nil> <nil> true [0xc0028de7c8 0xc0028de7e0 0xc0028de7f8] [0xc0028de7c8 0xc0028de7e0 0xc0028de7f8] [0xc0028de7d8 0xc0028de7f0] [0x9c00a0 0x9c00a0] 0xc0029737a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:42:18.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:42:18.916: INFO: rc: 1
Sep  6 09:42:18.917: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002e96db0 exit status 1 <nil> <nil> true [0xc002182f48 0xc002182f60 0xc002182f78] [0xc002182f48 0xc002182f60 0xc002182f78] [0xc002182f58 0xc002182f70] [0x9c00a0 0x9c00a0] 0xc002599d40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:42:28.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:42:29.043: INFO: rc: 1
Sep  6 09:42:29.043: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0014482d0 exit status 1 <nil> <nil> true [0xc001aee038 0xc001aee098 0xc001aee0f8] [0xc001aee038 0xc001aee098 0xc001aee0f8] [0xc001aee068 0xc001aee0d8] [0x9c00a0 0x9c00a0] 0xc00088e3c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:42:39.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:42:39.146: INFO: rc: 1
Sep  6 09:42:39.146: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0031f04e0 exit status 1 <nil> <nil> true [0xc0005978a8 0xc000597da8 0xc000597ec0] [0xc0005978a8 0xc000597da8 0xc000597ec0] [0xc000597d00 0xc000597e40] [0x9c00a0 0x9c00a0] 0xc001534360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:42:49.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:42:49.245: INFO: rc: 1
Sep  6 09:42:49.245: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001f8a3f0 exit status 1 <nil> <nil> true [0xc002416038 0xc002416068 0xc0024160a8] [0xc002416038 0xc002416068 0xc0024160a8] [0xc002416058 0xc0024160a0] [0x9c00a0 0x9c00a0] 0xc001dc6960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:42:59.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:42:59.338: INFO: rc: 1
Sep  6 09:42:59.338: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001448780 exit status 1 <nil> <nil> true [0xc001aee130 0xc001aee1a8 0xc001aee1c8] [0xc001aee130 0xc001aee1a8 0xc001aee1c8] [0xc001aee170 0xc001aee1b8] [0x9c00a0 0x9c00a0] 0xc00088e720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:43:09.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:43:09.437: INFO: rc: 1
Sep  6 09:43:09.437: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0031f0840 exit status 1 <nil> <nil> true [0xc000597f58 0xc000597fb8 0xc000011c80] [0xc000597f58 0xc000597fb8 0xc000011c80] [0xc000597f90 0xc000011b60] [0x9c00a0 0x9c00a0] 0xc001534960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:43:19.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:43:19.530: INFO: rc: 1
Sep  6 09:43:19.530: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0031f0ba0 exit status 1 <nil> <nil> true [0xc000011f90 0xc00018c490 0xc00018cb70] [0xc000011f90 0xc00018c490 0xc00018cb70] [0xc00018c370 0xc00018cb10] [0x9c00a0 0x9c00a0] 0xc001534ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:43:29.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:43:29.624: INFO: rc: 1
Sep  6 09:43:29.624: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0031f0ed0 exit status 1 <nil> <nil> true [0xc00018d0f8 0xc00018d990 0xc00018dd30] [0xc00018d0f8 0xc00018d990 0xc00018dd30] [0xc00018d648 0xc00018dc40] [0x9c00a0 0x9c00a0] 0xc001535380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:43:39.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:43:39.721: INFO: rc: 1
Sep  6 09:43:39.721: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001448ae0 exit status 1 <nil> <nil> true [0xc001aee1e8 0xc001aee210 0xc001aee250] [0xc001aee1e8 0xc001aee210 0xc001aee250] [0xc001aee208 0xc001aee230] [0x9c00a0 0x9c00a0] 0xc00088eae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:43:49.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:43:49.821: INFO: rc: 1
Sep  6 09:43:49.822: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001448e40 exit status 1 <nil> <nil> true [0xc001aee260 0xc001aee280 0xc001aee308] [0xc001aee260 0xc001aee280 0xc001aee308] [0xc001aee278 0xc001aee2f0] [0x9c00a0 0x9c00a0] 0xc00088ee40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:43:59.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:43:59.923: INFO: rc: 1
Sep  6 09:43:59.923: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0031f1260 exit status 1 <nil> <nil> true [0xc00018dd48 0xc0000cc088 0xc0000cc3e8] [0xc00018dd48 0xc0000cc088 0xc0000cc3e8] [0xc00018dfe8 0xc0000cc350] [0x9c00a0 0x9c00a0] 0xc001535860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:44:09.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:44:10.030: INFO: rc: 1
Sep  6 09:44:10.030: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0014494a0 exit status 1 <nil> <nil> true [0xc001aee330 0xc001aee3a0 0xc001aee3d8] [0xc001aee330 0xc001aee3a0 0xc001aee3d8] [0xc001aee380 0xc001aee3c8] [0x9c00a0 0x9c00a0] 0xc00088f2c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:44:20.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:44:20.121: INFO: rc: 1
Sep  6 09:44:20.121: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001449830 exit status 1 <nil> <nil> true [0xc001aee3e8 0xc001aee460 0xc001aee4e0] [0xc001aee3e8 0xc001aee460 0xc001aee4e0] [0xc001aee408 0xc001aee4c8] [0x9c00a0 0x9c00a0] 0xc00088fa40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:44:30.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:44:30.217: INFO: rc: 1
Sep  6 09:44:30.217: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001f8a270 exit status 1 <nil> <nil> true [0xc00018c370 0xc00018cb10 0xc00018d308] [0xc00018c370 0xc00018cb10 0xc00018d308] [0xc00018c520 0xc00018d0f8] [0x9c00a0 0x9c00a0] 0xc001dc6960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:44:40.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:44:40.306: INFO: rc: 1
Sep  6 09:44:40.306: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001448450 exit status 1 <nil> <nil> true [0xc000011910 0xc000011f90 0xc000597d00] [0xc000011910 0xc000011f90 0xc000597d00] [0xc000011c80 0xc000597ba0] [0x9c00a0 0x9c00a0] 0xc00088e3c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:44:50.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:44:50.410: INFO: rc: 1
Sep  6 09:44:50.410: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0031f0480 exit status 1 <nil> <nil> true [0xc002416020 0xc002416058 0xc0024160a0] [0xc002416020 0xc002416058 0xc0024160a0] [0xc002416048 0xc002416088] [0x9c00a0 0x9c00a0] 0xc001534360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:45:00.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:45:00.509: INFO: rc: 1
Sep  6 09:45:00.509: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0031f0810 exit status 1 <nil> <nil> true [0xc0024160a8 0xc0024160f0 0xc002416118] [0xc0024160a8 0xc0024160f0 0xc002416118] [0xc0024160b8 0xc002416110] [0x9c00a0 0x9c00a0] 0xc001534960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:45:10.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:45:10.621: INFO: rc: 1
Sep  6 09:45:10.621: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002dbe420 exit status 1 <nil> <nil> true [0xc0000cc088 0xc0000cc3e8 0xc0000cc460] [0xc0000cc088 0xc0000cc3e8 0xc0000cc460] [0xc0000cc350 0xc0000cc420] [0x9c00a0 0x9c00a0] 0xc002d14540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:45:20.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:45:20.709: INFO: rc: 1
Sep  6 09:45:20.709: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002dbe7b0 exit status 1 <nil> <nil> true [0xc001aee038 0xc001aee098 0xc001aee0f8] [0xc001aee038 0xc001aee098 0xc001aee0f8] [0xc001aee068 0xc001aee0d8] [0x9c00a0 0x9c00a0] 0xc002d14c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:45:30.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:45:30.817: INFO: rc: 1
Sep  6 09:45:30.818: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0031f0c60 exit status 1 <nil> <nil> true [0xc002416120 0xc002416170 0xc0024161d0] [0xc002416120 0xc002416170 0xc0024161d0] [0xc002416158 0xc0024161c8] [0x9c00a0 0x9c00a0] 0xc001534ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:45:40.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:45:40.918: INFO: rc: 1
Sep  6 09:45:40.918: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0031f0fc0 exit status 1 <nil> <nil> true [0xc0024161f0 0xc002416230 0xc002416278] [0xc0024161f0 0xc002416230 0xc002416278] [0xc002416220 0xc002416270] [0x9c00a0 0x9c00a0] 0xc001535380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:45:50.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:45:51.016: INFO: rc: 1
Sep  6 09:45:51.016: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0031f1320 exit status 1 <nil> <nil> true [0xc002416280 0xc0024162b0 0xc0024162f8] [0xc002416280 0xc0024162b0 0xc0024162f8] [0xc0024162a0 0xc0024162c0] [0x9c00a0 0x9c00a0] 0xc001535860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:46:01.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:46:01.138: INFO: rc: 1
Sep  6 09:46:01.138: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001f8bd40 exit status 1 <nil> <nil> true [0xc00018d648 0xc00018dc40 0xc00018deb8] [0xc00018d648 0xc00018dc40 0xc00018deb8] [0xc00018db00 0xc00018dd48] [0x9c00a0 0x9c00a0] 0xc000ef4060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:46:11.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:46:11.239: INFO: rc: 1
Sep  6 09:46:11.240: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0014489c0 exit status 1 <nil> <nil> true [0xc000597da8 0xc000597ec0 0xc000597f90] [0xc000597da8 0xc000597ec0 0xc000597f90] [0xc000597e40 0xc000597f70] [0x9c00a0 0x9c00a0] 0xc00088e720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:46:21.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:46:21.337: INFO: rc: 1
Sep  6 09:46:21.337: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0031f1680 exit status 1 <nil> <nil> true [0xc002416310 0xc002416340 0xc002416398] [0xc002416310 0xc002416340 0xc002416398] [0xc002416330 0xc002416390] [0x9c00a0 0x9c00a0] 0xc001535f20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Sep  6 09:46:31.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 exec --namespace=statefulset-9522 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 09:46:31.445: INFO: rc: 1
Sep  6 09:46:31.446: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
Sep  6 09:46:31.446: INFO: Scaling statefulset ss to 0
Sep  6 09:46:31.470: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep  6 09:46:31.476: INFO: Deleting all statefulset in ns statefulset-9522
Sep  6 09:46:31.484: INFO: Scaling statefulset ss to 0
Sep  6 09:46:31.508: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 09:46:31.513: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:46:31.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9522" for this suite.
Sep  6 09:46:37.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:46:37.757: INFO: namespace statefulset-9522 deletion completed in 6.204483201s

• [SLOW TEST:362.819 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:46:37.758: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4945
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-nsxc
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 09:46:37.964: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-nsxc" in namespace "subpath-4945" to be "success or failure"
Sep  6 09:46:37.979: INFO: Pod "pod-subpath-test-secret-nsxc": Phase="Pending", Reason="", readiness=false. Elapsed: 15.27466ms
Sep  6 09:46:39.988: INFO: Pod "pod-subpath-test-secret-nsxc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023941393s
Sep  6 09:46:41.996: INFO: Pod "pod-subpath-test-secret-nsxc": Phase="Running", Reason="", readiness=true. Elapsed: 4.032037702s
Sep  6 09:46:44.002: INFO: Pod "pod-subpath-test-secret-nsxc": Phase="Running", Reason="", readiness=true. Elapsed: 6.037736099s
Sep  6 09:46:46.009: INFO: Pod "pod-subpath-test-secret-nsxc": Phase="Running", Reason="", readiness=true. Elapsed: 8.045579549s
Sep  6 09:46:48.017: INFO: Pod "pod-subpath-test-secret-nsxc": Phase="Running", Reason="", readiness=true. Elapsed: 10.052769657s
Sep  6 09:46:50.025: INFO: Pod "pod-subpath-test-secret-nsxc": Phase="Running", Reason="", readiness=true. Elapsed: 12.061323257s
Sep  6 09:46:52.035: INFO: Pod "pod-subpath-test-secret-nsxc": Phase="Running", Reason="", readiness=true. Elapsed: 14.070765924s
Sep  6 09:46:54.042: INFO: Pod "pod-subpath-test-secret-nsxc": Phase="Running", Reason="", readiness=true. Elapsed: 16.07812499s
Sep  6 09:46:56.049: INFO: Pod "pod-subpath-test-secret-nsxc": Phase="Running", Reason="", readiness=true. Elapsed: 18.0854909s
Sep  6 09:46:58.060: INFO: Pod "pod-subpath-test-secret-nsxc": Phase="Running", Reason="", readiness=true. Elapsed: 20.0964111s
Sep  6 09:47:00.069: INFO: Pod "pod-subpath-test-secret-nsxc": Phase="Running", Reason="", readiness=true. Elapsed: 22.104748569s
Sep  6 09:47:02.075: INFO: Pod "pod-subpath-test-secret-nsxc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.111185313s
STEP: Saw pod success
Sep  6 09:47:02.075: INFO: Pod "pod-subpath-test-secret-nsxc" satisfied condition "success or failure"
Sep  6 09:47:02.079: INFO: Trying to get logs from node kube-node-20-104 pod pod-subpath-test-secret-nsxc container test-container-subpath-secret-nsxc: <nil>
STEP: delete the pod
Sep  6 09:47:02.121: INFO: Waiting for pod pod-subpath-test-secret-nsxc to disappear
Sep  6 09:47:02.126: INFO: Pod pod-subpath-test-secret-nsxc no longer exists
STEP: Deleting pod pod-subpath-test-secret-nsxc
Sep  6 09:47:02.126: INFO: Deleting pod "pod-subpath-test-secret-nsxc" in namespace "subpath-4945"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:47:02.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4945" for this suite.
Sep  6 09:47:08.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:47:08.329: INFO: namespace subpath-4945 deletion completed in 6.192943667s

• [SLOW TEST:30.571 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:47:08.331: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9089
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Sep  6 09:47:08.510: INFO: Waiting up to 5m0s for pod "var-expansion-4ac53fe4-d08b-11e9-b48b-b617478434fe" in namespace "var-expansion-9089" to be "success or failure"
Sep  6 09:47:08.514: INFO: Pod "var-expansion-4ac53fe4-d08b-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.116977ms
Sep  6 09:47:10.521: INFO: Pod "var-expansion-4ac53fe4-d08b-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010645171s
STEP: Saw pod success
Sep  6 09:47:10.521: INFO: Pod "var-expansion-4ac53fe4-d08b-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:47:10.526: INFO: Trying to get logs from node kube-node-20-104 pod var-expansion-4ac53fe4-d08b-11e9-b48b-b617478434fe container dapi-container: <nil>
STEP: delete the pod
Sep  6 09:47:10.561: INFO: Waiting for pod var-expansion-4ac53fe4-d08b-11e9-b48b-b617478434fe to disappear
Sep  6 09:47:10.567: INFO: Pod var-expansion-4ac53fe4-d08b-11e9-b48b-b617478434fe no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:47:10.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9089" for this suite.
Sep  6 09:47:16.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:47:16.768: INFO: namespace var-expansion-9089 deletion completed in 6.195119776s

• [SLOW TEST:8.437 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:47:16.769: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1889
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-1889
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1889 to expose endpoints map[]
Sep  6 09:47:16.985: INFO: Get endpoints failed (12.476828ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Sep  6 09:47:17.991: INFO: successfully validated that service multi-endpoint-test in namespace services-1889 exposes endpoints map[] (1.018426595s elapsed)
STEP: Creating pod pod1 in namespace services-1889
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1889 to expose endpoints map[pod1:[100]]
Sep  6 09:47:20.045: INFO: successfully validated that service multi-endpoint-test in namespace services-1889 exposes endpoints map[pod1:[100]] (2.038613608s elapsed)
STEP: Creating pod pod2 in namespace services-1889
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1889 to expose endpoints map[pod1:[100] pod2:[101]]
Sep  6 09:47:23.131: INFO: successfully validated that service multi-endpoint-test in namespace services-1889 exposes endpoints map[pod1:[100] pod2:[101]] (3.074390955s elapsed)
STEP: Deleting pod pod1 in namespace services-1889
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1889 to expose endpoints map[pod2:[101]]
Sep  6 09:47:24.163: INFO: successfully validated that service multi-endpoint-test in namespace services-1889 exposes endpoints map[pod2:[101]] (1.022245231s elapsed)
STEP: Deleting pod pod2 in namespace services-1889
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1889 to expose endpoints map[]
Sep  6 09:47:24.183: INFO: successfully validated that service multi-endpoint-test in namespace services-1889 exposes endpoints map[] (8.204931ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:47:24.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1889" for this suite.
Sep  6 09:47:46.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:47:46.465: INFO: namespace services-1889 deletion completed in 22.227719585s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:29.696 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:47:46.466: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-368
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-6184ce4a-d08b-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume configMaps
Sep  6 09:47:46.686: INFO: Waiting up to 5m0s for pod "pod-configmaps-6185fa00-d08b-11e9-b48b-b617478434fe" in namespace "configmap-368" to be "success or failure"
Sep  6 09:47:46.691: INFO: Pod "pod-configmaps-6185fa00-d08b-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.65944ms
Sep  6 09:47:48.697: INFO: Pod "pod-configmaps-6185fa00-d08b-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010907788s
STEP: Saw pod success
Sep  6 09:47:48.697: INFO: Pod "pod-configmaps-6185fa00-d08b-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:47:48.701: INFO: Trying to get logs from node kube-node-20-104 pod pod-configmaps-6185fa00-d08b-11e9-b48b-b617478434fe container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 09:47:48.733: INFO: Waiting for pod pod-configmaps-6185fa00-d08b-11e9-b48b-b617478434fe to disappear
Sep  6 09:47:48.738: INFO: Pod pod-configmaps-6185fa00-d08b-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:47:48.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-368" for this suite.
Sep  6 09:47:54.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:47:54.949: INFO: namespace configmap-368 deletion completed in 6.202899419s

• [SLOW TEST:8.483 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:47:54.949: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2013
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:47:57.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2013" for this suite.
Sep  6 09:48:41.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:48:41.544: INFO: namespace kubelet-test-2013 deletion completed in 44.28848553s

• [SLOW TEST:46.595 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:48:41.545: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2797
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Sep  6 09:48:41.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 create -f - --namespace=kubectl-2797'
Sep  6 09:48:42.374: INFO: stderr: ""
Sep  6 09:48:42.374: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 09:48:42.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2797'
Sep  6 09:48:42.513: INFO: stderr: ""
Sep  6 09:48:42.513: INFO: stdout: "update-demo-nautilus-7ct8w update-demo-nautilus-kk7ng "
Sep  6 09:48:42.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-7ct8w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2797'
Sep  6 09:48:42.634: INFO: stderr: ""
Sep  6 09:48:42.634: INFO: stdout: ""
Sep  6 09:48:42.634: INFO: update-demo-nautilus-7ct8w is created but not running
Sep  6 09:48:47.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2797'
Sep  6 09:48:47.738: INFO: stderr: ""
Sep  6 09:48:47.738: INFO: stdout: "update-demo-nautilus-7ct8w update-demo-nautilus-kk7ng "
Sep  6 09:48:47.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-7ct8w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2797'
Sep  6 09:48:47.849: INFO: stderr: ""
Sep  6 09:48:47.849: INFO: stdout: "true"
Sep  6 09:48:47.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-7ct8w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2797'
Sep  6 09:48:47.952: INFO: stderr: ""
Sep  6 09:48:47.952: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 09:48:47.952: INFO: validating pod update-demo-nautilus-7ct8w
Sep  6 09:48:47.962: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 09:48:47.962: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 09:48:47.963: INFO: update-demo-nautilus-7ct8w is verified up and running
Sep  6 09:48:47.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-kk7ng -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2797'
Sep  6 09:48:48.057: INFO: stderr: ""
Sep  6 09:48:48.057: INFO: stdout: "true"
Sep  6 09:48:48.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-nautilus-kk7ng -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2797'
Sep  6 09:48:48.160: INFO: stderr: ""
Sep  6 09:48:48.160: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 09:48:48.160: INFO: validating pod update-demo-nautilus-kk7ng
Sep  6 09:48:48.169: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 09:48:48.169: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 09:48:48.169: INFO: update-demo-nautilus-kk7ng is verified up and running
STEP: rolling-update to new replication controller
Sep  6 09:48:48.172: INFO: scanned /root for discovery docs: <nil>
Sep  6 09:48:48.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2797'
Sep  6 09:49:10.810: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep  6 09:49:10.810: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 09:49:10.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2797'
Sep  6 09:49:10.923: INFO: stderr: ""
Sep  6 09:49:10.923: INFO: stdout: "update-demo-kitten-2x86b update-demo-kitten-slhnm "
Sep  6 09:49:10.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-kitten-2x86b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2797'
Sep  6 09:49:11.049: INFO: stderr: ""
Sep  6 09:49:11.049: INFO: stdout: "true"
Sep  6 09:49:11.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-kitten-2x86b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2797'
Sep  6 09:49:11.154: INFO: stderr: ""
Sep  6 09:49:11.154: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep  6 09:49:11.154: INFO: validating pod update-demo-kitten-2x86b
Sep  6 09:49:11.164: INFO: got data: {
  "image": "kitten.jpg"
}

Sep  6 09:49:11.165: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep  6 09:49:11.165: INFO: update-demo-kitten-2x86b is verified up and running
Sep  6 09:49:11.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-kitten-slhnm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2797'
Sep  6 09:49:11.270: INFO: stderr: ""
Sep  6 09:49:11.270: INFO: stdout: "true"
Sep  6 09:49:11.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods update-demo-kitten-slhnm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2797'
Sep  6 09:49:11.384: INFO: stderr: ""
Sep  6 09:49:11.384: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep  6 09:49:11.384: INFO: validating pod update-demo-kitten-slhnm
Sep  6 09:49:11.395: INFO: got data: {
  "image": "kitten.jpg"
}

Sep  6 09:49:11.395: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep  6 09:49:11.395: INFO: update-demo-kitten-slhnm is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:49:11.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2797" for this suite.
Sep  6 09:49:35.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:49:35.604: INFO: namespace kubectl-2797 deletion completed in 24.198952629s

• [SLOW TEST:54.060 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:49:35.607: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4371
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
Sep  6 09:49:35.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 create -f - --namespace=kubectl-4371'
Sep  6 09:49:36.208: INFO: stderr: ""
Sep  6 09:49:36.208: INFO: stdout: "pod/pause created\n"
Sep  6 09:49:36.208: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep  6 09:49:36.208: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4371" to be "running and ready"
Sep  6 09:49:36.218: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 9.300781ms
Sep  6 09:49:38.224: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015942425s
Sep  6 09:49:40.235: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.026725101s
Sep  6 09:49:40.235: INFO: Pod "pause" satisfied condition "running and ready"
Sep  6 09:49:40.235: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Sep  6 09:49:40.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 label pods pause testing-label=testing-label-value --namespace=kubectl-4371'
Sep  6 09:49:40.353: INFO: stderr: ""
Sep  6 09:49:40.353: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep  6 09:49:40.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pod pause -L testing-label --namespace=kubectl-4371'
Sep  6 09:49:40.464: INFO: stderr: ""
Sep  6 09:49:40.464: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep  6 09:49:40.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 label pods pause testing-label- --namespace=kubectl-4371'
Sep  6 09:49:40.576: INFO: stderr: ""
Sep  6 09:49:40.576: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep  6 09:49:40.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pod pause -L testing-label --namespace=kubectl-4371'
Sep  6 09:49:40.690: INFO: stderr: ""
Sep  6 09:49:40.690: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
Sep  6 09:49:40.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 delete --grace-period=0 --force -f - --namespace=kubectl-4371'
Sep  6 09:49:40.809: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 09:49:40.809: INFO: stdout: "pod \"pause\" force deleted\n"
Sep  6 09:49:40.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get rc,svc -l name=pause --no-headers --namespace=kubectl-4371'
Sep  6 09:49:40.927: INFO: stderr: "No resources found.\n"
Sep  6 09:49:40.927: INFO: stdout: ""
Sep  6 09:49:40.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods -l name=pause --namespace=kubectl-4371 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 09:49:41.024: INFO: stderr: ""
Sep  6 09:49:41.024: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:49:41.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4371" for this suite.
Sep  6 09:49:47.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:49:47.207: INFO: namespace kubectl-4371 deletion completed in 6.175391286s

• [SLOW TEST:11.600 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:49:47.207: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7372
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-vjhx
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 09:49:47.396: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vjhx" in namespace "subpath-7372" to be "success or failure"
Sep  6 09:49:47.401: INFO: Pod "pod-subpath-test-configmap-vjhx": Phase="Pending", Reason="", readiness=false. Elapsed: 5.143656ms
Sep  6 09:49:49.408: INFO: Pod "pod-subpath-test-configmap-vjhx": Phase="Running", Reason="", readiness=true. Elapsed: 2.012407223s
Sep  6 09:49:51.415: INFO: Pod "pod-subpath-test-configmap-vjhx": Phase="Running", Reason="", readiness=true. Elapsed: 4.018647586s
Sep  6 09:49:53.421: INFO: Pod "pod-subpath-test-configmap-vjhx": Phase="Running", Reason="", readiness=true. Elapsed: 6.024969304s
Sep  6 09:49:55.430: INFO: Pod "pod-subpath-test-configmap-vjhx": Phase="Running", Reason="", readiness=true. Elapsed: 8.034210979s
Sep  6 09:49:57.437: INFO: Pod "pod-subpath-test-configmap-vjhx": Phase="Running", Reason="", readiness=true. Elapsed: 10.040646334s
Sep  6 09:49:59.444: INFO: Pod "pod-subpath-test-configmap-vjhx": Phase="Running", Reason="", readiness=true. Elapsed: 12.047522285s
Sep  6 09:50:01.451: INFO: Pod "pod-subpath-test-configmap-vjhx": Phase="Running", Reason="", readiness=true. Elapsed: 14.055321225s
Sep  6 09:50:03.458: INFO: Pod "pod-subpath-test-configmap-vjhx": Phase="Running", Reason="", readiness=true. Elapsed: 16.061485824s
Sep  6 09:50:05.553: INFO: Pod "pod-subpath-test-configmap-vjhx": Phase="Running", Reason="", readiness=true. Elapsed: 18.156521099s
Sep  6 09:50:07.562: INFO: Pod "pod-subpath-test-configmap-vjhx": Phase="Running", Reason="", readiness=true. Elapsed: 20.166426043s
Sep  6 09:50:09.568: INFO: Pod "pod-subpath-test-configmap-vjhx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.171945466s
STEP: Saw pod success
Sep  6 09:50:09.568: INFO: Pod "pod-subpath-test-configmap-vjhx" satisfied condition "success or failure"
Sep  6 09:50:09.573: INFO: Trying to get logs from node kube-node-20-104 pod pod-subpath-test-configmap-vjhx container test-container-subpath-configmap-vjhx: <nil>
STEP: delete the pod
Sep  6 09:50:09.610: INFO: Waiting for pod pod-subpath-test-configmap-vjhx to disappear
Sep  6 09:50:09.616: INFO: Pod pod-subpath-test-configmap-vjhx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-vjhx
Sep  6 09:50:09.616: INFO: Deleting pod "pod-subpath-test-configmap-vjhx" in namespace "subpath-7372"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:50:09.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7372" for this suite.
Sep  6 09:50:15.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:50:15.816: INFO: namespace subpath-7372 deletion completed in 6.183745623s

• [SLOW TEST:28.609 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:50:15.817: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4200
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-ba877738-d08b-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume secrets
Sep  6 09:50:16.031: INFO: Waiting up to 5m0s for pod "pod-secrets-ba8a64b8-d08b-11e9-b48b-b617478434fe" in namespace "secrets-4200" to be "success or failure"
Sep  6 09:50:16.036: INFO: Pod "pod-secrets-ba8a64b8-d08b-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.890119ms
Sep  6 09:50:18.044: INFO: Pod "pod-secrets-ba8a64b8-d08b-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012369921s
STEP: Saw pod success
Sep  6 09:50:18.044: INFO: Pod "pod-secrets-ba8a64b8-d08b-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:50:18.051: INFO: Trying to get logs from node kube-node-20-104 pod pod-secrets-ba8a64b8-d08b-11e9-b48b-b617478434fe container secret-env-test: <nil>
STEP: delete the pod
Sep  6 09:50:18.089: INFO: Waiting for pod pod-secrets-ba8a64b8-d08b-11e9-b48b-b617478434fe to disappear
Sep  6 09:50:18.094: INFO: Pod pod-secrets-ba8a64b8-d08b-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:50:18.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4200" for this suite.
Sep  6 09:50:24.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:50:24.305: INFO: namespace secrets-4200 deletion completed in 6.203116769s

• [SLOW TEST:8.488 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:50:24.306: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5528
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 09:50:24.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-5528'
Sep  6 09:50:24.630: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 09:50:24.631: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Sep  6 09:50:24.642: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Sep  6 09:50:24.650: INFO: scanned /root for discovery docs: <nil>
Sep  6 09:50:24.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-5528'
Sep  6 09:50:40.546: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep  6 09:50:40.546: INFO: stdout: "Created e2e-test-nginx-rc-e91d826bb8056922eb75e9d8646017c1\nScaling up e2e-test-nginx-rc-e91d826bb8056922eb75e9d8646017c1 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-e91d826bb8056922eb75e9d8646017c1 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-e91d826bb8056922eb75e9d8646017c1 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Sep  6 09:50:40.546: INFO: stdout: "Created e2e-test-nginx-rc-e91d826bb8056922eb75e9d8646017c1\nScaling up e2e-test-nginx-rc-e91d826bb8056922eb75e9d8646017c1 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-e91d826bb8056922eb75e9d8646017c1 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-e91d826bb8056922eb75e9d8646017c1 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Sep  6 09:50:40.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-5528'
Sep  6 09:50:40.671: INFO: stderr: ""
Sep  6 09:50:40.671: INFO: stdout: "e2e-test-nginx-rc-e91d826bb8056922eb75e9d8646017c1-qr7kg "
Sep  6 09:50:40.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods e2e-test-nginx-rc-e91d826bb8056922eb75e9d8646017c1-qr7kg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5528'
Sep  6 09:50:40.780: INFO: stderr: ""
Sep  6 09:50:40.780: INFO: stdout: "true"
Sep  6 09:50:40.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods e2e-test-nginx-rc-e91d826bb8056922eb75e9d8646017c1-qr7kg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5528'
Sep  6 09:50:40.901: INFO: stderr: ""
Sep  6 09:50:40.901: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Sep  6 09:50:40.901: INFO: e2e-test-nginx-rc-e91d826bb8056922eb75e9d8646017c1-qr7kg is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Sep  6 09:50:40.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 delete rc e2e-test-nginx-rc --namespace=kubectl-5528'
Sep  6 09:50:41.014: INFO: stderr: ""
Sep  6 09:50:41.014: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:50:41.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5528" for this suite.
Sep  6 09:50:47.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:50:47.240: INFO: namespace kubectl-5528 deletion completed in 6.215609298s

• [SLOW TEST:22.934 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:50:47.241: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8983
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep  6 09:50:49.954: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8983 pod-service-account-cd8fb93c-d08b-11e9-b48b-b617478434fe -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep  6 09:50:50.353: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8983 pod-service-account-cd8fb93c-d08b-11e9-b48b-b617478434fe -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep  6 09:50:50.738: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8983 pod-service-account-cd8fb93c-d08b-11e9-b48b-b617478434fe -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:50:51.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8983" for this suite.
Sep  6 09:50:57.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:50:57.313: INFO: namespace svcaccounts-8983 deletion completed in 6.17369254s

• [SLOW TEST:10.072 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:50:57.314: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-679
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep  6 09:50:57.506: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-679,SelfLink:/api/v1/namespaces/watch-679/configmaps/e2e-watch-test-label-changed,UID:d3412de0-d08b-11e9-b16e-525400186f34,ResourceVersion:866747,Generation:0,CreationTimestamp:2019-09-06 09:50:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 09:50:57.507: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-679,SelfLink:/api/v1/namespaces/watch-679/configmaps/e2e-watch-test-label-changed,UID:d3412de0-d08b-11e9-b16e-525400186f34,ResourceVersion:866748,Generation:0,CreationTimestamp:2019-09-06 09:50:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep  6 09:50:57.507: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-679,SelfLink:/api/v1/namespaces/watch-679/configmaps/e2e-watch-test-label-changed,UID:d3412de0-d08b-11e9-b16e-525400186f34,ResourceVersion:866749,Generation:0,CreationTimestamp:2019-09-06 09:50:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep  6 09:51:07.549: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-679,SelfLink:/api/v1/namespaces/watch-679/configmaps/e2e-watch-test-label-changed,UID:d3412de0-d08b-11e9-b16e-525400186f34,ResourceVersion:866771,Generation:0,CreationTimestamp:2019-09-06 09:50:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 09:51:07.549: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-679,SelfLink:/api/v1/namespaces/watch-679/configmaps/e2e-watch-test-label-changed,UID:d3412de0-d08b-11e9-b16e-525400186f34,ResourceVersion:866772,Generation:0,CreationTimestamp:2019-09-06 09:50:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Sep  6 09:51:07.549: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-679,SelfLink:/api/v1/namespaces/watch-679/configmaps/e2e-watch-test-label-changed,UID:d3412de0-d08b-11e9-b16e-525400186f34,ResourceVersion:866773,Generation:0,CreationTimestamp:2019-09-06 09:50:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:51:07.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-679" for this suite.
Sep  6 09:51:13.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:51:13.732: INFO: namespace watch-679 deletion completed in 6.175216803s

• [SLOW TEST:16.418 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:51:13.733: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2336
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  6 09:51:18.001: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 09:51:18.008: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 09:51:20.008: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 09:51:20.018: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 09:51:22.008: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 09:51:22.015: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 09:51:24.008: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 09:51:24.015: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 09:51:26.009: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 09:51:26.018: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 09:51:28.008: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 09:51:28.015: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 09:51:30.008: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 09:51:30.018: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 09:51:32.008: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 09:51:32.015: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:51:32.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2336" for this suite.
Sep  6 09:51:54.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:51:54.210: INFO: namespace container-lifecycle-hook-2336 deletion completed in 22.187383365s

• [SLOW TEST:40.477 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:51:54.211: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1156
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 09:51:54.403: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f52c772d-d08b-11e9-b48b-b617478434fe" in namespace "downward-api-1156" to be "success or failure"
Sep  6 09:51:54.409: INFO: Pod "downwardapi-volume-f52c772d-d08b-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.491312ms
Sep  6 09:51:56.417: INFO: Pod "downwardapi-volume-f52c772d-d08b-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014627769s
STEP: Saw pod success
Sep  6 09:51:56.417: INFO: Pod "downwardapi-volume-f52c772d-d08b-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:51:56.424: INFO: Trying to get logs from node kube-node-20-104 pod downwardapi-volume-f52c772d-d08b-11e9-b48b-b617478434fe container client-container: <nil>
STEP: delete the pod
Sep  6 09:51:56.461: INFO: Waiting for pod downwardapi-volume-f52c772d-d08b-11e9-b48b-b617478434fe to disappear
Sep  6 09:51:56.471: INFO: Pod downwardapi-volume-f52c772d-d08b-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:51:56.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1156" for this suite.
Sep  6 09:52:02.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:52:02.715: INFO: namespace downward-api-1156 deletion completed in 6.237369278s

• [SLOW TEST:8.504 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:52:02.716: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3664
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-fa42ca26-d08b-11e9-b48b-b617478434fe
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-fa42ca26-d08b-11e9-b48b-b617478434fe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:52:07.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3664" for this suite.
Sep  6 09:52:29.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:52:29.239: INFO: namespace configmap-3664 deletion completed in 22.215878977s

• [SLOW TEST:26.522 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:52:29.240: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7149
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep  6 09:52:29.427: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0a0ce480-d08c-11e9-b48b-b617478434fe" in namespace "projected-7149" to be "success or failure"
Sep  6 09:52:29.432: INFO: Pod "downwardapi-volume-0a0ce480-d08c-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.188442ms
Sep  6 09:52:31.441: INFO: Pod "downwardapi-volume-0a0ce480-d08c-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014735557s
STEP: Saw pod success
Sep  6 09:52:31.441: INFO: Pod "downwardapi-volume-0a0ce480-d08c-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:52:31.447: INFO: Trying to get logs from node kube-node-20-104 pod downwardapi-volume-0a0ce480-d08c-11e9-b48b-b617478434fe container client-container: <nil>
STEP: delete the pod
Sep  6 09:52:31.490: INFO: Waiting for pod downwardapi-volume-0a0ce480-d08c-11e9-b48b-b617478434fe to disappear
Sep  6 09:52:31.501: INFO: Pod downwardapi-volume-0a0ce480-d08c-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:52:31.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7149" for this suite.
Sep  6 09:52:37.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:52:37.705: INFO: namespace projected-7149 deletion completed in 6.195019834s

• [SLOW TEST:8.465 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:52:37.705: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5951
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  6 09:52:41.944: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 09:52:41.949: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 09:52:43.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 09:52:43.956: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 09:52:45.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 09:52:45.957: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 09:52:47.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 09:52:47.955: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 09:52:49.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 09:52:49.955: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 09:52:51.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 09:52:51.956: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 09:52:53.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 09:52:53.956: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 09:52:55.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 09:52:55.956: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 09:52:57.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 09:52:57.957: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 09:52:59.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 09:52:59.956: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 09:53:01.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 09:53:01.955: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 09:53:03.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 09:53:03.955: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 09:53:05.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 09:53:05.955: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 09:53:07.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 09:53:07.956: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 09:53:09.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 09:53:09.956: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 09:53:11.949: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 09:53:11.958: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:53:11.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5951" for this suite.
Sep  6 09:53:34.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:53:34.182: INFO: namespace container-lifecycle-hook-5951 deletion completed in 22.199488544s

• [SLOW TEST:56.477 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:53:34.183: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5870
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  6 09:53:34.371: INFO: Waiting up to 5m0s for pod "pod-30c26558-d08c-11e9-b48b-b617478434fe" in namespace "emptydir-5870" to be "success or failure"
Sep  6 09:53:34.377: INFO: Pod "pod-30c26558-d08c-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.573147ms
Sep  6 09:53:36.385: INFO: Pod "pod-30c26558-d08c-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014173107s
Sep  6 09:53:38.393: INFO: Pod "pod-30c26558-d08c-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021561997s
STEP: Saw pod success
Sep  6 09:53:38.393: INFO: Pod "pod-30c26558-d08c-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:53:38.400: INFO: Trying to get logs from node kube-node-20-104 pod pod-30c26558-d08c-11e9-b48b-b617478434fe container test-container: <nil>
STEP: delete the pod
Sep  6 09:53:38.445: INFO: Waiting for pod pod-30c26558-d08c-11e9-b48b-b617478434fe to disappear
Sep  6 09:53:38.450: INFO: Pod pod-30c26558-d08c-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:53:38.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5870" for this suite.
Sep  6 09:53:44.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:53:44.649: INFO: namespace emptydir-5870 deletion completed in 6.19234365s

• [SLOW TEST:10.466 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:53:44.650: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6634
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-6634
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 09:53:44.821: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  6 09:54:09.058: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.67.163:8080/dial?request=hostName&protocol=http&host=192.168.64.161&port=8080&tries=1'] Namespace:pod-network-test-6634 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 09:54:09.058: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 09:54:09.336: INFO: Waiting for endpoints: map[]
Sep  6 09:54:09.342: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.67.163:8080/dial?request=hostName&protocol=http&host=192.168.66.228&port=8080&tries=1'] Namespace:pod-network-test-6634 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 09:54:09.342: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 09:54:09.607: INFO: Waiting for endpoints: map[]
Sep  6 09:54:09.613: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.67.163:8080/dial?request=hostName&protocol=http&host=192.168.65.143&port=8080&tries=1'] Namespace:pod-network-test-6634 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 09:54:09.613: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 09:54:09.884: INFO: Waiting for endpoints: map[]
Sep  6 09:54:09.891: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.67.163:8080/dial?request=hostName&protocol=http&host=192.168.67.162&port=8080&tries=1'] Namespace:pod-network-test-6634 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 09:54:09.891: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 09:54:10.181: INFO: Waiting for endpoints: map[]
Sep  6 09:54:10.188: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.67.163:8080/dial?request=hostName&protocol=http&host=192.168.68.53&port=8080&tries=1'] Namespace:pod-network-test-6634 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 09:54:10.188: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
Sep  6 09:54:10.469: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:54:10.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6634" for this suite.
Sep  6 09:54:34.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:54:34.668: INFO: namespace pod-network-test-6634 deletion completed in 24.19085928s

• [SLOW TEST:50.019 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:54:34.669: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3166
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-54d1ec02-d08c-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume secrets
Sep  6 09:54:34.877: INFO: Waiting up to 5m0s for pod "pod-secrets-54d30b48-d08c-11e9-b48b-b617478434fe" in namespace "secrets-3166" to be "success or failure"
Sep  6 09:54:34.882: INFO: Pod "pod-secrets-54d30b48-d08c-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.896125ms
Sep  6 09:54:36.890: INFO: Pod "pod-secrets-54d30b48-d08c-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012239751s
Sep  6 09:54:38.897: INFO: Pod "pod-secrets-54d30b48-d08c-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019455975s
STEP: Saw pod success
Sep  6 09:54:38.897: INFO: Pod "pod-secrets-54d30b48-d08c-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:54:38.901: INFO: Trying to get logs from node kube-node-20-104 pod pod-secrets-54d30b48-d08c-11e9-b48b-b617478434fe container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 09:54:38.940: INFO: Waiting for pod pod-secrets-54d30b48-d08c-11e9-b48b-b617478434fe to disappear
Sep  6 09:54:38.945: INFO: Pod pod-secrets-54d30b48-d08c-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:54:38.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3166" for this suite.
Sep  6 09:54:44.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:54:45.135: INFO: namespace secrets-3166 deletion completed in 6.182697835s

• [SLOW TEST:10.466 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:54:45.136: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8095
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 09:54:45.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 version'
Sep  6 09:54:45.438: INFO: stderr: ""
Sep  6 09:54:45.438: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:44:30Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:36:19Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:54:45.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8095" for this suite.
Sep  6 09:54:51.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:54:51.637: INFO: namespace kubectl-8095 deletion completed in 6.189672886s

• [SLOW TEST:6.501 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:54:51.637: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1012
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep  6 09:54:54.869: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:54:55.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1012" for this suite.
Sep  6 09:55:17.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:55:18.073: INFO: namespace replicaset-1012 deletion completed in 22.171390974s

• [SLOW TEST:26.436 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:55:18.074: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1184
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 09:55:18.242: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:55:20.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1184" for this suite.
Sep  6 09:56:02.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:56:02.575: INFO: namespace pods-1184 deletion completed in 42.219253519s

• [SLOW TEST:44.501 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:56:02.576: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7059
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  6 09:56:02.759: INFO: Waiting up to 5m0s for pod "pod-8934cd83-d08c-11e9-b48b-b617478434fe" in namespace "emptydir-7059" to be "success or failure"
Sep  6 09:56:02.764: INFO: Pod "pod-8934cd83-d08c-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.074566ms
Sep  6 09:56:04.773: INFO: Pod "pod-8934cd83-d08c-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013985881s
STEP: Saw pod success
Sep  6 09:56:04.773: INFO: Pod "pod-8934cd83-d08c-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:56:04.779: INFO: Trying to get logs from node kube-node-20-104 pod pod-8934cd83-d08c-11e9-b48b-b617478434fe container test-container: <nil>
STEP: delete the pod
Sep  6 09:56:04.811: INFO: Waiting for pod pod-8934cd83-d08c-11e9-b48b-b617478434fe to disappear
Sep  6 09:56:04.816: INFO: Pod pod-8934cd83-d08c-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:56:04.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7059" for this suite.
Sep  6 09:56:10.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:56:11.011: INFO: namespace emptydir-7059 deletion completed in 6.186607031s

• [SLOW TEST:8.435 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:56:11.012: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1258
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 09:56:11.177: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:56:13.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1258" for this suite.
Sep  6 09:56:51.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:56:51.663: INFO: namespace pods-1258 deletion completed in 38.199669963s

• [SLOW TEST:40.652 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:56:51.664: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4643
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-a677495d-d08c-11e9-b48b-b617478434fe
STEP: Creating a pod to test consume secrets
Sep  6 09:56:51.858: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a678ae33-d08c-11e9-b48b-b617478434fe" in namespace "projected-4643" to be "success or failure"
Sep  6 09:56:51.866: INFO: Pod "pod-projected-secrets-a678ae33-d08c-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.423724ms
Sep  6 09:56:53.875: INFO: Pod "pod-projected-secrets-a678ae33-d08c-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016375305s
STEP: Saw pod success
Sep  6 09:56:53.875: INFO: Pod "pod-projected-secrets-a678ae33-d08c-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:56:53.881: INFO: Trying to get logs from node kube-node-20-104 pod pod-projected-secrets-a678ae33-d08c-11e9-b48b-b617478434fe container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 09:56:53.922: INFO: Waiting for pod pod-projected-secrets-a678ae33-d08c-11e9-b48b-b617478434fe to disappear
Sep  6 09:56:53.926: INFO: Pod pod-projected-secrets-a678ae33-d08c-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:56:53.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4643" for this suite.
Sep  6 09:56:59.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:57:00.127: INFO: namespace projected-4643 deletion completed in 6.194186019s

• [SLOW TEST:8.463 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:57:00.128: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9775
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  6 09:57:00.308: INFO: Waiting up to 5m0s for pod "pod-ab81f2ff-d08c-11e9-b48b-b617478434fe" in namespace "emptydir-9775" to be "success or failure"
Sep  6 09:57:00.320: INFO: Pod "pod-ab81f2ff-d08c-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.413961ms
Sep  6 09:57:02.327: INFO: Pod "pod-ab81f2ff-d08c-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018678242s
STEP: Saw pod success
Sep  6 09:57:02.327: INFO: Pod "pod-ab81f2ff-d08c-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 09:57:02.333: INFO: Trying to get logs from node kube-node-20-104 pod pod-ab81f2ff-d08c-11e9-b48b-b617478434fe container test-container: <nil>
STEP: delete the pod
Sep  6 09:57:02.365: INFO: Waiting for pod pod-ab81f2ff-d08c-11e9-b48b-b617478434fe to disappear
Sep  6 09:57:02.370: INFO: Pod pod-ab81f2ff-d08c-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:57:02.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9775" for this suite.
Sep  6 09:57:08.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:57:08.574: INFO: namespace emptydir-9775 deletion completed in 6.195918488s

• [SLOW TEST:8.446 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:57:08.574: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1454
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 09:57:08.786: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"b08e7dd8-d08c-11e9-b16e-525400186f34", Controller:(*bool)(0xc002582bf2), BlockOwnerDeletion:(*bool)(0xc002582bf3)}}
Sep  6 09:57:08.797: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b08bf4bc-d08c-11e9-b16e-525400186f34", Controller:(*bool)(0xc002823982), BlockOwnerDeletion:(*bool)(0xc002823983)}}
Sep  6 09:57:08.805: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"b08d1b9e-d08c-11e9-b16e-525400186f34", Controller:(*bool)(0xc002823af6), BlockOwnerDeletion:(*bool)(0xc002823af7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:57:13.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1454" for this suite.
Sep  6 09:57:19.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:57:20.007: INFO: namespace gc-1454 deletion completed in 6.177315325s

• [SLOW TEST:11.433 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:57:20.007: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8008
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 09:57:20.226: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep  6 09:57:20.252: INFO: Number of nodes with available pods: 0
Sep  6 09:57:20.252: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:57:21.270: INFO: Number of nodes with available pods: 0
Sep  6 09:57:21.270: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:57:22.270: INFO: Number of nodes with available pods: 0
Sep  6 09:57:22.270: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:57:23.268: INFO: Number of nodes with available pods: 5
Sep  6 09:57:23.268: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep  6 09:57:23.325: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:23.325: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:23.325: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:23.325: INFO: Wrong image for pod: daemon-set-qsrc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:23.325: INFO: Wrong image for pod: daemon-set-thr48. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:24.342: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:24.342: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:24.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:24.342: INFO: Wrong image for pod: daemon-set-qsrc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:24.342: INFO: Wrong image for pod: daemon-set-thr48. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:25.342: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:25.342: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:25.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:25.342: INFO: Wrong image for pod: daemon-set-qsrc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:25.342: INFO: Pod daemon-set-qsrc2 is not available
Sep  6 09:57:25.342: INFO: Wrong image for pod: daemon-set-thr48. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:26.345: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:26.345: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:26.345: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:26.345: INFO: Wrong image for pod: daemon-set-qsrc2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:26.345: INFO: Pod daemon-set-qsrc2 is not available
Sep  6 09:57:26.345: INFO: Wrong image for pod: daemon-set-thr48. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:27.384: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:27.384: INFO: Pod daemon-set-4whb8 is not available
Sep  6 09:57:27.384: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:27.384: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:27.384: INFO: Wrong image for pod: daemon-set-thr48. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:28.342: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:28.342: INFO: Pod daemon-set-4whb8 is not available
Sep  6 09:57:28.342: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:28.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:28.342: INFO: Wrong image for pod: daemon-set-thr48. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:29.341: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:29.341: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:29.341: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:29.341: INFO: Wrong image for pod: daemon-set-thr48. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:30.343: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:30.343: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:30.343: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:30.343: INFO: Wrong image for pod: daemon-set-thr48. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:30.343: INFO: Pod daemon-set-thr48 is not available
Sep  6 09:57:31.341: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:31.341: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:31.341: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:31.341: INFO: Pod daemon-set-wjj4w is not available
Sep  6 09:57:32.343: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:32.343: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:32.343: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:32.343: INFO: Pod daemon-set-wjj4w is not available
Sep  6 09:57:33.342: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:33.342: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:33.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:34.343: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:34.343: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:34.343: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:35.342: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:35.342: INFO: Pod daemon-set-2rn8w is not available
Sep  6 09:57:35.342: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:35.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:36.342: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:36.342: INFO: Pod daemon-set-2rn8w is not available
Sep  6 09:57:36.342: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:36.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:37.343: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:37.343: INFO: Pod daemon-set-2rn8w is not available
Sep  6 09:57:37.343: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:37.343: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:38.342: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:38.342: INFO: Pod daemon-set-2rn8w is not available
Sep  6 09:57:38.342: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:38.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:39.341: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:39.341: INFO: Pod daemon-set-2rn8w is not available
Sep  6 09:57:39.341: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:39.341: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:40.342: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:40.342: INFO: Pod daemon-set-2rn8w is not available
Sep  6 09:57:40.342: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:40.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:41.342: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:41.342: INFO: Pod daemon-set-2rn8w is not available
Sep  6 09:57:41.342: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:41.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:42.342: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:42.342: INFO: Pod daemon-set-2rn8w is not available
Sep  6 09:57:42.342: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:42.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:43.342: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:43.342: INFO: Pod daemon-set-2rn8w is not available
Sep  6 09:57:43.342: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:43.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:44.342: INFO: Wrong image for pod: daemon-set-2rn8w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:44.342: INFO: Pod daemon-set-2rn8w is not available
Sep  6 09:57:44.342: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:44.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:45.342: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:45.342: INFO: Pod daemon-set-jqtlm is not available
Sep  6 09:57:45.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:46.343: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:46.343: INFO: Pod daemon-set-jqtlm is not available
Sep  6 09:57:46.343: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:47.342: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:47.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:48.341: INFO: Wrong image for pod: daemon-set-bhx6z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:48.342: INFO: Pod daemon-set-bhx6z is not available
Sep  6 09:57:48.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:49.343: INFO: Pod daemon-set-ckv4n is not available
Sep  6 09:57:49.343: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:50.341: INFO: Pod daemon-set-ckv4n is not available
Sep  6 09:57:50.341: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:51.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:52.343: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:53.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:53.342: INFO: Pod daemon-set-p8bvr is not available
Sep  6 09:57:54.341: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:54.341: INFO: Pod daemon-set-p8bvr is not available
Sep  6 09:57:55.340: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:55.340: INFO: Pod daemon-set-p8bvr is not available
Sep  6 09:57:56.344: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:56.344: INFO: Pod daemon-set-p8bvr is not available
Sep  6 09:57:57.341: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:57.341: INFO: Pod daemon-set-p8bvr is not available
Sep  6 09:57:58.343: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:58.343: INFO: Pod daemon-set-p8bvr is not available
Sep  6 09:57:59.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:57:59.342: INFO: Pod daemon-set-p8bvr is not available
Sep  6 09:58:00.342: INFO: Wrong image for pod: daemon-set-p8bvr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 09:58:00.342: INFO: Pod daemon-set-p8bvr is not available
Sep  6 09:58:01.342: INFO: Pod daemon-set-97lvb is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Sep  6 09:58:01.374: INFO: Number of nodes with available pods: 4
Sep  6 09:58:01.374: INFO: Node kube-node-20-105 is running more than one daemon pod
Sep  6 09:58:02.394: INFO: Number of nodes with available pods: 4
Sep  6 09:58:02.394: INFO: Node kube-node-20-105 is running more than one daemon pod
Sep  6 09:58:03.391: INFO: Number of nodes with available pods: 5
Sep  6 09:58:03.392: INFO: Number of running nodes: 5, number of available pods: 5
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8008, will wait for the garbage collector to delete the pods
Sep  6 09:58:03.490: INFO: Deleting DaemonSet.extensions daemon-set took: 13.747876ms
Sep  6 09:58:03.990: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.412456ms
Sep  6 09:58:07.096: INFO: Number of nodes with available pods: 0
Sep  6 09:58:07.096: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 09:58:07.101: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8008/daemonsets","resourceVersion":"868152"},"items":null}

Sep  6 09:58:07.105: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8008/pods","resourceVersion":"868152"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:58:07.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8008" for this suite.
Sep  6 09:58:13.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:58:13.351: INFO: namespace daemonsets-8008 deletion completed in 6.204266652s

• [SLOW TEST:53.344 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:58:13.352: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2151
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-2151
Sep  6 09:58:17.568: INFO: Started pod liveness-exec in namespace container-probe-2151
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 09:58:17.573: INFO: Initial restart count of pod liveness-exec is 0
Sep  6 09:59:05.752: INFO: Restart count of pod container-probe-2151/liveness-exec is now 1 (48.178231217s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:59:05.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2151" for this suite.
Sep  6 09:59:11.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:59:11.979: INFO: namespace container-probe-2151 deletion completed in 6.197101585s

• [SLOW TEST:58.627 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:59:11.981: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4472
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 09:59:12.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-4472'
Sep  6 09:59:12.554: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 09:59:12.554: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Sep  6 09:59:12.569: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-kctfz]
Sep  6 09:59:12.569: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-kctfz" in namespace "kubectl-4472" to be "running and ready"
Sep  6 09:59:12.575: INFO: Pod "e2e-test-nginx-rc-kctfz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.617777ms
Sep  6 09:59:14.581: INFO: Pod "e2e-test-nginx-rc-kctfz": Phase="Running", Reason="", readiness=true. Elapsed: 2.012339224s
Sep  6 09:59:14.581: INFO: Pod "e2e-test-nginx-rc-kctfz" satisfied condition "running and ready"
Sep  6 09:59:14.581: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-kctfz]
Sep  6 09:59:14.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 logs rc/e2e-test-nginx-rc --namespace=kubectl-4472'
Sep  6 09:59:14.724: INFO: stderr: ""
Sep  6 09:59:14.724: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Sep  6 09:59:14.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 delete rc e2e-test-nginx-rc --namespace=kubectl-4472'
Sep  6 09:59:14.831: INFO: stderr: ""
Sep  6 09:59:14.831: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:59:14.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4472" for this suite.
Sep  6 09:59:20.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:59:21.021: INFO: namespace kubectl-4472 deletion completed in 6.18271931s

• [SLOW TEST:9.040 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:59:21.021: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-968
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 09:59:21.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-968'
Sep  6 09:59:21.375: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 09:59:21.375: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Sep  6 09:59:21.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 delete jobs e2e-test-nginx-job --namespace=kubectl-968'
Sep  6 09:59:21.518: INFO: stderr: ""
Sep  6 09:59:21.518: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:59:21.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-968" for this suite.
Sep  6 09:59:27.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:59:27.766: INFO: namespace kubectl-968 deletion completed in 6.234156881s

• [SLOW TEST:6.744 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:59:27.766: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-245
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-03830045-d08d-11e9-b48b-b617478434fe
STEP: Creating configMap with name cm-test-opt-upd-038300b0-d08d-11e9-b48b-b617478434fe
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-03830045-d08d-11e9-b48b-b617478434fe
STEP: Updating configmap cm-test-opt-upd-038300b0-d08d-11e9-b48b-b617478434fe
STEP: Creating configMap with name cm-test-opt-create-038300c9-d08d-11e9-b48b-b617478434fe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 09:59:34.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-245" for this suite.
Sep  6 09:59:56.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 09:59:56.319: INFO: namespace configmap-245 deletion completed in 22.191818472s

• [SLOW TEST:28.553 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 09:59:56.320: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2761
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep  6 09:59:56.540: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep  6 09:59:56.555: INFO: Number of nodes with available pods: 0
Sep  6 09:59:56.555: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep  6 09:59:56.592: INFO: Number of nodes with available pods: 0
Sep  6 09:59:56.592: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:59:57.599: INFO: Number of nodes with available pods: 0
Sep  6 09:59:57.599: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:59:58.598: INFO: Number of nodes with available pods: 0
Sep  6 09:59:58.598: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 09:59:59.598: INFO: Number of nodes with available pods: 1
Sep  6 09:59:59.598: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep  6 09:59:59.629: INFO: Number of nodes with available pods: 1
Sep  6 09:59:59.629: INFO: Number of running nodes: 0, number of available pods: 1
Sep  6 10:00:00.636: INFO: Number of nodes with available pods: 0
Sep  6 10:00:00.636: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep  6 10:00:00.652: INFO: Number of nodes with available pods: 0
Sep  6 10:00:00.652: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 10:00:01.659: INFO: Number of nodes with available pods: 0
Sep  6 10:00:01.659: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 10:00:02.660: INFO: Number of nodes with available pods: 0
Sep  6 10:00:02.660: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 10:00:03.660: INFO: Number of nodes with available pods: 0
Sep  6 10:00:03.660: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 10:00:04.660: INFO: Number of nodes with available pods: 0
Sep  6 10:00:04.661: INFO: Node kube-master-20-101 is running more than one daemon pod
Sep  6 10:00:05.808: INFO: Number of nodes with available pods: 1
Sep  6 10:00:05.808: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2761, will wait for the garbage collector to delete the pods
Sep  6 10:00:06.024: INFO: Deleting DaemonSet.extensions daemon-set took: 149.111783ms
Sep  6 10:00:06.625: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.411337ms
Sep  6 10:00:14.531: INFO: Number of nodes with available pods: 0
Sep  6 10:00:14.531: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 10:00:14.536: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2761/daemonsets","resourceVersion":"868554"},"items":null}

Sep  6 10:00:14.540: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2761/pods","resourceVersion":"868554"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 10:00:14.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2761" for this suite.
Sep  6 10:00:20.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 10:00:20.761: INFO: namespace daemonsets-2761 deletion completed in 6.165316262s

• [SLOW TEST:24.440 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 10:00:20.761: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5948
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-231a66db-d08d-11e9-b48b-b617478434fe
STEP: Creating secret with name s-test-opt-upd-231a6792-d08d-11e9-b48b-b617478434fe
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-231a66db-d08d-11e9-b48b-b617478434fe
STEP: Updating secret s-test-opt-upd-231a6792-d08d-11e9-b48b-b617478434fe
STEP: Creating secret with name s-test-opt-create-231a67cd-d08d-11e9-b48b-b617478434fe
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 10:00:29.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5948" for this suite.
Sep  6 10:00:51.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 10:00:51.349: INFO: namespace secrets-5948 deletion completed in 22.196275677s

• [SLOW TEST:30.588 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 10:00:51.350: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9043
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Sep  6 10:00:51.542: INFO: Waiting up to 5m0s for pod "client-containers-3555aa40-d08d-11e9-b48b-b617478434fe" in namespace "containers-9043" to be "success or failure"
Sep  6 10:00:51.547: INFO: Pod "client-containers-3555aa40-d08d-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.851627ms
Sep  6 10:00:53.556: INFO: Pod "client-containers-3555aa40-d08d-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01346584s
Sep  6 10:00:55.562: INFO: Pod "client-containers-3555aa40-d08d-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019168693s
STEP: Saw pod success
Sep  6 10:00:55.562: INFO: Pod "client-containers-3555aa40-d08d-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 10:00:55.566: INFO: Trying to get logs from node kube-node-20-104 pod client-containers-3555aa40-d08d-11e9-b48b-b617478434fe container test-container: <nil>
STEP: delete the pod
Sep  6 10:00:55.600: INFO: Waiting for pod client-containers-3555aa40-d08d-11e9-b48b-b617478434fe to disappear
Sep  6 10:00:55.604: INFO: Pod client-containers-3555aa40-d08d-11e9-b48b-b617478434fe no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 10:00:55.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9043" for this suite.
Sep  6 10:01:01.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 10:01:01.840: INFO: namespace containers-9043 deletion completed in 6.229720539s

• [SLOW TEST:10.490 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 10:01:01.840: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4327
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
Sep  6 10:01:02.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 create -f - --namespace=kubectl-4327'
Sep  6 10:01:02.432: INFO: stderr: ""
Sep  6 10:01:02.432: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Sep  6 10:01:03.440: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 10:01:03.440: INFO: Found 0 / 1
Sep  6 10:01:04.441: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 10:01:04.441: INFO: Found 0 / 1
Sep  6 10:01:05.438: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 10:01:05.439: INFO: Found 1 / 1
Sep  6 10:01:05.439: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  6 10:01:05.446: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 10:01:05.446: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Sep  6 10:01:05.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 logs redis-master-plrzr redis-master --namespace=kubectl-4327'
Sep  6 10:01:05.581: INFO: stderr: ""
Sep  6 10:01:05.581: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Sep 10:01:03.850 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Sep 10:01:03.850 # Server started, Redis version 3.2.12\n1:M 06 Sep 10:01:03.851 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Sep 10:01:03.851 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Sep  6 10:01:05.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 log redis-master-plrzr redis-master --namespace=kubectl-4327 --tail=1'
Sep  6 10:01:05.748: INFO: stderr: ""
Sep  6 10:01:05.748: INFO: stdout: "1:M 06 Sep 10:01:03.851 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Sep  6 10:01:05.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 log redis-master-plrzr redis-master --namespace=kubectl-4327 --limit-bytes=1'
Sep  6 10:01:05.881: INFO: stderr: ""
Sep  6 10:01:05.881: INFO: stdout: " "
STEP: exposing timestamps
Sep  6 10:01:05.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 log redis-master-plrzr redis-master --namespace=kubectl-4327 --tail=1 --timestamps'
Sep  6 10:01:06.013: INFO: stderr: ""
Sep  6 10:01:06.014: INFO: stdout: "2019-09-06T10:01:03.852486424Z 1:M 06 Sep 10:01:03.851 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Sep  6 10:01:08.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 log redis-master-plrzr redis-master --namespace=kubectl-4327 --since=1s'
Sep  6 10:01:08.654: INFO: stderr: ""
Sep  6 10:01:08.654: INFO: stdout: ""
Sep  6 10:01:08.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 log redis-master-plrzr redis-master --namespace=kubectl-4327 --since=24h'
Sep  6 10:01:08.789: INFO: stderr: ""
Sep  6 10:01:08.789: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Sep 10:01:03.850 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Sep 10:01:03.850 # Server started, Redis version 3.2.12\n1:M 06 Sep 10:01:03.851 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Sep 10:01:03.851 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
Sep  6 10:01:08.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 delete --grace-period=0 --force -f - --namespace=kubectl-4327'
Sep  6 10:01:08.898: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 10:01:08.898: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Sep  6 10:01:08.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get rc,svc -l name=nginx --no-headers --namespace=kubectl-4327'
Sep  6 10:01:09.003: INFO: stderr: "No resources found.\n"
Sep  6 10:01:09.003: INFO: stdout: ""
Sep  6 10:01:09.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 get pods -l name=nginx --namespace=kubectl-4327 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 10:01:09.097: INFO: stderr: ""
Sep  6 10:01:09.097: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 10:01:09.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4327" for this suite.
Sep  6 10:01:15.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 10:01:15.319: INFO: namespace kubectl-4327 deletion completed in 6.213502106s

• [SLOW TEST:13.479 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 10:01:15.320: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-503
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0906 10:01:46.051574      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 10:01:46.051: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 10:01:46.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-503" for this suite.
Sep  6 10:01:52.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 10:01:52.296: INFO: namespace gc-503 deletion completed in 6.234808122s

• [SLOW TEST:36.976 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 10:01:52.297: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6310
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Sep  6 10:01:52.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-369136095 cluster-info'
Sep  6 10:01:52.581: INFO: stderr: ""
Sep  6 10:01:52.581: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 10:01:52.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6310" for this suite.
Sep  6 10:01:58.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 10:01:58.766: INFO: namespace kubectl-6310 deletion completed in 6.176988338s

• [SLOW TEST:6.469 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 10:01:58.767: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3265
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  6 10:01:58.940: INFO: Waiting up to 5m0s for pod "pod-5d81d183-d08d-11e9-b48b-b617478434fe" in namespace "emptydir-3265" to be "success or failure"
Sep  6 10:01:58.946: INFO: Pod "pod-5d81d183-d08d-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.362136ms
Sep  6 10:02:00.953: INFO: Pod "pod-5d81d183-d08d-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012652322s
Sep  6 10:02:02.960: INFO: Pod "pod-5d81d183-d08d-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019113733s
STEP: Saw pod success
Sep  6 10:02:02.960: INFO: Pod "pod-5d81d183-d08d-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 10:02:02.966: INFO: Trying to get logs from node kube-node-20-104 pod pod-5d81d183-d08d-11e9-b48b-b617478434fe container test-container: <nil>
STEP: delete the pod
Sep  6 10:02:03.013: INFO: Waiting for pod pod-5d81d183-d08d-11e9-b48b-b617478434fe to disappear
Sep  6 10:02:03.020: INFO: Pod pod-5d81d183-d08d-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 10:02:03.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3265" for this suite.
Sep  6 10:02:09.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 10:02:09.325: INFO: namespace emptydir-3265 deletion completed in 6.296471313s

• [SLOW TEST:10.558 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep  6 10:02:09.326: INFO: >>> kubeConfig: /tmp/kubeconfig-369136095
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8041
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep  6 10:02:09.558: INFO: Waiting up to 5m0s for pod "downward-api-63d493eb-d08d-11e9-b48b-b617478434fe" in namespace "downward-api-8041" to be "success or failure"
Sep  6 10:02:09.568: INFO: Pod "downward-api-63d493eb-d08d-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.111833ms
Sep  6 10:02:11.581: INFO: Pod "downward-api-63d493eb-d08d-11e9-b48b-b617478434fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022626989s
Sep  6 10:02:13.598: INFO: Pod "downward-api-63d493eb-d08d-11e9-b48b-b617478434fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039653699s
STEP: Saw pod success
Sep  6 10:02:13.598: INFO: Pod "downward-api-63d493eb-d08d-11e9-b48b-b617478434fe" satisfied condition "success or failure"
Sep  6 10:02:13.607: INFO: Trying to get logs from node kube-node-20-104 pod downward-api-63d493eb-d08d-11e9-b48b-b617478434fe container dapi-container: <nil>
STEP: delete the pod
Sep  6 10:02:13.672: INFO: Waiting for pod downward-api-63d493eb-d08d-11e9-b48b-b617478434fe to disappear
Sep  6 10:02:13.681: INFO: Pod downward-api-63d493eb-d08d-11e9-b48b-b617478434fe no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep  6 10:02:13.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8041" for this suite.
Sep  6 10:02:19.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 10:02:19.997: INFO: namespace downward-api-8041 deletion completed in 6.304516151s

• [SLOW TEST:10.671 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSep  6 10:02:19.998: INFO: Running AfterSuite actions on all nodes
Sep  6 10:02:19.998: INFO: Running AfterSuite actions on node 1
Sep  6 10:02:19.998: INFO: Skipping dumping logs from cluster

Ran 204 of 3585 Specs in 5989.737 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3381 Skipped PASS

Ginkgo ran 1 suite in 1h39m51.389893751s
Test Suite Passed
