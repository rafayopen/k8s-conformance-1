I0617 14:35:44.649563      14 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-270483862
I0617 14:35:44.657671      14 e2e.go:240] Starting e2e run "2fe17ca0-910d-11e9-a8b9-dace53c98186" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1560782143 - Will randomize all specs
Will run 204 of 3585 specs

Jun 17 14:35:44.878: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 14:35:44.880: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 17 14:35:44.906: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 17 14:35:44.978: INFO: 36 / 36 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 17 14:35:44.978: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Jun 17 14:35:44.978: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 17 14:35:44.989: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jun 17 14:35:44.989: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jun 17 14:35:44.989: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
Jun 17 14:35:44.989: INFO: e2e test version: v1.14.3
Jun 17 14:35:44.991: INFO: kube-apiserver version: v1.14.3
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:35:44.991: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename configmap
Jun 17 14:35:45.103: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Jun 17 14:35:45.125: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2341
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-30f1278d-910d-11e9-a8b9-dace53c98186
STEP: Creating configMap with name cm-test-opt-upd-30f127de-910d-11e9-a8b9-dace53c98186
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-30f1278d-910d-11e9-a8b9-dace53c98186
STEP: Updating configmap cm-test-opt-upd-30f127de-910d-11e9-a8b9-dace53c98186
STEP: Creating configMap with name cm-test-opt-create-30f127f0-910d-11e9-a8b9-dace53c98186
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:37:09.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2341" for this suite.
Jun 17 14:37:29.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:37:30.034: INFO: namespace configmap-2341 deletion completed in 20.165903115s

• [SLOW TEST:105.043 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:37:30.034: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-153
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 14:37:30.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 version'
Jun 17 14:37:30.257: INFO: stderr: ""
Jun 17 14:37:30.257: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:44:30Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:36:19Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:37:30.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-153" for this suite.
Jun 17 14:37:36.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:37:36.421: INFO: namespace kubectl-153 deletion completed in 6.154994419s

• [SLOW TEST:6.387 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:37:36.423: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1737
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 14:37:36.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 version --client'
Jun 17 14:37:36.648: INFO: stderr: ""
Jun 17 14:37:36.648: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:44:30Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Jun 17 14:37:36.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 create -f - --namespace=kubectl-1737'
Jun 17 14:37:36.924: INFO: stderr: ""
Jun 17 14:37:36.924: INFO: stdout: "replicationcontroller/redis-master created\n"
Jun 17 14:37:36.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 create -f - --namespace=kubectl-1737'
Jun 17 14:37:37.151: INFO: stderr: ""
Jun 17 14:37:37.151: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 17 14:37:38.156: INFO: Selector matched 1 pods for map[app:redis]
Jun 17 14:37:38.156: INFO: Found 0 / 1
Jun 17 14:37:39.156: INFO: Selector matched 1 pods for map[app:redis]
Jun 17 14:37:39.156: INFO: Found 0 / 1
Jun 17 14:37:40.156: INFO: Selector matched 1 pods for map[app:redis]
Jun 17 14:37:40.156: INFO: Found 0 / 1
Jun 17 14:37:41.156: INFO: Selector matched 1 pods for map[app:redis]
Jun 17 14:37:41.156: INFO: Found 1 / 1
Jun 17 14:37:41.156: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 17 14:37:41.160: INFO: Selector matched 1 pods for map[app:redis]
Jun 17 14:37:41.160: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 17 14:37:41.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 describe pod redis-master-qdf5v --namespace=kubectl-1737'
Jun 17 14:37:41.270: INFO: stderr: ""
Jun 17 14:37:41.270: INFO: stdout: "Name:               redis-master-qdf5v\nNamespace:          kubectl-1737\nPriority:           0\nPriorityClassName:  <none>\nNode:               lab1-k8s-node-2/10.128.0.7\nStart Time:         Mon, 17 Jun 2019 14:37:36 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 10.233.64.3\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://cad31b1b06e24c77c31de5cd43482923157eb1fe89af3987bbeb0a12ff2dd378\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 17 Jun 2019 14:37:40 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-z28rj (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-z28rj:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-z28rj\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                      Message\n  ----    ------     ----  ----                      -------\n  Normal  Scheduled  5s    default-scheduler         Successfully assigned kubectl-1737/redis-master-qdf5v to lab1-k8s-node-2\n  Normal  Pulling    3s    kubelet, lab1-k8s-node-2  Pulling image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Pulled     1s    kubelet, lab1-k8s-node-2  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Created    1s    kubelet, lab1-k8s-node-2  Created container redis-master\n  Normal  Started    1s    kubelet, lab1-k8s-node-2  Started container redis-master\n"
Jun 17 14:37:41.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 describe rc redis-master --namespace=kubectl-1737'
Jun 17 14:37:41.371: INFO: stderr: ""
Jun 17 14:37:41.371: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-1737\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  5s    replication-controller  Created pod: redis-master-qdf5v\n"
Jun 17 14:37:41.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 describe service redis-master --namespace=kubectl-1737'
Jun 17 14:37:41.484: INFO: stderr: ""
Jun 17 14:37:41.484: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-1737\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.233.41.237\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.233.64.3:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 17 14:37:41.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 describe node lab1-k8s-master-1'
Jun 17 14:37:41.616: INFO: stderr: ""
Jun 17 14:37:41.616: INFO: stdout: "Name:               lab1-k8s-master-1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=3f73fc93-ec61-4808-88df-2580d94c1a9b\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=se-sto\n                    failure-domain.beta.kubernetes.io/zone=sto1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=lab1-k8s-master-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 17 Jun 2019 13:27:49 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 17 Jun 2019 14:36:53 +0000   Mon, 17 Jun 2019 13:27:41 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 17 Jun 2019 14:36:53 +0000   Mon, 17 Jun 2019 13:27:41 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 17 Jun 2019 14:36:53 +0000   Mon, 17 Jun 2019 13:27:41 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 17 Jun 2019 14:36:53 +0000   Mon, 17 Jun 2019 13:29:49 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.128.0.11\n  ExternalIP:  212.237.150.67\n  Hostname:    lab1-k8s-master-1\nCapacity:\n attachable-volumes-cinder:  256\n cpu:                        2\n ephemeral-storage:          40470732Ki\n hugepages-1Gi:              0\n hugepages-2Mi:              0\n memory:                     8168252Ki\n pods:                       110\nAllocatable:\n attachable-volumes-cinder:  256\n cpu:                        1800m\n ephemeral-storage:          37297826550\n hugepages-1Gi:              0\n hugepages-2Mi:              0\n memory:                     7565852Ki\n pods:                       110\nSystem Info:\n Machine ID:                 c5ecb262e52a41ddbc22a7771db41ebe\n System UUID:                C5ECB262-E52A-41DD-BC22-A7771DB41EBE\n Boot ID:                    fbc4362b-5039-4d63-99c2-24c3fe3b9f6f\n Kernel Version:             4.15.0-34-generic\n OS Image:                   Ubuntu 18.04.1 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.5\n Kubelet Version:            v1.14.3\n Kube-Proxy Version:         v1.14.3\nPodCIDR:                     10.233.65.0/24\nProviderID:                  openstack:///c5ecb262-e52a-41dd-bc22-a7771db41ebe\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-b6e1ac2b5ae94c02-xnmw2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m26s\n  kube-system                calico-node-6dgx4                                          150m (8%)     300m (16%)  64M (0%)         500M (6%)      67m\n  kube-system                kube-apiserver-lab1-k8s-master-1                           250m (13%)    0 (0%)      0 (0%)           0 (0%)         69m\n  kube-system                kube-controller-manager-lab1-k8s-master-1                  200m (11%)    0 (0%)      0 (0%)           0 (0%)         69m\n  kube-system                kube-proxy-ptljf                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         67m\n  kube-system                kube-scheduler-lab1-k8s-master-1                           100m (5%)     0 (0%)      0 (0%)           0 (0%)         69m\n  kube-system                nodelocaldns-8ns7v                                         100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     67m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests        Limits\n  --------                   --------        ------\n  cpu                        800m (44%)      300m (16%)\n  memory                     137400320 (1%)  678257920 (8%)\n  ephemeral-storage          0 (0%)          0 (0%)\n  attachable-volumes-cinder  0               0\nEvents:                      <none>\n"
Jun 17 14:37:41.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 describe namespace kubectl-1737'
Jun 17 14:37:41.713: INFO: stderr: ""
Jun 17 14:37:41.713: INFO: stdout: "Name:         kubectl-1737\nLabels:       e2e-framework=kubectl\n              e2e-run=2fe17ca0-910d-11e9-a8b9-dace53c98186\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:37:41.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1737" for this suite.
Jun 17 14:38:03.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:38:03.874: INFO: namespace kubectl-1737 deletion completed in 22.153163429s

• [SLOW TEST:27.451 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:38:03.876: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1613
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-839f2201-910d-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume configMaps
Jun 17 14:38:04.051: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-83a0308c-910d-11e9-a8b9-dace53c98186" in namespace "projected-1613" to be "success or failure"
Jun 17 14:38:04.056: INFO: Pod "pod-projected-configmaps-83a0308c-910d-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.292529ms
Jun 17 14:38:06.060: INFO: Pod "pod-projected-configmaps-83a0308c-910d-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009075836s
Jun 17 14:38:08.065: INFO: Pod "pod-projected-configmaps-83a0308c-910d-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013613834s
STEP: Saw pod success
Jun 17 14:38:08.065: INFO: Pod "pod-projected-configmaps-83a0308c-910d-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:38:08.069: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-configmaps-83a0308c-910d-11e9-a8b9-dace53c98186 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 17 14:38:08.092: INFO: Waiting for pod pod-projected-configmaps-83a0308c-910d-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:38:08.095: INFO: Pod pod-projected-configmaps-83a0308c-910d-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:38:08.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1613" for this suite.
Jun 17 14:38:14.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:38:14.255: INFO: namespace projected-1613 deletion completed in 6.153471803s

• [SLOW TEST:10.379 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:38:14.256: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5309
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 17 14:38:14.423: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89ce6dc6-910d-11e9-a8b9-dace53c98186" in namespace "downward-api-5309" to be "success or failure"
Jun 17 14:38:14.427: INFO: Pod "downwardapi-volume-89ce6dc6-910d-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 3.922153ms
Jun 17 14:38:16.432: INFO: Pod "downwardapi-volume-89ce6dc6-910d-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008932329s
Jun 17 14:38:18.436: INFO: Pod "downwardapi-volume-89ce6dc6-910d-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013260303s
STEP: Saw pod success
Jun 17 14:38:18.437: INFO: Pod "downwardapi-volume-89ce6dc6-910d-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:38:18.440: INFO: Trying to get logs from node lab1-k8s-node-2 pod downwardapi-volume-89ce6dc6-910d-11e9-a8b9-dace53c98186 container client-container: <nil>
STEP: delete the pod
Jun 17 14:38:18.469: INFO: Waiting for pod downwardapi-volume-89ce6dc6-910d-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:38:18.472: INFO: Pod downwardapi-volume-89ce6dc6-910d-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:38:18.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5309" for this suite.
Jun 17 14:38:24.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:38:24.633: INFO: namespace downward-api-5309 deletion completed in 6.155058465s

• [SLOW TEST:10.377 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:38:24.635: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7304
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 17 14:38:24.802: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8ffe3a90-910d-11e9-a8b9-dace53c98186" in namespace "projected-7304" to be "success or failure"
Jun 17 14:38:24.805: INFO: Pod "downwardapi-volume-8ffe3a90-910d-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 3.632777ms
Jun 17 14:38:26.811: INFO: Pod "downwardapi-volume-8ffe3a90-910d-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008803919s
STEP: Saw pod success
Jun 17 14:38:26.811: INFO: Pod "downwardapi-volume-8ffe3a90-910d-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:38:26.815: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-8ffe3a90-910d-11e9-a8b9-dace53c98186 container client-container: <nil>
STEP: delete the pod
Jun 17 14:38:26.839: INFO: Waiting for pod downwardapi-volume-8ffe3a90-910d-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:38:26.843: INFO: Pod downwardapi-volume-8ffe3a90-910d-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:38:26.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7304" for this suite.
Jun 17 14:38:32.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:38:33.014: INFO: namespace projected-7304 deletion completed in 6.16535573s

• [SLOW TEST:8.379 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:38:33.017: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0617 14:38:43.213313      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 17 14:38:43.213: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:38:43.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3818" for this suite.
Jun 17 14:38:49.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:38:49.367: INFO: namespace gc-3818 deletion completed in 6.147249349s

• [SLOW TEST:16.351 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:38:49.368: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2212
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:38:55.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2212" for this suite.
Jun 17 14:39:47.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:39:47.727: INFO: namespace kubelet-test-2212 deletion completed in 52.162836389s

• [SLOW TEST:58.359 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:39:47.727: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4012
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-c184b5e4-910d-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume secrets
Jun 17 14:39:47.896: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c185982f-910d-11e9-a8b9-dace53c98186" in namespace "projected-4012" to be "success or failure"
Jun 17 14:39:47.900: INFO: Pod "pod-projected-secrets-c185982f-910d-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 3.905528ms
Jun 17 14:39:49.906: INFO: Pod "pod-projected-secrets-c185982f-910d-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009330035s
Jun 17 14:39:51.910: INFO: Pod "pod-projected-secrets-c185982f-910d-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013953375s
STEP: Saw pod success
Jun 17 14:39:51.911: INFO: Pod "pod-projected-secrets-c185982f-910d-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:39:51.914: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-secrets-c185982f-910d-11e9-a8b9-dace53c98186 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 17 14:39:51.936: INFO: Waiting for pod pod-projected-secrets-c185982f-910d-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:39:51.939: INFO: Pod pod-projected-secrets-c185982f-910d-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:39:51.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4012" for this suite.
Jun 17 14:39:57.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:39:58.102: INFO: namespace projected-4012 deletion completed in 6.155408384s

• [SLOW TEST:10.375 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:39:58.104: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2089
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 17 14:39:58.291: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2089,SelfLink:/api/v1/namespaces/watch-2089/configmaps/e2e-watch-test-label-changed,UID:c7b6454b-910d-11e9-bdd5-fa163e38c6bd,ResourceVersion:11041,Generation:0,CreationTimestamp:2019-06-17 14:39:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 17 14:39:58.291: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2089,SelfLink:/api/v1/namespaces/watch-2089/configmaps/e2e-watch-test-label-changed,UID:c7b6454b-910d-11e9-bdd5-fa163e38c6bd,ResourceVersion:11042,Generation:0,CreationTimestamp:2019-06-17 14:39:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 17 14:39:58.291: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2089,SelfLink:/api/v1/namespaces/watch-2089/configmaps/e2e-watch-test-label-changed,UID:c7b6454b-910d-11e9-bdd5-fa163e38c6bd,ResourceVersion:11043,Generation:0,CreationTimestamp:2019-06-17 14:39:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 17 14:40:08.339: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2089,SelfLink:/api/v1/namespaces/watch-2089/configmaps/e2e-watch-test-label-changed,UID:c7b6454b-910d-11e9-bdd5-fa163e38c6bd,ResourceVersion:11099,Generation:0,CreationTimestamp:2019-06-17 14:39:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 17 14:40:08.339: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2089,SelfLink:/api/v1/namespaces/watch-2089/configmaps/e2e-watch-test-label-changed,UID:c7b6454b-910d-11e9-bdd5-fa163e38c6bd,ResourceVersion:11100,Generation:0,CreationTimestamp:2019-06-17 14:39:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jun 17 14:40:08.340: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2089,SelfLink:/api/v1/namespaces/watch-2089/configmaps/e2e-watch-test-label-changed,UID:c7b6454b-910d-11e9-bdd5-fa163e38c6bd,ResourceVersion:11101,Generation:0,CreationTimestamp:2019-06-17 14:39:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:40:08.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2089" for this suite.
Jun 17 14:40:14.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:40:14.506: INFO: namespace watch-2089 deletion completed in 6.1592917s

• [SLOW TEST:16.402 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:40:14.510: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7031
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 14:40:14.666: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:40:20.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7031" for this suite.
Jun 17 14:40:58.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:40:58.870: INFO: namespace pods-7031 deletion completed in 38.151158434s

• [SLOW TEST:44.360 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:40:58.870: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7289
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-ebec75de-910d-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume configMaps
Jun 17 14:40:59.040: INFO: Waiting up to 5m0s for pod "pod-configmaps-ebed7049-910d-11e9-a8b9-dace53c98186" in namespace "configmap-7289" to be "success or failure"
Jun 17 14:40:59.045: INFO: Pod "pod-configmaps-ebed7049-910d-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.183021ms
Jun 17 14:41:01.050: INFO: Pod "pod-configmaps-ebed7049-910d-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009145607s
Jun 17 14:41:03.055: INFO: Pod "pod-configmaps-ebed7049-910d-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014123396s
STEP: Saw pod success
Jun 17 14:41:03.055: INFO: Pod "pod-configmaps-ebed7049-910d-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:41:03.059: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-configmaps-ebed7049-910d-11e9-a8b9-dace53c98186 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 17 14:41:03.082: INFO: Waiting for pod pod-configmaps-ebed7049-910d-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:41:03.085: INFO: Pod pod-configmaps-ebed7049-910d-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:41:03.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7289" for this suite.
Jun 17 14:41:09.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:41:09.245: INFO: namespace configmap-7289 deletion completed in 6.153765207s

• [SLOW TEST:10.375 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:41:09.245: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8021
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0617 14:41:19.487751      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 17 14:41:19.487: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:41:19.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8021" for this suite.
Jun 17 14:41:25.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:41:25.648: INFO: namespace gc-8021 deletion completed in 6.151595368s

• [SLOW TEST:16.403 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:41:25.649: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3661
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Jun 17 14:41:25.820: INFO: Waiting up to 5m0s for pod "client-containers-fbe37130-910d-11e9-a8b9-dace53c98186" in namespace "containers-3661" to be "success or failure"
Jun 17 14:41:25.824: INFO: Pod "client-containers-fbe37130-910d-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.259723ms
Jun 17 14:41:27.829: INFO: Pod "client-containers-fbe37130-910d-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008928632s
Jun 17 14:41:29.834: INFO: Pod "client-containers-fbe37130-910d-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013643365s
Jun 17 14:41:31.838: INFO: Pod "client-containers-fbe37130-910d-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018384948s
STEP: Saw pod success
Jun 17 14:41:31.839: INFO: Pod "client-containers-fbe37130-910d-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:41:31.843: INFO: Trying to get logs from node lab1-k8s-node-2 pod client-containers-fbe37130-910d-11e9-a8b9-dace53c98186 container test-container: <nil>
STEP: delete the pod
Jun 17 14:41:31.870: INFO: Waiting for pod client-containers-fbe37130-910d-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:41:31.874: INFO: Pod client-containers-fbe37130-910d-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:41:31.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3661" for this suite.
Jun 17 14:41:37.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:41:38.034: INFO: namespace containers-3661 deletion completed in 6.152444291s

• [SLOW TEST:12.385 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:41:38.036: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4879
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Jun 17 14:41:38.201: INFO: Waiting up to 5m0s for pod "client-containers-034496fb-910e-11e9-a8b9-dace53c98186" in namespace "containers-4879" to be "success or failure"
Jun 17 14:41:38.207: INFO: Pod "client-containers-034496fb-910e-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 5.741056ms
Jun 17 14:41:40.214: INFO: Pod "client-containers-034496fb-910e-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013008005s
STEP: Saw pod success
Jun 17 14:41:40.214: INFO: Pod "client-containers-034496fb-910e-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:41:40.219: INFO: Trying to get logs from node lab1-k8s-node-2 pod client-containers-034496fb-910e-11e9-a8b9-dace53c98186 container test-container: <nil>
STEP: delete the pod
Jun 17 14:41:40.250: INFO: Waiting for pod client-containers-034496fb-910e-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:41:40.254: INFO: Pod client-containers-034496fb-910e-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:41:40.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4879" for this suite.
Jun 17 14:41:46.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:41:46.418: INFO: namespace containers-4879 deletion completed in 6.156667213s

• [SLOW TEST:8.383 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:41:46.419: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7269
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7269
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Jun 17 14:41:46.594: INFO: Found 0 stateful pods, waiting for 3
Jun 17 14:41:56.599: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 17 14:41:56.599: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 17 14:41:56.599: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 17 14:41:56.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-7269 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 17 14:41:56.847: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 17 14:41:56.847: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 17 14:41:56.847: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jun 17 14:42:06.885: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 17 14:42:16.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-7269 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 14:42:17.169: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 17 14:42:17.169: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 17 14:42:17.169: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 17 14:42:27.192: INFO: Waiting for StatefulSet statefulset-7269/ss2 to complete update
Jun 17 14:42:27.192: INFO: Waiting for Pod statefulset-7269/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 17 14:42:27.192: INFO: Waiting for Pod statefulset-7269/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 17 14:42:37.201: INFO: Waiting for StatefulSet statefulset-7269/ss2 to complete update
Jun 17 14:42:37.201: INFO: Waiting for Pod statefulset-7269/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 17 14:42:47.201: INFO: Waiting for StatefulSet statefulset-7269/ss2 to complete update
STEP: Rolling back to a previous revision
Jun 17 14:42:57.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-7269 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 17 14:42:57.434: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 17 14:42:57.434: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 17 14:42:57.434: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 17 14:43:07.477: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 17 14:43:17.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-7269 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 14:43:17.735: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 17 14:43:17.735: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 17 14:43:17.735: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 17 14:43:37.758: INFO: Waiting for StatefulSet statefulset-7269/ss2 to complete update
Jun 17 14:43:37.758: INFO: Waiting for Pod statefulset-7269/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jun 17 14:43:47.767: INFO: Deleting all statefulset in ns statefulset-7269
Jun 17 14:43:47.771: INFO: Scaling statefulset ss2 to 0
Jun 17 14:44:07.788: INFO: Waiting for statefulset status.replicas updated to 0
Jun 17 14:44:07.792: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:44:07.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7269" for this suite.
Jun 17 14:44:13.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:44:13.973: INFO: namespace statefulset-7269 deletion completed in 6.154741413s

• [SLOW TEST:147.554 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:44:13.973: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7231
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:44:14.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7231" for this suite.
Jun 17 14:44:36.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:44:36.318: INFO: namespace pods-7231 deletion completed in 22.161243162s

• [SLOW TEST:22.345 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:44:36.320: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5427
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-wv96
STEP: Creating a pod to test atomic-volume-subpath
Jun 17 14:44:36.500: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wv96" in namespace "subpath-5427" to be "success or failure"
Jun 17 14:44:36.505: INFO: Pod "pod-subpath-test-configmap-wv96": Phase="Pending", Reason="", readiness=false. Elapsed: 4.909491ms
Jun 17 14:44:38.510: INFO: Pod "pod-subpath-test-configmap-wv96": Phase="Running", Reason="", readiness=true. Elapsed: 2.010070369s
Jun 17 14:44:40.515: INFO: Pod "pod-subpath-test-configmap-wv96": Phase="Running", Reason="", readiness=true. Elapsed: 4.015417207s
Jun 17 14:44:42.520: INFO: Pod "pod-subpath-test-configmap-wv96": Phase="Running", Reason="", readiness=true. Elapsed: 6.019699202s
Jun 17 14:44:44.525: INFO: Pod "pod-subpath-test-configmap-wv96": Phase="Running", Reason="", readiness=true. Elapsed: 8.024849312s
Jun 17 14:44:46.530: INFO: Pod "pod-subpath-test-configmap-wv96": Phase="Running", Reason="", readiness=true. Elapsed: 10.029978748s
Jun 17 14:44:48.535: INFO: Pod "pod-subpath-test-configmap-wv96": Phase="Running", Reason="", readiness=true. Elapsed: 12.034771042s
Jun 17 14:44:50.539: INFO: Pod "pod-subpath-test-configmap-wv96": Phase="Running", Reason="", readiness=true. Elapsed: 14.039366704s
Jun 17 14:44:52.544: INFO: Pod "pod-subpath-test-configmap-wv96": Phase="Running", Reason="", readiness=true. Elapsed: 16.044279851s
Jun 17 14:44:54.550: INFO: Pod "pod-subpath-test-configmap-wv96": Phase="Running", Reason="", readiness=true. Elapsed: 18.049498432s
Jun 17 14:44:56.554: INFO: Pod "pod-subpath-test-configmap-wv96": Phase="Running", Reason="", readiness=true. Elapsed: 20.054224439s
Jun 17 14:44:58.559: INFO: Pod "pod-subpath-test-configmap-wv96": Phase="Running", Reason="", readiness=true. Elapsed: 22.059110147s
Jun 17 14:45:00.564: INFO: Pod "pod-subpath-test-configmap-wv96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.063731751s
STEP: Saw pod success
Jun 17 14:45:00.564: INFO: Pod "pod-subpath-test-configmap-wv96" satisfied condition "success or failure"
Jun 17 14:45:00.569: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-subpath-test-configmap-wv96 container test-container-subpath-configmap-wv96: <nil>
STEP: delete the pod
Jun 17 14:45:00.596: INFO: Waiting for pod pod-subpath-test-configmap-wv96 to disappear
Jun 17 14:45:00.599: INFO: Pod pod-subpath-test-configmap-wv96 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-wv96
Jun 17 14:45:00.599: INFO: Deleting pod "pod-subpath-test-configmap-wv96" in namespace "subpath-5427"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:45:00.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5427" for this suite.
Jun 17 14:45:06.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:45:06.773: INFO: namespace subpath-5427 deletion completed in 6.162249416s

• [SLOW TEST:30.453 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:45:06.778: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7276
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 17 14:45:06.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7276'
Jun 17 14:45:07.032: INFO: stderr: ""
Jun 17 14:45:07.032: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Jun 17 14:45:07.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 delete pods e2e-test-nginx-pod --namespace=kubectl-7276'
Jun 17 14:45:12.865: INFO: stderr: ""
Jun 17 14:45:12.865: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:45:12.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7276" for this suite.
Jun 17 14:45:18.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:45:19.020: INFO: namespace kubectl-7276 deletion completed in 6.148746311s

• [SLOW TEST:12.246 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:45:19.022: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4776
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-4776
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 17 14:45:19.176: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 17 14:45:45.300: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.74.9:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4776 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 14:45:45.300: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 14:45:45.530: INFO: Found all expected endpoints: [netserver-0]
Jun 17 14:45:45.535: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.16:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4776 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 14:45:45.535: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 14:45:45.760: INFO: Found all expected endpoints: [netserver-1]
Jun 17 14:45:45.765: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.95.18:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4776 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 14:45:45.765: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 14:45:45.990: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:45:45.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4776" for this suite.
Jun 17 14:46:08.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:46:08.164: INFO: namespace pod-network-test-4776 deletion completed in 22.16628258s

• [SLOW TEST:49.142 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:46:08.166: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6511
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 14:46:08.367: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"a44ca544-910e-11e9-bdd5-fa163e38c6bd", Controller:(*bool)(0xc000b95f56), BlockOwnerDeletion:(*bool)(0xc000b95f57)}}
Jun 17 14:46:08.374: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a44a2259-910e-11e9-bdd5-fa163e38c6bd", Controller:(*bool)(0xc0028b7ee6), BlockOwnerDeletion:(*bool)(0xc0028b7ee7)}}
Jun 17 14:46:08.387: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a44b2833-910e-11e9-bdd5-fa163e38c6bd", Controller:(*bool)(0xc0009e612a), BlockOwnerDeletion:(*bool)(0xc0009e612b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:46:13.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6511" for this suite.
Jun 17 14:46:19.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:46:19.569: INFO: namespace gc-6511 deletion completed in 6.155476043s

• [SLOW TEST:11.402 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:46:19.569: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-806
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 17 14:46:19.781: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-806,SelfLink:/api/v1/namespaces/watch-806/configmaps/e2e-watch-test-resource-version,UID:ab155ad6-910e-11e9-bdd5-fa163e38c6bd,ResourceVersion:13011,Generation:0,CreationTimestamp:2019-06-17 14:46:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 17 14:46:19.781: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-806,SelfLink:/api/v1/namespaces/watch-806/configmaps/e2e-watch-test-resource-version,UID:ab155ad6-910e-11e9-bdd5-fa163e38c6bd,ResourceVersion:13012,Generation:0,CreationTimestamp:2019-06-17 14:46:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:46:19.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-806" for this suite.
Jun 17 14:46:25.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:46:25.940: INFO: namespace watch-806 deletion completed in 6.151813746s

• [SLOW TEST:6.371 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:46:25.940: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8356
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 17 14:46:26.156: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:46:26.156: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:46:26.156: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:46:26.161: INFO: Number of nodes with available pods: 0
Jun 17 14:46:26.161: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 14:46:27.170: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:46:27.170: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:46:27.170: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:46:27.175: INFO: Number of nodes with available pods: 0
Jun 17 14:46:27.175: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 14:46:28.172: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:46:28.172: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:46:28.172: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:46:28.177: INFO: Number of nodes with available pods: 3
Jun 17 14:46:28.177: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 17 14:46:28.200: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:46:28.200: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:46:28.200: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:46:28.205: INFO: Number of nodes with available pods: 2
Jun 17 14:46:28.205: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Jun 17 14:46:29.212: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:46:29.212: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:46:29.212: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:46:29.217: INFO: Number of nodes with available pods: 2
Jun 17 14:46:29.217: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Jun 17 14:46:30.213: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:46:30.213: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:46:30.213: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:46:30.217: INFO: Number of nodes with available pods: 3
Jun 17 14:46:30.217: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8356, will wait for the garbage collector to delete the pods
Jun 17 14:46:30.305: INFO: Deleting DaemonSet.extensions daemon-set took: 19.227589ms
Jun 17 14:46:30.405: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.298677ms
Jun 17 14:46:43.410: INFO: Number of nodes with available pods: 0
Jun 17 14:46:43.410: INFO: Number of running nodes: 0, number of available pods: 0
Jun 17 14:46:43.416: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8356/daemonsets","resourceVersion":"13172"},"items":null}

Jun 17 14:46:43.419: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8356/pods","resourceVersion":"13172"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:46:43.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8356" for this suite.
Jun 17 14:46:49.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:46:49.597: INFO: namespace daemonsets-8356 deletion completed in 6.149715241s

• [SLOW TEST:23.657 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:46:49.598: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2955
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 17 14:46:49.813: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 17 14:46:54.819: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:46:55.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2955" for this suite.
Jun 17 14:47:01.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:47:02.011: INFO: namespace replication-controller-2955 deletion completed in 6.163934826s

• [SLOW TEST:12.413 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:47:02.011: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5494
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Jun 17 14:47:02.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 create -f - --namespace=kubectl-5494'
Jun 17 14:47:02.435: INFO: stderr: ""
Jun 17 14:47:02.435: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 17 14:47:02.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5494'
Jun 17 14:47:02.536: INFO: stderr: ""
Jun 17 14:47:02.536: INFO: stdout: "update-demo-nautilus-7s5td update-demo-nautilus-zwsgj "
Jun 17 14:47:02.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-7s5td -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5494'
Jun 17 14:47:02.625: INFO: stderr: ""
Jun 17 14:47:02.625: INFO: stdout: ""
Jun 17 14:47:02.625: INFO: update-demo-nautilus-7s5td is created but not running
Jun 17 14:47:07.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5494'
Jun 17 14:47:07.701: INFO: stderr: ""
Jun 17 14:47:07.701: INFO: stdout: "update-demo-nautilus-7s5td update-demo-nautilus-zwsgj "
Jun 17 14:47:07.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-7s5td -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5494'
Jun 17 14:47:07.776: INFO: stderr: ""
Jun 17 14:47:07.776: INFO: stdout: "true"
Jun 17 14:47:07.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-7s5td -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5494'
Jun 17 14:47:07.856: INFO: stderr: ""
Jun 17 14:47:07.856: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 17 14:47:07.856: INFO: validating pod update-demo-nautilus-7s5td
Jun 17 14:47:07.863: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 17 14:47:07.863: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 17 14:47:07.863: INFO: update-demo-nautilus-7s5td is verified up and running
Jun 17 14:47:07.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-zwsgj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5494'
Jun 17 14:47:07.955: INFO: stderr: ""
Jun 17 14:47:07.955: INFO: stdout: "true"
Jun 17 14:47:07.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-zwsgj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5494'
Jun 17 14:47:08.037: INFO: stderr: ""
Jun 17 14:47:08.037: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 17 14:47:08.037: INFO: validating pod update-demo-nautilus-zwsgj
Jun 17 14:47:08.050: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 17 14:47:08.050: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 17 14:47:08.050: INFO: update-demo-nautilus-zwsgj is verified up and running
STEP: rolling-update to new replication controller
Jun 17 14:47:08.051: INFO: scanned /root for discovery docs: <nil>
Jun 17 14:47:08.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-5494'
Jun 17 14:47:32.617: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 17 14:47:32.617: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 17 14:47:32.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5494'
Jun 17 14:47:32.720: INFO: stderr: ""
Jun 17 14:47:32.720: INFO: stdout: "update-demo-kitten-lgjgg update-demo-kitten-tsktp "
Jun 17 14:47:32.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-kitten-lgjgg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5494'
Jun 17 14:47:32.807: INFO: stderr: ""
Jun 17 14:47:32.807: INFO: stdout: "true"
Jun 17 14:47:32.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-kitten-lgjgg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5494'
Jun 17 14:47:32.894: INFO: stderr: ""
Jun 17 14:47:32.894: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 17 14:47:32.894: INFO: validating pod update-demo-kitten-lgjgg
Jun 17 14:47:32.901: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 17 14:47:32.901: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 17 14:47:32.901: INFO: update-demo-kitten-lgjgg is verified up and running
Jun 17 14:47:32.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-kitten-tsktp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5494'
Jun 17 14:47:33.004: INFO: stderr: ""
Jun 17 14:47:33.004: INFO: stdout: "true"
Jun 17 14:47:33.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-kitten-tsktp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5494'
Jun 17 14:47:33.117: INFO: stderr: ""
Jun 17 14:47:33.117: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 17 14:47:33.117: INFO: validating pod update-demo-kitten-tsktp
Jun 17 14:47:33.124: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 17 14:47:33.124: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 17 14:47:33.124: INFO: update-demo-kitten-tsktp is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:47:33.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5494" for this suite.
Jun 17 14:47:55.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:47:55.297: INFO: namespace kubectl-5494 deletion completed in 22.165108803s

• [SLOW TEST:53.286 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:47:55.297: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-3513
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Jun 17 14:47:55.462: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-3513" to be "success or failure"
Jun 17 14:47:55.466: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.800465ms
Jun 17 14:47:57.471: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008370046s
Jun 17 14:47:59.475: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013195567s
STEP: Saw pod success
Jun 17 14:47:59.476: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jun 17 14:47:59.479: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jun 17 14:47:59.509: INFO: Waiting for pod pod-host-path-test to disappear
Jun 17 14:47:59.513: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:47:59.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-3513" for this suite.
Jun 17 14:48:05.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:48:05.678: INFO: namespace hostpath-3513 deletion completed in 6.158108323s

• [SLOW TEST:10.382 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:48:05.681: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5143
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 14:48:05.880: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 17 14:48:05.894: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:05.894: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:05.894: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:05.898: INFO: Number of nodes with available pods: 0
Jun 17 14:48:05.898: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 14:48:06.908: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:06.908: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:06.908: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:06.912: INFO: Number of nodes with available pods: 0
Jun 17 14:48:06.912: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 14:48:07.906: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:07.906: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:07.906: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:07.914: INFO: Number of nodes with available pods: 0
Jun 17 14:48:07.914: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 14:48:08.905: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:08.905: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:08.905: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:08.909: INFO: Number of nodes with available pods: 3
Jun 17 14:48:08.909: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 17 14:48:08.956: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:08.956: INFO: Wrong image for pod: daemon-set-thh8x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:08.956: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:08.962: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:08.962: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:08.962: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:09.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:09.967: INFO: Wrong image for pod: daemon-set-thh8x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:09.967: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:09.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:09.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:09.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:10.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:10.967: INFO: Wrong image for pod: daemon-set-thh8x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:10.967: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:10.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:10.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:10.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:11.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:11.967: INFO: Wrong image for pod: daemon-set-thh8x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:11.967: INFO: Pod daemon-set-thh8x is not available
Jun 17 14:48:11.967: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:11.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:11.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:11.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:12.969: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:12.969: INFO: Wrong image for pod: daemon-set-thh8x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:12.969: INFO: Pod daemon-set-thh8x is not available
Jun 17 14:48:12.970: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:12.977: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:12.978: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:12.978: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:13.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:13.967: INFO: Wrong image for pod: daemon-set-thh8x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:13.967: INFO: Pod daemon-set-thh8x is not available
Jun 17 14:48:13.967: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:13.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:13.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:13.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:14.966: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:14.966: INFO: Wrong image for pod: daemon-set-thh8x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:14.966: INFO: Pod daemon-set-thh8x is not available
Jun 17 14:48:14.966: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:14.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:14.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:14.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:15.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:15.967: INFO: Wrong image for pod: daemon-set-thh8x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:15.967: INFO: Pod daemon-set-thh8x is not available
Jun 17 14:48:15.967: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:15.973: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:15.973: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:15.973: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:16.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:16.967: INFO: Wrong image for pod: daemon-set-thh8x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:16.967: INFO: Pod daemon-set-thh8x is not available
Jun 17 14:48:16.967: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:16.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:16.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:16.981: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:17.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:17.967: INFO: Wrong image for pod: daemon-set-thh8x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:17.967: INFO: Pod daemon-set-thh8x is not available
Jun 17 14:48:17.967: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:17.976: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:17.976: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:17.976: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:18.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:18.967: INFO: Wrong image for pod: daemon-set-thh8x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:18.967: INFO: Pod daemon-set-thh8x is not available
Jun 17 14:48:18.967: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:18.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:18.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:18.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:19.966: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:19.967: INFO: Wrong image for pod: daemon-set-thh8x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:19.967: INFO: Pod daemon-set-thh8x is not available
Jun 17 14:48:19.967: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:19.975: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:19.975: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:19.975: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:20.966: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:20.966: INFO: Wrong image for pod: daemon-set-thh8x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:20.966: INFO: Pod daemon-set-thh8x is not available
Jun 17 14:48:20.966: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:20.973: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:20.973: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:20.973: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:21.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:21.967: INFO: Wrong image for pod: daemon-set-thh8x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:21.967: INFO: Pod daemon-set-thh8x is not available
Jun 17 14:48:21.967: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:21.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:21.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:21.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:22.967: INFO: Pod daemon-set-4npsd is not available
Jun 17 14:48:22.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:22.967: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:22.975: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:22.975: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:22.975: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:23.967: INFO: Pod daemon-set-4npsd is not available
Jun 17 14:48:23.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:23.967: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:23.975: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:23.975: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:23.975: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:24.966: INFO: Pod daemon-set-4npsd is not available
Jun 17 14:48:24.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:24.967: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:24.973: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:24.973: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:24.973: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:25.967: INFO: Pod daemon-set-4npsd is not available
Jun 17 14:48:25.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:25.968: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:25.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:25.975: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:25.975: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:26.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:26.967: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:26.973: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:26.973: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:26.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:27.966: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:27.966: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:27.972: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:27.973: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:27.973: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:28.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:28.967: INFO: Wrong image for pod: daemon-set-zh5sv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:28.967: INFO: Pod daemon-set-zh5sv is not available
Jun 17 14:48:28.983: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:28.983: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:28.983: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:29.970: INFO: Pod daemon-set-54p5p is not available
Jun 17 14:48:29.970: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:29.977: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:29.977: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:29.977: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:30.967: INFO: Pod daemon-set-54p5p is not available
Jun 17 14:48:30.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:30.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:30.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:30.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:31.967: INFO: Pod daemon-set-54p5p is not available
Jun 17 14:48:31.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:31.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:31.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:31.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:32.967: INFO: Pod daemon-set-54p5p is not available
Jun 17 14:48:32.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:32.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:32.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:32.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:33.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:33.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:33.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:33.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:34.967: INFO: Wrong image for pod: daemon-set-dxmjm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 17 14:48:34.967: INFO: Pod daemon-set-dxmjm is not available
Jun 17 14:48:34.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:34.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:34.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:35.966: INFO: Pod daemon-set-dj5bc is not available
Jun 17 14:48:35.973: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:35.973: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:35.974: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 17 14:48:35.982: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:35.982: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:35.982: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:35.986: INFO: Number of nodes with available pods: 2
Jun 17 14:48:35.986: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Jun 17 14:48:36.997: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:36.997: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:36.997: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:37.002: INFO: Number of nodes with available pods: 2
Jun 17 14:48:37.002: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Jun 17 14:48:37.994: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:37.994: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:37.994: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:48:37.999: INFO: Number of nodes with available pods: 3
Jun 17 14:48:37.999: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5143, will wait for the garbage collector to delete the pods
Jun 17 14:48:38.092: INFO: Deleting DaemonSet.extensions daemon-set took: 13.711038ms
Jun 17 14:48:38.393: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.406475ms
Jun 17 14:48:43.097: INFO: Number of nodes with available pods: 0
Jun 17 14:48:43.097: INFO: Number of running nodes: 0, number of available pods: 0
Jun 17 14:48:43.102: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5143/daemonsets","resourceVersion":"13877"},"items":null}

Jun 17 14:48:43.105: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5143/pods","resourceVersion":"13877"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:48:43.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5143" for this suite.
Jun 17 14:48:49.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:48:49.295: INFO: namespace daemonsets-5143 deletion completed in 6.16068931s

• [SLOW TEST:43.614 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:48:49.297: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2453
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jun 17 14:48:49.466: INFO: Waiting up to 5m0s for pod "downward-api-04520041-910f-11e9-a8b9-dace53c98186" in namespace "downward-api-2453" to be "success or failure"
Jun 17 14:48:49.475: INFO: Pod "downward-api-04520041-910f-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 9.11076ms
Jun 17 14:48:51.480: INFO: Pod "downward-api-04520041-910f-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014197737s
Jun 17 14:48:53.485: INFO: Pod "downward-api-04520041-910f-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019213256s
STEP: Saw pod success
Jun 17 14:48:53.485: INFO: Pod "downward-api-04520041-910f-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:48:53.490: INFO: Trying to get logs from node lab1-k8s-node-2 pod downward-api-04520041-910f-11e9-a8b9-dace53c98186 container dapi-container: <nil>
STEP: delete the pod
Jun 17 14:48:53.527: INFO: Waiting for pod downward-api-04520041-910f-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:48:53.536: INFO: Pod downward-api-04520041-910f-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:48:53.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2453" for this suite.
Jun 17 14:48:59.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:48:59.707: INFO: namespace downward-api-2453 deletion completed in 6.160329046s

• [SLOW TEST:10.412 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:48:59.708: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6588
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-0a860198-910f-11e9-a8b9-dace53c98186
Jun 17 14:48:59.877: INFO: Pod name my-hostname-basic-0a860198-910f-11e9-a8b9-dace53c98186: Found 0 pods out of 1
Jun 17 14:49:04.882: INFO: Pod name my-hostname-basic-0a860198-910f-11e9-a8b9-dace53c98186: Found 1 pods out of 1
Jun 17 14:49:04.882: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0a860198-910f-11e9-a8b9-dace53c98186" are running
Jun 17 14:49:04.886: INFO: Pod "my-hostname-basic-0a860198-910f-11e9-a8b9-dace53c98186-mc7lh" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-17 14:48:59 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-17 14:49:04 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-17 14:49:04 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-17 14:48:59 +0000 UTC Reason: Message:}])
Jun 17 14:49:04.886: INFO: Trying to dial the pod
Jun 17 14:49:09.900: INFO: Controller my-hostname-basic-0a860198-910f-11e9-a8b9-dace53c98186: Got expected result from replica 1 [my-hostname-basic-0a860198-910f-11e9-a8b9-dace53c98186-mc7lh]: "my-hostname-basic-0a860198-910f-11e9-a8b9-dace53c98186-mc7lh", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:49:09.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6588" for this suite.
Jun 17 14:49:15.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:49:16.064: INFO: namespace replication-controller-6588 deletion completed in 6.157468564s

• [SLOW TEST:16.357 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:49:16.068: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7169
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Jun 17 14:49:16.242: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jun 17 14:49:16.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 create -f - --namespace=kubectl-7169'
Jun 17 14:49:16.516: INFO: stderr: ""
Jun 17 14:49:16.516: INFO: stdout: "service/redis-slave created\n"
Jun 17 14:49:16.516: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jun 17 14:49:16.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 create -f - --namespace=kubectl-7169'
Jun 17 14:49:16.784: INFO: stderr: ""
Jun 17 14:49:16.784: INFO: stdout: "service/redis-master created\n"
Jun 17 14:49:16.784: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 17 14:49:16.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 create -f - --namespace=kubectl-7169'
Jun 17 14:49:17.051: INFO: stderr: ""
Jun 17 14:49:17.051: INFO: stdout: "service/frontend created\n"
Jun 17 14:49:17.051: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jun 17 14:49:17.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 create -f - --namespace=kubectl-7169'
Jun 17 14:49:17.307: INFO: stderr: ""
Jun 17 14:49:17.307: INFO: stdout: "deployment.apps/frontend created\n"
Jun 17 14:49:17.308: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 17 14:49:17.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 create -f - --namespace=kubectl-7169'
Jun 17 14:49:17.550: INFO: stderr: ""
Jun 17 14:49:17.550: INFO: stdout: "deployment.apps/redis-master created\n"
Jun 17 14:49:17.551: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jun 17 14:49:17.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 create -f - --namespace=kubectl-7169'
Jun 17 14:49:17.848: INFO: stderr: ""
Jun 17 14:49:17.848: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jun 17 14:49:17.848: INFO: Waiting for all frontend pods to be Running.
Jun 17 14:49:32.898: INFO: Waiting for frontend to serve content.
Jun 17 14:49:33.931: INFO: Trying to add a new entry to the guestbook.
Jun 17 14:49:33.943: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jun 17 14:49:33.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 delete --grace-period=0 --force -f - --namespace=kubectl-7169'
Jun 17 14:49:34.103: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 17 14:49:34.103: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jun 17 14:49:34.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 delete --grace-period=0 --force -f - --namespace=kubectl-7169'
Jun 17 14:49:34.317: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 17 14:49:34.317: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 17 14:49:34.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 delete --grace-period=0 --force -f - --namespace=kubectl-7169'
Jun 17 14:49:34.468: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 17 14:49:34.468: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 17 14:49:34.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 delete --grace-period=0 --force -f - --namespace=kubectl-7169'
Jun 17 14:49:34.620: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 17 14:49:34.620: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 17 14:49:34.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 delete --grace-period=0 --force -f - --namespace=kubectl-7169'
Jun 17 14:49:34.759: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 17 14:49:34.759: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 17 14:49:34.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 delete --grace-period=0 --force -f - --namespace=kubectl-7169'
Jun 17 14:49:34.903: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 17 14:49:34.903: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:49:34.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7169" for this suite.
Jun 17 14:50:14.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:50:15.076: INFO: namespace kubectl-7169 deletion completed in 40.164432518s

• [SLOW TEST:59.009 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:50:15.076: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7642
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:50:38.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7642" for this suite.
Jun 17 14:50:44.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:50:44.743: INFO: namespace container-runtime-7642 deletion completed in 6.179956952s

• [SLOW TEST:29.667 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:50:44.753: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9274
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-4924da46-910f-11e9-a8b9-dace53c98186
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-4924da46-910f-11e9-a8b9-dace53c98186
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:50:48.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9274" for this suite.
Jun 17 14:51:11.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:51:11.159: INFO: namespace configmap-9274 deletion completed in 22.161969517s

• [SLOW TEST:26.407 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:51:11.164: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2090
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 17 14:51:11.329: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58e10236-910f-11e9-a8b9-dace53c98186" in namespace "projected-2090" to be "success or failure"
Jun 17 14:51:11.333: INFO: Pod "downwardapi-volume-58e10236-910f-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.491506ms
Jun 17 14:51:13.338: INFO: Pod "downwardapi-volume-58e10236-910f-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00961209s
Jun 17 14:51:15.343: INFO: Pod "downwardapi-volume-58e10236-910f-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014576157s
STEP: Saw pod success
Jun 17 14:51:15.344: INFO: Pod "downwardapi-volume-58e10236-910f-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:51:15.348: INFO: Trying to get logs from node lab1-k8s-node-2 pod downwardapi-volume-58e10236-910f-11e9-a8b9-dace53c98186 container client-container: <nil>
STEP: delete the pod
Jun 17 14:51:15.373: INFO: Waiting for pod downwardapi-volume-58e10236-910f-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:51:15.376: INFO: Pod downwardapi-volume-58e10236-910f-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:51:15.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2090" for this suite.
Jun 17 14:51:21.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:51:21.548: INFO: namespace projected-2090 deletion completed in 6.162158315s

• [SLOW TEST:10.384 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:51:21.548: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8928
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-8928
Jun 17 14:51:25.732: INFO: Started pod liveness-http in namespace container-probe-8928
STEP: checking the pod's current state and verifying that restartCount is present
Jun 17 14:51:25.736: INFO: Initial restart count of pod liveness-http is 0
Jun 17 14:51:43.783: INFO: Restart count of pod container-probe-8928/liveness-http is now 1 (18.046640036s elapsed)
Jun 17 14:52:03.833: INFO: Restart count of pod container-probe-8928/liveness-http is now 2 (38.096887307s elapsed)
Jun 17 14:52:23.882: INFO: Restart count of pod container-probe-8928/liveness-http is now 3 (58.145301388s elapsed)
Jun 17 14:52:43.929: INFO: Restart count of pod container-probe-8928/liveness-http is now 4 (1m18.192264745s elapsed)
Jun 17 14:53:46.080: INFO: Restart count of pod container-probe-8928/liveness-http is now 5 (2m20.3442158s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:53:46.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8928" for this suite.
Jun 17 14:53:52.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:53:52.258: INFO: namespace container-probe-8928 deletion completed in 6.15360374s

• [SLOW TEST:150.713 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:53:52.262: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2325
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jun 17 14:53:52.432: INFO: Waiting up to 5m0s for pod "downward-api-b8e769c3-910f-11e9-a8b9-dace53c98186" in namespace "downward-api-2325" to be "success or failure"
Jun 17 14:53:52.441: INFO: Pod "downward-api-b8e769c3-910f-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 8.28801ms
Jun 17 14:53:54.446: INFO: Pod "downward-api-b8e769c3-910f-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013577335s
Jun 17 14:53:56.451: INFO: Pod "downward-api-b8e769c3-910f-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018994638s
STEP: Saw pod success
Jun 17 14:53:56.451: INFO: Pod "downward-api-b8e769c3-910f-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:53:56.456: INFO: Trying to get logs from node lab1-k8s-node-2 pod downward-api-b8e769c3-910f-11e9-a8b9-dace53c98186 container dapi-container: <nil>
STEP: delete the pod
Jun 17 14:53:56.481: INFO: Waiting for pod downward-api-b8e769c3-910f-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:53:56.485: INFO: Pod downward-api-b8e769c3-910f-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:53:56.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2325" for this suite.
Jun 17 14:54:02.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:54:02.652: INFO: namespace downward-api-2325 deletion completed in 6.159871203s

• [SLOW TEST:10.390 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:54:02.654: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7232
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-bf193faf-910f-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume configMaps
Jun 17 14:54:02.833: INFO: Waiting up to 5m0s for pod "pod-configmaps-bf1a8f7d-910f-11e9-a8b9-dace53c98186" in namespace "configmap-7232" to be "success or failure"
Jun 17 14:54:02.837: INFO: Pod "pod-configmaps-bf1a8f7d-910f-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025453ms
Jun 17 14:54:04.842: INFO: Pod "pod-configmaps-bf1a8f7d-910f-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009310339s
STEP: Saw pod success
Jun 17 14:54:04.842: INFO: Pod "pod-configmaps-bf1a8f7d-910f-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:54:04.847: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-configmaps-bf1a8f7d-910f-11e9-a8b9-dace53c98186 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 17 14:54:04.873: INFO: Waiting for pod pod-configmaps-bf1a8f7d-910f-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:54:04.877: INFO: Pod pod-configmaps-bf1a8f7d-910f-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:54:04.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7232" for this suite.
Jun 17 14:54:10.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:54:11.037: INFO: namespace configmap-7232 deletion completed in 6.147897882s

• [SLOW TEST:8.383 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:54:11.045: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-250
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 17 14:54:11.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-250'
Jun 17 14:54:11.300: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 17 14:54:11.300: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Jun 17 14:54:15.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 delete deployment e2e-test-nginx-deployment --namespace=kubectl-250'
Jun 17 14:54:15.407: INFO: stderr: ""
Jun 17 14:54:15.407: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:54:15.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-250" for this suite.
Jun 17 14:54:21.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:54:21.586: INFO: namespace kubectl-250 deletion completed in 6.172260237s

• [SLOW TEST:10.542 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:54:21.589: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8997
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 14:54:21.779: INFO: Create a RollingUpdate DaemonSet
Jun 17 14:54:21.787: INFO: Check that daemon pods launch on every node of the cluster
Jun 17 14:54:21.796: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:21.796: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:21.797: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:21.804: INFO: Number of nodes with available pods: 0
Jun 17 14:54:21.804: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 14:54:22.814: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:22.814: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:22.814: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:22.818: INFO: Number of nodes with available pods: 0
Jun 17 14:54:22.818: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 14:54:23.812: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:23.812: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:23.812: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:23.816: INFO: Number of nodes with available pods: 2
Jun 17 14:54:23.816: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Jun 17 14:54:24.811: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:24.811: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:24.811: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:24.816: INFO: Number of nodes with available pods: 3
Jun 17 14:54:24.816: INFO: Number of running nodes: 3, number of available pods: 3
Jun 17 14:54:24.816: INFO: Update the DaemonSet to trigger a rollout
Jun 17 14:54:24.831: INFO: Updating DaemonSet daemon-set
Jun 17 14:54:33.847: INFO: Roll back the DaemonSet before rollout is complete
Jun 17 14:54:33.861: INFO: Updating DaemonSet daemon-set
Jun 17 14:54:33.861: INFO: Make sure DaemonSet rollback is complete
Jun 17 14:54:33.868: INFO: Wrong image for pod: daemon-set-8pkk2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 17 14:54:33.868: INFO: Pod daemon-set-8pkk2 is not available
Jun 17 14:54:33.875: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:33.875: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:33.875: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:34.880: INFO: Wrong image for pod: daemon-set-8pkk2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 17 14:54:34.880: INFO: Pod daemon-set-8pkk2 is not available
Jun 17 14:54:34.886: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:34.887: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:34.887: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:35.880: INFO: Wrong image for pod: daemon-set-8pkk2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 17 14:54:35.880: INFO: Pod daemon-set-8pkk2 is not available
Jun 17 14:54:35.888: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:35.888: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:35.888: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:36.880: INFO: Pod daemon-set-g49vl is not available
Jun 17 14:54:36.886: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:36.886: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:54:36.886: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8997, will wait for the garbage collector to delete the pods
Jun 17 14:54:36.964: INFO: Deleting DaemonSet.extensions daemon-set took: 14.049279ms
Jun 17 14:54:37.064: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.238055ms
Jun 17 14:54:43.369: INFO: Number of nodes with available pods: 0
Jun 17 14:54:43.369: INFO: Number of running nodes: 0, number of available pods: 0
Jun 17 14:54:43.375: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8997/daemonsets","resourceVersion":"15491"},"items":null}

Jun 17 14:54:43.379: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8997/pods","resourceVersion":"15491"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:54:43.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8997" for this suite.
Jun 17 14:54:49.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:54:49.571: INFO: namespace daemonsets-8997 deletion completed in 6.16516103s

• [SLOW TEST:27.982 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:54:49.572: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6005
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jun 17 14:54:49.728: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 17 14:54:49.741: INFO: Waiting for terminating namespaces to be deleted...
Jun 17 14:54:49.745: INFO: 
Logging pods the kubelet thinks is on node lab1-k8s-node-1 before test
Jun 17 14:54:49.753: INFO: dns-autoscaler-56c969bdb8-5brv9 from kube-system started at 2019-06-17 13:30:26 +0000 UTC (1 container statuses recorded)
Jun 17 14:54:49.753: INFO: 	Container autoscaler ready: true, restart count 0
Jun 17 14:54:49.753: INFO: nodelocaldns-zc5f8 from kube-system started at 2019-06-17 13:30:28 +0000 UTC (1 container statuses recorded)
Jun 17 14:54:49.753: INFO: 	Container node-cache ready: true, restart count 0
Jun 17 14:54:49.753: INFO: sonobuoy-systemd-logs-daemon-set-b6e1ac2b5ae94c02-2mwvd from heptio-sonobuoy started at 2019-06-17 14:35:15 +0000 UTC (2 container statuses recorded)
Jun 17 14:54:49.753: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 17 14:54:49.753: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 17 14:54:49.753: INFO: nginx-proxy-lab1-k8s-node-1 from kube-system started at <nil> (0 container statuses recorded)
Jun 17 14:54:49.753: INFO: kube-proxy-b4j5w from kube-system started at 2019-06-17 13:30:32 +0000 UTC (1 container statuses recorded)
Jun 17 14:54:49.753: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 17 14:54:49.753: INFO: calico-node-dlffw from kube-system started at 2019-06-17 13:29:45 +0000 UTC (1 container statuses recorded)
Jun 17 14:54:49.753: INFO: 	Container calico-node ready: true, restart count 0
Jun 17 14:54:49.753: INFO: tiller-deploy-86c8b7897c-f2sdt from kube-system started at 2019-06-17 13:30:50 +0000 UTC (1 container statuses recorded)
Jun 17 14:54:49.753: INFO: 	Container tiller ready: true, restart count 0
Jun 17 14:54:49.753: INFO: 
Logging pods the kubelet thinks is on node lab1-k8s-node-2 before test
Jun 17 14:54:49.762: INFO: nginx-proxy-lab1-k8s-node-2 from kube-system started at <nil> (0 container statuses recorded)
Jun 17 14:54:49.762: INFO: calico-kube-controllers-57c89fc87d-sk646 from kube-system started at 2019-06-17 13:29:55 +0000 UTC (1 container statuses recorded)
Jun 17 14:54:49.762: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 17 14:54:49.762: INFO: nodelocaldns-4979m from kube-system started at 2019-06-17 13:30:28 +0000 UTC (1 container statuses recorded)
Jun 17 14:54:49.762: INFO: 	Container node-cache ready: true, restart count 0
Jun 17 14:54:49.762: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-17 14:35:09 +0000 UTC (1 container statuses recorded)
Jun 17 14:54:49.762: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 17 14:54:49.762: INFO: kube-proxy-gtfkl from kube-system started at 2019-06-17 13:30:03 +0000 UTC (1 container statuses recorded)
Jun 17 14:54:49.762: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 17 14:54:49.762: INFO: calico-node-fz8bf from kube-system started at 2019-06-17 13:29:45 +0000 UTC (1 container statuses recorded)
Jun 17 14:54:49.763: INFO: 	Container calico-node ready: true, restart count 0
Jun 17 14:54:49.763: INFO: kubernetes-dashboard-57875dc8c7-mmp6z from kube-system started at 2019-06-17 13:30:30 +0000 UTC (1 container statuses recorded)
Jun 17 14:54:49.763: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun 17 14:54:49.763: INFO: sonobuoy-systemd-logs-daemon-set-b6e1ac2b5ae94c02-ngs2t from heptio-sonobuoy started at 2019-06-17 14:35:15 +0000 UTC (2 container statuses recorded)
Jun 17 14:54:49.763: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 17 14:54:49.763: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 17 14:54:49.763: INFO: 
Logging pods the kubelet thinks is on node lab1-k8s-node-3 before test
Jun 17 14:54:49.780: INFO: coredns-7646874c97-76qgd from kube-system started at 2019-06-17 13:30:33 +0000 UTC (1 container statuses recorded)
Jun 17 14:54:49.780: INFO: 	Container coredns ready: true, restart count 0
Jun 17 14:54:49.780: INFO: sonobuoy-e2e-job-383f0a2362094705 from heptio-sonobuoy started at 2019-06-17 14:35:15 +0000 UTC (2 container statuses recorded)
Jun 17 14:54:49.780: INFO: 	Container e2e ready: true, restart count 0
Jun 17 14:54:49.780: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 17 14:54:49.780: INFO: sonobuoy-systemd-logs-daemon-set-b6e1ac2b5ae94c02-xndrs from heptio-sonobuoy started at 2019-06-17 14:35:15 +0000 UTC (2 container statuses recorded)
Jun 17 14:54:49.780: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 17 14:54:49.780: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 17 14:54:49.780: INFO: kube-proxy-gdc49 from kube-system started at 2019-06-17 13:30:13 +0000 UTC (1 container statuses recorded)
Jun 17 14:54:49.780: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 17 14:54:49.780: INFO: nodelocaldns-psdz6 from kube-system started at 2019-06-17 13:30:28 +0000 UTC (1 container statuses recorded)
Jun 17 14:54:49.780: INFO: 	Container node-cache ready: true, restart count 0
Jun 17 14:54:49.780: INFO: nginx-proxy-lab1-k8s-node-3 from kube-system started at <nil> (0 container statuses recorded)
Jun 17 14:54:49.780: INFO: calico-node-rr2wf from kube-system started at 2019-06-17 13:29:45 +0000 UTC (1 container statuses recorded)
Jun 17 14:54:49.780: INFO: 	Container calico-node ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node lab1-k8s-node-1
STEP: verifying the node has the label node lab1-k8s-node-2
STEP: verifying the node has the label node lab1-k8s-node-3
Jun 17 14:54:49.868: INFO: Pod sonobuoy requesting resource cpu=0m on Node lab1-k8s-node-2
Jun 17 14:54:49.868: INFO: Pod sonobuoy-e2e-job-383f0a2362094705 requesting resource cpu=0m on Node lab1-k8s-node-3
Jun 17 14:54:49.868: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6e1ac2b5ae94c02-2mwvd requesting resource cpu=0m on Node lab1-k8s-node-1
Jun 17 14:54:49.868: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6e1ac2b5ae94c02-ngs2t requesting resource cpu=0m on Node lab1-k8s-node-2
Jun 17 14:54:49.868: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6e1ac2b5ae94c02-xndrs requesting resource cpu=0m on Node lab1-k8s-node-3
Jun 17 14:54:49.868: INFO: Pod calico-kube-controllers-57c89fc87d-sk646 requesting resource cpu=30m on Node lab1-k8s-node-2
Jun 17 14:54:49.868: INFO: Pod calico-node-dlffw requesting resource cpu=150m on Node lab1-k8s-node-1
Jun 17 14:54:49.868: INFO: Pod calico-node-fz8bf requesting resource cpu=150m on Node lab1-k8s-node-2
Jun 17 14:54:49.868: INFO: Pod calico-node-rr2wf requesting resource cpu=150m on Node lab1-k8s-node-3
Jun 17 14:54:49.868: INFO: Pod coredns-7646874c97-76qgd requesting resource cpu=100m on Node lab1-k8s-node-3
Jun 17 14:54:49.868: INFO: Pod dns-autoscaler-56c969bdb8-5brv9 requesting resource cpu=20m on Node lab1-k8s-node-1
Jun 17 14:54:49.868: INFO: Pod kube-proxy-b4j5w requesting resource cpu=0m on Node lab1-k8s-node-1
Jun 17 14:54:49.868: INFO: Pod kube-proxy-gdc49 requesting resource cpu=0m on Node lab1-k8s-node-3
Jun 17 14:54:49.868: INFO: Pod kube-proxy-gtfkl requesting resource cpu=0m on Node lab1-k8s-node-2
Jun 17 14:54:49.868: INFO: Pod kubernetes-dashboard-57875dc8c7-mmp6z requesting resource cpu=50m on Node lab1-k8s-node-2
Jun 17 14:54:49.869: INFO: Pod nginx-proxy-lab1-k8s-node-1 requesting resource cpu=25m on Node lab1-k8s-node-1
Jun 17 14:54:49.869: INFO: Pod nginx-proxy-lab1-k8s-node-2 requesting resource cpu=25m on Node lab1-k8s-node-2
Jun 17 14:54:49.869: INFO: Pod nginx-proxy-lab1-k8s-node-3 requesting resource cpu=25m on Node lab1-k8s-node-3
Jun 17 14:54:49.869: INFO: Pod nodelocaldns-4979m requesting resource cpu=100m on Node lab1-k8s-node-2
Jun 17 14:54:49.869: INFO: Pod nodelocaldns-psdz6 requesting resource cpu=100m on Node lab1-k8s-node-3
Jun 17 14:54:49.869: INFO: Pod nodelocaldns-zc5f8 requesting resource cpu=100m on Node lab1-k8s-node-1
Jun 17 14:54:49.869: INFO: Pod tiller-deploy-86c8b7897c-f2sdt requesting resource cpu=0m on Node lab1-k8s-node-1
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db2520c5-910f-11e9-a8b9-dace53c98186.15a90421fdb588ca], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6005/filler-pod-db2520c5-910f-11e9-a8b9-dace53c98186 to lab1-k8s-node-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db2520c5-910f-11e9-a8b9-dace53c98186.15a90422422ce856], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db2520c5-910f-11e9-a8b9-dace53c98186.15a904229decccfa], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db2520c5-910f-11e9-a8b9-dace53c98186.15a90422a12a7ae0], Reason = [Created], Message = [Created container filler-pod-db2520c5-910f-11e9-a8b9-dace53c98186]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db2520c5-910f-11e9-a8b9-dace53c98186.15a90422b3218018], Reason = [Started], Message = [Started container filler-pod-db2520c5-910f-11e9-a8b9-dace53c98186]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db26b848-910f-11e9-a8b9-dace53c98186.15a90421fe5059b2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6005/filler-pod-db26b848-910f-11e9-a8b9-dace53c98186 to lab1-k8s-node-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db26b848-910f-11e9-a8b9-dace53c98186.15a9042242c7e627], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db26b848-910f-11e9-a8b9-dace53c98186.15a90422aeefd5b5], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db26b848-910f-11e9-a8b9-dace53c98186.15a90422b2b7eec4], Reason = [Created], Message = [Created container filler-pod-db26b848-910f-11e9-a8b9-dace53c98186]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db26b848-910f-11e9-a8b9-dace53c98186.15a90422c5a8f77c], Reason = [Started], Message = [Started container filler-pod-db26b848-910f-11e9-a8b9-dace53c98186]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db284b4b-910f-11e9-a8b9-dace53c98186.15a90421fea89561], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6005/filler-pod-db284b4b-910f-11e9-a8b9-dace53c98186 to lab1-k8s-node-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db284b4b-910f-11e9-a8b9-dace53c98186.15a90422416c8110], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db284b4b-910f-11e9-a8b9-dace53c98186.15a90422b2db7927], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db284b4b-910f-11e9-a8b9-dace53c98186.15a90422b6826bae], Reason = [Created], Message = [Created container filler-pod-db284b4b-910f-11e9-a8b9-dace53c98186]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-db284b4b-910f-11e9-a8b9-dace53c98186.15a90422ca315751], Reason = [Started], Message = [Started container filler-pod-db284b4b-910f-11e9-a8b9-dace53c98186]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15a9042365c0aec6], Reason = [FailedScheduling], Message = [0/6 nodes are available: 2 node(s) had taints that the pod didn't tolerate, 4 Insufficient cpu.]
STEP: removing the label node off the node lab1-k8s-node-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node lab1-k8s-node-3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node lab1-k8s-node-1
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:54:57.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6005" for this suite.
Jun 17 14:55:03.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:55:03.168: INFO: namespace sched-pred-6005 deletion completed in 6.15324055s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:13.597 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:55:03.169: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3356
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 17 14:55:03.357: INFO: Waiting up to 5m0s for pod "pod-e32d9c5a-910f-11e9-a8b9-dace53c98186" in namespace "emptydir-3356" to be "success or failure"
Jun 17 14:55:03.363: INFO: Pod "pod-e32d9c5a-910f-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 5.5413ms
Jun 17 14:55:05.368: INFO: Pod "pod-e32d9c5a-910f-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010376146s
Jun 17 14:55:07.373: INFO: Pod "pod-e32d9c5a-910f-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01532678s
Jun 17 14:55:09.377: INFO: Pod "pod-e32d9c5a-910f-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019793579s
STEP: Saw pod success
Jun 17 14:55:09.377: INFO: Pod "pod-e32d9c5a-910f-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:55:09.382: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-e32d9c5a-910f-11e9-a8b9-dace53c98186 container test-container: <nil>
STEP: delete the pod
Jun 17 14:55:09.406: INFO: Waiting for pod pod-e32d9c5a-910f-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:55:09.409: INFO: Pod pod-e32d9c5a-910f-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:55:09.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3356" for this suite.
Jun 17 14:55:15.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:55:15.571: INFO: namespace emptydir-3356 deletion completed in 6.154452417s

• [SLOW TEST:12.401 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:55:15.571: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1753
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-ea8ebd51-910f-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume configMaps
Jun 17 14:55:15.745: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ea9010e4-910f-11e9-a8b9-dace53c98186" in namespace "projected-1753" to be "success or failure"
Jun 17 14:55:15.750: INFO: Pod "pod-projected-configmaps-ea9010e4-910f-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.174945ms
Jun 17 14:55:17.754: INFO: Pod "pod-projected-configmaps-ea9010e4-910f-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008935334s
STEP: Saw pod success
Jun 17 14:55:17.754: INFO: Pod "pod-projected-configmaps-ea9010e4-910f-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:55:17.758: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-projected-configmaps-ea9010e4-910f-11e9-a8b9-dace53c98186 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 17 14:55:17.783: INFO: Waiting for pod pod-projected-configmaps-ea9010e4-910f-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:55:17.786: INFO: Pod pod-projected-configmaps-ea9010e4-910f-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:55:17.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1753" for this suite.
Jun 17 14:55:23.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:55:23.941: INFO: namespace projected-1753 deletion completed in 6.145931959s

• [SLOW TEST:8.371 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:55:23.943: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1312
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 17 14:55:24.118: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ef8d15af-910f-11e9-a8b9-dace53c98186" in namespace "downward-api-1312" to be "success or failure"
Jun 17 14:55:24.123: INFO: Pod "downwardapi-volume-ef8d15af-910f-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 5.094868ms
Jun 17 14:55:26.128: INFO: Pod "downwardapi-volume-ef8d15af-910f-11e9-a8b9-dace53c98186": Phase="Running", Reason="", readiness=true. Elapsed: 2.00977267s
Jun 17 14:55:28.133: INFO: Pod "downwardapi-volume-ef8d15af-910f-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01512147s
STEP: Saw pod success
Jun 17 14:55:28.133: INFO: Pod "downwardapi-volume-ef8d15af-910f-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:55:28.137: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-ef8d15af-910f-11e9-a8b9-dace53c98186 container client-container: <nil>
STEP: delete the pod
Jun 17 14:55:28.164: INFO: Waiting for pod downwardapi-volume-ef8d15af-910f-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:55:28.167: INFO: Pod downwardapi-volume-ef8d15af-910f-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:55:28.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1312" for this suite.
Jun 17 14:55:34.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:55:34.326: INFO: namespace downward-api-1312 deletion completed in 6.149500301s

• [SLOW TEST:10.383 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:55:34.327: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8922
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 17 14:55:34.504: INFO: Waiting up to 5m0s for pod "pod-f5be346f-910f-11e9-a8b9-dace53c98186" in namespace "emptydir-8922" to be "success or failure"
Jun 17 14:55:34.510: INFO: Pod "pod-f5be346f-910f-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 6.340702ms
Jun 17 14:55:36.516: INFO: Pod "pod-f5be346f-910f-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011590292s
Jun 17 14:55:38.521: INFO: Pod "pod-f5be346f-910f-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016775714s
STEP: Saw pod success
Jun 17 14:55:38.521: INFO: Pod "pod-f5be346f-910f-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:55:38.533: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-f5be346f-910f-11e9-a8b9-dace53c98186 container test-container: <nil>
STEP: delete the pod
Jun 17 14:55:38.559: INFO: Waiting for pod pod-f5be346f-910f-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:55:38.563: INFO: Pod pod-f5be346f-910f-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:55:38.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8922" for this suite.
Jun 17 14:55:44.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:55:44.730: INFO: namespace emptydir-8922 deletion completed in 6.160272246s

• [SLOW TEST:10.404 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:55:44.731: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8292
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 17 14:55:44.898: INFO: Waiting up to 5m0s for pod "pod-fbf06f39-910f-11e9-a8b9-dace53c98186" in namespace "emptydir-8292" to be "success or failure"
Jun 17 14:55:44.902: INFO: Pod "pod-fbf06f39-910f-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 3.978219ms
Jun 17 14:55:46.907: INFO: Pod "pod-fbf06f39-910f-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00852532s
STEP: Saw pod success
Jun 17 14:55:46.907: INFO: Pod "pod-fbf06f39-910f-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:55:46.911: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-fbf06f39-910f-11e9-a8b9-dace53c98186 container test-container: <nil>
STEP: delete the pod
Jun 17 14:55:46.933: INFO: Waiting for pod pod-fbf06f39-910f-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:55:46.936: INFO: Pod pod-fbf06f39-910f-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:55:46.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8292" for this suite.
Jun 17 14:55:52.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:55:53.114: INFO: namespace emptydir-8292 deletion completed in 6.166636537s

• [SLOW TEST:8.383 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:55:53.122: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3245
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 17 14:55:55.832: INFO: Successfully updated pod "pod-update-activedeadlineseconds-00f23e57-9110-11e9-a8b9-dace53c98186"
Jun 17 14:55:55.833: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-00f23e57-9110-11e9-a8b9-dace53c98186" in namespace "pods-3245" to be "terminated due to deadline exceeded"
Jun 17 14:55:55.836: INFO: Pod "pod-update-activedeadlineseconds-00f23e57-9110-11e9-a8b9-dace53c98186": Phase="Running", Reason="", readiness=true. Elapsed: 3.40006ms
Jun 17 14:55:57.843: INFO: Pod "pod-update-activedeadlineseconds-00f23e57-9110-11e9-a8b9-dace53c98186": Phase="Running", Reason="", readiness=true. Elapsed: 2.010398752s
Jun 17 14:55:59.848: INFO: Pod "pod-update-activedeadlineseconds-00f23e57-9110-11e9-a8b9-dace53c98186": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.014987925s
Jun 17 14:55:59.848: INFO: Pod "pod-update-activedeadlineseconds-00f23e57-9110-11e9-a8b9-dace53c98186" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:55:59.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3245" for this suite.
Jun 17 14:56:05.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:56:06.015: INFO: namespace pods-3245 deletion completed in 6.15983454s

• [SLOW TEST:12.893 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:56:06.015: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9261
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0617 14:56:07.255897      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 17 14:56:07.256: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:56:07.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9261" for this suite.
Jun 17 14:56:13.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:56:13.424: INFO: namespace gc-9261 deletion completed in 6.161083153s

• [SLOW TEST:7.409 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:56:13.425: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7455
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-0d0b8eb3-9110-11e9-a8b9-dace53c98186
STEP: Creating configMap with name cm-test-opt-upd-0d0b8f47-9110-11e9-a8b9-dace53c98186
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-0d0b8eb3-9110-11e9-a8b9-dace53c98186
STEP: Updating configmap cm-test-opt-upd-0d0b8f47-9110-11e9-a8b9-dace53c98186
STEP: Creating configMap with name cm-test-opt-create-0d0b8f5a-9110-11e9-a8b9-dace53c98186
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:56:17.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7455" for this suite.
Jun 17 14:56:39.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:56:39.920: INFO: namespace projected-7455 deletion completed in 22.185242032s

• [SLOW TEST:26.495 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:56:39.920: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8698
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-1cdcd0e0-9110-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume configMaps
Jun 17 14:56:40.154: INFO: Waiting up to 5m0s for pod "pod-configmaps-1cdea2c4-9110-11e9-a8b9-dace53c98186" in namespace "configmap-8698" to be "success or failure"
Jun 17 14:56:40.161: INFO: Pod "pod-configmaps-1cdea2c4-9110-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 6.423132ms
Jun 17 14:56:42.166: INFO: Pod "pod-configmaps-1cdea2c4-9110-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011779298s
STEP: Saw pod success
Jun 17 14:56:42.166: INFO: Pod "pod-configmaps-1cdea2c4-9110-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:56:42.170: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-configmaps-1cdea2c4-9110-11e9-a8b9-dace53c98186 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 17 14:56:42.194: INFO: Waiting for pod pod-configmaps-1cdea2c4-9110-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:56:42.198: INFO: Pod pod-configmaps-1cdea2c4-9110-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:56:42.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8698" for this suite.
Jun 17 14:56:48.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:56:48.356: INFO: namespace configmap-8698 deletion completed in 6.150174783s

• [SLOW TEST:8.436 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:56:48.357: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 14:56:48.518: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:56:52.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8773" for this suite.
Jun 17 14:57:34.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:57:34.837: INFO: namespace pods-8773 deletion completed in 42.156474767s

• [SLOW TEST:46.481 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:57:34.842: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8675
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 17 14:57:35.015: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d931c00-9110-11e9-a8b9-dace53c98186" in namespace "downward-api-8675" to be "success or failure"
Jun 17 14:57:35.020: INFO: Pod "downwardapi-volume-3d931c00-9110-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.358086ms
Jun 17 14:57:37.024: INFO: Pod "downwardapi-volume-3d931c00-9110-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008893029s
STEP: Saw pod success
Jun 17 14:57:37.024: INFO: Pod "downwardapi-volume-3d931c00-9110-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:57:37.029: INFO: Trying to get logs from node lab1-k8s-node-2 pod downwardapi-volume-3d931c00-9110-11e9-a8b9-dace53c98186 container client-container: <nil>
STEP: delete the pod
Jun 17 14:57:37.056: INFO: Waiting for pod downwardapi-volume-3d931c00-9110-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:57:37.061: INFO: Pod downwardapi-volume-3d931c00-9110-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:57:37.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8675" for this suite.
Jun 17 14:57:43.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:57:43.229: INFO: namespace downward-api-8675 deletion completed in 6.160443516s

• [SLOW TEST:8.387 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:57:43.233: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2031
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 17 14:57:43.404: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4292d842-9110-11e9-a8b9-dace53c98186" in namespace "projected-2031" to be "success or failure"
Jun 17 14:57:43.408: INFO: Pod "downwardapi-volume-4292d842-9110-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.342155ms
Jun 17 14:57:45.414: INFO: Pod "downwardapi-volume-4292d842-9110-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010287506s
STEP: Saw pod success
Jun 17 14:57:45.414: INFO: Pod "downwardapi-volume-4292d842-9110-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:57:45.418: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-4292d842-9110-11e9-a8b9-dace53c98186 container client-container: <nil>
STEP: delete the pod
Jun 17 14:57:45.449: INFO: Waiting for pod downwardapi-volume-4292d842-9110-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:57:45.455: INFO: Pod downwardapi-volume-4292d842-9110-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:57:45.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2031" for this suite.
Jun 17 14:57:51.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:57:51.620: INFO: namespace projected-2031 deletion completed in 6.158063237s

• [SLOW TEST:8.388 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:57:51.623: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4178
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 17 14:57:51.829: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:51.829: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:51.829: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:51.833: INFO: Number of nodes with available pods: 0
Jun 17 14:57:51.833: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 14:57:52.840: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:52.840: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:52.840: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:52.845: INFO: Number of nodes with available pods: 0
Jun 17 14:57:52.845: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 14:57:53.841: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:53.841: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:53.841: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:53.845: INFO: Number of nodes with available pods: 2
Jun 17 14:57:53.845: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Jun 17 14:57:54.840: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:54.840: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:54.840: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:54.845: INFO: Number of nodes with available pods: 3
Jun 17 14:57:54.845: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 17 14:57:54.868: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:54.868: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:54.868: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:54.872: INFO: Number of nodes with available pods: 2
Jun 17 14:57:54.872: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Jun 17 14:57:55.879: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:55.879: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:55.879: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:55.885: INFO: Number of nodes with available pods: 2
Jun 17 14:57:55.885: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Jun 17 14:57:56.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:56.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:56.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:56.885: INFO: Number of nodes with available pods: 2
Jun 17 14:57:56.885: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Jun 17 14:57:57.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:57.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:57.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:57.885: INFO: Number of nodes with available pods: 2
Jun 17 14:57:57.885: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Jun 17 14:57:58.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:58.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:58.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:58.888: INFO: Number of nodes with available pods: 2
Jun 17 14:57:58.888: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Jun 17 14:57:59.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:59.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:59.881: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:57:59.885: INFO: Number of nodes with available pods: 2
Jun 17 14:57:59.885: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Jun 17 14:58:00.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:58:00.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:58:00.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:58:00.884: INFO: Number of nodes with available pods: 2
Jun 17 14:58:00.884: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Jun 17 14:58:01.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:58:01.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:58:01.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:58:01.885: INFO: Number of nodes with available pods: 2
Jun 17 14:58:01.885: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Jun 17 14:58:02.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:58:02.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:58:02.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:58:02.884: INFO: Number of nodes with available pods: 2
Jun 17 14:58:02.884: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Jun 17 14:58:03.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:58:03.881: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:58:03.881: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:58:03.887: INFO: Number of nodes with available pods: 2
Jun 17 14:58:03.887: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Jun 17 14:58:04.879: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:58:04.879: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:58:04.879: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:58:04.884: INFO: Number of nodes with available pods: 2
Jun 17 14:58:04.884: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Jun 17 14:58:05.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:58:05.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:58:05.880: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 17 14:58:05.885: INFO: Number of nodes with available pods: 3
Jun 17 14:58:05.885: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4178, will wait for the garbage collector to delete the pods
Jun 17 14:58:05.960: INFO: Deleting DaemonSet.extensions daemon-set took: 12.926452ms
Jun 17 14:58:06.261: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.308106ms
Jun 17 14:58:13.065: INFO: Number of nodes with available pods: 0
Jun 17 14:58:13.065: INFO: Number of running nodes: 0, number of available pods: 0
Jun 17 14:58:13.069: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4178/daemonsets","resourceVersion":"16658"},"items":null}

Jun 17 14:58:13.073: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4178/pods","resourceVersion":"16658"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:58:13.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4178" for this suite.
Jun 17 14:58:19.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:58:19.258: INFO: namespace daemonsets-4178 deletion completed in 6.155737978s

• [SLOW TEST:27.635 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:58:19.259: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3385
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-lm52
STEP: Creating a pod to test atomic-volume-subpath
Jun 17 14:58:19.446: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-lm52" in namespace "subpath-3385" to be "success or failure"
Jun 17 14:58:19.449: INFO: Pod "pod-subpath-test-projected-lm52": Phase="Pending", Reason="", readiness=false. Elapsed: 3.526802ms
Jun 17 14:58:21.454: INFO: Pod "pod-subpath-test-projected-lm52": Phase="Running", Reason="", readiness=true. Elapsed: 2.00831828s
Jun 17 14:58:23.459: INFO: Pod "pod-subpath-test-projected-lm52": Phase="Running", Reason="", readiness=true. Elapsed: 4.013141213s
Jun 17 14:58:25.464: INFO: Pod "pod-subpath-test-projected-lm52": Phase="Running", Reason="", readiness=true. Elapsed: 6.01827408s
Jun 17 14:58:27.469: INFO: Pod "pod-subpath-test-projected-lm52": Phase="Running", Reason="", readiness=true. Elapsed: 8.023497417s
Jun 17 14:58:29.474: INFO: Pod "pod-subpath-test-projected-lm52": Phase="Running", Reason="", readiness=true. Elapsed: 10.028167415s
Jun 17 14:58:31.479: INFO: Pod "pod-subpath-test-projected-lm52": Phase="Running", Reason="", readiness=true. Elapsed: 12.033372143s
Jun 17 14:58:33.484: INFO: Pod "pod-subpath-test-projected-lm52": Phase="Running", Reason="", readiness=true. Elapsed: 14.037957767s
Jun 17 14:58:35.489: INFO: Pod "pod-subpath-test-projected-lm52": Phase="Running", Reason="", readiness=true. Elapsed: 16.04279181s
Jun 17 14:58:37.494: INFO: Pod "pod-subpath-test-projected-lm52": Phase="Running", Reason="", readiness=true. Elapsed: 18.047885632s
Jun 17 14:58:39.498: INFO: Pod "pod-subpath-test-projected-lm52": Phase="Running", Reason="", readiness=true. Elapsed: 20.052376256s
Jun 17 14:58:41.503: INFO: Pod "pod-subpath-test-projected-lm52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.05746244s
STEP: Saw pod success
Jun 17 14:58:41.503: INFO: Pod "pod-subpath-test-projected-lm52" satisfied condition "success or failure"
Jun 17 14:58:41.508: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-subpath-test-projected-lm52 container test-container-subpath-projected-lm52: <nil>
STEP: delete the pod
Jun 17 14:58:41.539: INFO: Waiting for pod pod-subpath-test-projected-lm52 to disappear
Jun 17 14:58:41.542: INFO: Pod pod-subpath-test-projected-lm52 no longer exists
STEP: Deleting pod pod-subpath-test-projected-lm52
Jun 17 14:58:41.542: INFO: Deleting pod "pod-subpath-test-projected-lm52" in namespace "subpath-3385"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:58:41.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3385" for this suite.
Jun 17 14:58:47.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:58:47.710: INFO: namespace subpath-3385 deletion completed in 6.156677562s

• [SLOW TEST:28.451 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:58:47.710: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3970
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-69018f16-9110-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume secrets
Jun 17 14:58:47.888: INFO: Waiting up to 5m0s for pod "pod-secrets-690284dc-9110-11e9-a8b9-dace53c98186" in namespace "secrets-3970" to be "success or failure"
Jun 17 14:58:47.892: INFO: Pod "pod-secrets-690284dc-9110-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.276194ms
Jun 17 14:58:49.897: INFO: Pod "pod-secrets-690284dc-9110-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008960292s
STEP: Saw pod success
Jun 17 14:58:49.897: INFO: Pod "pod-secrets-690284dc-9110-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:58:49.901: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-secrets-690284dc-9110-11e9-a8b9-dace53c98186 container secret-volume-test: <nil>
STEP: delete the pod
Jun 17 14:58:49.927: INFO: Waiting for pod pod-secrets-690284dc-9110-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:58:49.930: INFO: Pod pod-secrets-690284dc-9110-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:58:49.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3970" for this suite.
Jun 17 14:58:55.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:58:56.097: INFO: namespace secrets-3970 deletion completed in 6.159796327s

• [SLOW TEST:8.387 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:58:56.098: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3149
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-3149/configmap-test-6e008775-9110-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume configMaps
Jun 17 14:58:56.270: INFO: Waiting up to 5m0s for pod "pod-configmaps-6e018dd8-9110-11e9-a8b9-dace53c98186" in namespace "configmap-3149" to be "success or failure"
Jun 17 14:58:56.276: INFO: Pod "pod-configmaps-6e018dd8-9110-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 5.457212ms
Jun 17 14:58:58.281: INFO: Pod "pod-configmaps-6e018dd8-9110-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01019593s
STEP: Saw pod success
Jun 17 14:58:58.281: INFO: Pod "pod-configmaps-6e018dd8-9110-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 14:58:58.286: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-configmaps-6e018dd8-9110-11e9-a8b9-dace53c98186 container env-test: <nil>
STEP: delete the pod
Jun 17 14:58:58.315: INFO: Waiting for pod pod-configmaps-6e018dd8-9110-11e9-a8b9-dace53c98186 to disappear
Jun 17 14:58:58.318: INFO: Pod pod-configmaps-6e018dd8-9110-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:58:58.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3149" for this suite.
Jun 17 14:59:04.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:59:04.491: INFO: namespace configmap-3149 deletion completed in 6.166011153s

• [SLOW TEST:8.393 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:59:04.491: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3684
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 14:59:04.659: INFO: (0) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.073574ms)
Jun 17 14:59:04.666: INFO: (1) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.367307ms)
Jun 17 14:59:04.671: INFO: (2) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.537081ms)
Jun 17 14:59:04.679: INFO: (3) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.46348ms)
Jun 17 14:59:04.685: INFO: (4) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.048156ms)
Jun 17 14:59:04.690: INFO: (5) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.548675ms)
Jun 17 14:59:04.697: INFO: (6) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.077683ms)
Jun 17 14:59:04.702: INFO: (7) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.732836ms)
Jun 17 14:59:04.708: INFO: (8) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.770027ms)
Jun 17 14:59:04.715: INFO: (9) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.951222ms)
Jun 17 14:59:04.721: INFO: (10) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.98468ms)
Jun 17 14:59:04.729: INFO: (11) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.041335ms)
Jun 17 14:59:04.735: INFO: (12) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.180601ms)
Jun 17 14:59:04.740: INFO: (13) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.392494ms)
Jun 17 14:59:04.746: INFO: (14) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.521134ms)
Jun 17 14:59:04.751: INFO: (15) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.339426ms)
Jun 17 14:59:04.756: INFO: (16) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.001858ms)
Jun 17 14:59:04.761: INFO: (17) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.120249ms)
Jun 17 14:59:04.766: INFO: (18) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.346206ms)
Jun 17 14:59:04.772: INFO: (19) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.536444ms)
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:59:04.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3684" for this suite.
Jun 17 14:59:10.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:59:10.926: INFO: namespace proxy-3684 deletion completed in 6.147908456s

• [SLOW TEST:6.435 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:59:10.927: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5899
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 17 14:59:11.087: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jun 17 14:59:18.136: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 14:59:18.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5899" for this suite.
Jun 17 14:59:24.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 14:59:24.314: INFO: namespace pods-5899 deletion completed in 6.163428084s

• [SLOW TEST:13.387 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 14:59:24.315: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6604
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6604
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6604
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6604
Jun 17 14:59:24.504: INFO: Found 0 stateful pods, waiting for 1
Jun 17 14:59:34.509: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 17 14:59:34.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-6604 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 17 14:59:34.752: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 17 14:59:34.752: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 17 14:59:34.752: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 17 14:59:34.758: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 17 14:59:44.763: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 17 14:59:44.763: INFO: Waiting for statefulset status.replicas updated to 0
Jun 17 14:59:44.787: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999618s
Jun 17 14:59:45.792: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992954294s
Jun 17 14:59:46.796: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988136889s
Jun 17 14:59:47.801: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983393244s
Jun 17 14:59:48.806: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.97856468s
Jun 17 14:59:49.811: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.973531991s
Jun 17 14:59:50.815: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.969043332s
Jun 17 14:59:51.820: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.964513518s
Jun 17 14:59:52.825: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.959551703s
Jun 17 14:59:53.830: INFO: Verifying statefulset ss doesn't scale past 1 for another 954.656279ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6604
Jun 17 14:59:54.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-6604 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 14:59:55.079: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 17 14:59:55.079: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 17 14:59:55.079: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 17 14:59:55.084: INFO: Found 1 stateful pods, waiting for 3
Jun 17 15:00:05.090: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 17 15:00:05.090: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 17 15:00:05.090: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 17 15:00:05.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-6604 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 17 15:00:05.319: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 17 15:00:05.319: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 17 15:00:05.319: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 17 15:00:05.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-6604 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 17 15:00:05.619: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 17 15:00:05.619: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 17 15:00:05.619: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 17 15:00:05.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-6604 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 17 15:00:05.911: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 17 15:00:05.911: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 17 15:00:05.911: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 17 15:00:05.911: INFO: Waiting for statefulset status.replicas updated to 0
Jun 17 15:00:05.916: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 17 15:00:15.924: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 17 15:00:15.924: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 17 15:00:15.924: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 17 15:00:15.938: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999632s
Jun 17 15:00:16.943: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995765183s
Jun 17 15:00:17.948: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990705826s
Jun 17 15:00:18.953: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985882948s
Jun 17 15:00:19.958: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980901567s
Jun 17 15:00:20.963: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975988579s
Jun 17 15:00:21.968: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970418556s
Jun 17 15:00:22.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.96554053s
Jun 17 15:00:23.978: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.960541728s
Jun 17 15:00:24.983: INFO: Verifying statefulset ss doesn't scale past 3 for another 955.511185ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6604
Jun 17 15:00:25.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-6604 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:00:26.213: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 17 15:00:26.213: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 17 15:00:26.213: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 17 15:00:26.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-6604 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:00:26.478: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 17 15:00:26.478: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 17 15:00:26.478: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 17 15:00:26.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-6604 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:00:26.767: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 17 15:00:26.767: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 17 15:00:26.767: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 17 15:00:26.767: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jun 17 15:00:56.788: INFO: Deleting all statefulset in ns statefulset-6604
Jun 17 15:00:56.792: INFO: Scaling statefulset ss to 0
Jun 17 15:00:56.804: INFO: Waiting for statefulset status.replicas updated to 0
Jun 17 15:00:56.808: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:00:56.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6604" for this suite.
Jun 17 15:01:02.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:01:02.999: INFO: namespace statefulset-6604 deletion completed in 6.162826623s

• [SLOW TEST:98.684 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:01:03.001: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5435
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 15:01:21.185: INFO: Container started at 2019-06-17 15:01:05 +0000 UTC, pod became ready at 2019-06-17 15:01:20 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:01:21.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5435" for this suite.
Jun 17 15:01:43.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:01:43.348: INFO: namespace container-probe-5435 deletion completed in 22.153470608s

• [SLOW TEST:40.347 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:01:43.350: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2024
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-d1b22653-9110-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume configMaps
Jun 17 15:01:43.531: INFO: Waiting up to 5m0s for pod "pod-configmaps-d1b33814-9110-11e9-a8b9-dace53c98186" in namespace "configmap-2024" to be "success or failure"
Jun 17 15:01:43.535: INFO: Pod "pod-configmaps-d1b33814-9110-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.474849ms
Jun 17 15:01:45.540: INFO: Pod "pod-configmaps-d1b33814-9110-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008890159s
Jun 17 15:01:47.545: INFO: Pod "pod-configmaps-d1b33814-9110-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013957084s
STEP: Saw pod success
Jun 17 15:01:47.545: INFO: Pod "pod-configmaps-d1b33814-9110-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:01:47.548: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-configmaps-d1b33814-9110-11e9-a8b9-dace53c98186 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 17 15:01:47.577: INFO: Waiting for pod pod-configmaps-d1b33814-9110-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:01:47.580: INFO: Pod pod-configmaps-d1b33814-9110-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:01:47.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2024" for this suite.
Jun 17 15:01:53.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:01:53.755: INFO: namespace configmap-2024 deletion completed in 6.16494599s

• [SLOW TEST:10.406 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:01:53.757: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2538
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-d7e629ea-9110-11e9-a8b9-dace53c98186
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:01:53.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2538" for this suite.
Jun 17 15:01:59.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:02:00.083: INFO: namespace configmap-2538 deletion completed in 6.152988281s

• [SLOW TEST:6.326 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:02:00.085: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7247
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 17 15:02:00.251: INFO: Waiting up to 5m0s for pod "pod-dbaa4d91-9110-11e9-a8b9-dace53c98186" in namespace "emptydir-7247" to be "success or failure"
Jun 17 15:02:00.256: INFO: Pod "pod-dbaa4d91-9110-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.251348ms
Jun 17 15:02:02.261: INFO: Pod "pod-dbaa4d91-9110-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009394746s
Jun 17 15:02:04.265: INFO: Pod "pod-dbaa4d91-9110-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013817759s
STEP: Saw pod success
Jun 17 15:02:04.265: INFO: Pod "pod-dbaa4d91-9110-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:02:04.271: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-dbaa4d91-9110-11e9-a8b9-dace53c98186 container test-container: <nil>
STEP: delete the pod
Jun 17 15:02:04.301: INFO: Waiting for pod pod-dbaa4d91-9110-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:02:04.304: INFO: Pod pod-dbaa4d91-9110-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:02:04.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7247" for this suite.
Jun 17 15:02:10.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:02:10.485: INFO: namespace emptydir-7247 deletion completed in 6.173919257s

• [SLOW TEST:10.400 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:02:10.485: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1724
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 17 15:02:10.656: INFO: Waiting up to 5m0s for pod "pod-e1de8907-9110-11e9-a8b9-dace53c98186" in namespace "emptydir-1724" to be "success or failure"
Jun 17 15:02:10.663: INFO: Pod "pod-e1de8907-9110-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 6.258117ms
Jun 17 15:02:12.667: INFO: Pod "pod-e1de8907-9110-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010985669s
Jun 17 15:02:14.672: INFO: Pod "pod-e1de8907-9110-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015597244s
Jun 17 15:02:16.676: INFO: Pod "pod-e1de8907-9110-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020131099s
STEP: Saw pod success
Jun 17 15:02:16.677: INFO: Pod "pod-e1de8907-9110-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:02:16.680: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-e1de8907-9110-11e9-a8b9-dace53c98186 container test-container: <nil>
STEP: delete the pod
Jun 17 15:02:16.712: INFO: Waiting for pod pod-e1de8907-9110-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:02:16.715: INFO: Pod pod-e1de8907-9110-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:02:16.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1724" for this suite.
Jun 17 15:02:22.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:02:22.888: INFO: namespace emptydir-1724 deletion completed in 6.16102327s

• [SLOW TEST:12.403 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:02:22.890: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7051
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-7051
Jun 17 15:02:27.069: INFO: Started pod liveness-exec in namespace container-probe-7051
STEP: checking the pod's current state and verifying that restartCount is present
Jun 17 15:02:27.073: INFO: Initial restart count of pod liveness-exec is 0
Jun 17 15:03:17.230: INFO: Restart count of pod container-probe-7051/liveness-exec is now 1 (50.156895613s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:03:17.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7051" for this suite.
Jun 17 15:03:23.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:03:23.407: INFO: namespace container-probe-7051 deletion completed in 6.156988681s

• [SLOW TEST:60.517 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:03:23.408: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9396
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jun 17 15:03:28.127: INFO: Successfully updated pod "labelsupdate0d55dfdc-9111-11e9-a8b9-dace53c98186"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:03:30.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9396" for this suite.
Jun 17 15:03:52.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:03:52.320: INFO: namespace projected-9396 deletion completed in 22.159015178s

• [SLOW TEST:28.911 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:03:52.320: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5121
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 17 15:03:52.499: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5121,SelfLink:/api/v1/namespaces/watch-5121/configmaps/e2e-watch-test-watch-closed,UID:1e91455f-9111-11e9-bdd5-fa163e38c6bd,ResourceVersion:18055,Generation:0,CreationTimestamp:2019-06-17 15:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 17 15:03:52.500: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5121,SelfLink:/api/v1/namespaces/watch-5121/configmaps/e2e-watch-test-watch-closed,UID:1e91455f-9111-11e9-bdd5-fa163e38c6bd,ResourceVersion:18056,Generation:0,CreationTimestamp:2019-06-17 15:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 17 15:03:52.529: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5121,SelfLink:/api/v1/namespaces/watch-5121/configmaps/e2e-watch-test-watch-closed,UID:1e91455f-9111-11e9-bdd5-fa163e38c6bd,ResourceVersion:18057,Generation:0,CreationTimestamp:2019-06-17 15:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 17 15:03:52.530: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5121,SelfLink:/api/v1/namespaces/watch-5121/configmaps/e2e-watch-test-watch-closed,UID:1e91455f-9111-11e9-bdd5-fa163e38c6bd,ResourceVersion:18058,Generation:0,CreationTimestamp:2019-06-17 15:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:03:52.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5121" for this suite.
Jun 17 15:03:58.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:03:58.708: INFO: namespace watch-5121 deletion completed in 6.164742273s

• [SLOW TEST:6.388 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:03:58.709: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5578
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jun 17 15:03:58.871: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:04:03.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5578" for this suite.
Jun 17 15:04:25.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:04:25.602: INFO: namespace init-container-5578 deletion completed in 22.144871617s

• [SLOW TEST:26.893 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:04:25.604: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Jun 17 15:04:25.777: INFO: Waiting up to 5m0s for pod "client-containers-3268221b-9111-11e9-a8b9-dace53c98186" in namespace "containers-9974" to be "success or failure"
Jun 17 15:04:25.782: INFO: Pod "client-containers-3268221b-9111-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.184641ms
Jun 17 15:04:27.786: INFO: Pod "client-containers-3268221b-9111-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008913014s
STEP: Saw pod success
Jun 17 15:04:27.786: INFO: Pod "client-containers-3268221b-9111-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:04:27.790: INFO: Trying to get logs from node lab1-k8s-node-2 pod client-containers-3268221b-9111-11e9-a8b9-dace53c98186 container test-container: <nil>
STEP: delete the pod
Jun 17 15:04:27.813: INFO: Waiting for pod client-containers-3268221b-9111-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:04:27.817: INFO: Pod client-containers-3268221b-9111-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:04:27.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9974" for this suite.
Jun 17 15:04:33.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:04:33.978: INFO: namespace containers-9974 deletion completed in 6.152722703s

• [SLOW TEST:8.374 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:04:33.979: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2969
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jun 17 15:04:36.684: INFO: Successfully updated pod "annotationupdate376505c6-9111-11e9-a8b9-dace53c98186"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:04:38.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2969" for this suite.
Jun 17 15:05:00.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:05:00.899: INFO: namespace projected-2969 deletion completed in 22.172216106s

• [SLOW TEST:26.921 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:05:00.899: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9753
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:06:01.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9753" for this suite.
Jun 17 15:06:23.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:06:23.246: INFO: namespace container-probe-9753 deletion completed in 22.158549283s

• [SLOW TEST:82.347 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:06:23.247: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7490
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7490.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7490.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7490.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7490.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7490.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7490.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7490.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7490.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7490.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7490.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7490.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7490.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7490.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 120.59.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.59.120_udp@PTR;check="$$(dig +tcp +noall +answer +search 120.59.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.59.120_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7490.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7490.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7490.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7490.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7490.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7490.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7490.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7490.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7490.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7490.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7490.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7490.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7490.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 120.59.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.59.120_udp@PTR;check="$$(dig +tcp +noall +answer +search 120.59.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.59.120_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 17 15:06:35.476: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7490.svc.cluster.local from pod dns-7490/dns-test-788b3ba7-9111-11e9-a8b9-dace53c98186: the server could not find the requested resource (get pods dns-test-788b3ba7-9111-11e9-a8b9-dace53c98186)
Jun 17 15:06:35.480: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7490.svc.cluster.local from pod dns-7490/dns-test-788b3ba7-9111-11e9-a8b9-dace53c98186: the server could not find the requested resource (get pods dns-test-788b3ba7-9111-11e9-a8b9-dace53c98186)
Jun 17 15:06:35.485: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7490.svc.cluster.local from pod dns-7490/dns-test-788b3ba7-9111-11e9-a8b9-dace53c98186: the server could not find the requested resource (get pods dns-test-788b3ba7-9111-11e9-a8b9-dace53c98186)
Jun 17 15:06:35.516: INFO: Unable to read jessie_udp@dns-test-service.dns-7490.svc.cluster.local from pod dns-7490/dns-test-788b3ba7-9111-11e9-a8b9-dace53c98186: the server could not find the requested resource (get pods dns-test-788b3ba7-9111-11e9-a8b9-dace53c98186)
Jun 17 15:06:35.520: INFO: Unable to read jessie_tcp@dns-test-service.dns-7490.svc.cluster.local from pod dns-7490/dns-test-788b3ba7-9111-11e9-a8b9-dace53c98186: the server could not find the requested resource (get pods dns-test-788b3ba7-9111-11e9-a8b9-dace53c98186)
Jun 17 15:06:35.525: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7490.svc.cluster.local from pod dns-7490/dns-test-788b3ba7-9111-11e9-a8b9-dace53c98186: the server could not find the requested resource (get pods dns-test-788b3ba7-9111-11e9-a8b9-dace53c98186)
Jun 17 15:06:35.530: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7490.svc.cluster.local from pod dns-7490/dns-test-788b3ba7-9111-11e9-a8b9-dace53c98186: the server could not find the requested resource (get pods dns-test-788b3ba7-9111-11e9-a8b9-dace53c98186)
Jun 17 15:06:35.559: INFO: Lookups using dns-7490/dns-test-788b3ba7-9111-11e9-a8b9-dace53c98186 failed for: [wheezy_tcp@dns-test-service.dns-7490.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7490.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7490.svc.cluster.local jessie_udp@dns-test-service.dns-7490.svc.cluster.local jessie_tcp@dns-test-service.dns-7490.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7490.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7490.svc.cluster.local]

Jun 17 15:06:40.656: INFO: DNS probes using dns-7490/dns-test-788b3ba7-9111-11e9-a8b9-dace53c98186 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:06:40.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7490" for this suite.
Jun 17 15:06:46.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:06:46.907: INFO: namespace dns-7490 deletion completed in 6.157239101s

• [SLOW TEST:23.660 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:06:46.907: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-534
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 17 15:06:47.082: INFO: Waiting up to 5m0s for pod "pod-86a17371-9111-11e9-a8b9-dace53c98186" in namespace "emptydir-534" to be "success or failure"
Jun 17 15:06:47.090: INFO: Pod "pod-86a17371-9111-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 8.312696ms
Jun 17 15:06:49.095: INFO: Pod "pod-86a17371-9111-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013408766s
STEP: Saw pod success
Jun 17 15:06:49.095: INFO: Pod "pod-86a17371-9111-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:06:49.099: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-86a17371-9111-11e9-a8b9-dace53c98186 container test-container: <nil>
STEP: delete the pod
Jun 17 15:06:49.122: INFO: Waiting for pod pod-86a17371-9111-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:06:49.126: INFO: Pod pod-86a17371-9111-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:06:49.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-534" for this suite.
Jun 17 15:06:55.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:06:55.302: INFO: namespace emptydir-534 deletion completed in 6.169753983s

• [SLOW TEST:8.395 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:06:55.303: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7182
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jun 17 15:06:55.458: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 17 15:06:55.470: INFO: Waiting for terminating namespaces to be deleted...
Jun 17 15:06:55.476: INFO: 
Logging pods the kubelet thinks is on node lab1-k8s-node-1 before test
Jun 17 15:06:55.484: INFO: nginx-proxy-lab1-k8s-node-1 from kube-system started at <nil> (0 container statuses recorded)
Jun 17 15:06:55.484: INFO: kube-proxy-b4j5w from kube-system started at 2019-06-17 13:30:32 +0000 UTC (1 container statuses recorded)
Jun 17 15:06:55.484: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 17 15:06:55.484: INFO: calico-node-dlffw from kube-system started at 2019-06-17 13:29:45 +0000 UTC (1 container statuses recorded)
Jun 17 15:06:55.484: INFO: 	Container calico-node ready: true, restart count 0
Jun 17 15:06:55.484: INFO: tiller-deploy-86c8b7897c-f2sdt from kube-system started at 2019-06-17 13:30:50 +0000 UTC (1 container statuses recorded)
Jun 17 15:06:55.484: INFO: 	Container tiller ready: true, restart count 0
Jun 17 15:06:55.484: INFO: dns-autoscaler-56c969bdb8-5brv9 from kube-system started at 2019-06-17 13:30:26 +0000 UTC (1 container statuses recorded)
Jun 17 15:06:55.484: INFO: 	Container autoscaler ready: true, restart count 0
Jun 17 15:06:55.484: INFO: nodelocaldns-zc5f8 from kube-system started at 2019-06-17 13:30:28 +0000 UTC (1 container statuses recorded)
Jun 17 15:06:55.485: INFO: 	Container node-cache ready: true, restart count 0
Jun 17 15:06:55.485: INFO: sonobuoy-systemd-logs-daemon-set-b6e1ac2b5ae94c02-2mwvd from heptio-sonobuoy started at 2019-06-17 14:35:15 +0000 UTC (2 container statuses recorded)
Jun 17 15:06:55.485: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 17 15:06:55.485: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 17 15:06:55.485: INFO: 
Logging pods the kubelet thinks is on node lab1-k8s-node-2 before test
Jun 17 15:06:55.495: INFO: kube-proxy-gtfkl from kube-system started at 2019-06-17 13:30:03 +0000 UTC (1 container statuses recorded)
Jun 17 15:06:55.495: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 17 15:06:55.495: INFO: calico-node-fz8bf from kube-system started at 2019-06-17 13:29:45 +0000 UTC (1 container statuses recorded)
Jun 17 15:06:55.495: INFO: 	Container calico-node ready: true, restart count 0
Jun 17 15:06:55.495: INFO: kubernetes-dashboard-57875dc8c7-mmp6z from kube-system started at 2019-06-17 13:30:30 +0000 UTC (1 container statuses recorded)
Jun 17 15:06:55.495: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun 17 15:06:55.495: INFO: sonobuoy-systemd-logs-daemon-set-b6e1ac2b5ae94c02-ngs2t from heptio-sonobuoy started at 2019-06-17 14:35:15 +0000 UTC (2 container statuses recorded)
Jun 17 15:06:55.495: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 17 15:06:55.495: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 17 15:06:55.495: INFO: nginx-proxy-lab1-k8s-node-2 from kube-system started at <nil> (0 container statuses recorded)
Jun 17 15:06:55.495: INFO: calico-kube-controllers-57c89fc87d-sk646 from kube-system started at 2019-06-17 13:29:55 +0000 UTC (1 container statuses recorded)
Jun 17 15:06:55.495: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 17 15:06:55.495: INFO: nodelocaldns-4979m from kube-system started at 2019-06-17 13:30:28 +0000 UTC (1 container statuses recorded)
Jun 17 15:06:55.495: INFO: 	Container node-cache ready: true, restart count 0
Jun 17 15:06:55.495: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-17 14:35:09 +0000 UTC (1 container statuses recorded)
Jun 17 15:06:55.495: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 17 15:06:55.495: INFO: 
Logging pods the kubelet thinks is on node lab1-k8s-node-3 before test
Jun 17 15:06:55.512: INFO: kube-proxy-gdc49 from kube-system started at 2019-06-17 13:30:13 +0000 UTC (1 container statuses recorded)
Jun 17 15:06:55.512: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 17 15:06:55.512: INFO: nodelocaldns-psdz6 from kube-system started at 2019-06-17 13:30:28 +0000 UTC (1 container statuses recorded)
Jun 17 15:06:55.513: INFO: 	Container node-cache ready: true, restart count 0
Jun 17 15:06:55.513: INFO: coredns-7646874c97-76qgd from kube-system started at 2019-06-17 13:30:33 +0000 UTC (1 container statuses recorded)
Jun 17 15:06:55.513: INFO: 	Container coredns ready: true, restart count 0
Jun 17 15:06:55.513: INFO: sonobuoy-e2e-job-383f0a2362094705 from heptio-sonobuoy started at 2019-06-17 14:35:15 +0000 UTC (2 container statuses recorded)
Jun 17 15:06:55.513: INFO: 	Container e2e ready: true, restart count 0
Jun 17 15:06:55.513: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 17 15:06:55.513: INFO: sonobuoy-systemd-logs-daemon-set-b6e1ac2b5ae94c02-xndrs from heptio-sonobuoy started at 2019-06-17 14:35:15 +0000 UTC (2 container statuses recorded)
Jun 17 15:06:55.513: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 17 15:06:55.513: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 17 15:06:55.513: INFO: nginx-proxy-lab1-k8s-node-3 from kube-system started at <nil> (0 container statuses recorded)
Jun 17 15:06:55.513: INFO: calico-node-rr2wf from kube-system started at 2019-06-17 13:29:45 +0000 UTC (1 container statuses recorded)
Jun 17 15:06:55.513: INFO: 	Container calico-node ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15a904caf22b7406], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:06:56.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7182" for this suite.
Jun 17 15:07:02.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:07:02.727: INFO: namespace sched-pred-7182 deletion completed in 6.170413034s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.424 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:07:02.727: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2848
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-900e22ac-9111-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume configMaps
Jun 17 15:07:02.902: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-900f97ce-9111-11e9-a8b9-dace53c98186" in namespace "projected-2848" to be "success or failure"
Jun 17 15:07:02.906: INFO: Pod "pod-projected-configmaps-900f97ce-9111-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.17804ms
Jun 17 15:07:04.911: INFO: Pod "pod-projected-configmaps-900f97ce-9111-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008883592s
Jun 17 15:07:06.920: INFO: Pod "pod-projected-configmaps-900f97ce-9111-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017829857s
STEP: Saw pod success
Jun 17 15:07:06.920: INFO: Pod "pod-projected-configmaps-900f97ce-9111-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:07:06.925: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-configmaps-900f97ce-9111-11e9-a8b9-dace53c98186 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 17 15:07:06.952: INFO: Waiting for pod pod-projected-configmaps-900f97ce-9111-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:07:06.955: INFO: Pod pod-projected-configmaps-900f97ce-9111-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:07:06.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2848" for this suite.
Jun 17 15:07:12.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:07:13.117: INFO: namespace projected-2848 deletion completed in 6.154673266s

• [SLOW TEST:10.390 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:07:13.119: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7537
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-9641d725-9111-11e9-a8b9-dace53c98186
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-9641d725-9111-11e9-a8b9-dace53c98186
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:07:19.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7537" for this suite.
Jun 17 15:07:41.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:07:41.532: INFO: namespace projected-7537 deletion completed in 22.166494184s

• [SLOW TEST:28.413 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:07:41.532: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7733
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7733
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Jun 17 15:07:41.721: INFO: Found 0 stateful pods, waiting for 3
Jun 17 15:07:51.726: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 17 15:07:51.726: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 17 15:07:51.726: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jun 17 15:07:51.759: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 17 15:08:01.798: INFO: Updating stateful set ss2
Jun 17 15:08:01.811: INFO: Waiting for Pod statefulset-7733/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Jun 17 15:08:11.858: INFO: Found 1 stateful pods, waiting for 3
Jun 17 15:08:21.863: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 17 15:08:21.863: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 17 15:08:21.863: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 17 15:08:21.892: INFO: Updating stateful set ss2
Jun 17 15:08:21.903: INFO: Waiting for Pod statefulset-7733/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 17 15:08:31.912: INFO: Waiting for Pod statefulset-7733/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 17 15:08:41.934: INFO: Updating stateful set ss2
Jun 17 15:08:41.945: INFO: Waiting for StatefulSet statefulset-7733/ss2 to complete update
Jun 17 15:08:41.945: INFO: Waiting for Pod statefulset-7733/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 17 15:08:51.954: INFO: Waiting for StatefulSet statefulset-7733/ss2 to complete update
Jun 17 15:08:51.954: INFO: Waiting for Pod statefulset-7733/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jun 17 15:09:01.954: INFO: Deleting all statefulset in ns statefulset-7733
Jun 17 15:09:01.958: INFO: Scaling statefulset ss2 to 0
Jun 17 15:09:41.980: INFO: Waiting for statefulset status.replicas updated to 0
Jun 17 15:09:41.984: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:09:42.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7733" for this suite.
Jun 17 15:09:48.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:09:48.167: INFO: namespace statefulset-7733 deletion completed in 6.156487921s

• [SLOW TEST:126.635 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:09:48.170: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9723
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 17 15:09:48.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-9723'
Jun 17 15:09:48.493: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 17 15:09:48.493: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Jun 17 15:09:48.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 delete jobs e2e-test-nginx-job --namespace=kubectl-9723'
Jun 17 15:09:48.597: INFO: stderr: ""
Jun 17 15:09:48.597: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:09:48.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9723" for this suite.
Jun 17 15:10:10.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:10:10.767: INFO: namespace kubectl-9723 deletion completed in 22.161654692s

• [SLOW TEST:22.600 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:10:10.768: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1651
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 17 15:10:10.938: INFO: Waiting up to 5m0s for pod "pod-0022ee98-9112-11e9-a8b9-dace53c98186" in namespace "emptydir-1651" to be "success or failure"
Jun 17 15:10:10.942: INFO: Pod "pod-0022ee98-9112-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 3.470521ms
Jun 17 15:10:12.946: INFO: Pod "pod-0022ee98-9112-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0082416s
Jun 17 15:10:14.952: INFO: Pod "pod-0022ee98-9112-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013697585s
STEP: Saw pod success
Jun 17 15:10:14.952: INFO: Pod "pod-0022ee98-9112-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:10:14.957: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-0022ee98-9112-11e9-a8b9-dace53c98186 container test-container: <nil>
STEP: delete the pod
Jun 17 15:10:14.985: INFO: Waiting for pod pod-0022ee98-9112-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:10:14.988: INFO: Pod pod-0022ee98-9112-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:10:14.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1651" for this suite.
Jun 17 15:10:21.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:10:21.154: INFO: namespace emptydir-1651 deletion completed in 6.158918679s

• [SLOW TEST:10.386 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:10:21.155: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3020
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-06544031-9112-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume configMaps
Jun 17 15:10:21.330: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-065558a7-9112-11e9-a8b9-dace53c98186" in namespace "projected-3020" to be "success or failure"
Jun 17 15:10:21.334: INFO: Pod "pod-projected-configmaps-065558a7-9112-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 3.779163ms
Jun 17 15:10:23.339: INFO: Pod "pod-projected-configmaps-065558a7-9112-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008438546s
Jun 17 15:10:25.344: INFO: Pod "pod-projected-configmaps-065558a7-9112-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013265798s
STEP: Saw pod success
Jun 17 15:10:25.344: INFO: Pod "pod-projected-configmaps-065558a7-9112-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:10:25.348: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-configmaps-065558a7-9112-11e9-a8b9-dace53c98186 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 17 15:10:25.374: INFO: Waiting for pod pod-projected-configmaps-065558a7-9112-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:10:25.377: INFO: Pod pod-projected-configmaps-065558a7-9112-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:10:25.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3020" for this suite.
Jun 17 15:10:31.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:10:31.557: INFO: namespace projected-3020 deletion completed in 6.16630295s

• [SLOW TEST:10.402 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:10:31.561: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4506
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:10:31.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4506" for this suite.
Jun 17 15:10:37.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:10:37.909: INFO: namespace kubelet-test-4506 deletion completed in 6.149915345s

• [SLOW TEST:6.348 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:10:37.911: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8591
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 17 15:10:38.091: INFO: Waiting up to 5m0s for pod "pod-105298bb-9112-11e9-a8b9-dace53c98186" in namespace "emptydir-8591" to be "success or failure"
Jun 17 15:10:38.096: INFO: Pod "pod-105298bb-9112-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.398548ms
Jun 17 15:10:40.100: INFO: Pod "pod-105298bb-9112-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008574882s
STEP: Saw pod success
Jun 17 15:10:40.100: INFO: Pod "pod-105298bb-9112-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:10:40.104: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-105298bb-9112-11e9-a8b9-dace53c98186 container test-container: <nil>
STEP: delete the pod
Jun 17 15:10:40.127: INFO: Waiting for pod pod-105298bb-9112-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:10:40.130: INFO: Pod pod-105298bb-9112-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:10:40.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8591" for this suite.
Jun 17 15:10:46.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:10:46.302: INFO: namespace emptydir-8591 deletion completed in 6.165985785s

• [SLOW TEST:8.391 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:10:46.303: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 17 15:10:54.513: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 17 15:10:54.517: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 17 15:10:56.518: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 17 15:10:56.523: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 17 15:10:58.518: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 17 15:10:58.523: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 17 15:11:00.518: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 17 15:11:00.523: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 17 15:11:02.518: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 17 15:11:02.523: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 17 15:11:04.518: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 17 15:11:04.524: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 17 15:11:06.518: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 17 15:11:06.523: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 17 15:11:08.518: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 17 15:11:08.524: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 17 15:11:10.518: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 17 15:11:10.523: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 17 15:11:12.518: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 17 15:11:12.523: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 17 15:11:14.518: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 17 15:11:14.523: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:11:14.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2096" for this suite.
Jun 17 15:11:36.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:11:36.703: INFO: namespace container-lifecycle-hook-2096 deletion completed in 22.159887389s

• [SLOW TEST:50.399 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:11:36.704: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5982
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 17 15:11:36.880: INFO: Waiting up to 5m0s for pod "downwardapi-volume-335d045e-9112-11e9-a8b9-dace53c98186" in namespace "projected-5982" to be "success or failure"
Jun 17 15:11:36.884: INFO: Pod "downwardapi-volume-335d045e-9112-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 3.920292ms
Jun 17 15:11:38.888: INFO: Pod "downwardapi-volume-335d045e-9112-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008429021s
STEP: Saw pod success
Jun 17 15:11:38.888: INFO: Pod "downwardapi-volume-335d045e-9112-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:11:38.892: INFO: Trying to get logs from node lab1-k8s-node-2 pod downwardapi-volume-335d045e-9112-11e9-a8b9-dace53c98186 container client-container: <nil>
STEP: delete the pod
Jun 17 15:11:38.918: INFO: Waiting for pod downwardapi-volume-335d045e-9112-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:11:38.921: INFO: Pod downwardapi-volume-335d045e-9112-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:11:38.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5982" for this suite.
Jun 17 15:11:44.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:11:45.129: INFO: namespace projected-5982 deletion completed in 6.197300195s

• [SLOW TEST:8.425 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:11:45.147: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3357
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-3866440c-9112-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume secrets
Jun 17 15:11:45.335: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-386755da-9112-11e9-a8b9-dace53c98186" in namespace "projected-3357" to be "success or failure"
Jun 17 15:11:45.343: INFO: Pod "pod-projected-secrets-386755da-9112-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 7.471377ms
Jun 17 15:11:47.349: INFO: Pod "pod-projected-secrets-386755da-9112-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014218641s
STEP: Saw pod success
Jun 17 15:11:47.349: INFO: Pod "pod-projected-secrets-386755da-9112-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:11:47.353: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-secrets-386755da-9112-11e9-a8b9-dace53c98186 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 17 15:11:47.379: INFO: Waiting for pod pod-projected-secrets-386755da-9112-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:11:47.386: INFO: Pod pod-projected-secrets-386755da-9112-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:11:47.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3357" for this suite.
Jun 17 15:11:53.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:11:53.548: INFO: namespace projected-3357 deletion completed in 6.152673164s

• [SLOW TEST:8.401 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:11:53.551: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5763
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:11:58.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5763" for this suite.
Jun 17 15:12:20.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:12:20.915: INFO: namespace replication-controller-5763 deletion completed in 22.16057587s

• [SLOW TEST:27.364 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:12:20.919: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5340
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5340.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5340.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 17 15:12:35.151: INFO: DNS probes using dns-5340/dns-test-4db67846-9112-11e9-a8b9-dace53c98186 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:12:35.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5340" for this suite.
Jun 17 15:12:41.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:12:41.346: INFO: namespace dns-5340 deletion completed in 6.164956225s

• [SLOW TEST:20.427 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:12:41.347: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1238
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 17 15:12:41.902: INFO: Pod name wrapped-volume-race-5a1cf252-9112-11e9-a8b9-dace53c98186: Found 0 pods out of 5
Jun 17 15:12:46.909: INFO: Pod name wrapped-volume-race-5a1cf252-9112-11e9-a8b9-dace53c98186: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5a1cf252-9112-11e9-a8b9-dace53c98186 in namespace emptydir-wrapper-1238, will wait for the garbage collector to delete the pods
Jun 17 15:12:59.017: INFO: Deleting ReplicationController wrapped-volume-race-5a1cf252-9112-11e9-a8b9-dace53c98186 took: 16.09335ms
Jun 17 15:12:59.317: INFO: Terminating ReplicationController wrapped-volume-race-5a1cf252-9112-11e9-a8b9-dace53c98186 pods took: 300.35364ms
STEP: Creating RC which spawns configmap-volume pods
Jun 17 15:13:43.044: INFO: Pod name wrapped-volume-race-7e8def0c-9112-11e9-a8b9-dace53c98186: Found 0 pods out of 5
Jun 17 15:13:48.053: INFO: Pod name wrapped-volume-race-7e8def0c-9112-11e9-a8b9-dace53c98186: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7e8def0c-9112-11e9-a8b9-dace53c98186 in namespace emptydir-wrapper-1238, will wait for the garbage collector to delete the pods
Jun 17 15:13:58.157: INFO: Deleting ReplicationController wrapped-volume-race-7e8def0c-9112-11e9-a8b9-dace53c98186 took: 13.729103ms
Jun 17 15:13:58.457: INFO: Terminating ReplicationController wrapped-volume-race-7e8def0c-9112-11e9-a8b9-dace53c98186 pods took: 300.562839ms
STEP: Creating RC which spawns configmap-volume pods
Jun 17 15:14:43.984: INFO: Pod name wrapped-volume-race-a2e0ac7e-9112-11e9-a8b9-dace53c98186: Found 0 pods out of 5
Jun 17 15:14:48.992: INFO: Pod name wrapped-volume-race-a2e0ac7e-9112-11e9-a8b9-dace53c98186: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a2e0ac7e-9112-11e9-a8b9-dace53c98186 in namespace emptydir-wrapper-1238, will wait for the garbage collector to delete the pods
Jun 17 15:15:27.861: INFO: Deleting ReplicationController wrapped-volume-race-a2e0ac7e-9112-11e9-a8b9-dace53c98186 took: 13.415181ms
Jun 17 15:15:31.062: INFO: Terminating ReplicationController wrapped-volume-race-a2e0ac7e-9112-11e9-a8b9-dace53c98186 pods took: 3.200253567s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:16:24.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1238" for this suite.
Jun 17 15:16:32.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:16:32.590: INFO: namespace emptydir-wrapper-1238 deletion completed in 8.156471648s

• [SLOW TEST:231.243 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:16:32.592: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5692
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 17 15:16:32.770: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3ba4de4-9112-11e9-a8b9-dace53c98186" in namespace "projected-5692" to be "success or failure"
Jun 17 15:16:32.777: INFO: Pod "downwardapi-volume-e3ba4de4-9112-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 6.188474ms
Jun 17 15:16:34.781: INFO: Pod "downwardapi-volume-e3ba4de4-9112-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010991893s
STEP: Saw pod success
Jun 17 15:16:34.781: INFO: Pod "downwardapi-volume-e3ba4de4-9112-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:16:34.786: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-e3ba4de4-9112-11e9-a8b9-dace53c98186 container client-container: <nil>
STEP: delete the pod
Jun 17 15:16:34.814: INFO: Waiting for pod downwardapi-volume-e3ba4de4-9112-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:16:34.818: INFO: Pod downwardapi-volume-e3ba4de4-9112-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:16:34.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5692" for this suite.
Jun 17 15:16:40.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:16:40.967: INFO: namespace projected-5692 deletion completed in 6.143515552s

• [SLOW TEST:8.375 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:16:40.968: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9148
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-e8b8e741-9112-11e9-a8b9-dace53c98186
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:16:43.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9148" for this suite.
Jun 17 15:17:05.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:17:05.342: INFO: namespace configmap-9148 deletion completed in 22.150995094s

• [SLOW TEST:24.374 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:17:05.349: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2984
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jun 17 15:17:05.507: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:17:08.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2984" for this suite.
Jun 17 15:17:14.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:17:14.797: INFO: namespace init-container-2984 deletion completed in 6.152052156s

• [SLOW TEST:9.448 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:17:14.799: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2237
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 15:17:14.958: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:17:21.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2237" for this suite.
Jun 17 15:17:27.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:17:27.185: INFO: namespace custom-resource-definition-2237 deletion completed in 6.153948844s

• [SLOW TEST:12.386 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:17:27.186: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3015
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Jun 17 15:17:27.944: INFO: created pod pod-service-account-defaultsa
Jun 17 15:17:27.944: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 17 15:17:27.957: INFO: created pod pod-service-account-mountsa
Jun 17 15:17:27.957: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 17 15:17:27.963: INFO: created pod pod-service-account-nomountsa
Jun 17 15:17:27.963: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 17 15:17:27.975: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 17 15:17:27.975: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 17 15:17:27.985: INFO: created pod pod-service-account-mountsa-mountspec
Jun 17 15:17:27.985: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 17 15:17:28.025: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 17 15:17:28.025: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 17 15:17:28.034: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 17 15:17:28.034: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 17 15:17:28.054: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 17 15:17:28.054: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 17 15:17:28.062: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 17 15:17:28.062: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:17:28.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3015" for this suite.
Jun 17 15:17:34.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:17:34.242: INFO: namespace svcaccounts-3015 deletion completed in 6.149589084s

• [SLOW TEST:7.057 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:17:34.242: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-857
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:17:36.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-857" for this suite.
Jun 17 15:17:42.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:17:42.651: INFO: namespace emptydir-wrapper-857 deletion completed in 6.153071731s

• [SLOW TEST:8.408 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:17:42.651: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4258
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-2pwb
STEP: Creating a pod to test atomic-volume-subpath
Jun 17 15:17:42.828: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-2pwb" in namespace "subpath-4258" to be "success or failure"
Jun 17 15:17:42.835: INFO: Pod "pod-subpath-test-secret-2pwb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.486101ms
Jun 17 15:17:44.840: INFO: Pod "pod-subpath-test-secret-2pwb": Phase="Running", Reason="", readiness=true. Elapsed: 2.011695398s
Jun 17 15:17:46.844: INFO: Pod "pod-subpath-test-secret-2pwb": Phase="Running", Reason="", readiness=true. Elapsed: 4.016373246s
Jun 17 15:17:48.849: INFO: Pod "pod-subpath-test-secret-2pwb": Phase="Running", Reason="", readiness=true. Elapsed: 6.020833346s
Jun 17 15:17:50.854: INFO: Pod "pod-subpath-test-secret-2pwb": Phase="Running", Reason="", readiness=true. Elapsed: 8.025805105s
Jun 17 15:17:52.859: INFO: Pod "pod-subpath-test-secret-2pwb": Phase="Running", Reason="", readiness=true. Elapsed: 10.030636555s
Jun 17 15:17:54.863: INFO: Pod "pod-subpath-test-secret-2pwb": Phase="Running", Reason="", readiness=true. Elapsed: 12.03529146s
Jun 17 15:17:56.868: INFO: Pod "pod-subpath-test-secret-2pwb": Phase="Running", Reason="", readiness=true. Elapsed: 14.040103659s
Jun 17 15:17:58.873: INFO: Pod "pod-subpath-test-secret-2pwb": Phase="Running", Reason="", readiness=true. Elapsed: 16.04485195s
Jun 17 15:18:00.878: INFO: Pod "pod-subpath-test-secret-2pwb": Phase="Running", Reason="", readiness=true. Elapsed: 18.049769643s
Jun 17 15:18:02.882: INFO: Pod "pod-subpath-test-secret-2pwb": Phase="Running", Reason="", readiness=true. Elapsed: 20.054265206s
Jun 17 15:18:04.887: INFO: Pod "pod-subpath-test-secret-2pwb": Phase="Running", Reason="", readiness=true. Elapsed: 22.058778832s
Jun 17 15:18:06.891: INFO: Pod "pod-subpath-test-secret-2pwb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.063375201s
STEP: Saw pod success
Jun 17 15:18:06.892: INFO: Pod "pod-subpath-test-secret-2pwb" satisfied condition "success or failure"
Jun 17 15:18:06.895: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-subpath-test-secret-2pwb container test-container-subpath-secret-2pwb: <nil>
STEP: delete the pod
Jun 17 15:18:06.921: INFO: Waiting for pod pod-subpath-test-secret-2pwb to disappear
Jun 17 15:18:06.925: INFO: Pod pod-subpath-test-secret-2pwb no longer exists
STEP: Deleting pod pod-subpath-test-secret-2pwb
Jun 17 15:18:06.925: INFO: Deleting pod "pod-subpath-test-secret-2pwb" in namespace "subpath-4258"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:18:06.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4258" for this suite.
Jun 17 15:18:12.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:18:13.079: INFO: namespace subpath-4258 deletion completed in 6.14489775s

• [SLOW TEST:30.429 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:18:13.083: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9145
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Jun 17 15:18:13.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 cluster-info'
Jun 17 15:18:13.327: INFO: stderr: ""
Jun 17 15:18:13.327: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\x1b[0;32mcoredns\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:18:13.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9145" for this suite.
Jun 17 15:18:19.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:18:19.475: INFO: namespace kubectl-9145 deletion completed in 6.14255476s

• [SLOW TEST:6.393 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:18:19.476: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1066
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Jun 17 15:18:19.644: INFO: Waiting up to 5m0s for pod "var-expansion-236e1dd8-9113-11e9-a8b9-dace53c98186" in namespace "var-expansion-1066" to be "success or failure"
Jun 17 15:18:19.651: INFO: Pod "var-expansion-236e1dd8-9113-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 6.760501ms
Jun 17 15:18:21.657: INFO: Pod "var-expansion-236e1dd8-9113-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012474033s
STEP: Saw pod success
Jun 17 15:18:21.657: INFO: Pod "var-expansion-236e1dd8-9113-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:18:21.661: INFO: Trying to get logs from node lab1-k8s-node-1 pod var-expansion-236e1dd8-9113-11e9-a8b9-dace53c98186 container dapi-container: <nil>
STEP: delete the pod
Jun 17 15:18:21.685: INFO: Waiting for pod var-expansion-236e1dd8-9113-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:18:21.689: INFO: Pod var-expansion-236e1dd8-9113-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:18:21.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1066" for this suite.
Jun 17 15:18:27.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:18:27.855: INFO: namespace var-expansion-1066 deletion completed in 6.159509666s

• [SLOW TEST:8.379 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:18:27.855: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9021
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Jun 17 15:18:28.019: INFO: namespace kubectl-9021
Jun 17 15:18:28.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 create -f - --namespace=kubectl-9021'
Jun 17 15:18:28.260: INFO: stderr: ""
Jun 17 15:18:28.260: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 17 15:18:29.264: INFO: Selector matched 1 pods for map[app:redis]
Jun 17 15:18:29.264: INFO: Found 0 / 1
Jun 17 15:18:30.264: INFO: Selector matched 1 pods for map[app:redis]
Jun 17 15:18:30.264: INFO: Found 1 / 1
Jun 17 15:18:30.264: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 17 15:18:30.272: INFO: Selector matched 1 pods for map[app:redis]
Jun 17 15:18:30.272: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 17 15:18:30.272: INFO: wait on redis-master startup in kubectl-9021 
Jun 17 15:18:30.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 logs redis-master-rhtks redis-master --namespace=kubectl-9021'
Jun 17 15:18:30.377: INFO: stderr: ""
Jun 17 15:18:30.377: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 17 Jun 15:18:29.711 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 17 Jun 15:18:29.712 # Server started, Redis version 3.2.12\n1:M 17 Jun 15:18:29.712 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 17 Jun 15:18:29.712 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jun 17 15:18:30.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9021'
Jun 17 15:18:30.509: INFO: stderr: ""
Jun 17 15:18:30.509: INFO: stdout: "service/rm2 exposed\n"
Jun 17 15:18:30.516: INFO: Service rm2 in namespace kubectl-9021 found.
STEP: exposing service
Jun 17 15:18:32.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9021'
Jun 17 15:18:32.646: INFO: stderr: ""
Jun 17 15:18:32.646: INFO: stdout: "service/rm3 exposed\n"
Jun 17 15:18:32.651: INFO: Service rm3 in namespace kubectl-9021 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:18:34.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9021" for this suite.
Jun 17 15:18:56.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:18:56.821: INFO: namespace kubectl-9021 deletion completed in 22.15487689s

• [SLOW TEST:28.966 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:18:56.822: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8594
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Jun 17 15:18:56.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 create -f - --namespace=kubectl-8594'
Jun 17 15:18:57.216: INFO: stderr: ""
Jun 17 15:18:57.216: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 17 15:18:57.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8594'
Jun 17 15:18:57.304: INFO: stderr: ""
Jun 17 15:18:57.304: INFO: stdout: "update-demo-nautilus-4tstw update-demo-nautilus-vzxzs "
Jun 17 15:18:57.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-4tstw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8594'
Jun 17 15:18:57.392: INFO: stderr: ""
Jun 17 15:18:57.392: INFO: stdout: ""
Jun 17 15:18:57.392: INFO: update-demo-nautilus-4tstw is created but not running
Jun 17 15:19:02.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8594'
Jun 17 15:19:02.477: INFO: stderr: ""
Jun 17 15:19:02.477: INFO: stdout: "update-demo-nautilus-4tstw update-demo-nautilus-vzxzs "
Jun 17 15:19:02.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-4tstw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8594'
Jun 17 15:19:02.553: INFO: stderr: ""
Jun 17 15:19:02.553: INFO: stdout: "true"
Jun 17 15:19:02.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-4tstw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8594'
Jun 17 15:19:02.635: INFO: stderr: ""
Jun 17 15:19:02.635: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 17 15:19:02.635: INFO: validating pod update-demo-nautilus-4tstw
Jun 17 15:19:02.642: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 17 15:19:02.642: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 17 15:19:02.642: INFO: update-demo-nautilus-4tstw is verified up and running
Jun 17 15:19:02.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-vzxzs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8594'
Jun 17 15:19:02.736: INFO: stderr: ""
Jun 17 15:19:02.737: INFO: stdout: "true"
Jun 17 15:19:02.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-vzxzs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8594'
Jun 17 15:19:02.844: INFO: stderr: ""
Jun 17 15:19:02.844: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 17 15:19:02.844: INFO: validating pod update-demo-nautilus-vzxzs
Jun 17 15:19:02.851: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 17 15:19:02.851: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 17 15:19:02.851: INFO: update-demo-nautilus-vzxzs is verified up and running
STEP: scaling down the replication controller
Jun 17 15:19:02.853: INFO: scanned /root for discovery docs: <nil>
Jun 17 15:19:02.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-8594'
Jun 17 15:19:03.991: INFO: stderr: ""
Jun 17 15:19:03.991: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 17 15:19:03.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8594'
Jun 17 15:19:04.072: INFO: stderr: ""
Jun 17 15:19:04.072: INFO: stdout: "update-demo-nautilus-4tstw update-demo-nautilus-vzxzs "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 17 15:19:09.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8594'
Jun 17 15:19:09.158: INFO: stderr: ""
Jun 17 15:19:09.158: INFO: stdout: "update-demo-nautilus-4tstw update-demo-nautilus-vzxzs "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 17 15:19:14.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8594'
Jun 17 15:19:14.265: INFO: stderr: ""
Jun 17 15:19:14.265: INFO: stdout: "update-demo-nautilus-vzxzs "
Jun 17 15:19:14.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-vzxzs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8594'
Jun 17 15:19:14.363: INFO: stderr: ""
Jun 17 15:19:14.363: INFO: stdout: "true"
Jun 17 15:19:14.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-vzxzs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8594'
Jun 17 15:19:14.444: INFO: stderr: ""
Jun 17 15:19:14.444: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 17 15:19:14.444: INFO: validating pod update-demo-nautilus-vzxzs
Jun 17 15:19:14.450: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 17 15:19:14.450: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 17 15:19:14.450: INFO: update-demo-nautilus-vzxzs is verified up and running
STEP: scaling up the replication controller
Jun 17 15:19:14.455: INFO: scanned /root for discovery docs: <nil>
Jun 17 15:19:14.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-8594'
Jun 17 15:19:15.574: INFO: stderr: ""
Jun 17 15:19:15.574: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 17 15:19:15.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8594'
Jun 17 15:19:15.669: INFO: stderr: ""
Jun 17 15:19:15.669: INFO: stdout: "update-demo-nautilus-mb522 update-demo-nautilus-vzxzs "
Jun 17 15:19:15.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-mb522 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8594'
Jun 17 15:19:15.744: INFO: stderr: ""
Jun 17 15:19:15.744: INFO: stdout: ""
Jun 17 15:19:15.744: INFO: update-demo-nautilus-mb522 is created but not running
Jun 17 15:19:20.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8594'
Jun 17 15:19:20.824: INFO: stderr: ""
Jun 17 15:19:20.825: INFO: stdout: "update-demo-nautilus-mb522 update-demo-nautilus-vzxzs "
Jun 17 15:19:20.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-mb522 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8594'
Jun 17 15:19:20.913: INFO: stderr: ""
Jun 17 15:19:20.913: INFO: stdout: "true"
Jun 17 15:19:20.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-mb522 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8594'
Jun 17 15:19:21.020: INFO: stderr: ""
Jun 17 15:19:21.020: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 17 15:19:21.020: INFO: validating pod update-demo-nautilus-mb522
Jun 17 15:19:21.026: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 17 15:19:21.026: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 17 15:19:21.026: INFO: update-demo-nautilus-mb522 is verified up and running
Jun 17 15:19:21.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-vzxzs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8594'
Jun 17 15:19:21.104: INFO: stderr: ""
Jun 17 15:19:21.104: INFO: stdout: "true"
Jun 17 15:19:21.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-vzxzs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8594'
Jun 17 15:19:21.189: INFO: stderr: ""
Jun 17 15:19:21.189: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 17 15:19:21.189: INFO: validating pod update-demo-nautilus-vzxzs
Jun 17 15:19:21.194: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 17 15:19:21.194: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 17 15:19:21.194: INFO: update-demo-nautilus-vzxzs is verified up and running
STEP: using delete to clean up resources
Jun 17 15:19:21.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 delete --grace-period=0 --force -f - --namespace=kubectl-8594'
Jun 17 15:19:21.286: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 17 15:19:21.286: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 17 15:19:21.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8594'
Jun 17 15:19:21.373: INFO: stderr: "No resources found.\n"
Jun 17 15:19:21.373: INFO: stdout: ""
Jun 17 15:19:21.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -l name=update-demo --namespace=kubectl-8594 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 17 15:19:21.455: INFO: stderr: ""
Jun 17 15:19:21.455: INFO: stdout: "update-demo-nautilus-mb522\nupdate-demo-nautilus-vzxzs\n"
Jun 17 15:19:21.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8594'
Jun 17 15:19:22.065: INFO: stderr: "No resources found.\n"
Jun 17 15:19:22.065: INFO: stdout: ""
Jun 17 15:19:22.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -l name=update-demo --namespace=kubectl-8594 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 17 15:19:22.150: INFO: stderr: ""
Jun 17 15:19:22.150: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:19:22.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8594" for this suite.
Jun 17 15:19:44.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:19:44.306: INFO: namespace kubectl-8594 deletion completed in 22.144344128s

• [SLOW TEST:47.484 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:19:44.309: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1982
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-55fe4fb5-9113-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume secrets
Jun 17 15:19:44.493: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-55ff8a23-9113-11e9-a8b9-dace53c98186" in namespace "projected-1982" to be "success or failure"
Jun 17 15:19:44.500: INFO: Pod "pod-projected-secrets-55ff8a23-9113-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 6.464541ms
Jun 17 15:19:46.508: INFO: Pod "pod-projected-secrets-55ff8a23-9113-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01502802s
STEP: Saw pod success
Jun 17 15:19:46.508: INFO: Pod "pod-projected-secrets-55ff8a23-9113-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:19:46.514: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-projected-secrets-55ff8a23-9113-11e9-a8b9-dace53c98186 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 17 15:19:46.539: INFO: Waiting for pod pod-projected-secrets-55ff8a23-9113-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:19:46.543: INFO: Pod pod-projected-secrets-55ff8a23-9113-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:19:46.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1982" for this suite.
Jun 17 15:19:52.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:19:52.686: INFO: namespace projected-1982 deletion completed in 6.136974153s

• [SLOW TEST:8.377 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:19:52.686: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3739
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 17 15:19:52.869: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5afecad8-9113-11e9-a8b9-dace53c98186" in namespace "projected-3739" to be "success or failure"
Jun 17 15:19:52.875: INFO: Pod "downwardapi-volume-5afecad8-9113-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 5.450684ms
Jun 17 15:19:54.880: INFO: Pod "downwardapi-volume-5afecad8-9113-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01042046s
STEP: Saw pod success
Jun 17 15:19:54.880: INFO: Pod "downwardapi-volume-5afecad8-9113-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:19:54.884: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-5afecad8-9113-11e9-a8b9-dace53c98186 container client-container: <nil>
STEP: delete the pod
Jun 17 15:19:54.907: INFO: Waiting for pod downwardapi-volume-5afecad8-9113-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:19:54.911: INFO: Pod downwardapi-volume-5afecad8-9113-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:19:54.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3739" for this suite.
Jun 17 15:20:00.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:20:01.078: INFO: namespace projected-3739 deletion completed in 6.157030911s

• [SLOW TEST:8.392 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:20:01.079: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5174
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Jun 17 15:20:01.243: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-270483862 proxy --unix-socket=/tmp/kubectl-proxy-unix462062589/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:20:01.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5174" for this suite.
Jun 17 15:20:07.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:20:07.462: INFO: namespace kubectl-5174 deletion completed in 6.143982952s

• [SLOW TEST:6.383 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:20:07.462: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7738
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 17 15:20:10.175: INFO: Successfully updated pod "pod-update-63ccf69a-9113-11e9-a8b9-dace53c98186"
STEP: verifying the updated pod is in kubernetes
Jun 17 15:20:10.194: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:20:10.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7738" for this suite.
Jun 17 15:20:32.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:20:32.360: INFO: namespace pods-7738 deletion completed in 22.158452968s

• [SLOW TEST:24.897 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:20:32.360: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-821
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jun 17 15:20:32.520: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:20:36.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-821" for this suite.
Jun 17 15:20:42.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:20:42.601: INFO: namespace init-container-821 deletion completed in 6.15349589s

• [SLOW TEST:10.241 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:20:42.601: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-8299
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-8299
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8299
STEP: Deleting pre-stop pod
Jun 17 15:20:55.818: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:20:55.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8299" for this suite.
Jun 17 15:21:33.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:21:33.974: INFO: namespace prestop-8299 deletion completed in 38.141584343s

• [SLOW TEST:51.373 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:21:33.977: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8589
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Jun 17 15:21:38.166: INFO: Pod pod-hostip-975d1fc1-9113-11e9-a8b9-dace53c98186 has hostIP: 10.128.0.7
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:21:38.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8589" for this suite.
Jun 17 15:22:00.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:22:00.329: INFO: namespace pods-8589 deletion completed in 22.15612938s

• [SLOW TEST:26.352 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:22:00.329: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4878
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 17 15:22:00.500: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a71201e7-9113-11e9-a8b9-dace53c98186" in namespace "downward-api-4878" to be "success or failure"
Jun 17 15:22:00.507: INFO: Pod "downwardapi-volume-a71201e7-9113-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 5.996582ms
Jun 17 15:22:02.511: INFO: Pod "downwardapi-volume-a71201e7-9113-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010854007s
STEP: Saw pod success
Jun 17 15:22:02.511: INFO: Pod "downwardapi-volume-a71201e7-9113-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:22:02.516: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-a71201e7-9113-11e9-a8b9-dace53c98186 container client-container: <nil>
STEP: delete the pod
Jun 17 15:22:02.541: INFO: Waiting for pod downwardapi-volume-a71201e7-9113-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:22:02.545: INFO: Pod downwardapi-volume-a71201e7-9113-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:22:02.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4878" for this suite.
Jun 17 15:22:08.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:22:08.703: INFO: namespace downward-api-4878 deletion completed in 6.145546151s

• [SLOW TEST:8.374 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:22:08.703: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9667
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 17 15:22:08.875: INFO: Waiting up to 5m0s for pod "pod-ac103295-9113-11e9-a8b9-dace53c98186" in namespace "emptydir-9667" to be "success or failure"
Jun 17 15:22:08.881: INFO: Pod "pod-ac103295-9113-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 5.603001ms
Jun 17 15:22:10.886: INFO: Pod "pod-ac103295-9113-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010227155s
Jun 17 15:22:12.891: INFO: Pod "pod-ac103295-9113-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015137194s
STEP: Saw pod success
Jun 17 15:22:12.891: INFO: Pod "pod-ac103295-9113-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:22:12.894: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-ac103295-9113-11e9-a8b9-dace53c98186 container test-container: <nil>
STEP: delete the pod
Jun 17 15:22:12.919: INFO: Waiting for pod pod-ac103295-9113-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:22:12.923: INFO: Pod pod-ac103295-9113-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:22:12.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9667" for this suite.
Jun 17 15:22:18.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:22:19.075: INFO: namespace emptydir-9667 deletion completed in 6.145367024s

• [SLOW TEST:10.372 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:22:19.076: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2088
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-b23e38f8-9113-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume secrets
Jun 17 15:22:19.249: INFO: Waiting up to 5m0s for pod "pod-secrets-b23f3556-9113-11e9-a8b9-dace53c98186" in namespace "secrets-2088" to be "success or failure"
Jun 17 15:22:19.254: INFO: Pod "pod-secrets-b23f3556-9113-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.375693ms
Jun 17 15:22:21.259: INFO: Pod "pod-secrets-b23f3556-9113-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00949351s
Jun 17 15:22:23.263: INFO: Pod "pod-secrets-b23f3556-9113-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013887638s
STEP: Saw pod success
Jun 17 15:22:23.263: INFO: Pod "pod-secrets-b23f3556-9113-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:22:23.269: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-secrets-b23f3556-9113-11e9-a8b9-dace53c98186 container secret-env-test: <nil>
STEP: delete the pod
Jun 17 15:22:23.293: INFO: Waiting for pod pod-secrets-b23f3556-9113-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:22:23.296: INFO: Pod pod-secrets-b23f3556-9113-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:22:23.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2088" for this suite.
Jun 17 15:22:29.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:22:29.456: INFO: namespace secrets-2088 deletion completed in 6.154270943s

• [SLOW TEST:10.380 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:22:29.456: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6691
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0617 15:22:35.667090      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 17 15:22:35.667: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:22:35.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6691" for this suite.
Jun 17 15:22:41.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:22:41.822: INFO: namespace gc-6691 deletion completed in 6.148173254s

• [SLOW TEST:12.366 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:22:41.824: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-7624
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8703
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7728
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:22:48.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7624" for this suite.
Jun 17 15:22:54.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:22:54.497: INFO: namespace namespaces-7624 deletion completed in 6.152640842s
STEP: Destroying namespace "nsdeletetest-8703" for this suite.
Jun 17 15:22:54.502: INFO: Namespace nsdeletetest-8703 was already deleted
STEP: Destroying namespace "nsdeletetest-7728" for this suite.
Jun 17 15:23:00.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:23:00.647: INFO: namespace nsdeletetest-7728 deletion completed in 6.14418844s

• [SLOW TEST:18.824 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:23:00.653: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3108
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 15:23:00.821: INFO: Creating ReplicaSet my-hostname-basic-cb07edab-9113-11e9-a8b9-dace53c98186
Jun 17 15:23:00.835: INFO: Pod name my-hostname-basic-cb07edab-9113-11e9-a8b9-dace53c98186: Found 0 pods out of 1
Jun 17 15:23:05.841: INFO: Pod name my-hostname-basic-cb07edab-9113-11e9-a8b9-dace53c98186: Found 1 pods out of 1
Jun 17 15:23:05.842: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-cb07edab-9113-11e9-a8b9-dace53c98186" is running
Jun 17 15:23:05.846: INFO: Pod "my-hostname-basic-cb07edab-9113-11e9-a8b9-dace53c98186-5gw7n" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-17 15:23:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-17 15:23:04 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-17 15:23:04 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-17 15:23:00 +0000 UTC Reason: Message:}])
Jun 17 15:23:05.846: INFO: Trying to dial the pod
Jun 17 15:23:10.861: INFO: Controller my-hostname-basic-cb07edab-9113-11e9-a8b9-dace53c98186: Got expected result from replica 1 [my-hostname-basic-cb07edab-9113-11e9-a8b9-dace53c98186-5gw7n]: "my-hostname-basic-cb07edab-9113-11e9-a8b9-dace53c98186-5gw7n", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:23:10.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3108" for this suite.
Jun 17 15:23:16.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:23:17.023: INFO: namespace replicaset-3108 deletion completed in 6.155886727s

• [SLOW TEST:16.371 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:23:17.025: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7040
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-7040
Jun 17 15:23:19.206: INFO: Started pod liveness-exec in namespace container-probe-7040
STEP: checking the pod's current state and verifying that restartCount is present
Jun 17 15:23:19.210: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:27:19.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7040" for this suite.
Jun 17 15:27:25.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:27:26.000: INFO: namespace container-probe-7040 deletion completed in 6.155107877s

• [SLOW TEST:248.975 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:27:26.000: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2506
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 15:27:26.158: INFO: Creating deployment "test-recreate-deployment"
Jun 17 15:27:26.170: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 17 15:27:26.187: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun 17 15:27:28.197: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 17 15:27:28.202: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 17 15:27:28.213: INFO: Updating deployment test-recreate-deployment
Jun 17 15:27:28.213: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jun 17 15:27:28.323: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-2506,SelfLink:/apis/apps/v1/namespaces/deployment-2506/deployments/test-recreate-deployment,UID:692fca5d-9114-11e9-bdd5-fa163e38c6bd,ResourceVersion:24681,Generation:2,CreationTimestamp:2019-06-17 15:27:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-06-17 15:27:28 +0000 UTC 2019-06-17 15:27:28 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-06-17 15:27:28 +0000 UTC 2019-06-17 15:27:26 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jun 17 15:27:28.328: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-2506,SelfLink:/apis/apps/v1/namespaces/deployment-2506/replicasets/test-recreate-deployment-c9cbd8684,UID:6a70c340-9114-11e9-bdd5-fa163e38c6bd,ResourceVersion:24679,Generation:1,CreationTimestamp:2019-06-17 15:27:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 692fca5d-9114-11e9-bdd5-fa163e38c6bd 0xc001746250 0xc001746251}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 17 15:27:28.328: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 17 15:27:28.328: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-2506,SelfLink:/apis/apps/v1/namespaces/deployment-2506/replicasets/test-recreate-deployment-7d57d5ff7c,UID:69315326-9114-11e9-bdd5-fa163e38c6bd,ResourceVersion:24668,Generation:2,CreationTimestamp:2019-06-17 15:27:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 692fca5d-9114-11e9-bdd5-fa163e38c6bd 0xc001746187 0xc001746188}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 17 15:27:28.333: INFO: Pod "test-recreate-deployment-c9cbd8684-mwg5r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-mwg5r,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-2506,SelfLink:/api/v1/namespaces/deployment-2506/pods/test-recreate-deployment-c9cbd8684-mwg5r,UID:6a71dcc9-9114-11e9-bdd5-fa163e38c6bd,ResourceVersion:24677,Generation:0,CreationTimestamp:2019-06-17 15:27:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 6a70c340-9114-11e9-bdd5-fa163e38c6bd 0xc001746a80 0xc001746a81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lpnsh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lpnsh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lpnsh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001746af0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001746b20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:27:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:27:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:27:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:27:28 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2019-06-17 15:27:28 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:27:28.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2506" for this suite.
Jun 17 15:27:34.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:27:34.491: INFO: namespace deployment-2506 deletion completed in 6.15180784s

• [SLOW TEST:8.491 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:27:34.493: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4135
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 17 15:27:34.669: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e3fc99d-9114-11e9-a8b9-dace53c98186" in namespace "projected-4135" to be "success or failure"
Jun 17 15:27:34.676: INFO: Pod "downwardapi-volume-6e3fc99d-9114-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 6.789424ms
Jun 17 15:27:36.681: INFO: Pod "downwardapi-volume-6e3fc99d-9114-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011306386s
Jun 17 15:27:38.686: INFO: Pod "downwardapi-volume-6e3fc99d-9114-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016324548s
STEP: Saw pod success
Jun 17 15:27:38.687: INFO: Pod "downwardapi-volume-6e3fc99d-9114-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:27:38.692: INFO: Trying to get logs from node lab1-k8s-node-2 pod downwardapi-volume-6e3fc99d-9114-11e9-a8b9-dace53c98186 container client-container: <nil>
STEP: delete the pod
Jun 17 15:27:38.718: INFO: Waiting for pod downwardapi-volume-6e3fc99d-9114-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:27:38.732: INFO: Pod downwardapi-volume-6e3fc99d-9114-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:27:38.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4135" for this suite.
Jun 17 15:27:44.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:27:44.882: INFO: namespace projected-4135 deletion completed in 6.143646578s

• [SLOW TEST:10.389 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:27:44.883: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Jun 17 15:27:45.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 --namespace=kubectl-8387 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jun 17 15:27:47.348: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jun 17 15:27:47.348: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:27:49.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8387" for this suite.
Jun 17 15:27:55.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:27:55.520: INFO: namespace kubectl-8387 deletion completed in 6.158657352s

• [SLOW TEST:10.637 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:27:55.521: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-988
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Jun 17 15:27:55.691: INFO: Waiting up to 5m0s for pod "client-containers-7ac827a0-9114-11e9-a8b9-dace53c98186" in namespace "containers-988" to be "success or failure"
Jun 17 15:27:55.696: INFO: Pod "client-containers-7ac827a0-9114-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 3.984294ms
Jun 17 15:27:57.703: INFO: Pod "client-containers-7ac827a0-9114-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011134544s
Jun 17 15:27:59.707: INFO: Pod "client-containers-7ac827a0-9114-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015860422s
STEP: Saw pod success
Jun 17 15:27:59.707: INFO: Pod "client-containers-7ac827a0-9114-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:27:59.712: INFO: Trying to get logs from node lab1-k8s-node-2 pod client-containers-7ac827a0-9114-11e9-a8b9-dace53c98186 container test-container: <nil>
STEP: delete the pod
Jun 17 15:27:59.738: INFO: Waiting for pod client-containers-7ac827a0-9114-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:27:59.741: INFO: Pod client-containers-7ac827a0-9114-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:27:59.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-988" for this suite.
Jun 17 15:28:05.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:28:05.898: INFO: namespace containers-988 deletion completed in 6.150780586s

• [SLOW TEST:10.377 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:28:05.898: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5163
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jun 17 15:28:06.075: INFO: Waiting up to 5m0s for pod "downward-api-80f7bfee-9114-11e9-a8b9-dace53c98186" in namespace "downward-api-5163" to be "success or failure"
Jun 17 15:28:06.080: INFO: Pod "downward-api-80f7bfee-9114-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.521302ms
Jun 17 15:28:08.086: INFO: Pod "downward-api-80f7bfee-9114-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010155024s
Jun 17 15:28:10.090: INFO: Pod "downward-api-80f7bfee-9114-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014754701s
STEP: Saw pod success
Jun 17 15:28:10.090: INFO: Pod "downward-api-80f7bfee-9114-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:28:10.094: INFO: Trying to get logs from node lab1-k8s-node-1 pod downward-api-80f7bfee-9114-11e9-a8b9-dace53c98186 container dapi-container: <nil>
STEP: delete the pod
Jun 17 15:28:10.121: INFO: Waiting for pod downward-api-80f7bfee-9114-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:28:10.125: INFO: Pod downward-api-80f7bfee-9114-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:28:10.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5163" for this suite.
Jun 17 15:28:16.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:28:16.286: INFO: namespace downward-api-5163 deletion completed in 6.152652046s

• [SLOW TEST:10.388 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:28:16.294: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4757
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 17 15:28:22.527: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 17 15:28:22.531: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 17 15:28:24.531: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 17 15:28:24.536: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 17 15:28:26.531: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 17 15:28:26.536: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 17 15:28:28.531: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 17 15:28:28.537: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 17 15:28:30.531: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 17 15:28:30.536: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 17 15:28:32.531: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 17 15:28:32.537: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 17 15:28:34.531: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 17 15:28:34.535: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 17 15:28:36.531: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 17 15:28:36.536: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 17 15:28:38.531: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 17 15:28:38.536: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 17 15:28:40.531: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 17 15:28:40.535: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:28:40.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4757" for this suite.
Jun 17 15:29:02.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:29:02.697: INFO: namespace container-lifecycle-hook-4757 deletion completed in 22.150131748s

• [SLOW TEST:46.403 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:29:02.699: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9206
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Jun 17 15:29:02.873: INFO: Waiting up to 5m0s for pod "var-expansion-a2d3484c-9114-11e9-a8b9-dace53c98186" in namespace "var-expansion-9206" to be "success or failure"
Jun 17 15:29:02.881: INFO: Pod "var-expansion-a2d3484c-9114-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 7.392857ms
Jun 17 15:29:04.886: INFO: Pod "var-expansion-a2d3484c-9114-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012337678s
STEP: Saw pod success
Jun 17 15:29:04.886: INFO: Pod "var-expansion-a2d3484c-9114-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:29:04.891: INFO: Trying to get logs from node lab1-k8s-node-2 pod var-expansion-a2d3484c-9114-11e9-a8b9-dace53c98186 container dapi-container: <nil>
STEP: delete the pod
Jun 17 15:29:04.916: INFO: Waiting for pod var-expansion-a2d3484c-9114-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:29:04.920: INFO: Pod var-expansion-a2d3484c-9114-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:29:04.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9206" for this suite.
Jun 17 15:29:10.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:29:11.076: INFO: namespace var-expansion-9206 deletion completed in 6.148898401s

• [SLOW TEST:8.378 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:29:11.077: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9224
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9484
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9320
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:29:35.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9224" for this suite.
Jun 17 15:29:41.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:29:41.775: INFO: namespace namespaces-9224 deletion completed in 6.159101402s
STEP: Destroying namespace "nsdeletetest-9484" for this suite.
Jun 17 15:29:41.778: INFO: Namespace nsdeletetest-9484 was already deleted
STEP: Destroying namespace "nsdeletetest-9320" for this suite.
Jun 17 15:29:47.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:29:47.923: INFO: namespace nsdeletetest-9320 deletion completed in 6.145255341s

• [SLOW TEST:36.846 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:29:47.925: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1286
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-bdc878dd-9114-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume configMaps
Jun 17 15:29:48.117: INFO: Waiting up to 5m0s for pod "pod-configmaps-bdc97211-9114-11e9-a8b9-dace53c98186" in namespace "configmap-1286" to be "success or failure"
Jun 17 15:29:48.122: INFO: Pod "pod-configmaps-bdc97211-9114-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.490451ms
Jun 17 15:29:50.127: INFO: Pod "pod-configmaps-bdc97211-9114-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00914284s
Jun 17 15:29:52.132: INFO: Pod "pod-configmaps-bdc97211-9114-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014188173s
STEP: Saw pod success
Jun 17 15:29:52.132: INFO: Pod "pod-configmaps-bdc97211-9114-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:29:52.137: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-configmaps-bdc97211-9114-11e9-a8b9-dace53c98186 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 17 15:29:52.173: INFO: Waiting for pod pod-configmaps-bdc97211-9114-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:29:52.178: INFO: Pod pod-configmaps-bdc97211-9114-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:29:52.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1286" for this suite.
Jun 17 15:29:58.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:29:58.345: INFO: namespace configmap-1286 deletion completed in 6.161197345s

• [SLOW TEST:10.421 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:29:58.346: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9568
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-9568
Jun 17 15:30:00.527: INFO: Started pod liveness-http in namespace container-probe-9568
STEP: checking the pod's current state and verifying that restartCount is present
Jun 17 15:30:00.531: INFO: Initial restart count of pod liveness-http is 0
Jun 17 15:30:22.590: INFO: Restart count of pod container-probe-9568/liveness-http is now 1 (22.059097007s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:30:22.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9568" for this suite.
Jun 17 15:30:28.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:30:28.775: INFO: namespace container-probe-9568 deletion completed in 6.161042732s

• [SLOW TEST:30.429 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:30:28.776: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2668
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 17 15:30:28.947: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d620a380-9114-11e9-a8b9-dace53c98186" in namespace "downward-api-2668" to be "success or failure"
Jun 17 15:30:28.953: INFO: Pod "downwardapi-volume-d620a380-9114-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 5.280293ms
Jun 17 15:30:30.958: INFO: Pod "downwardapi-volume-d620a380-9114-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0108482s
STEP: Saw pod success
Jun 17 15:30:30.958: INFO: Pod "downwardapi-volume-d620a380-9114-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:30:30.963: INFO: Trying to get logs from node lab1-k8s-node-2 pod downwardapi-volume-d620a380-9114-11e9-a8b9-dace53c98186 container client-container: <nil>
STEP: delete the pod
Jun 17 15:30:30.994: INFO: Waiting for pod downwardapi-volume-d620a380-9114-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:30:30.998: INFO: Pod downwardapi-volume-d620a380-9114-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:30:30.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2668" for this suite.
Jun 17 15:30:37.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:30:37.158: INFO: namespace downward-api-2668 deletion completed in 6.152709752s

• [SLOW TEST:8.382 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:30:37.159: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2248
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 17 15:30:37.335: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db2046f7-9114-11e9-a8b9-dace53c98186" in namespace "downward-api-2248" to be "success or failure"
Jun 17 15:30:37.339: INFO: Pod "downwardapi-volume-db2046f7-9114-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046994ms
Jun 17 15:30:39.343: INFO: Pod "downwardapi-volume-db2046f7-9114-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008853359s
STEP: Saw pod success
Jun 17 15:30:39.343: INFO: Pod "downwardapi-volume-db2046f7-9114-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:30:39.348: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-db2046f7-9114-11e9-a8b9-dace53c98186 container client-container: <nil>
STEP: delete the pod
Jun 17 15:30:39.370: INFO: Waiting for pod downwardapi-volume-db2046f7-9114-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:30:39.374: INFO: Pod downwardapi-volume-db2046f7-9114-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:30:39.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2248" for this suite.
Jun 17 15:30:45.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:30:45.529: INFO: namespace downward-api-2248 deletion completed in 6.14896908s

• [SLOW TEST:8.371 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:30:45.530: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1646
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jun 17 15:30:45.710: INFO: Waiting up to 5m0s for pod "downward-api-e01ed11a-9114-11e9-a8b9-dace53c98186" in namespace "downward-api-1646" to be "success or failure"
Jun 17 15:30:45.716: INFO: Pod "downward-api-e01ed11a-9114-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 6.113786ms
Jun 17 15:30:47.721: INFO: Pod "downward-api-e01ed11a-9114-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011013667s
Jun 17 15:30:49.730: INFO: Pod "downward-api-e01ed11a-9114-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019648973s
STEP: Saw pod success
Jun 17 15:30:49.730: INFO: Pod "downward-api-e01ed11a-9114-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:30:49.735: INFO: Trying to get logs from node lab1-k8s-node-2 pod downward-api-e01ed11a-9114-11e9-a8b9-dace53c98186 container dapi-container: <nil>
STEP: delete the pod
Jun 17 15:30:49.761: INFO: Waiting for pod downward-api-e01ed11a-9114-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:30:49.764: INFO: Pod downward-api-e01ed11a-9114-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:30:49.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1646" for this suite.
Jun 17 15:30:55.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:30:55.914: INFO: namespace downward-api-1646 deletion completed in 6.144228433s

• [SLOW TEST:10.384 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:30:55.914: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5887
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-e64e4d99-9114-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume configMaps
Jun 17 15:30:56.097: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e64f5b63-9114-11e9-a8b9-dace53c98186" in namespace "projected-5887" to be "success or failure"
Jun 17 15:30:56.107: INFO: Pod "pod-projected-configmaps-e64f5b63-9114-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 10.402658ms
Jun 17 15:30:58.112: INFO: Pod "pod-projected-configmaps-e64f5b63-9114-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015449655s
Jun 17 15:31:00.116: INFO: Pod "pod-projected-configmaps-e64f5b63-9114-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019528418s
STEP: Saw pod success
Jun 17 15:31:00.117: INFO: Pod "pod-projected-configmaps-e64f5b63-9114-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:31:00.121: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-configmaps-e64f5b63-9114-11e9-a8b9-dace53c98186 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 17 15:31:00.145: INFO: Waiting for pod pod-projected-configmaps-e64f5b63-9114-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:31:00.149: INFO: Pod pod-projected-configmaps-e64f5b63-9114-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:31:00.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5887" for this suite.
Jun 17 15:31:06.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:31:06.312: INFO: namespace projected-5887 deletion completed in 6.156648578s

• [SLOW TEST:10.398 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:31:06.314: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-ec8195dd-9114-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume secrets
Jun 17 15:31:06.495: INFO: Waiting up to 5m0s for pod "pod-secrets-ec8281b6-9114-11e9-a8b9-dace53c98186" in namespace "secrets-3576" to be "success or failure"
Jun 17 15:31:06.503: INFO: Pod "pod-secrets-ec8281b6-9114-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 7.883239ms
Jun 17 15:31:08.507: INFO: Pod "pod-secrets-ec8281b6-9114-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012438692s
STEP: Saw pod success
Jun 17 15:31:08.508: INFO: Pod "pod-secrets-ec8281b6-9114-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:31:08.512: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-secrets-ec8281b6-9114-11e9-a8b9-dace53c98186 container secret-volume-test: <nil>
STEP: delete the pod
Jun 17 15:31:08.541: INFO: Waiting for pod pod-secrets-ec8281b6-9114-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:31:08.545: INFO: Pod pod-secrets-ec8281b6-9114-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:31:08.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3576" for this suite.
Jun 17 15:31:14.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:31:14.704: INFO: namespace secrets-3576 deletion completed in 6.152505934s

• [SLOW TEST:8.391 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:31:14.707: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3697
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Jun 17 15:31:14.909: INFO: Waiting up to 5m0s for pod "var-expansion-f1862b3f-9114-11e9-a8b9-dace53c98186" in namespace "var-expansion-3697" to be "success or failure"
Jun 17 15:31:14.917: INFO: Pod "var-expansion-f1862b3f-9114-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 7.994569ms
Jun 17 15:31:16.925: INFO: Pod "var-expansion-f1862b3f-9114-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015911044s
STEP: Saw pod success
Jun 17 15:31:16.925: INFO: Pod "var-expansion-f1862b3f-9114-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:31:16.933: INFO: Trying to get logs from node lab1-k8s-node-1 pod var-expansion-f1862b3f-9114-11e9-a8b9-dace53c98186 container dapi-container: <nil>
STEP: delete the pod
Jun 17 15:31:16.971: INFO: Waiting for pod var-expansion-f1862b3f-9114-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:31:16.977: INFO: Pod var-expansion-f1862b3f-9114-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:31:16.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3697" for this suite.
Jun 17 15:31:23.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:31:23.143: INFO: namespace var-expansion-3697 deletion completed in 6.159876438s

• [SLOW TEST:8.437 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:31:23.144: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3852
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-f68a22f2-9114-11e9-a8b9-dace53c98186
STEP: Creating secret with name s-test-opt-upd-f68a232f-9114-11e9-a8b9-dace53c98186
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-f68a22f2-9114-11e9-a8b9-dace53c98186
STEP: Updating secret s-test-opt-upd-f68a232f-9114-11e9-a8b9-dace53c98186
STEP: Creating secret with name s-test-opt-create-f68a2345-9114-11e9-a8b9-dace53c98186
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:32:29.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3852" for this suite.
Jun 17 15:32:51.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:32:51.959: INFO: namespace secrets-3852 deletion completed in 22.14963753s

• [SLOW TEST:88.815 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:32:51.960: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-883
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-2b7927d6-9115-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume secrets
Jun 17 15:32:52.141: INFO: Waiting up to 5m0s for pod "pod-secrets-2b7ad717-9115-11e9-a8b9-dace53c98186" in namespace "secrets-883" to be "success or failure"
Jun 17 15:32:52.148: INFO: Pod "pod-secrets-2b7ad717-9115-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 6.607743ms
Jun 17 15:32:54.154: INFO: Pod "pod-secrets-2b7ad717-9115-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012595906s
Jun 17 15:32:56.159: INFO: Pod "pod-secrets-2b7ad717-9115-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017607196s
STEP: Saw pod success
Jun 17 15:32:56.159: INFO: Pod "pod-secrets-2b7ad717-9115-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:32:56.164: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-secrets-2b7ad717-9115-11e9-a8b9-dace53c98186 container secret-volume-test: <nil>
STEP: delete the pod
Jun 17 15:32:56.189: INFO: Waiting for pod pod-secrets-2b7ad717-9115-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:32:56.193: INFO: Pod pod-secrets-2b7ad717-9115-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:32:56.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-883" for this suite.
Jun 17 15:33:02.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:33:02.352: INFO: namespace secrets-883 deletion completed in 6.153216592s

• [SLOW TEST:10.392 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:33:02.354: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5809
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-31b2440f-9115-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume configMaps
Jun 17 15:33:02.582: INFO: Waiting up to 5m0s for pod "pod-configmaps-31b36b90-9115-11e9-a8b9-dace53c98186" in namespace "configmap-5809" to be "success or failure"
Jun 17 15:33:02.587: INFO: Pod "pod-configmaps-31b36b90-9115-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 5.374935ms
Jun 17 15:33:04.592: INFO: Pod "pod-configmaps-31b36b90-9115-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009995172s
STEP: Saw pod success
Jun 17 15:33:04.592: INFO: Pod "pod-configmaps-31b36b90-9115-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:33:04.596: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-configmaps-31b36b90-9115-11e9-a8b9-dace53c98186 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 17 15:33:04.627: INFO: Waiting for pod pod-configmaps-31b36b90-9115-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:33:04.630: INFO: Pod pod-configmaps-31b36b90-9115-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:33:04.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5809" for this suite.
Jun 17 15:33:10.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:33:10.806: INFO: namespace configmap-5809 deletion completed in 6.170066781s

• [SLOW TEST:8.452 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:33:10.808: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1948
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 17 15:33:10.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-1948'
Jun 17 15:33:11.069: INFO: stderr: ""
Jun 17 15:33:11.069: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jun 17 15:33:16.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pod e2e-test-nginx-pod --namespace=kubectl-1948 -o json'
Jun 17 15:33:16.201: INFO: stderr: ""
Jun 17 15:33:16.201: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-06-17T15:33:11Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-1948\",\n        \"resourceVersion\": \"26214\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-1948/pods/e2e-test-nginx-pod\",\n        \"uid\": \"36c22e44-9115-11e9-bdd5-fa163e38c6bd\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-fn4ll\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"lab1-k8s-node-1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-fn4ll\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-fn4ll\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-17T15:33:11Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-17T15:33:13Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-17T15:33:13Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-17T15:33:11Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://6829c3e5610294a3430a6840bde04c9d65a097c1344605790e9abdb6876f726d\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-06-17T15:33:12Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.128.0.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.95.106\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-06-17T15:33:11Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 17 15:33:16.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 replace -f - --namespace=kubectl-1948'
Jun 17 15:33:16.441: INFO: stderr: ""
Jun 17 15:33:16.441: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Jun 17 15:33:16.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 delete pods e2e-test-nginx-pod --namespace=kubectl-1948'
Jun 17 15:33:22.881: INFO: stderr: ""
Jun 17 15:33:22.881: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:33:22.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1948" for this suite.
Jun 17 15:33:28.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:33:29.041: INFO: namespace kubectl-1948 deletion completed in 6.152073256s

• [SLOW TEST:18.233 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:33:29.045: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8393
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 17 15:33:29.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8393'
Jun 17 15:33:29.313: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 17 15:33:29.313: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Jun 17 15:33:31.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 delete deployment e2e-test-nginx-deployment --namespace=kubectl-8393'
Jun 17 15:33:31.423: INFO: stderr: ""
Jun 17 15:33:31.423: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:33:31.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8393" for this suite.
Jun 17 15:33:53.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:33:53.586: INFO: namespace kubectl-8393 deletion completed in 22.153934685s

• [SLOW TEST:24.541 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:33:53.587: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5546
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun 17 15:33:57.784: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-50352ca4-9115-11e9-a8b9-dace53c98186,GenerateName:,Namespace:events-5546,SelfLink:/api/v1/namespaces/events-5546/pods/send-events-50352ca4-9115-11e9-a8b9-dace53c98186,UID:5036166b-9115-11e9-bdd5-fa163e38c6bd,ResourceVersion:26399,Generation:0,CreationTimestamp:2019-06-17 15:33:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 751368387,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tqrfw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tqrfw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-tqrfw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b00860} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b00880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:33:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:33:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:33:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:33:53 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.4,PodIP:10.233.95.107,StartTime:2019-06-17 15:33:53 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-06-17 15:33:55 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://6e937a264d395dcc0035944302db8a1f7d7f9b1889353c9a5b2e270378b81aaa}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jun 17 15:33:59.790: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun 17 15:34:01.795: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:34:01.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5546" for this suite.
Jun 17 15:34:45.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:34:45.964: INFO: namespace events-5546 deletion completed in 44.150190327s

• [SLOW TEST:52.377 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:34:45.965: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8667
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 17 15:34:46.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-8667'
Jun 17 15:34:46.275: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 17 15:34:46.276: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jun 17 15:34:46.292: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jun 17 15:34:46.299: INFO: scanned /root for discovery docs: <nil>
Jun 17 15:34:46.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-8667'
Jun 17 15:35:02.131: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 17 15:35:02.131: INFO: stdout: "Created e2e-test-nginx-rc-5037a678e0836804aa9a3c921bdd05d2\nScaling up e2e-test-nginx-rc-5037a678e0836804aa9a3c921bdd05d2 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-5037a678e0836804aa9a3c921bdd05d2 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-5037a678e0836804aa9a3c921bdd05d2 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jun 17 15:35:02.131: INFO: stdout: "Created e2e-test-nginx-rc-5037a678e0836804aa9a3c921bdd05d2\nScaling up e2e-test-nginx-rc-5037a678e0836804aa9a3c921bdd05d2 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-5037a678e0836804aa9a3c921bdd05d2 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-5037a678e0836804aa9a3c921bdd05d2 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jun 17 15:35:02.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-8667'
Jun 17 15:35:02.214: INFO: stderr: ""
Jun 17 15:35:02.214: INFO: stdout: "e2e-test-nginx-rc-5037a678e0836804aa9a3c921bdd05d2-bzvgm e2e-test-nginx-rc-rs72g "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Jun 17 15:35:07.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-8667'
Jun 17 15:35:07.314: INFO: stderr: ""
Jun 17 15:35:07.314: INFO: stdout: "e2e-test-nginx-rc-5037a678e0836804aa9a3c921bdd05d2-bzvgm "
Jun 17 15:35:07.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods e2e-test-nginx-rc-5037a678e0836804aa9a3c921bdd05d2-bzvgm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8667'
Jun 17 15:35:07.396: INFO: stderr: ""
Jun 17 15:35:07.396: INFO: stdout: "true"
Jun 17 15:35:07.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods e2e-test-nginx-rc-5037a678e0836804aa9a3c921bdd05d2-bzvgm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8667'
Jun 17 15:35:07.476: INFO: stderr: ""
Jun 17 15:35:07.476: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jun 17 15:35:07.476: INFO: e2e-test-nginx-rc-5037a678e0836804aa9a3c921bdd05d2-bzvgm is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Jun 17 15:35:07.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 delete rc e2e-test-nginx-rc --namespace=kubectl-8667'
Jun 17 15:35:07.579: INFO: stderr: ""
Jun 17 15:35:07.579: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:35:07.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8667" for this suite.
Jun 17 15:35:13.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:35:13.772: INFO: namespace kubectl-8667 deletion completed in 6.18663358s

• [SLOW TEST:27.808 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:35:13.772: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9507
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:35:17.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9507" for this suite.
Jun 17 15:35:57.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:35:58.121: INFO: namespace kubelet-test-9507 deletion completed in 40.142587316s

• [SLOW TEST:44.348 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:35:58.121: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-205
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 17 15:35:58.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-205'
Jun 17 15:35:58.390: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 17 15:35:58.390: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jun 17 15:35:58.406: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-sd5d9]
Jun 17 15:35:58.406: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-sd5d9" in namespace "kubectl-205" to be "running and ready"
Jun 17 15:35:58.414: INFO: Pod "e2e-test-nginx-rc-sd5d9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.208394ms
Jun 17 15:36:00.418: INFO: Pod "e2e-test-nginx-rc-sd5d9": Phase="Running", Reason="", readiness=true. Elapsed: 2.012612471s
Jun 17 15:36:00.418: INFO: Pod "e2e-test-nginx-rc-sd5d9" satisfied condition "running and ready"
Jun 17 15:36:00.418: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-sd5d9]
Jun 17 15:36:00.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 logs rc/e2e-test-nginx-rc --namespace=kubectl-205'
Jun 17 15:36:00.531: INFO: stderr: ""
Jun 17 15:36:00.531: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Jun 17 15:36:00.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 delete rc e2e-test-nginx-rc --namespace=kubectl-205'
Jun 17 15:36:00.630: INFO: stderr: ""
Jun 17 15:36:00.630: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:36:00.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-205" for this suite.
Jun 17 15:36:22.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:36:22.792: INFO: namespace kubectl-205 deletion completed in 22.155144136s

• [SLOW TEST:24.672 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:36:22.804: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9763
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-9763/secret-test-a92528c3-9115-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume secrets
Jun 17 15:36:22.985: INFO: Waiting up to 5m0s for pod "pod-configmaps-a9263f88-9115-11e9-a8b9-dace53c98186" in namespace "secrets-9763" to be "success or failure"
Jun 17 15:36:22.993: INFO: Pod "pod-configmaps-a9263f88-9115-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 8.235094ms
Jun 17 15:36:24.998: INFO: Pod "pod-configmaps-a9263f88-9115-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012898181s
Jun 17 15:36:27.002: INFO: Pod "pod-configmaps-a9263f88-9115-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017615627s
STEP: Saw pod success
Jun 17 15:36:27.002: INFO: Pod "pod-configmaps-a9263f88-9115-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:36:27.006: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-configmaps-a9263f88-9115-11e9-a8b9-dace53c98186 container env-test: <nil>
STEP: delete the pod
Jun 17 15:36:27.032: INFO: Waiting for pod pod-configmaps-a9263f88-9115-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:36:27.035: INFO: Pod pod-configmaps-a9263f88-9115-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:36:27.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9763" for this suite.
Jun 17 15:36:33.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:36:33.201: INFO: namespace secrets-9763 deletion completed in 6.159512814s

• [SLOW TEST:10.397 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:36:33.201: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6084
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jun 17 15:36:37.911: INFO: Successfully updated pod "labelsupdateaf578b7d-9115-11e9-a8b9-dace53c98186"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:36:39.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6084" for this suite.
Jun 17 15:37:01.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:37:02.096: INFO: namespace downward-api-6084 deletion completed in 22.155428291s

• [SLOW TEST:28.895 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:37:02.097: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9680
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:37:06.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9680" for this suite.
Jun 17 15:37:46.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:37:46.501: INFO: namespace kubelet-test-9680 deletion completed in 40.151912187s

• [SLOW TEST:44.405 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:37:46.502: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2136
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
Jun 17 15:37:46.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 create -f - --namespace=kubectl-2136'
Jun 17 15:37:46.934: INFO: stderr: ""
Jun 17 15:37:46.934: INFO: stdout: "pod/pause created\n"
Jun 17 15:37:46.934: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 17 15:37:46.934: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2136" to be "running and ready"
Jun 17 15:37:46.939: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.96237ms
Jun 17 15:37:48.946: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012155071s
Jun 17 15:37:50.951: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.017055866s
Jun 17 15:37:50.951: INFO: Pod "pause" satisfied condition "running and ready"
Jun 17 15:37:50.951: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 17 15:37:50.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 label pods pause testing-label=testing-label-value --namespace=kubectl-2136'
Jun 17 15:37:51.095: INFO: stderr: ""
Jun 17 15:37:51.095: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 17 15:37:51.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pod pause -L testing-label --namespace=kubectl-2136'
Jun 17 15:37:51.174: INFO: stderr: ""
Jun 17 15:37:51.174: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 17 15:37:51.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 label pods pause testing-label- --namespace=kubectl-2136'
Jun 17 15:37:51.269: INFO: stderr: ""
Jun 17 15:37:51.269: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 17 15:37:51.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pod pause -L testing-label --namespace=kubectl-2136'
Jun 17 15:37:51.355: INFO: stderr: ""
Jun 17 15:37:51.355: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
Jun 17 15:37:51.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 delete --grace-period=0 --force -f - --namespace=kubectl-2136'
Jun 17 15:37:51.456: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 17 15:37:51.456: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 17 15:37:51.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get rc,svc -l name=pause --no-headers --namespace=kubectl-2136'
Jun 17 15:37:51.547: INFO: stderr: "No resources found.\n"
Jun 17 15:37:51.547: INFO: stdout: ""
Jun 17 15:37:51.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -l name=pause --namespace=kubectl-2136 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 17 15:37:51.630: INFO: stderr: ""
Jun 17 15:37:51.630: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:37:51.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2136" for this suite.
Jun 17 15:37:57.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:37:57.793: INFO: namespace kubectl-2136 deletion completed in 6.156237746s

• [SLOW TEST:11.291 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:37:57.793: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7530
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 17 15:37:57.975: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1c4ae42-9115-11e9-a8b9-dace53c98186" in namespace "downward-api-7530" to be "success or failure"
Jun 17 15:37:57.982: INFO: Pod "downwardapi-volume-e1c4ae42-9115-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 6.992728ms
Jun 17 15:37:59.989: INFO: Pod "downwardapi-volume-e1c4ae42-9115-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013384536s
Jun 17 15:38:01.994: INFO: Pod "downwardapi-volume-e1c4ae42-9115-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018858181s
STEP: Saw pod success
Jun 17 15:38:01.994: INFO: Pod "downwardapi-volume-e1c4ae42-9115-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:38:01.998: INFO: Trying to get logs from node lab1-k8s-node-2 pod downwardapi-volume-e1c4ae42-9115-11e9-a8b9-dace53c98186 container client-container: <nil>
STEP: delete the pod
Jun 17 15:38:02.035: INFO: Waiting for pod downwardapi-volume-e1c4ae42-9115-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:38:02.041: INFO: Pod downwardapi-volume-e1c4ae42-9115-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:38:02.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7530" for this suite.
Jun 17 15:38:08.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:38:08.208: INFO: namespace downward-api-7530 deletion completed in 6.160792954s

• [SLOW TEST:10.415 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:38:08.209: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2144
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 17 15:38:08.387: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e7f9abdc-9115-11e9-a8b9-dace53c98186" in namespace "projected-2144" to be "success or failure"
Jun 17 15:38:08.393: INFO: Pod "downwardapi-volume-e7f9abdc-9115-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 5.984703ms
Jun 17 15:38:10.400: INFO: Pod "downwardapi-volume-e7f9abdc-9115-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012999218s
STEP: Saw pod success
Jun 17 15:38:10.400: INFO: Pod "downwardapi-volume-e7f9abdc-9115-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:38:10.406: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-e7f9abdc-9115-11e9-a8b9-dace53c98186 container client-container: <nil>
STEP: delete the pod
Jun 17 15:38:10.451: INFO: Waiting for pod downwardapi-volume-e7f9abdc-9115-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:38:10.455: INFO: Pod downwardapi-volume-e7f9abdc-9115-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:38:10.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2144" for this suite.
Jun 17 15:38:16.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:38:16.608: INFO: namespace projected-2144 deletion completed in 6.145508833s

• [SLOW TEST:8.399 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:38:16.609: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6766
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-ecfaa2d8-9115-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume secrets
Jun 17 15:38:16.785: INFO: Waiting up to 5m0s for pod "pod-secrets-ecfb82eb-9115-11e9-a8b9-dace53c98186" in namespace "secrets-6766" to be "success or failure"
Jun 17 15:38:16.790: INFO: Pod "pod-secrets-ecfb82eb-9115-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.745346ms
Jun 17 15:38:18.795: INFO: Pod "pod-secrets-ecfb82eb-9115-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009650271s
STEP: Saw pod success
Jun 17 15:38:18.795: INFO: Pod "pod-secrets-ecfb82eb-9115-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:38:18.799: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-secrets-ecfb82eb-9115-11e9-a8b9-dace53c98186 container secret-volume-test: <nil>
STEP: delete the pod
Jun 17 15:38:18.832: INFO: Waiting for pod pod-secrets-ecfb82eb-9115-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:38:18.836: INFO: Pod pod-secrets-ecfb82eb-9115-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:38:18.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6766" for this suite.
Jun 17 15:38:24.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:38:25.002: INFO: namespace secrets-6766 deletion completed in 6.158506847s

• [SLOW TEST:8.393 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:38:25.003: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2781
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-f1fe4385-9115-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume secrets
Jun 17 15:38:25.198: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f1ff3405-9115-11e9-a8b9-dace53c98186" in namespace "projected-2781" to be "success or failure"
Jun 17 15:38:25.204: INFO: Pod "pod-projected-secrets-f1ff3405-9115-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 5.739608ms
Jun 17 15:38:27.209: INFO: Pod "pod-projected-secrets-f1ff3405-9115-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010367099s
Jun 17 15:38:29.214: INFO: Pod "pod-projected-secrets-f1ff3405-9115-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015320657s
STEP: Saw pod success
Jun 17 15:38:29.214: INFO: Pod "pod-projected-secrets-f1ff3405-9115-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:38:29.218: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-secrets-f1ff3405-9115-11e9-a8b9-dace53c98186 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 17 15:38:29.247: INFO: Waiting for pod pod-projected-secrets-f1ff3405-9115-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:38:29.251: INFO: Pod pod-projected-secrets-f1ff3405-9115-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:38:29.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2781" for this suite.
Jun 17 15:38:35.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:38:35.429: INFO: namespace projected-2781 deletion completed in 6.172520156s

• [SLOW TEST:10.427 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:38:35.429: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6824
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-6824
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 17 15:38:35.590: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 17 15:38:55.713: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.95.115:8080/dial?request=hostName&protocol=http&host=10.233.64.91&port=8080&tries=1'] Namespace:pod-network-test-6824 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 15:38:55.713: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 15:38:55.914: INFO: Waiting for endpoints: map[]
Jun 17 15:38:55.919: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.95.115:8080/dial?request=hostName&protocol=http&host=10.233.74.30&port=8080&tries=1'] Namespace:pod-network-test-6824 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 15:38:55.919: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 15:38:56.152: INFO: Waiting for endpoints: map[]
Jun 17 15:38:56.159: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.95.115:8080/dial?request=hostName&protocol=http&host=10.233.95.114&port=8080&tries=1'] Namespace:pod-network-test-6824 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 15:38:56.159: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 15:38:56.401: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:38:56.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6824" for this suite.
Jun 17 15:39:18.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:39:18.569: INFO: namespace pod-network-test-6824 deletion completed in 22.162022937s

• [SLOW TEST:43.140 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:39:18.573: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8246
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 17 15:39:18.743: INFO: Waiting up to 5m0s for pod "downwardapi-volume-11e912dd-9116-11e9-a8b9-dace53c98186" in namespace "projected-8246" to be "success or failure"
Jun 17 15:39:18.750: INFO: Pod "downwardapi-volume-11e912dd-9116-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 7.582746ms
Jun 17 15:39:20.755: INFO: Pod "downwardapi-volume-11e912dd-9116-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012534178s
STEP: Saw pod success
Jun 17 15:39:20.755: INFO: Pod "downwardapi-volume-11e912dd-9116-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:39:20.760: INFO: Trying to get logs from node lab1-k8s-node-2 pod downwardapi-volume-11e912dd-9116-11e9-a8b9-dace53c98186 container client-container: <nil>
STEP: delete the pod
Jun 17 15:39:20.791: INFO: Waiting for pod downwardapi-volume-11e912dd-9116-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:39:20.796: INFO: Pod downwardapi-volume-11e912dd-9116-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:39:20.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8246" for this suite.
Jun 17 15:39:26.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:39:26.948: INFO: namespace projected-8246 deletion completed in 6.146499662s

• [SLOW TEST:8.376 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:39:26.949: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-952
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun 17 15:39:29.664: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-952 pod-service-account-1738664d-9116-11e9-a8b9-dace53c98186 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 17 15:39:29.918: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-952 pod-service-account-1738664d-9116-11e9-a8b9-dace53c98186 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 17 15:39:30.145: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-952 pod-service-account-1738664d-9116-11e9-a8b9-dace53c98186 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:39:30.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-952" for this suite.
Jun 17 15:39:36.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:39:36.527: INFO: namespace svcaccounts-952 deletion completed in 6.151668941s

• [SLOW TEST:9.578 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:39:36.529: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6507
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6507.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6507.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6507.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6507.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6507.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6507.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 17 15:39:40.765: INFO: DNS probes using dns-6507/dns-test-1c9d4e13-9116-11e9-a8b9-dace53c98186 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:39:40.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6507" for this suite.
Jun 17 15:39:46.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:39:46.940: INFO: namespace dns-6507 deletion completed in 6.149679045s

• [SLOW TEST:10.411 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:39:46.940: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1977
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-22d2195d-9116-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume secrets
Jun 17 15:39:47.122: INFO: Waiting up to 5m0s for pod "pod-secrets-22d3337b-9116-11e9-a8b9-dace53c98186" in namespace "secrets-1977" to be "success or failure"
Jun 17 15:39:47.127: INFO: Pod "pod-secrets-22d3337b-9116-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 5.137587ms
Jun 17 15:39:49.133: INFO: Pod "pod-secrets-22d3337b-9116-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010733042s
STEP: Saw pod success
Jun 17 15:39:49.133: INFO: Pod "pod-secrets-22d3337b-9116-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:39:49.138: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-secrets-22d3337b-9116-11e9-a8b9-dace53c98186 container secret-volume-test: <nil>
STEP: delete the pod
Jun 17 15:39:49.162: INFO: Waiting for pod pod-secrets-22d3337b-9116-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:39:49.166: INFO: Pod pod-secrets-22d3337b-9116-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:39:49.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1977" for this suite.
Jun 17 15:39:55.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:39:55.354: INFO: namespace secrets-1977 deletion completed in 6.178911298s

• [SLOW TEST:8.414 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:39:55.354: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5983
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jun 17 15:39:55.513: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 17 15:39:55.523: INFO: Waiting for terminating namespaces to be deleted...
Jun 17 15:39:55.527: INFO: 
Logging pods the kubelet thinks is on node lab1-k8s-node-1 before test
Jun 17 15:39:55.535: INFO: kube-proxy-b4j5w from kube-system started at 2019-06-17 13:30:32 +0000 UTC (1 container statuses recorded)
Jun 17 15:39:55.535: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 17 15:39:55.536: INFO: calico-node-dlffw from kube-system started at 2019-06-17 13:29:45 +0000 UTC (1 container statuses recorded)
Jun 17 15:39:55.536: INFO: 	Container calico-node ready: true, restart count 0
Jun 17 15:39:55.536: INFO: tiller-deploy-86c8b7897c-f2sdt from kube-system started at 2019-06-17 13:30:50 +0000 UTC (1 container statuses recorded)
Jun 17 15:39:55.536: INFO: 	Container tiller ready: true, restart count 0
Jun 17 15:39:55.536: INFO: nginx-proxy-lab1-k8s-node-1 from kube-system started at <nil> (0 container statuses recorded)
Jun 17 15:39:55.536: INFO: nodelocaldns-zc5f8 from kube-system started at 2019-06-17 13:30:28 +0000 UTC (1 container statuses recorded)
Jun 17 15:39:55.536: INFO: 	Container node-cache ready: true, restart count 0
Jun 17 15:39:55.536: INFO: sonobuoy-systemd-logs-daemon-set-b6e1ac2b5ae94c02-2mwvd from heptio-sonobuoy started at 2019-06-17 14:35:15 +0000 UTC (2 container statuses recorded)
Jun 17 15:39:55.536: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 17 15:39:55.536: INFO: 	Container systemd-logs ready: true, restart count 1
Jun 17 15:39:55.536: INFO: dns-autoscaler-56c969bdb8-5brv9 from kube-system started at 2019-06-17 13:30:26 +0000 UTC (1 container statuses recorded)
Jun 17 15:39:55.537: INFO: 	Container autoscaler ready: true, restart count 0
Jun 17 15:39:55.537: INFO: 
Logging pods the kubelet thinks is on node lab1-k8s-node-2 before test
Jun 17 15:39:55.546: INFO: kube-proxy-gtfkl from kube-system started at 2019-06-17 13:30:03 +0000 UTC (1 container statuses recorded)
Jun 17 15:39:55.546: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 17 15:39:55.546: INFO: calico-node-fz8bf from kube-system started at 2019-06-17 13:29:45 +0000 UTC (1 container statuses recorded)
Jun 17 15:39:55.546: INFO: 	Container calico-node ready: true, restart count 0
Jun 17 15:39:55.546: INFO: kubernetes-dashboard-57875dc8c7-mmp6z from kube-system started at 2019-06-17 13:30:30 +0000 UTC (1 container statuses recorded)
Jun 17 15:39:55.546: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun 17 15:39:55.546: INFO: sonobuoy-systemd-logs-daemon-set-b6e1ac2b5ae94c02-ngs2t from heptio-sonobuoy started at 2019-06-17 14:35:15 +0000 UTC (2 container statuses recorded)
Jun 17 15:39:55.547: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 17 15:39:55.547: INFO: 	Container systemd-logs ready: true, restart count 1
Jun 17 15:39:55.547: INFO: nginx-proxy-lab1-k8s-node-2 from kube-system started at <nil> (0 container statuses recorded)
Jun 17 15:39:55.547: INFO: calico-kube-controllers-57c89fc87d-sk646 from kube-system started at 2019-06-17 13:29:55 +0000 UTC (1 container statuses recorded)
Jun 17 15:39:55.547: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 17 15:39:55.547: INFO: nodelocaldns-4979m from kube-system started at 2019-06-17 13:30:28 +0000 UTC (1 container statuses recorded)
Jun 17 15:39:55.547: INFO: 	Container node-cache ready: true, restart count 0
Jun 17 15:39:55.547: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-17 14:35:09 +0000 UTC (1 container statuses recorded)
Jun 17 15:39:55.547: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 17 15:39:55.547: INFO: 
Logging pods the kubelet thinks is on node lab1-k8s-node-3 before test
Jun 17 15:39:55.556: INFO: calico-node-rr2wf from kube-system started at 2019-06-17 13:29:45 +0000 UTC (1 container statuses recorded)
Jun 17 15:39:55.556: INFO: 	Container calico-node ready: true, restart count 0
Jun 17 15:39:55.556: INFO: nginx-proxy-lab1-k8s-node-3 from kube-system started at <nil> (0 container statuses recorded)
Jun 17 15:39:55.556: INFO: nodelocaldns-psdz6 from kube-system started at 2019-06-17 13:30:28 +0000 UTC (1 container statuses recorded)
Jun 17 15:39:55.556: INFO: 	Container node-cache ready: true, restart count 0
Jun 17 15:39:55.556: INFO: coredns-7646874c97-76qgd from kube-system started at 2019-06-17 13:30:33 +0000 UTC (1 container statuses recorded)
Jun 17 15:39:55.556: INFO: 	Container coredns ready: true, restart count 0
Jun 17 15:39:55.556: INFO: sonobuoy-e2e-job-383f0a2362094705 from heptio-sonobuoy started at 2019-06-17 14:35:15 +0000 UTC (2 container statuses recorded)
Jun 17 15:39:55.556: INFO: 	Container e2e ready: true, restart count 0
Jun 17 15:39:55.556: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 17 15:39:55.556: INFO: sonobuoy-systemd-logs-daemon-set-b6e1ac2b5ae94c02-xndrs from heptio-sonobuoy started at 2019-06-17 14:35:15 +0000 UTC (2 container statuses recorded)
Jun 17 15:39:55.556: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 17 15:39:55.556: INFO: 	Container systemd-logs ready: true, restart count 1
Jun 17 15:39:55.556: INFO: kube-proxy-gdc49 from kube-system started at 2019-06-17 13:30:13 +0000 UTC (1 container statuses recorded)
Jun 17 15:39:55.556: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-2913fa3c-9116-11e9-a8b9-dace53c98186 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-2913fa3c-9116-11e9-a8b9-dace53c98186 off the node lab1-k8s-node-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2913fa3c-9116-11e9-a8b9-dace53c98186
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:40:01.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5983" for this suite.
Jun 17 15:40:15.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:40:15.823: INFO: namespace sched-pred-5983 deletion completed in 14.153281173s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:20.469 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:40:15.824: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2087
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 17 15:40:21.031: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:40:22.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2087" for this suite.
Jun 17 15:40:36.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:40:36.224: INFO: namespace replicaset-2087 deletion completed in 14.159644044s

• [SLOW TEST:20.400 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:40:36.224: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4463
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 17 15:40:36.407: INFO: Waiting up to 5m0s for pod "pod-40328d15-9116-11e9-a8b9-dace53c98186" in namespace "emptydir-4463" to be "success or failure"
Jun 17 15:40:36.415: INFO: Pod "pod-40328d15-9116-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 8.188708ms
Jun 17 15:40:38.420: INFO: Pod "pod-40328d15-9116-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012785s
Jun 17 15:40:40.425: INFO: Pod "pod-40328d15-9116-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017568019s
STEP: Saw pod success
Jun 17 15:40:40.425: INFO: Pod "pod-40328d15-9116-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:40:40.429: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-40328d15-9116-11e9-a8b9-dace53c98186 container test-container: <nil>
STEP: delete the pod
Jun 17 15:40:40.459: INFO: Waiting for pod pod-40328d15-9116-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:40:40.462: INFO: Pod pod-40328d15-9116-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:40:40.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4463" for this suite.
Jun 17 15:40:46.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:40:46.625: INFO: namespace emptydir-4463 deletion completed in 6.156304522s

• [SLOW TEST:10.401 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:40:46.629: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-399
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-wqczg in namespace proxy-399
I0617 15:40:46.817531      14 runners.go:184] Created replication controller with name: proxy-service-wqczg, namespace: proxy-399, replica count: 1
I0617 15:40:47.867892      14 runners.go:184] proxy-service-wqczg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0617 15:40:48.868091      14 runners.go:184] proxy-service-wqczg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0617 15:40:49.868294      14 runners.go:184] proxy-service-wqczg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0617 15:40:50.868488      14 runners.go:184] proxy-service-wqczg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0617 15:40:51.868689      14 runners.go:184] proxy-service-wqczg Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 17 15:40:51.873: INFO: setup took 5.082242021s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 17 15:40:51.888: INFO: (0) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 14.042486ms)
Jun 17 15:40:51.888: INFO: (0) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 15.120505ms)
Jun 17 15:40:51.889: INFO: (0) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 14.114179ms)
Jun 17 15:40:51.898: INFO: (0) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 23.323841ms)
Jun 17 15:40:51.899: INFO: (0) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 24.210836ms)
Jun 17 15:40:51.899: INFO: (0) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 24.227787ms)
Jun 17 15:40:51.899: INFO: (0) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 24.220849ms)
Jun 17 15:40:51.899: INFO: (0) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 24.02819ms)
Jun 17 15:40:51.899: INFO: (0) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 24.681365ms)
Jun 17 15:40:51.901: INFO: (0) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 25.691567ms)
Jun 17 15:40:51.901: INFO: (0) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 26.052487ms)
Jun 17 15:40:51.902: INFO: (0) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 28.904807ms)
Jun 17 15:40:51.902: INFO: (0) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 27.251892ms)
Jun 17 15:40:51.903: INFO: (0) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 29.608013ms)
Jun 17 15:40:51.903: INFO: (0) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 28.576545ms)
Jun 17 15:40:51.905: INFO: (0) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 29.99986ms)
Jun 17 15:40:51.914: INFO: (1) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 9.353375ms)
Jun 17 15:40:51.915: INFO: (1) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 9.379908ms)
Jun 17 15:40:51.920: INFO: (1) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 14.837789ms)
Jun 17 15:40:51.921: INFO: (1) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 15.120651ms)
Jun 17 15:40:51.921: INFO: (1) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 15.375009ms)
Jun 17 15:40:51.921: INFO: (1) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 15.612778ms)
Jun 17 15:40:51.923: INFO: (1) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 17.482998ms)
Jun 17 15:40:51.923: INFO: (1) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 17.624124ms)
Jun 17 15:40:51.923: INFO: (1) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 17.579959ms)
Jun 17 15:40:51.924: INFO: (1) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 18.151373ms)
Jun 17 15:40:51.924: INFO: (1) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 18.761178ms)
Jun 17 15:40:51.925: INFO: (1) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 19.628161ms)
Jun 17 15:40:51.925: INFO: (1) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 19.530145ms)
Jun 17 15:40:51.926: INFO: (1) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 19.700801ms)
Jun 17 15:40:51.926: INFO: (1) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 19.74416ms)
Jun 17 15:40:51.926: INFO: (1) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 19.870038ms)
Jun 17 15:40:51.938: INFO: (2) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 10.811136ms)
Jun 17 15:40:51.938: INFO: (2) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 11.704881ms)
Jun 17 15:40:51.939: INFO: (2) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 11.810311ms)
Jun 17 15:40:51.939: INFO: (2) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 12.212178ms)
Jun 17 15:40:51.939: INFO: (2) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 13.051983ms)
Jun 17 15:40:51.939: INFO: (2) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 13.033592ms)
Jun 17 15:40:51.939: INFO: (2) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 12.798978ms)
Jun 17 15:40:51.940: INFO: (2) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 14.168574ms)
Jun 17 15:40:51.940: INFO: (2) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 13.293278ms)
Jun 17 15:40:51.940: INFO: (2) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 14.168033ms)
Jun 17 15:40:51.940: INFO: (2) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 13.717403ms)
Jun 17 15:40:51.942: INFO: (2) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 16.342089ms)
Jun 17 15:40:51.943: INFO: (2) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 16.59758ms)
Jun 17 15:40:51.943: INFO: (2) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 16.586928ms)
Jun 17 15:40:51.943: INFO: (2) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 16.09274ms)
Jun 17 15:40:51.943: INFO: (2) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 16.353788ms)
Jun 17 15:40:51.956: INFO: (3) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 12.808014ms)
Jun 17 15:40:51.956: INFO: (3) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 13.08061ms)
Jun 17 15:40:51.957: INFO: (3) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 13.288695ms)
Jun 17 15:40:51.957: INFO: (3) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 13.284301ms)
Jun 17 15:40:51.957: INFO: (3) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 13.47378ms)
Jun 17 15:40:51.957: INFO: (3) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 13.698281ms)
Jun 17 15:40:51.959: INFO: (3) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 15.98145ms)
Jun 17 15:40:51.959: INFO: (3) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 16.06178ms)
Jun 17 15:40:51.960: INFO: (3) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 16.724248ms)
Jun 17 15:40:51.960: INFO: (3) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 17.092253ms)
Jun 17 15:40:51.960: INFO: (3) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 16.878784ms)
Jun 17 15:40:51.960: INFO: (3) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 17.043688ms)
Jun 17 15:40:51.961: INFO: (3) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 17.587806ms)
Jun 17 15:40:51.962: INFO: (3) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 18.398421ms)
Jun 17 15:40:51.962: INFO: (3) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 18.546415ms)
Jun 17 15:40:51.962: INFO: (3) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 18.248915ms)
Jun 17 15:40:51.972: INFO: (4) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 10.10777ms)
Jun 17 15:40:51.973: INFO: (4) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 10.911158ms)
Jun 17 15:40:51.974: INFO: (4) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 11.024775ms)
Jun 17 15:40:51.974: INFO: (4) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 11.201158ms)
Jun 17 15:40:51.974: INFO: (4) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 11.466954ms)
Jun 17 15:40:51.974: INFO: (4) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 11.167669ms)
Jun 17 15:40:51.974: INFO: (4) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 11.652586ms)
Jun 17 15:40:51.974: INFO: (4) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 12.190678ms)
Jun 17 15:40:51.974: INFO: (4) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 11.682988ms)
Jun 17 15:40:51.974: INFO: (4) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 12.298153ms)
Jun 17 15:40:51.976: INFO: (4) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 14.011984ms)
Jun 17 15:40:51.976: INFO: (4) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 14.13274ms)
Jun 17 15:40:51.977: INFO: (4) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 14.571118ms)
Jun 17 15:40:51.978: INFO: (4) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 15.266877ms)
Jun 17 15:40:51.978: INFO: (4) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 14.92048ms)
Jun 17 15:40:51.978: INFO: (4) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 14.977102ms)
Jun 17 15:40:51.982: INFO: (5) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 4.375615ms)
Jun 17 15:40:51.985: INFO: (5) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 7.304982ms)
Jun 17 15:40:51.986: INFO: (5) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 7.708184ms)
Jun 17 15:40:51.990: INFO: (5) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 11.825289ms)
Jun 17 15:40:51.992: INFO: (5) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 13.817402ms)
Jun 17 15:40:51.992: INFO: (5) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 13.696093ms)
Jun 17 15:40:51.993: INFO: (5) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 14.092692ms)
Jun 17 15:40:51.993: INFO: (5) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 14.59675ms)
Jun 17 15:40:51.993: INFO: (5) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 15.091144ms)
Jun 17 15:40:51.993: INFO: (5) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 15.548226ms)
Jun 17 15:40:51.995: INFO: (5) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 15.633045ms)
Jun 17 15:40:51.995: INFO: (5) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 16.067235ms)
Jun 17 15:40:51.995: INFO: (5) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 16.183325ms)
Jun 17 15:40:51.995: INFO: (5) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 15.991079ms)
Jun 17 15:40:51.995: INFO: (5) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 17.106714ms)
Jun 17 15:40:51.996: INFO: (5) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 17.003173ms)
Jun 17 15:40:52.001: INFO: (6) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 5.539251ms)
Jun 17 15:40:52.007: INFO: (6) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 10.220143ms)
Jun 17 15:40:52.007: INFO: (6) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 11.066533ms)
Jun 17 15:40:52.007: INFO: (6) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 11.22008ms)
Jun 17 15:40:52.007: INFO: (6) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 11.175909ms)
Jun 17 15:40:52.008: INFO: (6) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 11.763157ms)
Jun 17 15:40:52.008: INFO: (6) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 12.536452ms)
Jun 17 15:40:52.009: INFO: (6) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 12.132401ms)
Jun 17 15:40:52.009: INFO: (6) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 12.925696ms)
Jun 17 15:40:52.010: INFO: (6) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 14.065298ms)
Jun 17 15:40:52.010: INFO: (6) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 13.385205ms)
Jun 17 15:40:52.012: INFO: (6) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 16.062315ms)
Jun 17 15:40:52.013: INFO: (6) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 16.862755ms)
Jun 17 15:40:52.014: INFO: (6) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 17.217624ms)
Jun 17 15:40:52.014: INFO: (6) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 17.424537ms)
Jun 17 15:40:52.014: INFO: (6) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 18.336448ms)
Jun 17 15:40:52.025: INFO: (7) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 10.566136ms)
Jun 17 15:40:52.025: INFO: (7) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 11.111823ms)
Jun 17 15:40:52.026: INFO: (7) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 11.290361ms)
Jun 17 15:40:52.026: INFO: (7) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 10.749736ms)
Jun 17 15:40:52.026: INFO: (7) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 11.118663ms)
Jun 17 15:40:52.026: INFO: (7) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 11.475864ms)
Jun 17 15:40:52.027: INFO: (7) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 12.117298ms)
Jun 17 15:40:52.027: INFO: (7) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 11.971665ms)
Jun 17 15:40:52.027: INFO: (7) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 13.140981ms)
Jun 17 15:40:52.028: INFO: (7) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 13.212079ms)
Jun 17 15:40:52.029: INFO: (7) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 14.204877ms)
Jun 17 15:40:52.030: INFO: (7) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 14.5639ms)
Jun 17 15:40:52.031: INFO: (7) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 16.671713ms)
Jun 17 15:40:52.031: INFO: (7) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 16.9777ms)
Jun 17 15:40:52.032: INFO: (7) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 17.213342ms)
Jun 17 15:40:52.032: INFO: (7) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 17.003015ms)
Jun 17 15:40:52.041: INFO: (8) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 8.660237ms)
Jun 17 15:40:52.043: INFO: (8) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 10.89175ms)
Jun 17 15:40:52.044: INFO: (8) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 11.437751ms)
Jun 17 15:40:52.044: INFO: (8) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 11.63695ms)
Jun 17 15:40:52.044: INFO: (8) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 11.709092ms)
Jun 17 15:40:52.045: INFO: (8) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 12.270459ms)
Jun 17 15:40:52.045: INFO: (8) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 12.148129ms)
Jun 17 15:40:52.045: INFO: (8) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 12.36116ms)
Jun 17 15:40:52.045: INFO: (8) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 12.744ms)
Jun 17 15:40:52.045: INFO: (8) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 12.94897ms)
Jun 17 15:40:52.051: INFO: (8) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 18.014681ms)
Jun 17 15:40:52.051: INFO: (8) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 18.990196ms)
Jun 17 15:40:52.051: INFO: (8) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 18.812933ms)
Jun 17 15:40:52.052: INFO: (8) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 19.228776ms)
Jun 17 15:40:52.052: INFO: (8) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 19.008021ms)
Jun 17 15:40:52.054: INFO: (8) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 21.140647ms)
Jun 17 15:40:52.064: INFO: (9) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 10.684576ms)
Jun 17 15:40:52.068: INFO: (9) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 13.15415ms)
Jun 17 15:40:52.068: INFO: (9) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 14.139767ms)
Jun 17 15:40:52.069: INFO: (9) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 14.87596ms)
Jun 17 15:40:52.069: INFO: (9) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 14.572032ms)
Jun 17 15:40:52.070: INFO: (9) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 15.690896ms)
Jun 17 15:40:52.070: INFO: (9) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 16.285773ms)
Jun 17 15:40:52.072: INFO: (9) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 17.446146ms)
Jun 17 15:40:52.072: INFO: (9) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 17.271854ms)
Jun 17 15:40:52.075: INFO: (9) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 20.423065ms)
Jun 17 15:40:52.075: INFO: (9) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 20.800106ms)
Jun 17 15:40:52.077: INFO: (9) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 22.733767ms)
Jun 17 15:40:52.077: INFO: (9) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 23.23995ms)
Jun 17 15:40:52.077: INFO: (9) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 22.857144ms)
Jun 17 15:40:52.077: INFO: (9) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 23.284491ms)
Jun 17 15:40:52.078: INFO: (9) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 23.764718ms)
Jun 17 15:40:52.091: INFO: (10) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 12.321196ms)
Jun 17 15:40:52.091: INFO: (10) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 12.990314ms)
Jun 17 15:40:52.091: INFO: (10) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 12.717713ms)
Jun 17 15:40:52.092: INFO: (10) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 13.077489ms)
Jun 17 15:40:52.092: INFO: (10) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 13.052999ms)
Jun 17 15:40:52.092: INFO: (10) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 13.510863ms)
Jun 17 15:40:52.092: INFO: (10) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 13.420147ms)
Jun 17 15:40:52.095: INFO: (10) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 16.326066ms)
Jun 17 15:40:52.095: INFO: (10) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 16.881532ms)
Jun 17 15:40:52.095: INFO: (10) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 16.422278ms)
Jun 17 15:40:52.096: INFO: (10) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 17.238598ms)
Jun 17 15:40:52.098: INFO: (10) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 19.134907ms)
Jun 17 15:40:52.099: INFO: (10) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 20.426357ms)
Jun 17 15:40:52.102: INFO: (10) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 23.035625ms)
Jun 17 15:40:52.104: INFO: (10) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 25.080973ms)
Jun 17 15:40:52.104: INFO: (10) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 25.323157ms)
Jun 17 15:40:52.117: INFO: (11) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 11.738075ms)
Jun 17 15:40:52.117: INFO: (11) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 11.63601ms)
Jun 17 15:40:52.120: INFO: (11) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 15.148975ms)
Jun 17 15:40:52.121: INFO: (11) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 15.401721ms)
Jun 17 15:40:52.121: INFO: (11) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 15.517921ms)
Jun 17 15:40:52.121: INFO: (11) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 14.572046ms)
Jun 17 15:40:52.121: INFO: (11) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 16.758209ms)
Jun 17 15:40:52.121: INFO: (11) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 14.76286ms)
Jun 17 15:40:52.121: INFO: (11) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 15.760647ms)
Jun 17 15:40:52.121: INFO: (11) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 14.96849ms)
Jun 17 15:40:52.121: INFO: (11) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 16.699614ms)
Jun 17 15:40:52.124: INFO: (11) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 17.540169ms)
Jun 17 15:40:52.124: INFO: (11) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 18.902585ms)
Jun 17 15:40:52.124: INFO: (11) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 19.176224ms)
Jun 17 15:40:52.124: INFO: (11) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 17.844141ms)
Jun 17 15:40:52.124: INFO: (11) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 19.293324ms)
Jun 17 15:40:52.130: INFO: (12) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 5.562205ms)
Jun 17 15:40:52.137: INFO: (12) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 12.844682ms)
Jun 17 15:40:52.138: INFO: (12) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 12.584996ms)
Jun 17 15:40:52.140: INFO: (12) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 14.996469ms)
Jun 17 15:40:52.140: INFO: (12) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 14.997809ms)
Jun 17 15:40:52.140: INFO: (12) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 15.040559ms)
Jun 17 15:40:52.140: INFO: (12) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 15.756794ms)
Jun 17 15:40:52.141: INFO: (12) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 15.696664ms)
Jun 17 15:40:52.142: INFO: (12) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 17.210257ms)
Jun 17 15:40:52.142: INFO: (12) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 17.809195ms)
Jun 17 15:40:52.143: INFO: (12) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 17.683189ms)
Jun 17 15:40:52.143: INFO: (12) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 17.989117ms)
Jun 17 15:40:52.144: INFO: (12) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 19.423684ms)
Jun 17 15:40:52.145: INFO: (12) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 20.101246ms)
Jun 17 15:40:52.148: INFO: (12) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 22.705242ms)
Jun 17 15:40:52.149: INFO: (12) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 24.600066ms)
Jun 17 15:40:52.162: INFO: (13) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 12.226266ms)
Jun 17 15:40:52.162: INFO: (13) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 12.380585ms)
Jun 17 15:40:52.162: INFO: (13) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 12.957246ms)
Jun 17 15:40:52.163: INFO: (13) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 13.487071ms)
Jun 17 15:40:52.163: INFO: (13) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 14.015957ms)
Jun 17 15:40:52.164: INFO: (13) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 14.5828ms)
Jun 17 15:40:52.164: INFO: (13) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 14.496941ms)
Jun 17 15:40:52.165: INFO: (13) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 15.136483ms)
Jun 17 15:40:52.165: INFO: (13) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 15.080398ms)
Jun 17 15:40:52.165: INFO: (13) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 15.254088ms)
Jun 17 15:40:52.165: INFO: (13) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 15.265726ms)
Jun 17 15:40:52.168: INFO: (13) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 18.090174ms)
Jun 17 15:40:52.168: INFO: (13) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 18.445068ms)
Jun 17 15:40:52.168: INFO: (13) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 18.568139ms)
Jun 17 15:40:52.168: INFO: (13) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 18.952008ms)
Jun 17 15:40:52.169: INFO: (13) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 18.889601ms)
Jun 17 15:40:52.177: INFO: (14) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 7.970522ms)
Jun 17 15:40:52.178: INFO: (14) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 8.887355ms)
Jun 17 15:40:52.178: INFO: (14) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 8.886119ms)
Jun 17 15:40:52.178: INFO: (14) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 9.122583ms)
Jun 17 15:40:52.181: INFO: (14) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 11.509019ms)
Jun 17 15:40:52.181: INFO: (14) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 12.408544ms)
Jun 17 15:40:52.181: INFO: (14) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 12.379471ms)
Jun 17 15:40:52.183: INFO: (14) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 13.564943ms)
Jun 17 15:40:52.183: INFO: (14) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 13.937342ms)
Jun 17 15:40:52.183: INFO: (14) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 13.965019ms)
Jun 17 15:40:52.184: INFO: (14) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 14.52609ms)
Jun 17 15:40:52.184: INFO: (14) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 14.558887ms)
Jun 17 15:40:52.186: INFO: (14) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 17.350587ms)
Jun 17 15:40:52.187: INFO: (14) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 17.369432ms)
Jun 17 15:40:52.187: INFO: (14) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 17.419968ms)
Jun 17 15:40:52.187: INFO: (14) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 17.482657ms)
Jun 17 15:40:52.193: INFO: (15) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 6.430358ms)
Jun 17 15:40:52.200: INFO: (15) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 12.974413ms)
Jun 17 15:40:52.200: INFO: (15) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 13.552719ms)
Jun 17 15:40:52.201: INFO: (15) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 13.199092ms)
Jun 17 15:40:52.201: INFO: (15) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 13.417524ms)
Jun 17 15:40:52.201: INFO: (15) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 13.640443ms)
Jun 17 15:40:52.202: INFO: (15) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 14.341793ms)
Jun 17 15:40:52.202: INFO: (15) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 14.054085ms)
Jun 17 15:40:52.202: INFO: (15) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 14.642553ms)
Jun 17 15:40:52.202: INFO: (15) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 14.119357ms)
Jun 17 15:40:52.202: INFO: (15) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 14.422892ms)
Jun 17 15:40:52.202: INFO: (15) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 14.73215ms)
Jun 17 15:40:52.203: INFO: (15) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 15.142195ms)
Jun 17 15:40:52.204: INFO: (15) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 16.679717ms)
Jun 17 15:40:52.205: INFO: (15) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 17.471033ms)
Jun 17 15:40:52.205: INFO: (15) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 17.77302ms)
Jun 17 15:40:52.212: INFO: (16) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 6.557332ms)
Jun 17 15:40:52.213: INFO: (16) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 7.72543ms)
Jun 17 15:40:52.216: INFO: (16) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 10.021574ms)
Jun 17 15:40:52.216: INFO: (16) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 10.301132ms)
Jun 17 15:40:52.216: INFO: (16) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 10.806028ms)
Jun 17 15:40:52.217: INFO: (16) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 10.953167ms)
Jun 17 15:40:52.217: INFO: (16) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 11.060396ms)
Jun 17 15:40:52.217: INFO: (16) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 11.275401ms)
Jun 17 15:40:52.223: INFO: (16) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 17.13525ms)
Jun 17 15:40:52.230: INFO: (16) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 24.067489ms)
Jun 17 15:40:52.230: INFO: (16) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 24.138715ms)
Jun 17 15:40:52.232: INFO: (16) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 25.945786ms)
Jun 17 15:40:52.232: INFO: (16) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 26.065207ms)
Jun 17 15:40:52.234: INFO: (16) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 27.717021ms)
Jun 17 15:40:52.234: INFO: (16) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 28.200622ms)
Jun 17 15:40:52.234: INFO: (16) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 28.892251ms)
Jun 17 15:40:52.257: INFO: (17) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 22.925094ms)
Jun 17 15:40:52.258: INFO: (17) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 23.043913ms)
Jun 17 15:40:52.258: INFO: (17) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 24.044611ms)
Jun 17 15:40:52.259: INFO: (17) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 23.843493ms)
Jun 17 15:40:52.259: INFO: (17) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 24.330407ms)
Jun 17 15:40:52.259: INFO: (17) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 24.724348ms)
Jun 17 15:40:52.259: INFO: (17) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 24.782918ms)
Jun 17 15:40:52.260: INFO: (17) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 25.375418ms)
Jun 17 15:40:52.260: INFO: (17) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 26.026544ms)
Jun 17 15:40:52.261: INFO: (17) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 26.401036ms)
Jun 17 15:40:52.261: INFO: (17) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 26.94783ms)
Jun 17 15:40:52.263: INFO: (17) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 29.033446ms)
Jun 17 15:40:52.264: INFO: (17) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 29.080779ms)
Jun 17 15:40:52.266: INFO: (17) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 30.967566ms)
Jun 17 15:40:52.266: INFO: (17) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 31.161268ms)
Jun 17 15:40:52.267: INFO: (17) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 32.030152ms)
Jun 17 15:40:52.276: INFO: (18) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 9.764185ms)
Jun 17 15:40:52.279: INFO: (18) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 11.973854ms)
Jun 17 15:40:52.280: INFO: (18) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 13.122086ms)
Jun 17 15:40:52.280: INFO: (18) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 12.284839ms)
Jun 17 15:40:52.280: INFO: (18) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 12.855368ms)
Jun 17 15:40:52.280: INFO: (18) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 13.258133ms)
Jun 17 15:40:52.282: INFO: (18) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 15.033407ms)
Jun 17 15:40:52.283: INFO: (18) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 15.500247ms)
Jun 17 15:40:52.284: INFO: (18) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 16.736619ms)
Jun 17 15:40:52.284: INFO: (18) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 16.828242ms)
Jun 17 15:40:52.284: INFO: (18) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 16.713713ms)
Jun 17 15:40:52.286: INFO: (18) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 19.320985ms)
Jun 17 15:40:52.288: INFO: (18) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 20.776385ms)
Jun 17 15:40:52.288: INFO: (18) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 20.862947ms)
Jun 17 15:40:52.289: INFO: (18) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 21.504735ms)
Jun 17 15:40:52.290: INFO: (18) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 22.110879ms)
Jun 17 15:40:52.297: INFO: (19) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 6.957374ms)
Jun 17 15:40:52.297: INFO: (19) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 7.651139ms)
Jun 17 15:40:52.299: INFO: (19) /api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/http:proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">t... (200; 8.095559ms)
Jun 17 15:40:52.299: INFO: (19) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:443/proxy/tlsrewriteme... (200; 8.903134ms)
Jun 17 15:40:52.299: INFO: (19) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7/proxy/rewriteme">test</a> (200; 8.911293ms)
Jun 17 15:40:52.302: INFO: (19) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:160/proxy/: foo (200; 11.538405ms)
Jun 17 15:40:52.303: INFO: (19) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname2/proxy/: bar (200; 12.846798ms)
Jun 17 15:40:52.303: INFO: (19) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname1/proxy/: tls baz (200; 13.068061ms)
Jun 17 15:40:52.303: INFO: (19) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:162/proxy/: bar (200; 12.83511ms)
Jun 17 15:40:52.304: INFO: (19) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:462/proxy/: tls qux (200; 13.100667ms)
Jun 17 15:40:52.304: INFO: (19) /api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/: <a href="/api/v1/namespaces/proxy-399/pods/proxy-service-wqczg-rwwn7:1080/proxy/rewriteme">test</... (200; 13.287636ms)
Jun 17 15:40:52.305: INFO: (19) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname2/proxy/: bar (200; 14.598351ms)
Jun 17 15:40:52.305: INFO: (19) /api/v1/namespaces/proxy-399/pods/https:proxy-service-wqczg-rwwn7:460/proxy/: tls baz (200; 14.04139ms)
Jun 17 15:40:52.306: INFO: (19) /api/v1/namespaces/proxy-399/services/https:proxy-service-wqczg:tlsportname2/proxy/: tls qux (200; 14.732228ms)
Jun 17 15:40:52.306: INFO: (19) /api/v1/namespaces/proxy-399/services/proxy-service-wqczg:portname1/proxy/: foo (200; 14.863749ms)
Jun 17 15:40:52.306: INFO: (19) /api/v1/namespaces/proxy-399/services/http:proxy-service-wqczg:portname1/proxy/: foo (200; 14.840246ms)
STEP: deleting ReplicationController proxy-service-wqczg in namespace proxy-399, will wait for the garbage collector to delete the pods
Jun 17 15:40:52.371: INFO: Deleting ReplicationController proxy-service-wqczg took: 10.825335ms
Jun 17 15:40:52.671: INFO: Terminating ReplicationController proxy-service-wqczg pods took: 300.201547ms
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:41:03.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-399" for this suite.
Jun 17 15:41:09.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:41:09.536: INFO: namespace proxy-399 deletion completed in 6.158909737s

• [SLOW TEST:22.907 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:41:09.538: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9786
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:41:13.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9786" for this suite.
Jun 17 15:41:19.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:41:19.876: INFO: namespace kubelet-test-9786 deletion completed in 6.147670088s

• [SLOW TEST:10.339 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:41:19.877: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5769
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jun 17 15:41:20.048: INFO: Waiting up to 5m0s for pod "downward-api-5a36dcee-9116-11e9-a8b9-dace53c98186" in namespace "downward-api-5769" to be "success or failure"
Jun 17 15:41:20.053: INFO: Pod "downward-api-5a36dcee-9116-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 5.483465ms
Jun 17 15:41:22.058: INFO: Pod "downward-api-5a36dcee-9116-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010338443s
Jun 17 15:41:24.063: INFO: Pod "downward-api-5a36dcee-9116-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01546396s
STEP: Saw pod success
Jun 17 15:41:24.063: INFO: Pod "downward-api-5a36dcee-9116-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:41:24.071: INFO: Trying to get logs from node lab1-k8s-node-2 pod downward-api-5a36dcee-9116-11e9-a8b9-dace53c98186 container dapi-container: <nil>
STEP: delete the pod
Jun 17 15:41:24.098: INFO: Waiting for pod downward-api-5a36dcee-9116-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:41:24.103: INFO: Pod downward-api-5a36dcee-9116-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:41:24.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5769" for this suite.
Jun 17 15:41:30.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:41:30.281: INFO: namespace downward-api-5769 deletion completed in 6.169921929s

• [SLOW TEST:10.404 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:41:30.285: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 15:41:30.469: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 17 15:41:35.475: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 17 15:41:35.475: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 17 15:41:37.479: INFO: Creating deployment "test-rollover-deployment"
Jun 17 15:41:37.490: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 17 15:41:39.504: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 17 15:41:39.513: INFO: Ensure that both replica sets have 1 created replica
Jun 17 15:41:39.521: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 17 15:41:39.531: INFO: Updating deployment test-rollover-deployment
Jun 17 15:41:39.531: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 17 15:41:41.540: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 17 15:41:41.548: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 17 15:41:41.558: INFO: all replica sets need to contain the pod-template-hash label
Jun 17 15:41:41.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382897, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382897, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382899, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382897, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 17 15:41:43.567: INFO: all replica sets need to contain the pod-template-hash label
Jun 17 15:41:43.567: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382897, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382897, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382901, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382897, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 17 15:41:45.567: INFO: all replica sets need to contain the pod-template-hash label
Jun 17 15:41:45.567: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382897, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382897, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382901, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382897, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 17 15:41:47.566: INFO: all replica sets need to contain the pod-template-hash label
Jun 17 15:41:47.567: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382897, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382897, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382901, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382897, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 17 15:41:49.566: INFO: all replica sets need to contain the pod-template-hash label
Jun 17 15:41:49.566: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382897, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382897, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382901, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382897, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 17 15:41:51.568: INFO: all replica sets need to contain the pod-template-hash label
Jun 17 15:41:51.568: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382897, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382897, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382901, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696382897, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 17 15:41:53.567: INFO: 
Jun 17 15:41:53.567: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jun 17 15:41:53.582: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-2109,SelfLink:/apis/apps/v1/namespaces/deployment-2109/deployments/test-rollover-deployment,UID:649cee34-9116-11e9-bdd5-fa163e38c6bd,ResourceVersion:28660,Generation:2,CreationTimestamp:2019-06-17 15:41:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-17 15:41:37 +0000 UTC 2019-06-17 15:41:37 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-17 15:41:52 +0000 UTC 2019-06-17 15:41:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 17 15:41:53.586: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-2109,SelfLink:/apis/apps/v1/namespaces/deployment-2109/replicasets/test-rollover-deployment-766b4d6c9d,UID:65d5f090-9116-11e9-bdd5-fa163e38c6bd,ResourceVersion:28649,Generation:2,CreationTimestamp:2019-06-17 15:41:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 649cee34-9116-11e9-bdd5-fa163e38c6bd 0xc0025f9917 0xc0025f9918}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 17 15:41:53.587: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 17 15:41:53.587: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-2109,SelfLink:/apis/apps/v1/namespaces/deployment-2109/replicasets/test-rollover-controller,UID:606cf1fa-9116-11e9-bdd5-fa163e38c6bd,ResourceVersion:28659,Generation:2,CreationTimestamp:2019-06-17 15:41:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 649cee34-9116-11e9-bdd5-fa163e38c6bd 0xc0025f9767 0xc0025f9768}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 17 15:41:53.587: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-2109,SelfLink:/apis/apps/v1/namespaces/deployment-2109/replicasets/test-rollover-deployment-6455657675,UID:649fd2e3-9116-11e9-bdd5-fa163e38c6bd,ResourceVersion:28607,Generation:2,CreationTimestamp:2019-06-17 15:41:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 649cee34-9116-11e9-bdd5-fa163e38c6bd 0xc0025f9837 0xc0025f9838}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 17 15:41:53.591: INFO: Pod "test-rollover-deployment-766b4d6c9d-2l7qv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-2l7qv,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-2109,SelfLink:/api/v1/namespaces/deployment-2109/pods/test-rollover-deployment-766b4d6c9d-2l7qv,UID:65dbf487-9116-11e9-bdd5-fa163e38c6bd,ResourceVersion:28627,Generation:0,CreationTimestamp:2019-06-17 15:41:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d 65d5f090-9116-11e9-bdd5-fa163e38c6bd 0xc000699207 0xc000699208}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-46mcf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-46mcf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-46mcf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000699280} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0006992a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:41:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:41:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:41:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:41:39 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.4,PodIP:10.233.95.122,StartTime:2019-06-17 15:41:39 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-17 15:41:40 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://7e6014e8126f402b795c9b21ac901e1b586b45c3ecf39cced819d865ed84441e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:41:53.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2109" for this suite.
Jun 17 15:41:59.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:41:59.744: INFO: namespace deployment-2109 deletion completed in 6.146432016s

• [SLOW TEST:29.460 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:41:59.745: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-952
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Jun 17 15:41:59.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 create -f - --namespace=kubectl-952'
Jun 17 15:42:00.123: INFO: stderr: ""
Jun 17 15:42:00.123: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 17 15:42:00.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-952'
Jun 17 15:42:00.210: INFO: stderr: ""
Jun 17 15:42:00.210: INFO: stdout: "update-demo-nautilus-4cbbd update-demo-nautilus-4zw8n "
Jun 17 15:42:00.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-4cbbd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-952'
Jun 17 15:42:00.290: INFO: stderr: ""
Jun 17 15:42:00.290: INFO: stdout: ""
Jun 17 15:42:00.290: INFO: update-demo-nautilus-4cbbd is created but not running
Jun 17 15:42:05.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-952'
Jun 17 15:42:05.397: INFO: stderr: ""
Jun 17 15:42:05.397: INFO: stdout: "update-demo-nautilus-4cbbd update-demo-nautilus-4zw8n "
Jun 17 15:42:05.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-4cbbd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-952'
Jun 17 15:42:05.484: INFO: stderr: ""
Jun 17 15:42:05.484: INFO: stdout: "true"
Jun 17 15:42:05.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-4cbbd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-952'
Jun 17 15:42:05.565: INFO: stderr: ""
Jun 17 15:42:05.565: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 17 15:42:05.565: INFO: validating pod update-demo-nautilus-4cbbd
Jun 17 15:42:05.572: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 17 15:42:05.572: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 17 15:42:05.572: INFO: update-demo-nautilus-4cbbd is verified up and running
Jun 17 15:42:05.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-4zw8n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-952'
Jun 17 15:42:05.651: INFO: stderr: ""
Jun 17 15:42:05.651: INFO: stdout: "true"
Jun 17 15:42:05.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods update-demo-nautilus-4zw8n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-952'
Jun 17 15:42:05.734: INFO: stderr: ""
Jun 17 15:42:05.734: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 17 15:42:05.734: INFO: validating pod update-demo-nautilus-4zw8n
Jun 17 15:42:05.742: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 17 15:42:05.742: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 17 15:42:05.742: INFO: update-demo-nautilus-4zw8n is verified up and running
STEP: using delete to clean up resources
Jun 17 15:42:05.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 delete --grace-period=0 --force -f - --namespace=kubectl-952'
Jun 17 15:42:05.832: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 17 15:42:05.832: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 17 15:42:05.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-952'
Jun 17 15:42:05.932: INFO: stderr: "No resources found.\n"
Jun 17 15:42:05.932: INFO: stdout: ""
Jun 17 15:42:05.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -l name=update-demo --namespace=kubectl-952 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 17 15:42:06.010: INFO: stderr: ""
Jun 17 15:42:06.010: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:42:06.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-952" for this suite.
Jun 17 15:42:28.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:42:28.170: INFO: namespace kubectl-952 deletion completed in 22.153663002s

• [SLOW TEST:28.425 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:42:28.171: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6856
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 15:42:28.355: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 17 15:42:28.367: INFO: Number of nodes with available pods: 0
Jun 17 15:42:28.367: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun 17 15:42:28.392: INFO: Number of nodes with available pods: 0
Jun 17 15:42:28.392: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 15:42:29.396: INFO: Number of nodes with available pods: 0
Jun 17 15:42:29.397: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 15:42:30.397: INFO: Number of nodes with available pods: 0
Jun 17 15:42:30.397: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 15:42:31.397: INFO: Number of nodes with available pods: 1
Jun 17 15:42:31.397: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 17 15:42:31.429: INFO: Number of nodes with available pods: 1
Jun 17 15:42:31.429: INFO: Number of running nodes: 0, number of available pods: 1
Jun 17 15:42:32.434: INFO: Number of nodes with available pods: 0
Jun 17 15:42:32.435: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 17 15:42:32.447: INFO: Number of nodes with available pods: 0
Jun 17 15:42:32.447: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 15:42:33.452: INFO: Number of nodes with available pods: 0
Jun 17 15:42:33.452: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 15:42:34.453: INFO: Number of nodes with available pods: 0
Jun 17 15:42:34.453: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 15:42:35.452: INFO: Number of nodes with available pods: 0
Jun 17 15:42:35.452: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 15:42:36.452: INFO: Number of nodes with available pods: 0
Jun 17 15:42:36.452: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 15:42:37.453: INFO: Number of nodes with available pods: 0
Jun 17 15:42:37.453: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 15:42:38.452: INFO: Number of nodes with available pods: 0
Jun 17 15:42:38.452: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 15:42:39.452: INFO: Number of nodes with available pods: 0
Jun 17 15:42:39.452: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 15:42:40.452: INFO: Number of nodes with available pods: 0
Jun 17 15:42:40.453: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 15:42:41.452: INFO: Number of nodes with available pods: 0
Jun 17 15:42:41.452: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 15:42:42.452: INFO: Number of nodes with available pods: 0
Jun 17 15:42:42.452: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 15:42:43.452: INFO: Number of nodes with available pods: 0
Jun 17 15:42:43.452: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 15:42:44.453: INFO: Number of nodes with available pods: 0
Jun 17 15:42:44.453: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Jun 17 15:42:45.453: INFO: Number of nodes with available pods: 1
Jun 17 15:42:45.453: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6856, will wait for the garbage collector to delete the pods
Jun 17 15:42:45.526: INFO: Deleting DaemonSet.extensions daemon-set took: 10.216494ms
Jun 17 15:42:45.826: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.210573ms
Jun 17 15:42:52.931: INFO: Number of nodes with available pods: 0
Jun 17 15:42:52.931: INFO: Number of running nodes: 0, number of available pods: 0
Jun 17 15:42:52.935: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6856/daemonsets","resourceVersion":"28975"},"items":null}

Jun 17 15:42:52.939: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6856/pods","resourceVersion":"28975"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:42:52.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6856" for this suite.
Jun 17 15:42:58.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:42:59.119: INFO: namespace daemonsets-6856 deletion completed in 6.143582474s

• [SLOW TEST:30.949 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:42:59.120: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1617
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1617
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-1617
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1617
Jun 17 15:42:59.318: INFO: Found 0 stateful pods, waiting for 1
Jun 17 15:43:09.323: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 17 15:43:09.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 17 15:43:09.603: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 17 15:43:09.603: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 17 15:43:09.603: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 17 15:43:09.608: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 17 15:43:09.608: INFO: Waiting for statefulset status.replicas updated to 0
Jun 17 15:43:09.633: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Jun 17 15:43:09.633: INFO: ss-0  lab1-k8s-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  }]
Jun 17 15:43:09.634: INFO: 
Jun 17 15:43:09.634: INFO: StatefulSet ss has not reached scale 3, at 1
Jun 17 15:43:10.640: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99191198s
Jun 17 15:43:11.646: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985274807s
Jun 17 15:43:12.651: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979938207s
Jun 17 15:43:13.656: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974544596s
Jun 17 15:43:14.662: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.969143261s
Jun 17 15:43:15.668: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963351803s
Jun 17 15:43:16.673: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957924123s
Jun 17 15:43:17.678: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.952757205s
Jun 17 15:43:18.686: INFO: Verifying statefulset ss doesn't scale past 3 for another 947.329472ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1617
Jun 17 15:43:19.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:43:19.967: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 17 15:43:19.967: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 17 15:43:19.967: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 17 15:43:19.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:43:20.223: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 17 15:43:20.223: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 17 15:43:20.223: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 17 15:43:20.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:43:20.495: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 17 15:43:20.495: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 17 15:43:20.495: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 17 15:43:20.500: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jun 17 15:43:30.504: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 17 15:43:30.504: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 17 15:43:30.504: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 17 15:43:30.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 17 15:43:30.759: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 17 15:43:30.759: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 17 15:43:30.759: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 17 15:43:30.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 17 15:43:30.988: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 17 15:43:30.988: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 17 15:43:30.988: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 17 15:43:30.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 17 15:43:31.257: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 17 15:43:31.257: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 17 15:43:31.257: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 17 15:43:31.257: INFO: Waiting for statefulset status.replicas updated to 0
Jun 17 15:43:31.262: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun 17 15:43:41.271: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 17 15:43:41.271: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 17 15:43:41.271: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 17 15:43:41.287: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Jun 17 15:43:41.287: INFO: ss-0  lab1-k8s-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  }]
Jun 17 15:43:41.287: INFO: ss-1  lab1-k8s-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:41.287: INFO: ss-2  lab1-k8s-node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:41.287: INFO: 
Jun 17 15:43:41.287: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 17 15:43:42.293: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Jun 17 15:43:42.293: INFO: ss-0  lab1-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  }]
Jun 17 15:43:42.293: INFO: ss-1  lab1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:42.293: INFO: ss-2  lab1-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:42.293: INFO: 
Jun 17 15:43:42.293: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 17 15:43:43.299: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Jun 17 15:43:43.299: INFO: ss-0  lab1-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  }]
Jun 17 15:43:43.299: INFO: ss-1  lab1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:43.299: INFO: ss-2  lab1-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:43.299: INFO: 
Jun 17 15:43:43.299: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 17 15:43:44.304: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Jun 17 15:43:44.304: INFO: ss-0  lab1-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  }]
Jun 17 15:43:44.304: INFO: ss-1  lab1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:44.304: INFO: ss-2  lab1-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:44.304: INFO: 
Jun 17 15:43:44.304: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 17 15:43:45.309: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Jun 17 15:43:45.310: INFO: ss-0  lab1-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  }]
Jun 17 15:43:45.310: INFO: ss-1  lab1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:45.311: INFO: ss-2  lab1-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:45.311: INFO: 
Jun 17 15:43:45.311: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 17 15:43:46.316: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Jun 17 15:43:46.316: INFO: ss-0  lab1-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  }]
Jun 17 15:43:46.316: INFO: ss-1  lab1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:46.316: INFO: ss-2  lab1-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:46.316: INFO: 
Jun 17 15:43:46.316: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 17 15:43:47.322: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Jun 17 15:43:47.322: INFO: ss-0  lab1-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  }]
Jun 17 15:43:47.322: INFO: ss-1  lab1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:47.322: INFO: ss-2  lab1-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:47.322: INFO: 
Jun 17 15:43:47.322: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 17 15:43:48.328: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Jun 17 15:43:48.328: INFO: ss-0  lab1-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  }]
Jun 17 15:43:48.328: INFO: ss-1  lab1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:48.328: INFO: ss-2  lab1-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:48.328: INFO: 
Jun 17 15:43:48.328: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 17 15:43:49.333: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Jun 17 15:43:49.334: INFO: ss-0  lab1-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  }]
Jun 17 15:43:49.334: INFO: ss-1  lab1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:49.334: INFO: ss-2  lab1-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:49.334: INFO: 
Jun 17 15:43:49.334: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 17 15:43:50.340: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Jun 17 15:43:50.340: INFO: ss-0  lab1-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:42:59 +0000 UTC  }]
Jun 17 15:43:50.341: INFO: ss-1  lab1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:50.341: INFO: ss-2  lab1-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:43:09 +0000 UTC  }]
Jun 17 15:43:50.341: INFO: 
Jun 17 15:43:50.341: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1617
Jun 17 15:43:51.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:43:51.473: INFO: rc: 1
Jun 17 15:43:51.473: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc00174be60 exit status 1 <nil> <nil> true [0xc0028121d0 0xc0028121e8 0xc002812200] [0xc0028121d0 0xc0028121e8 0xc002812200] [0xc0028121e0 0xc0028121f8] [0x9c00a0 0x9c00a0] 0xc0014d83c0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Jun 17 15:44:01.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:44:01.557: INFO: rc: 1
Jun 17 15:44:01.558: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002f7a210 exit status 1 <nil> <nil> true [0xc002812208 0xc002812220 0xc002812238] [0xc002812208 0xc002812220 0xc002812238] [0xc002812218 0xc002812230] [0x9c00a0 0x9c00a0] 0xc0014d8840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:44:11.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:44:11.641: INFO: rc: 1
Jun 17 15:44:11.641: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002f7a5a0 exit status 1 <nil> <nil> true [0xc002812240 0xc002812258 0xc002812270] [0xc002812240 0xc002812258 0xc002812270] [0xc002812250 0xc002812268] [0x9c00a0 0x9c00a0] 0xc0014d8f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:44:21.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:44:21.732: INFO: rc: 1
Jun 17 15:44:21.732: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002f7aa80 exit status 1 <nil> <nil> true [0xc002812278 0xc002812290 0xc0028122a8] [0xc002812278 0xc002812290 0xc0028122a8] [0xc002812288 0xc0028122a0] [0x9c00a0 0x9c00a0] 0xc0014d9620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:44:31.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:44:31.816: INFO: rc: 1
Jun 17 15:44:31.816: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002f7ae10 exit status 1 <nil> <nil> true [0xc0028122b0 0xc0028122c8 0xc0028122e0] [0xc0028122b0 0xc0028122c8 0xc0028122e0] [0xc0028122c0 0xc0028122d8] [0x9c00a0 0x9c00a0] 0xc0014d9aa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:44:41.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:44:41.896: INFO: rc: 1
Jun 17 15:44:41.896: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0014e6fc0 exit status 1 <nil> <nil> true [0xc0019e82d0 0xc0019e82e8 0xc0019e8300] [0xc0019e82d0 0xc0019e82e8 0xc0019e8300] [0xc0019e82e0 0xc0019e82f8] [0x9c00a0 0x9c00a0] 0xc0030a5c80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:44:51.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:44:51.986: INFO: rc: 1
Jun 17 15:44:51.986: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002f7b1a0 exit status 1 <nil> <nil> true [0xc0028122e8 0xc002812300 0xc002812318] [0xc0028122e8 0xc002812300 0xc002812318] [0xc0028122f8 0xc002812310] [0x9c00a0 0x9c00a0] 0xc0014d9f20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:45:01.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:45:02.085: INFO: rc: 1
Jun 17 15:45:02.085: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002f7b530 exit status 1 <nil> <nil> true [0xc002812320 0xc002812338 0xc002812350] [0xc002812320 0xc002812338 0xc002812350] [0xc002812330 0xc002812348] [0x9c00a0 0x9c00a0] 0xc0020d4300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:45:12.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:45:12.167: INFO: rc: 1
Jun 17 15:45:12.167: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0014e7380 exit status 1 <nil> <nil> true [0xc0019e8308 0xc0019e8320 0xc0019e8360] [0xc0019e8308 0xc0019e8320 0xc0019e8360] [0xc0019e8318 0xc0019e8348] [0x9c00a0 0x9c00a0] 0xc002892420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:45:22.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:45:22.272: INFO: rc: 1
Jun 17 15:45:22.273: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00174a360 exit status 1 <nil> <nil> true [0xc002812008 0xc002812020 0xc002812038] [0xc002812008 0xc002812020 0xc002812038] [0xc002812018 0xc002812030] [0x9c00a0 0x9c00a0] 0xc0014d83c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:45:32.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:45:32.373: INFO: rc: 1
Jun 17 15:45:32.379: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002848360 exit status 1 <nil> <nil> true [0xc0019e8000 0xc0019e8018 0xc0019e8030] [0xc0019e8000 0xc0019e8018 0xc0019e8030] [0xc0019e8010 0xc0019e8028] [0x9c00a0 0x9c00a0] 0xc0030a4000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:45:42.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:45:42.461: INFO: rc: 1
Jun 17 15:45:42.464: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0028486f0 exit status 1 <nil> <nil> true [0xc0019e8048 0xc0019e8060 0xc0019e8078] [0xc0019e8048 0xc0019e8060 0xc0019e8078] [0xc0019e8058 0xc0019e8070] [0x9c00a0 0x9c00a0] 0xc0030a4720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:45:52.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:45:52.532: INFO: rc: 1
Jun 17 15:45:52.532: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00174a750 exit status 1 <nil> <nil> true [0xc002812040 0xc002812058 0xc002812070] [0xc002812040 0xc002812058 0xc002812070] [0xc002812050 0xc002812068] [0x9c00a0 0x9c00a0] 0xc0014d8840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:46:02.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:46:02.609: INFO: rc: 1
Jun 17 15:46:02.609: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002848ba0 exit status 1 <nil> <nil> true [0xc0019e8080 0xc0019e8098 0xc0019e80b0] [0xc0019e8080 0xc0019e8098 0xc0019e80b0] [0xc0019e8090 0xc0019e80a8] [0x9c00a0 0x9c00a0] 0xc0030a4ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:46:12.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:46:12.680: INFO: rc: 1
Jun 17 15:46:12.680: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002848f60 exit status 1 <nil> <nil> true [0xc0019e80b8 0xc0019e80d0 0xc0019e80e8] [0xc0019e80b8 0xc0019e80d0 0xc0019e80e8] [0xc0019e80c8 0xc0019e80e0] [0x9c00a0 0x9c00a0] 0xc0030a58c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:46:22.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:46:22.764: INFO: rc: 1
Jun 17 15:46:22.764: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00174ab40 exit status 1 <nil> <nil> true [0xc002812078 0xc002812090 0xc0028120a8] [0xc002812078 0xc002812090 0xc0028120a8] [0xc002812088 0xc0028120a0] [0x9c00a0 0x9c00a0] 0xc0014d8f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:46:32.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:46:32.848: INFO: rc: 1
Jun 17 15:46:32.848: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00174af30 exit status 1 <nil> <nil> true [0xc0028120b0 0xc0028120c8 0xc0028120e0] [0xc0028120b0 0xc0028120c8 0xc0028120e0] [0xc0028120c0 0xc0028120d8] [0x9c00a0 0x9c00a0] 0xc0014d9620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:46:42.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:46:42.924: INFO: rc: 1
Jun 17 15:46:42.924: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0028492f0 exit status 1 <nil> <nil> true [0xc0019e80f0 0xc0019e8108 0xc0019e8120] [0xc0019e80f0 0xc0019e8108 0xc0019e8120] [0xc0019e8100 0xc0019e8118] [0x9c00a0 0x9c00a0] 0xc002ffc0c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:46:52.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:46:53.022: INFO: rc: 1
Jun 17 15:46:53.023: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00174b590 exit status 1 <nil> <nil> true [0xc0028120e8 0xc002812100 0xc002812118] [0xc0028120e8 0xc002812100 0xc002812118] [0xc0028120f8 0xc002812110] [0x9c00a0 0x9c00a0] 0xc0014d9aa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:47:03.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:47:03.107: INFO: rc: 1
Jun 17 15:47:03.107: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00174be30 exit status 1 <nil> <nil> true [0xc002812120 0xc002812138 0xc002812158] [0xc002812120 0xc002812138 0xc002812158] [0xc002812130 0xc002812148] [0x9c00a0 0x9c00a0] 0xc0014d9f20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:47:13.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:47:13.186: INFO: rc: 1
Jun 17 15:47:13.186: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002eca1e0 exit status 1 <nil> <nil> true [0xc002812160 0xc002812178 0xc002812190] [0xc002812160 0xc002812178 0xc002812190] [0xc002812170 0xc002812188] [0x9c00a0 0x9c00a0] 0xc00304c600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:47:23.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:47:23.270: INFO: rc: 1
Jun 17 15:47:23.270: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002eca570 exit status 1 <nil> <nil> true [0xc0028121a0 0xc0028121b8 0xc0028121d0] [0xc0028121a0 0xc0028121b8 0xc0028121d0] [0xc0028121b0 0xc0028121c8] [0x9c00a0 0x9c00a0] 0xc00304cc60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:47:33.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:47:33.353: INFO: rc: 1
Jun 17 15:47:33.354: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002848330 exit status 1 <nil> <nil> true [0xc0019e8000 0xc0019e8018 0xc0019e8030] [0xc0019e8000 0xc0019e8018 0xc0019e8030] [0xc0019e8010 0xc0019e8028] [0x9c00a0 0x9c00a0] 0xc0030a45a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:47:43.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:47:43.429: INFO: rc: 1
Jun 17 15:47:43.429: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002848720 exit status 1 <nil> <nil> true [0xc0019e8048 0xc0019e8060 0xc0019e8078] [0xc0019e8048 0xc0019e8060 0xc0019e8078] [0xc0019e8058 0xc0019e8070] [0x9c00a0 0x9c00a0] 0xc0030a4c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:47:53.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:47:53.506: INFO: rc: 1
Jun 17 15:47:53.507: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002848c00 exit status 1 <nil> <nil> true [0xc0019e8080 0xc0019e8098 0xc0019e80b0] [0xc0019e8080 0xc0019e8098 0xc0019e80b0] [0xc0019e8090 0xc0019e80a8] [0x9c00a0 0x9c00a0] 0xc0030a5620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:48:03.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:48:03.579: INFO: rc: 1
Jun 17 15:48:03.579: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00174a3f0 exit status 1 <nil> <nil> true [0xc002812000 0xc002812018 0xc002812030] [0xc002812000 0xc002812018 0xc002812030] [0xc002812010 0xc002812028] [0x9c00a0 0x9c00a0] 0xc0014d8000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:48:13.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:48:13.659: INFO: rc: 1
Jun 17 15:48:13.659: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00174a840 exit status 1 <nil> <nil> true [0xc002812038 0xc002812050 0xc002812068] [0xc002812038 0xc002812050 0xc002812068] [0xc002812048 0xc002812060] [0x9c00a0 0x9c00a0] 0xc0014d8480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:48:23.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:48:23.743: INFO: rc: 1
Jun 17 15:48:23.743: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00174abd0 exit status 1 <nil> <nil> true [0xc002812070 0xc002812088 0xc0028120a0] [0xc002812070 0xc002812088 0xc0028120a0] [0xc002812080 0xc002812098] [0x9c00a0 0x9c00a0] 0xc0014d89c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:48:33.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:48:33.821: INFO: rc: 1
Jun 17 15:48:33.821: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002849020 exit status 1 <nil> <nil> true [0xc0019e80b8 0xc0019e80d0 0xc0019e80e8] [0xc0019e80b8 0xc0019e80d0 0xc0019e80e8] [0xc0019e80c8 0xc0019e80e0] [0x9c00a0 0x9c00a0] 0xc0030a5f20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:48:43.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:48:43.912: INFO: rc: 1
Jun 17 15:48:43.912: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00174b110 exit status 1 <nil> <nil> true [0xc0028120a8 0xc0028120c0 0xc0028120d8] [0xc0028120a8 0xc0028120c0 0xc0028120d8] [0xc0028120b8 0xc0028120d0] [0x9c00a0 0x9c00a0] 0xc0014d90e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Jun 17 15:48:53.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 exec --namespace=statefulset-1617 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 17 15:48:53.991: INFO: rc: 1
Jun 17 15:48:53.991: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Jun 17 15:48:53.991: INFO: Scaling statefulset ss to 0
Jun 17 15:48:54.004: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jun 17 15:48:54.007: INFO: Deleting all statefulset in ns statefulset-1617
Jun 17 15:48:54.011: INFO: Scaling statefulset ss to 0
Jun 17 15:48:54.023: INFO: Waiting for statefulset status.replicas updated to 0
Jun 17 15:48:54.026: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:48:54.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1617" for this suite.
Jun 17 15:49:00.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:49:00.230: INFO: namespace statefulset-1617 deletion completed in 6.170862435s

• [SLOW TEST:361.110 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:49:00.230: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4544
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 17 15:49:00.427: INFO: Waiting up to 5m0s for pod "pod-6c9f0e98-9117-11e9-a8b9-dace53c98186" in namespace "emptydir-4544" to be "success or failure"
Jun 17 15:49:00.434: INFO: Pod "pod-6c9f0e98-9117-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 6.72185ms
Jun 17 15:49:02.439: INFO: Pod "pod-6c9f0e98-9117-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011693596s
STEP: Saw pod success
Jun 17 15:49:02.439: INFO: Pod "pod-6c9f0e98-9117-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:49:02.443: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-6c9f0e98-9117-11e9-a8b9-dace53c98186 container test-container: <nil>
STEP: delete the pod
Jun 17 15:49:02.467: INFO: Waiting for pod pod-6c9f0e98-9117-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:49:02.471: INFO: Pod pod-6c9f0e98-9117-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:49:02.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4544" for this suite.
Jun 17 15:49:08.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:49:08.634: INFO: namespace emptydir-4544 deletion completed in 6.158015873s

• [SLOW TEST:8.404 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:49:08.636: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9164
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 17 15:49:14.859: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 17 15:49:14.862: INFO: Pod pod-with-prestop-http-hook still exists
Jun 17 15:49:16.863: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 17 15:49:16.868: INFO: Pod pod-with-prestop-http-hook still exists
Jun 17 15:49:18.863: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 17 15:49:18.868: INFO: Pod pod-with-prestop-http-hook still exists
Jun 17 15:49:20.863: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 17 15:49:20.868: INFO: Pod pod-with-prestop-http-hook still exists
Jun 17 15:49:22.863: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 17 15:49:22.868: INFO: Pod pod-with-prestop-http-hook still exists
Jun 17 15:49:24.863: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 17 15:49:24.868: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:49:24.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9164" for this suite.
Jun 17 15:49:46.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:49:47.029: INFO: namespace container-lifecycle-hook-9164 deletion completed in 22.145476889s

• [SLOW TEST:38.393 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:49:47.029: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6401
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-311
STEP: Creating secret with name secret-test-887fba19-9117-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume secrets
Jun 17 15:49:47.364: INFO: Waiting up to 5m0s for pod "pod-secrets-8899a2f2-9117-11e9-a8b9-dace53c98186" in namespace "secrets-6401" to be "success or failure"
Jun 17 15:49:47.369: INFO: Pod "pod-secrets-8899a2f2-9117-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.544378ms
Jun 17 15:49:49.374: INFO: Pod "pod-secrets-8899a2f2-9117-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009509209s
Jun 17 15:49:51.378: INFO: Pod "pod-secrets-8899a2f2-9117-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013637122s
STEP: Saw pod success
Jun 17 15:49:51.378: INFO: Pod "pod-secrets-8899a2f2-9117-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:49:51.382: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-secrets-8899a2f2-9117-11e9-a8b9-dace53c98186 container secret-volume-test: <nil>
STEP: delete the pod
Jun 17 15:49:51.408: INFO: Waiting for pod pod-secrets-8899a2f2-9117-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:49:51.411: INFO: Pod pod-secrets-8899a2f2-9117-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:49:51.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6401" for this suite.
Jun 17 15:49:57.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:49:57.555: INFO: namespace secrets-6401 deletion completed in 6.138072707s
STEP: Destroying namespace "secret-namespace-311" for this suite.
Jun 17 15:50:03.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:50:03.701: INFO: namespace secret-namespace-311 deletion completed in 6.146201717s

• [SLOW TEST:16.672 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:50:03.702: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-596
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-92713c65-9117-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume configMaps
Jun 17 15:50:03.885: INFO: Waiting up to 5m0s for pod "pod-configmaps-92726fc1-9117-11e9-a8b9-dace53c98186" in namespace "configmap-596" to be "success or failure"
Jun 17 15:50:03.889: INFO: Pod "pod-configmaps-92726fc1-9117-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.165446ms
Jun 17 15:50:05.894: INFO: Pod "pod-configmaps-92726fc1-9117-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009102958s
Jun 17 15:50:07.899: INFO: Pod "pod-configmaps-92726fc1-9117-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014013559s
STEP: Saw pod success
Jun 17 15:50:07.900: INFO: Pod "pod-configmaps-92726fc1-9117-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:50:07.905: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-configmaps-92726fc1-9117-11e9-a8b9-dace53c98186 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 17 15:50:07.929: INFO: Waiting for pod pod-configmaps-92726fc1-9117-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:50:07.933: INFO: Pod pod-configmaps-92726fc1-9117-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:50:07.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-596" for this suite.
Jun 17 15:50:13.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:50:14.082: INFO: namespace configmap-596 deletion completed in 6.143095674s

• [SLOW TEST:10.380 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:50:14.086: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4799
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0617 15:50:44.795002      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 17 15:50:44.795: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:50:44.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4799" for this suite.
Jun 17 15:50:50.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:50:50.947: INFO: namespace gc-4799 deletion completed in 6.14718574s

• [SLOW TEST:36.862 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:50:50.952: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8774
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 17 15:50:51.119: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ae9983ad-9117-11e9-a8b9-dace53c98186" in namespace "downward-api-8774" to be "success or failure"
Jun 17 15:50:51.125: INFO: Pod "downwardapi-volume-ae9983ad-9117-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 6.787891ms
Jun 17 15:50:53.131: INFO: Pod "downwardapi-volume-ae9983ad-9117-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012123019s
Jun 17 15:50:55.136: INFO: Pod "downwardapi-volume-ae9983ad-9117-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017297734s
STEP: Saw pod success
Jun 17 15:50:55.136: INFO: Pod "downwardapi-volume-ae9983ad-9117-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:50:55.140: INFO: Trying to get logs from node lab1-k8s-node-2 pod downwardapi-volume-ae9983ad-9117-11e9-a8b9-dace53c98186 container client-container: <nil>
STEP: delete the pod
Jun 17 15:50:55.181: INFO: Waiting for pod downwardapi-volume-ae9983ad-9117-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:50:55.188: INFO: Pod downwardapi-volume-ae9983ad-9117-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:50:55.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8774" for this suite.
Jun 17 15:51:01.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:51:01.352: INFO: namespace downward-api-8774 deletion completed in 6.158400324s

• [SLOW TEST:10.401 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:51:01.352: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9301
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Jun 17 15:51:01.515: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-270483862 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:51:01.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9301" for this suite.
Jun 17 15:51:07.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:51:07.749: INFO: namespace kubectl-9301 deletion completed in 6.148601527s

• [SLOW TEST:6.397 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:51:07.752: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5111
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-b89d636f-9117-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume secrets
Jun 17 15:51:07.928: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b89e5f08-9117-11e9-a8b9-dace53c98186" in namespace "projected-5111" to be "success or failure"
Jun 17 15:51:07.936: INFO: Pod "pod-projected-secrets-b89e5f08-9117-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 7.473882ms
Jun 17 15:51:09.941: INFO: Pod "pod-projected-secrets-b89e5f08-9117-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01246322s
STEP: Saw pod success
Jun 17 15:51:09.941: INFO: Pod "pod-projected-secrets-b89e5f08-9117-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:51:09.945: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-secrets-b89e5f08-9117-11e9-a8b9-dace53c98186 container secret-volume-test: <nil>
STEP: delete the pod
Jun 17 15:51:09.971: INFO: Waiting for pod pod-projected-secrets-b89e5f08-9117-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:51:09.975: INFO: Pod pod-projected-secrets-b89e5f08-9117-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:51:09.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5111" for this suite.
Jun 17 15:51:15.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:51:16.119: INFO: namespace projected-5111 deletion completed in 6.137527952s

• [SLOW TEST:8.367 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:51:16.121: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6104
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 17 15:51:16.290: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bd9a7175-9117-11e9-a8b9-dace53c98186" in namespace "downward-api-6104" to be "success or failure"
Jun 17 15:51:16.297: INFO: Pod "downwardapi-volume-bd9a7175-9117-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 6.616901ms
Jun 17 15:51:18.302: INFO: Pod "downwardapi-volume-bd9a7175-9117-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011362195s
Jun 17 15:51:20.307: INFO: Pod "downwardapi-volume-bd9a7175-9117-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016951662s
STEP: Saw pod success
Jun 17 15:51:20.307: INFO: Pod "downwardapi-volume-bd9a7175-9117-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:51:20.312: INFO: Trying to get logs from node lab1-k8s-node-2 pod downwardapi-volume-bd9a7175-9117-11e9-a8b9-dace53c98186 container client-container: <nil>
STEP: delete the pod
Jun 17 15:51:20.353: INFO: Waiting for pod downwardapi-volume-bd9a7175-9117-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:51:20.357: INFO: Pod downwardapi-volume-bd9a7175-9117-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:51:20.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6104" for this suite.
Jun 17 15:51:26.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:51:26.505: INFO: namespace downward-api-6104 deletion completed in 6.140180088s

• [SLOW TEST:10.384 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:51:26.507: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6608
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 17 15:51:26.678: INFO: Waiting up to 5m0s for pod "pod-c3cb95e6-9117-11e9-a8b9-dace53c98186" in namespace "emptydir-6608" to be "success or failure"
Jun 17 15:51:26.682: INFO: Pod "pod-c3cb95e6-9117-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.171679ms
Jun 17 15:51:28.687: INFO: Pod "pod-c3cb95e6-9117-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00943981s
STEP: Saw pod success
Jun 17 15:51:28.687: INFO: Pod "pod-c3cb95e6-9117-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:51:28.692: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-c3cb95e6-9117-11e9-a8b9-dace53c98186 container test-container: <nil>
STEP: delete the pod
Jun 17 15:51:28.719: INFO: Waiting for pod pod-c3cb95e6-9117-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:51:28.724: INFO: Pod pod-c3cb95e6-9117-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:51:28.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6608" for this suite.
Jun 17 15:51:34.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:51:34.875: INFO: namespace emptydir-6608 deletion completed in 6.142581576s

• [SLOW TEST:8.368 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:51:34.875: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-4568
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Jun 17 15:51:35.526: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 17 15:51:37.586: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383495, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383495, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383495, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383495, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 17 15:51:39.591: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383495, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383495, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383495, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383495, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 17 15:51:41.591: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383495, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383495, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383495, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383495, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 17 15:51:45.322: INFO: Waited 1.724093495s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:51:45.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4568" for this suite.
Jun 17 15:51:51.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:51:52.009: INFO: namespace aggregator-4568 deletion completed in 6.231759462s

• [SLOW TEST:17.134 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:51:52.010: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4524
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jun 17 15:51:52.168: INFO: PodSpec: initContainers in spec.initContainers
Jun 17 15:52:39.238: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-d2fea39c-9117-11e9-a8b9-dace53c98186", GenerateName:"", Namespace:"init-container-4524", SelfLink:"/api/v1/namespaces/init-container-4524/pods/pod-init-d2fea39c-9117-11e9-a8b9-dace53c98186", UID:"d2ff7b4d-9117-11e9-bdd5-fa163e38c6bd", ResourceVersion:"30987", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63696383512, loc:(*time.Location)(0x8a1a0e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"168953133"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-pqkdn", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc000be1c80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-pqkdn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-pqkdn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-pqkdn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000efa1a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"lab1-k8s-node-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001b6ede0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000efa230)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000efa250)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000efa258), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000efa25c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383512, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383512, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383512, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383512, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.128.0.4", PodIP:"10.233.95.133", StartTime:(*v1.Time)(0xc0018b4260), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00049b420)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00049b490)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://ae1833aa8136c5a649ef43ebab00ea07d16550f65770481de3ed9915ab741d6a"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0018b42a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0018b4280), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:52:39.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4524" for this suite.
Jun 17 15:53:01.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:53:01.398: INFO: namespace init-container-4524 deletion completed in 22.150721729s

• [SLOW TEST:69.388 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:53:01.399: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8333
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 15:53:01.560: INFO: Creating deployment "nginx-deployment"
Jun 17 15:53:01.568: INFO: Waiting for observed generation 1
Jun 17 15:53:03.579: INFO: Waiting for all required pods to come up
Jun 17 15:53:03.586: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 17 15:53:05.604: INFO: Waiting for deployment "nginx-deployment" to complete
Jun 17 15:53:05.613: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jun 17 15:53:05.623: INFO: Updating deployment nginx-deployment
Jun 17 15:53:05.623: INFO: Waiting for observed generation 2
Jun 17 15:53:07.637: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 17 15:53:07.641: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 17 15:53:07.645: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jun 17 15:53:07.655: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 17 15:53:07.655: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 17 15:53:07.659: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jun 17 15:53:07.666: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jun 17 15:53:07.666: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jun 17 15:53:07.679: INFO: Updating deployment nginx-deployment
Jun 17 15:53:07.679: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jun 17 15:53:07.704: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 17 15:53:09.720: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jun 17 15:53:09.728: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-8333,SelfLink:/apis/apps/v1/namespaces/deployment-8333/deployments/nginx-deployment,UID:fc5b5638-9117-11e9-bdd5-fa163e38c6bd,ResourceVersion:31373,Generation:3,CreationTimestamp:2019-06-17 15:53:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-06-17 15:53:07 +0000 UTC 2019-06-17 15:53:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-06-17 15:53:07 +0000 UTC 2019-06-17 15:53:01 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Jun 17 15:53:09.733: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-8333,SelfLink:/apis/apps/v1/namespaces/deployment-8333/replicasets/nginx-deployment-5f9595f595,UID:fec71493-9117-11e9-bdd5-fa163e38c6bd,ResourceVersion:31368,Generation:3,CreationTimestamp:2019-06-17 15:53:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment fc5b5638-9117-11e9-bdd5-fa163e38c6bd 0xc002be68e7 0xc002be68e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 17 15:53:09.733: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jun 17 15:53:09.733: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-8333,SelfLink:/apis/apps/v1/namespaces/deployment-8333/replicasets/nginx-deployment-6f478d8d8,UID:fc5c6c60-9117-11e9-bdd5-fa163e38c6bd,ResourceVersion:31358,Generation:3,CreationTimestamp:2019-06-17 15:53:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment fc5b5638-9117-11e9-bdd5-fa163e38c6bd 0xc002be69b7 0xc002be69b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jun 17 15:53:09.741: INFO: Pod "nginx-deployment-5f9595f595-674kq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-674kq,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-5f9595f595-674kq,UID:fed75469-9117-11e9-bdd5-fa163e38c6bd,ResourceVersion:31261,Generation:0,CreationTimestamp:2019-06-17 15:53:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 fec71493-9117-11e9-bdd5-fa163e38c6bd 0xc0028b65c7 0xc0028b65c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028b6640} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028b6660}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.7,PodIP:,StartTime:2019-06-17 15:53:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.741: INFO: Pod "nginx-deployment-5f9595f595-6dhbh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-6dhbh,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-5f9595f595-6dhbh,UID:fed5174d-9117-11e9-bdd5-fa163e38c6bd,ResourceVersion:31259,Generation:0,CreationTimestamp:2019-06-17 15:53:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 fec71493-9117-11e9-bdd5-fa163e38c6bd 0xc0028b6740 0xc0028b6741}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028b6840} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028b68d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2019-06-17 15:53:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.741: INFO: Pod "nginx-deployment-5f9595f595-7cmr9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-7cmr9,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-5f9595f595-7cmr9,UID:fec860b1-9117-11e9-bdd5-fa163e38c6bd,ResourceVersion:31233,Generation:0,CreationTimestamp:2019-06-17 15:53:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 fec71493-9117-11e9-bdd5-fa163e38c6bd 0xc0028b6b00 0xc0028b6b01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028b6bb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028b6bf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.7,PodIP:,StartTime:2019-06-17 15:53:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.741: INFO: Pod "nginx-deployment-5f9595f595-9pwzb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-9pwzb,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-5f9595f595-9pwzb,UID:fecaaccf-9117-11e9-bdd5-fa163e38c6bd,ResourceVersion:31238,Generation:0,CreationTimestamp:2019-06-17 15:53:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 fec71493-9117-11e9-bdd5-fa163e38c6bd 0xc0028b6ed0 0xc0028b6ed1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028b7000} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028b7030}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2019-06-17 15:53:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.741: INFO: Pod "nginx-deployment-5f9595f595-cmkp6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-cmkp6,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-5f9595f595-cmkp6,UID:0007d2b1-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31346,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 fec71493-9117-11e9-bdd5-fa163e38c6bd 0xc0028b7110 0xc0028b7111}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028b7190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028b71b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.742: INFO: Pod "nginx-deployment-5f9595f595-f86c7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-f86c7,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-5f9595f595-f86c7,UID:00042e1d-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31345,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 fec71493-9117-11e9-bdd5-fa163e38c6bd 0xc0028b7290 0xc0028b7291}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028b7310} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028b7330}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.742: INFO: Pod "nginx-deployment-5f9595f595-fjmg2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-fjmg2,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-5f9595f595-fjmg2,UID:000e3371-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31371,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 fec71493-9117-11e9-bdd5-fa163e38c6bd 0xc0028b7470 0xc0028b7471}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028b7690} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028b76d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.742: INFO: Pod "nginx-deployment-5f9595f595-j4nt9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-j4nt9,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-5f9595f595-j4nt9,UID:000e82bb-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31376,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 fec71493-9117-11e9-bdd5-fa163e38c6bd 0xc0028b78a0 0xc0028b78a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028b7930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028b7950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.7,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.742: INFO: Pod "nginx-deployment-5f9595f595-jx2gd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-jx2gd,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-5f9595f595-jx2gd,UID:00080ee0-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31369,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 fec71493-9117-11e9-bdd5-fa163e38c6bd 0xc0028b7a20 0xc0028b7a21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028b7b20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028b7b40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.7,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.742: INFO: Pod "nginx-deployment-5f9595f595-ld6lt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-ld6lt,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-5f9595f595-ld6lt,UID:0013432b-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31399,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 fec71493-9117-11e9-bdd5-fa163e38c6bd 0xc0028b7d30 0xc0028b7d31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028b7ec0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028b7ee0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.742: INFO: Pod "nginx-deployment-5f9595f595-s5tqq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-s5tqq,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-5f9595f595-s5tqq,UID:000e5fd2-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31398,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 fec71493-9117-11e9-bdd5-fa163e38c6bd 0xc002a80060 0xc002a80061}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a800e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a80100}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.742: INFO: Pod "nginx-deployment-5f9595f595-xgxxm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-xgxxm,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-5f9595f595-xgxxm,UID:feca73d3-9117-11e9-bdd5-fa163e38c6bd,ResourceVersion:31243,Generation:0,CreationTimestamp:2019-06-17 15:53:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 fec71493-9117-11e9-bdd5-fa163e38c6bd 0xc002a801d0 0xc002a801d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a80250} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a80270}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:05 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:,StartTime:2019-06-17 15:53:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.742: INFO: Pod "nginx-deployment-5f9595f595-zgl5s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-zgl5s,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-5f9595f595-zgl5s,UID:000d6f5e-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31380,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 fec71493-9117-11e9-bdd5-fa163e38c6bd 0xc002a80340 0xc002a80341}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a803c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a803e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.743: INFO: Pod "nginx-deployment-6f478d8d8-26dd5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-26dd5,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-26dd5,UID:fc67a2bc-9117-11e9-bdd5-fa163e38c6bd,ResourceVersion:31204,Generation:0,CreationTimestamp:2019-06-17 15:53:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a804b0 0xc002a804b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a80520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a80540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:01 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.7,PodIP:10.233.64.109,StartTime:2019-06-17 15:53:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-17 15:53:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://237f56b7903868b9d4bad24e04f976afa81349b920fb87e49574b3fad90cc0f7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.743: INFO: Pod "nginx-deployment-6f478d8d8-45bpk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-45bpk,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-45bpk,UID:fc676ed0-9117-11e9-bdd5-fa163e38c6bd,ResourceVersion:31190,Generation:0,CreationTimestamp:2019-06-17 15:53:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a80610 0xc002a80611}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a80680} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a806a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:01 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.4,PodIP:10.233.95.134,StartTime:2019-06-17 15:53:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-17 15:53:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://293a303bfac71e457533bbd0b6d45117198d96052c6298621b480653022973b2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.743: INFO: Pod "nginx-deployment-6f478d8d8-4vpxg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-4vpxg,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-4vpxg,UID:0003b5fa-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31341,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a80770 0xc002a80771}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a807f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a80810}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.7,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.743: INFO: Pod "nginx-deployment-6f478d8d8-625mz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-625mz,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-625mz,UID:0001693c-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31321,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a808d0 0xc002a808d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a80940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a80960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.7,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.743: INFO: Pod "nginx-deployment-6f478d8d8-9fl8k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-9fl8k,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-9fl8k,UID:0007ebcf-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31365,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a80a20 0xc002a80a21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a80a90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a80ab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.743: INFO: Pod "nginx-deployment-6f478d8d8-9gzgv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-9gzgv,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-9gzgv,UID:00083e7f-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31374,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a80b70 0xc002a80b71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a80be0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a80c00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.743: INFO: Pod "nginx-deployment-6f478d8d8-9kw5v" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-9kw5v,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-9kw5v,UID:fc68667f-9117-11e9-bdd5-fa163e38c6bd,ResourceVersion:31188,Generation:0,CreationTimestamp:2019-06-17 15:53:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a80cc0 0xc002a80cc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a80d30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a80d50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:01 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.4,PodIP:10.233.95.137,StartTime:2019-06-17 15:53:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-17 15:53:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://3ee7cdf9508d9e23b51430e1338485cd6d50c5a41af91b6e026eafb6cf6a4d86}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.743: INFO: Pod "nginx-deployment-6f478d8d8-g2z6j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-g2z6j,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-g2z6j,UID:000f6bce-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31415,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a80e20 0xc002a80e21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a80e90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a80eb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.7,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.744: INFO: Pod "nginx-deployment-6f478d8d8-g54d8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-g54d8,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-g54d8,UID:000fb7df-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31377,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a80f70 0xc002a80f71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a80ff0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a81010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.744: INFO: Pod "nginx-deployment-6f478d8d8-mgrbm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-mgrbm,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-mgrbm,UID:000f6c1e-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31417,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a810d0 0xc002a810d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a81180} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a811a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.744: INFO: Pod "nginx-deployment-6f478d8d8-p8vjl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-p8vjl,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-p8vjl,UID:fc61710e-9117-11e9-bdd5-fa163e38c6bd,ResourceVersion:31194,Generation:0,CreationTimestamp:2019-06-17 15:53:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a81260 0xc002a81261}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a812d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a812f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:01 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.4,PodIP:10.233.95.136,StartTime:2019-06-17 15:53:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-17 15:53:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f7f9d74f67d64243a5219f4f712b17794af5cb195d4f77db2df7abf0340e7e29}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.744: INFO: Pod "nginx-deployment-6f478d8d8-ppjlb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ppjlb,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-ppjlb,UID:000f92a9-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31396,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a813c0 0xc002a813c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a81430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a81450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.7,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.744: INFO: Pod "nginx-deployment-6f478d8d8-pw779" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-pw779,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-pw779,UID:fc5f8582-9117-11e9-bdd5-fa163e38c6bd,ResourceVersion:31200,Generation:0,CreationTimestamp:2019-06-17 15:53:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a81510 0xc002a81511}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a81580} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a815a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:01 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.7,PodIP:10.233.64.107,StartTime:2019-06-17 15:53:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-17 15:53:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://df12276485179bc97c946527e152eb143ba8bedbcd17545421e7c7fee62bb839}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.744: INFO: Pod "nginx-deployment-6f478d8d8-qb925" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-qb925,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-qb925,UID:fc63bd6c-9117-11e9-bdd5-fa163e38c6bd,ResourceVersion:31201,Generation:0,CreationTimestamp:2019-06-17 15:53:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a81670 0xc002a81671}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a816e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a81700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:01 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:10.233.74.34,StartTime:2019-06-17 15:53:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-17 15:53:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c069ae603d9e777d5f187b42b3e9d59f247eeb891dad54d646216c9c20133ad2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.744: INFO: Pod "nginx-deployment-6f478d8d8-rfrj6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rfrj6,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-rfrj6,UID:000804ac-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31364,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a817d0 0xc002a817d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a81840} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a81860}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.744: INFO: Pod "nginx-deployment-6f478d8d8-vgn9v" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-vgn9v,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-vgn9v,UID:fc63ed4c-9117-11e9-bdd5-fa163e38c6bd,ResourceVersion:31185,Generation:0,CreationTimestamp:2019-06-17 15:53:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a81920 0xc002a81921}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a81990} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a819b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:01 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.4,PodIP:10.233.95.135,StartTime:2019-06-17 15:53:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-17 15:53:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2506755ede6e228cef5563dcea92286ad5aa5e84cb63a0b87aadb04ffb108245}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.744: INFO: Pod "nginx-deployment-6f478d8d8-vkhqs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-vkhqs,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-vkhqs,UID:000f49df-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31383,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a81a80 0xc002a81a81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a81af0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a81b10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.4,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.744: INFO: Pod "nginx-deployment-6f478d8d8-vzhtx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-vzhtx,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-vzhtx,UID:000384db-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31325,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a81bd0 0xc002a81bd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a81c40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a81c60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.745: INFO: Pod "nginx-deployment-6f478d8d8-z4dqs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-z4dqs,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-z4dqs,UID:000856c0-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:31361,Generation:0,CreationTimestamp:2019-06-17 15:53:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a81d20 0xc002a81d21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a81d90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a81db0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.7,PodIP:,StartTime:2019-06-17 15:53:07 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 17 15:53:09.745: INFO: Pod "nginx-deployment-6f478d8d8-zqxjn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-zqxjn,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8333,SelfLink:/api/v1/namespaces/deployment-8333/pods/nginx-deployment-6f478d8d8-zqxjn,UID:fc63c09f-9117-11e9-bdd5-fa163e38c6bd,ResourceVersion:31209,Generation:0,CreationTimestamp:2019-06-17 15:53:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 fc5c6c60-9117-11e9-bdd5-fa163e38c6bd 0xc002a81e70 0xc002a81e71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zm2l6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zm2l6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-zm2l6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a81f00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a81f30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:53:01 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.10,PodIP:10.233.74.35,StartTime:2019-06-17 15:53:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-17 15:53:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://27136435486ca37cff9eab2cc2d0f5385a6ca245695ce415eb361cc0d9c73acf}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:53:09.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8333" for this suite.
Jun 17 15:53:17.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:53:17.959: INFO: namespace deployment-8333 deletion completed in 8.209925007s

• [SLOW TEST:16.560 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:53:17.959: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8864
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 17 15:53:18.130: INFO: Waiting up to 5m0s for pod "pod-063994aa-9118-11e9-a8b9-dace53c98186" in namespace "emptydir-8864" to be "success or failure"
Jun 17 15:53:18.138: INFO: Pod "pod-063994aa-9118-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 8.112985ms
Jun 17 15:53:20.144: INFO: Pod "pod-063994aa-9118-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013306944s
Jun 17 15:53:22.149: INFO: Pod "pod-063994aa-9118-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018300762s
STEP: Saw pod success
Jun 17 15:53:22.149: INFO: Pod "pod-063994aa-9118-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:53:22.157: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-063994aa-9118-11e9-a8b9-dace53c98186 container test-container: <nil>
STEP: delete the pod
Jun 17 15:53:22.180: INFO: Waiting for pod pod-063994aa-9118-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:53:22.183: INFO: Pod pod-063994aa-9118-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:53:22.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8864" for this suite.
Jun 17 15:53:28.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:53:28.337: INFO: namespace emptydir-8864 deletion completed in 6.149213477s

• [SLOW TEST:10.378 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:53:28.338: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7545
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 15:53:30.550: INFO: Waiting up to 5m0s for pod "client-envvars-0da0cd82-9118-11e9-a8b9-dace53c98186" in namespace "pods-7545" to be "success or failure"
Jun 17 15:53:30.557: INFO: Pod "client-envvars-0da0cd82-9118-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 7.514514ms
Jun 17 15:53:32.562: INFO: Pod "client-envvars-0da0cd82-9118-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012057052s
STEP: Saw pod success
Jun 17 15:53:32.562: INFO: Pod "client-envvars-0da0cd82-9118-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:53:32.567: INFO: Trying to get logs from node lab1-k8s-node-1 pod client-envvars-0da0cd82-9118-11e9-a8b9-dace53c98186 container env3cont: <nil>
STEP: delete the pod
Jun 17 15:53:32.593: INFO: Waiting for pod client-envvars-0da0cd82-9118-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:53:32.605: INFO: Pod client-envvars-0da0cd82-9118-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:53:32.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7545" for this suite.
Jun 17 15:54:10.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:54:10.779: INFO: namespace pods-7545 deletion completed in 38.167397023s

• [SLOW TEST:42.440 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:54:10.779: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6065
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-6065/configmap-test-25b5d46f-9118-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume configMaps
Jun 17 15:54:10.959: INFO: Waiting up to 5m0s for pod "pod-configmaps-25b6cdf9-9118-11e9-a8b9-dace53c98186" in namespace "configmap-6065" to be "success or failure"
Jun 17 15:54:10.966: INFO: Pod "pod-configmaps-25b6cdf9-9118-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 6.771044ms
Jun 17 15:54:12.971: INFO: Pod "pod-configmaps-25b6cdf9-9118-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011246888s
STEP: Saw pod success
Jun 17 15:54:12.971: INFO: Pod "pod-configmaps-25b6cdf9-9118-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:54:12.974: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-configmaps-25b6cdf9-9118-11e9-a8b9-dace53c98186 container env-test: <nil>
STEP: delete the pod
Jun 17 15:54:12.999: INFO: Waiting for pod pod-configmaps-25b6cdf9-9118-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:54:13.002: INFO: Pod pod-configmaps-25b6cdf9-9118-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:54:13.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6065" for this suite.
Jun 17 15:54:19.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:54:19.160: INFO: namespace configmap-6065 deletion completed in 6.152842698s

• [SLOW TEST:8.381 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:54:19.162: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9234
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 15:54:19.341: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 17 15:54:24.346: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 17 15:54:24.347: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jun 17 15:54:28.386: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-9234,SelfLink:/apis/apps/v1/namespaces/deployment-9234/deployments/test-cleanup-deployment,UID:2db49341-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:32196,Generation:1,CreationTimestamp:2019-06-17 15:54:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-17 15:54:24 +0000 UTC 2019-06-17 15:54:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-17 15:54:26 +0000 UTC 2019-06-17 15:54:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55cbfbc8f5" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 17 15:54:28.390: INFO: New ReplicaSet "test-cleanup-deployment-55cbfbc8f5" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5,GenerateName:,Namespace:deployment-9234,SelfLink:/apis/apps/v1/namespaces/deployment-9234/replicasets/test-cleanup-deployment-55cbfbc8f5,UID:2db70ec0-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:32185,Generation:1,CreationTimestamp:2019-06-17 15:54:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 2db49341-9118-11e9-bdd5-fa163e38c6bd 0xc002781097 0xc002781098}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 17 15:54:28.394: INFO: Pod "test-cleanup-deployment-55cbfbc8f5-2qdj4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5-2qdj4,GenerateName:test-cleanup-deployment-55cbfbc8f5-,Namespace:deployment-9234,SelfLink:/api/v1/namespaces/deployment-9234/pods/test-cleanup-deployment-55cbfbc8f5-2qdj4,UID:2db7d28b-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:32184,Generation:0,CreationTimestamp:2019-06-17 15:54:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55cbfbc8f5 2db70ec0-9118-11e9-bdd5-fa163e38c6bd 0xc002781647 0xc002781648}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lrtxm {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lrtxm,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-lrtxm true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027816d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027816f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:54:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:54:26 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:54:26 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:54:24 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.7,PodIP:10.233.64.121,StartTime:2019-06-17 15:54:24 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-17 15:54:25 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://03b52e9e20b83c36a92b6201bfee824bf837fbd033e6448ae75acc970f303d7d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:54:28.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9234" for this suite.
Jun 17 15:54:34.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:54:34.543: INFO: namespace deployment-9234 deletion completed in 6.143806055s

• [SLOW TEST:15.381 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:54:34.543: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7670
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-33df66d2-9118-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume secrets
Jun 17 15:54:34.721: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-33e07509-9118-11e9-a8b9-dace53c98186" in namespace "projected-7670" to be "success or failure"
Jun 17 15:54:34.730: INFO: Pod "pod-projected-secrets-33e07509-9118-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 9.751687ms
Jun 17 15:54:36.736: INFO: Pod "pod-projected-secrets-33e07509-9118-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015023052s
STEP: Saw pod success
Jun 17 15:54:36.736: INFO: Pod "pod-projected-secrets-33e07509-9118-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:54:36.740: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-secrets-33e07509-9118-11e9-a8b9-dace53c98186 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 17 15:54:36.768: INFO: Waiting for pod pod-projected-secrets-33e07509-9118-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:54:36.772: INFO: Pod pod-projected-secrets-33e07509-9118-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:54:36.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7670" for this suite.
Jun 17 15:54:42.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:54:42.927: INFO: namespace projected-7670 deletion completed in 6.149618053s

• [SLOW TEST:8.384 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:54:42.929: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2655
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 17 15:54:43.144: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2655,SelfLink:/api/v1/namespaces/watch-2655/configmaps/e2e-watch-test-configmap-a,UID:38e6c757-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:32307,Generation:0,CreationTimestamp:2019-06-17 15:54:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 17 15:54:43.144: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2655,SelfLink:/api/v1/namespaces/watch-2655/configmaps/e2e-watch-test-configmap-a,UID:38e6c757-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:32307,Generation:0,CreationTimestamp:2019-06-17 15:54:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 17 15:54:53.154: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2655,SelfLink:/api/v1/namespaces/watch-2655/configmaps/e2e-watch-test-configmap-a,UID:38e6c757-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:32327,Generation:0,CreationTimestamp:2019-06-17 15:54:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 17 15:54:53.154: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2655,SelfLink:/api/v1/namespaces/watch-2655/configmaps/e2e-watch-test-configmap-a,UID:38e6c757-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:32327,Generation:0,CreationTimestamp:2019-06-17 15:54:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 17 15:55:03.164: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2655,SelfLink:/api/v1/namespaces/watch-2655/configmaps/e2e-watch-test-configmap-a,UID:38e6c757-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:32350,Generation:0,CreationTimestamp:2019-06-17 15:54:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 17 15:55:03.165: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2655,SelfLink:/api/v1/namespaces/watch-2655/configmaps/e2e-watch-test-configmap-a,UID:38e6c757-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:32350,Generation:0,CreationTimestamp:2019-06-17 15:54:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 17 15:55:13.175: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2655,SelfLink:/api/v1/namespaces/watch-2655/configmaps/e2e-watch-test-configmap-a,UID:38e6c757-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:32370,Generation:0,CreationTimestamp:2019-06-17 15:54:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 17 15:55:13.175: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2655,SelfLink:/api/v1/namespaces/watch-2655/configmaps/e2e-watch-test-configmap-a,UID:38e6c757-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:32370,Generation:0,CreationTimestamp:2019-06-17 15:54:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 17 15:55:23.186: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2655,SelfLink:/api/v1/namespaces/watch-2655/configmaps/e2e-watch-test-configmap-b,UID:50c44220-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:32391,Generation:0,CreationTimestamp:2019-06-17 15:55:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 17 15:55:23.186: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2655,SelfLink:/api/v1/namespaces/watch-2655/configmaps/e2e-watch-test-configmap-b,UID:50c44220-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:32391,Generation:0,CreationTimestamp:2019-06-17 15:55:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 17 15:55:33.196: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2655,SelfLink:/api/v1/namespaces/watch-2655/configmaps/e2e-watch-test-configmap-b,UID:50c44220-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:32445,Generation:0,CreationTimestamp:2019-06-17 15:55:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 17 15:55:33.196: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2655,SelfLink:/api/v1/namespaces/watch-2655/configmaps/e2e-watch-test-configmap-b,UID:50c44220-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:32445,Generation:0,CreationTimestamp:2019-06-17 15:55:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:55:43.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2655" for this suite.
Jun 17 15:55:49.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:55:49.348: INFO: namespace watch-2655 deletion completed in 6.145187068s

• [SLOW TEST:66.419 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:55:49.349: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-784
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-784
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-784 to expose endpoints map[]
Jun 17 15:55:49.540: INFO: successfully validated that service multi-endpoint-test in namespace services-784 exposes endpoints map[] (4.036549ms elapsed)
STEP: Creating pod pod1 in namespace services-784
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-784 to expose endpoints map[pod1:[100]]
Jun 17 15:55:52.586: INFO: successfully validated that service multi-endpoint-test in namespace services-784 exposes endpoints map[pod1:[100]] (3.035657947s elapsed)
STEP: Creating pod pod2 in namespace services-784
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-784 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 17 15:55:54.638: INFO: successfully validated that service multi-endpoint-test in namespace services-784 exposes endpoints map[pod1:[100] pod2:[101]] (2.044027467s elapsed)
STEP: Deleting pod pod1 in namespace services-784
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-784 to expose endpoints map[pod2:[101]]
Jun 17 15:55:55.666: INFO: successfully validated that service multi-endpoint-test in namespace services-784 exposes endpoints map[pod2:[101]] (1.017365829s elapsed)
STEP: Deleting pod pod2 in namespace services-784
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-784 to expose endpoints map[]
Jun 17 15:55:56.688: INFO: successfully validated that service multi-endpoint-test in namespace services-784 exposes endpoints map[] (1.011508852s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:55:56.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-784" for this suite.
Jun 17 15:56:18.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:56:18.886: INFO: namespace services-784 deletion completed in 22.142032987s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:29.537 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:56:18.888: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1332
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:56:19.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1332" for this suite.
Jun 17 15:56:25.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:56:25.212: INFO: namespace services-1332 deletion completed in 6.152938274s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.325 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:56:25.213: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-75d6a34a-9118-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume secrets
Jun 17 15:56:25.399: INFO: Waiting up to 5m0s for pod "pod-secrets-75d8576e-9118-11e9-a8b9-dace53c98186" in namespace "secrets-7381" to be "success or failure"
Jun 17 15:56:25.404: INFO: Pod "pod-secrets-75d8576e-9118-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 4.968079ms
Jun 17 15:56:27.409: INFO: Pod "pod-secrets-75d8576e-9118-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01023571s
STEP: Saw pod success
Jun 17 15:56:27.409: INFO: Pod "pod-secrets-75d8576e-9118-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:56:27.414: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-secrets-75d8576e-9118-11e9-a8b9-dace53c98186 container secret-volume-test: <nil>
STEP: delete the pod
Jun 17 15:56:27.443: INFO: Waiting for pod pod-secrets-75d8576e-9118-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:56:27.446: INFO: Pod pod-secrets-75d8576e-9118-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:56:27.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7381" for this suite.
Jun 17 15:56:33.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:56:33.597: INFO: namespace secrets-7381 deletion completed in 6.145121873s

• [SLOW TEST:8.384 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:56:33.598: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8900
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0617 15:57:13.816451      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 17 15:57:13.816: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:57:13.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8900" for this suite.
Jun 17 15:57:19.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:57:20.025: INFO: namespace gc-8900 deletion completed in 6.205247767s

• [SLOW TEST:46.428 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:57:20.026: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3049
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-9681b5c6-9118-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume configMaps
Jun 17 15:57:20.199: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-96829115-9118-11e9-a8b9-dace53c98186" in namespace "projected-3049" to be "success or failure"
Jun 17 15:57:20.211: INFO: Pod "pod-projected-configmaps-96829115-9118-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 12.078544ms
Jun 17 15:57:22.216: INFO: Pod "pod-projected-configmaps-96829115-9118-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0168134s
Jun 17 15:57:24.221: INFO: Pod "pod-projected-configmaps-96829115-9118-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022007913s
STEP: Saw pod success
Jun 17 15:57:24.221: INFO: Pod "pod-projected-configmaps-96829115-9118-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:57:24.225: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-configmaps-96829115-9118-11e9-a8b9-dace53c98186 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 17 15:57:24.248: INFO: Waiting for pod pod-projected-configmaps-96829115-9118-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:57:24.252: INFO: Pod pod-projected-configmaps-96829115-9118-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:57:24.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3049" for this suite.
Jun 17 15:57:30.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:57:30.416: INFO: namespace projected-3049 deletion completed in 6.158628557s

• [SLOW TEST:10.390 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:57:30.418: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-2164
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2164
I0617 15:57:30.594509      14 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2164, replica count: 1
I0617 15:57:31.645090      14 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0617 15:57:32.645322      14 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 17 15:57:32.762: INFO: Created: latency-svc-9vznc
Jun 17 15:57:32.769: INFO: Got endpoints: latency-svc-9vznc [23.899283ms]
Jun 17 15:57:32.787: INFO: Created: latency-svc-xd5zg
Jun 17 15:57:32.792: INFO: Got endpoints: latency-svc-xd5zg [22.599475ms]
Jun 17 15:57:32.797: INFO: Created: latency-svc-kdj84
Jun 17 15:57:32.804: INFO: Got endpoints: latency-svc-kdj84 [34.861106ms]
Jun 17 15:57:32.811: INFO: Created: latency-svc-c2k27
Jun 17 15:57:32.818: INFO: Got endpoints: latency-svc-c2k27 [47.612143ms]
Jun 17 15:57:32.822: INFO: Created: latency-svc-frz9l
Jun 17 15:57:32.828: INFO: Got endpoints: latency-svc-frz9l [58.303744ms]
Jun 17 15:57:32.837: INFO: Created: latency-svc-qjd8p
Jun 17 15:57:32.844: INFO: Got endpoints: latency-svc-qjd8p [73.97194ms]
Jun 17 15:57:32.845: INFO: Created: latency-svc-gm9wh
Jun 17 15:57:32.853: INFO: Created: latency-svc-7wwhk
Jun 17 15:57:32.854: INFO: Got endpoints: latency-svc-gm9wh [83.627281ms]
Jun 17 15:57:32.860: INFO: Created: latency-svc-l2j9v
Jun 17 15:57:32.866: INFO: Got endpoints: latency-svc-l2j9v [95.636216ms]
Jun 17 15:57:32.867: INFO: Got endpoints: latency-svc-7wwhk [97.822311ms]
Jun 17 15:57:32.873: INFO: Created: latency-svc-n2p6f
Jun 17 15:57:32.880: INFO: Got endpoints: latency-svc-n2p6f [109.2203ms]
Jun 17 15:57:32.887: INFO: Created: latency-svc-pm6z5
Jun 17 15:57:32.893: INFO: Got endpoints: latency-svc-pm6z5 [122.36558ms]
Jun 17 15:57:32.902: INFO: Created: latency-svc-h2mqm
Jun 17 15:57:32.910: INFO: Got endpoints: latency-svc-h2mqm [138.807457ms]
Jun 17 15:57:32.911: INFO: Created: latency-svc-jw6kr
Jun 17 15:57:32.921: INFO: Got endpoints: latency-svc-jw6kr [149.854838ms]
Jun 17 15:57:32.921: INFO: Created: latency-svc-dvrgm
Jun 17 15:57:32.925: INFO: Got endpoints: latency-svc-dvrgm [153.857206ms]
Jun 17 15:57:32.929: INFO: Created: latency-svc-jwfbl
Jun 17 15:57:32.938: INFO: Got endpoints: latency-svc-jwfbl [166.551066ms]
Jun 17 15:57:32.941: INFO: Created: latency-svc-vvwrz
Jun 17 15:57:32.949: INFO: Got endpoints: latency-svc-vvwrz [178.239496ms]
Jun 17 15:57:32.953: INFO: Created: latency-svc-bhm5f
Jun 17 15:57:32.960: INFO: Created: latency-svc-jbf7b
Jun 17 15:57:32.960: INFO: Got endpoints: latency-svc-bhm5f [168.103391ms]
Jun 17 15:57:32.965: INFO: Got endpoints: latency-svc-jbf7b [160.636191ms]
Jun 17 15:57:32.981: INFO: Created: latency-svc-xjttp
Jun 17 15:57:32.986: INFO: Got endpoints: latency-svc-xjttp [167.965261ms]
Jun 17 15:57:32.987: INFO: Created: latency-svc-ln6tj
Jun 17 15:57:32.994: INFO: Got endpoints: latency-svc-ln6tj [165.786206ms]
Jun 17 15:57:32.998: INFO: Created: latency-svc-fctbg
Jun 17 15:57:33.004: INFO: Created: latency-svc-276q5
Jun 17 15:57:33.005: INFO: Got endpoints: latency-svc-fctbg [160.592399ms]
Jun 17 15:57:33.009: INFO: Got endpoints: latency-svc-276q5 [155.551581ms]
Jun 17 15:57:33.020: INFO: Created: latency-svc-llz5q
Jun 17 15:57:33.027: INFO: Got endpoints: latency-svc-llz5q [160.559582ms]
Jun 17 15:57:33.028: INFO: Created: latency-svc-95pf8
Jun 17 15:57:33.032: INFO: Created: latency-svc-865gw
Jun 17 15:57:33.038: INFO: Got endpoints: latency-svc-95pf8 [170.746555ms]
Jun 17 15:57:33.042: INFO: Got endpoints: latency-svc-865gw [161.063557ms]
Jun 17 15:57:33.046: INFO: Created: latency-svc-dvwxf
Jun 17 15:57:33.051: INFO: Created: latency-svc-5sxlk
Jun 17 15:57:33.056: INFO: Got endpoints: latency-svc-dvwxf [162.31722ms]
Jun 17 15:57:33.062: INFO: Got endpoints: latency-svc-5sxlk [152.36488ms]
Jun 17 15:57:33.063: INFO: Created: latency-svc-kqckz
Jun 17 15:57:33.070: INFO: Got endpoints: latency-svc-kqckz [149.683531ms]
Jun 17 15:57:33.074: INFO: Created: latency-svc-kvcwm
Jun 17 15:57:33.079: INFO: Got endpoints: latency-svc-kvcwm [154.605898ms]
Jun 17 15:57:33.082: INFO: Created: latency-svc-2bzwn
Jun 17 15:57:33.089: INFO: Created: latency-svc-n5bxz
Jun 17 15:57:33.091: INFO: Got endpoints: latency-svc-2bzwn [153.867621ms]
Jun 17 15:57:33.096: INFO: Got endpoints: latency-svc-n5bxz [146.88168ms]
Jun 17 15:57:33.097: INFO: Created: latency-svc-6gv7t
Jun 17 15:57:33.103: INFO: Got endpoints: latency-svc-6gv7t [143.015488ms]
Jun 17 15:57:33.106: INFO: Created: latency-svc-54ckz
Jun 17 15:57:33.113: INFO: Created: latency-svc-tr69c
Jun 17 15:57:33.114: INFO: Got endpoints: latency-svc-54ckz [149.007691ms]
Jun 17 15:57:33.118: INFO: Created: latency-svc-wrm9z
Jun 17 15:57:33.124: INFO: Got endpoints: latency-svc-tr69c [138.276727ms]
Jun 17 15:57:33.129: INFO: Got endpoints: latency-svc-wrm9z [135.163735ms]
Jun 17 15:57:33.136: INFO: Created: latency-svc-5xk5j
Jun 17 15:57:33.138: INFO: Created: latency-svc-57xqt
Jun 17 15:57:33.144: INFO: Got endpoints: latency-svc-5xk5j [138.934091ms]
Jun 17 15:57:33.145: INFO: Got endpoints: latency-svc-57xqt [135.262042ms]
Jun 17 15:57:33.151: INFO: Created: latency-svc-smwc2
Jun 17 15:57:33.157: INFO: Created: latency-svc-rg8w6
Jun 17 15:57:33.165: INFO: Created: latency-svc-gqshx
Jun 17 15:57:33.169: INFO: Got endpoints: latency-svc-smwc2 [142.660518ms]
Jun 17 15:57:33.170: INFO: Created: latency-svc-srj6w
Jun 17 15:57:33.182: INFO: Created: latency-svc-g5xf4
Jun 17 15:57:33.186: INFO: Created: latency-svc-s2dqd
Jun 17 15:57:33.197: INFO: Created: latency-svc-vjjm5
Jun 17 15:57:33.205: INFO: Created: latency-svc-cchr6
Jun 17 15:57:33.216: INFO: Created: latency-svc-t6qhc
Jun 17 15:57:33.217: INFO: Created: latency-svc-78j4x
Jun 17 15:57:33.221: INFO: Got endpoints: latency-svc-rg8w6 [182.354643ms]
Jun 17 15:57:33.232: INFO: Created: latency-svc-5ndwg
Jun 17 15:57:33.237: INFO: Created: latency-svc-jtmwf
Jun 17 15:57:33.242: INFO: Created: latency-svc-7mhsg
Jun 17 15:57:33.250: INFO: Created: latency-svc-9krk4
Jun 17 15:57:33.255: INFO: Created: latency-svc-4xmpd
Jun 17 15:57:33.264: INFO: Created: latency-svc-z5xls
Jun 17 15:57:33.269: INFO: Got endpoints: latency-svc-gqshx [226.952185ms]
Jun 17 15:57:33.272: INFO: Created: latency-svc-txctt
Jun 17 15:57:33.283: INFO: Created: latency-svc-dc6vp
Jun 17 15:57:33.324: INFO: Got endpoints: latency-svc-srj6w [268.225866ms]
Jun 17 15:57:33.341: INFO: Created: latency-svc-dlzhw
Jun 17 15:57:33.368: INFO: Got endpoints: latency-svc-g5xf4 [305.703095ms]
Jun 17 15:57:33.384: INFO: Created: latency-svc-5kzn9
Jun 17 15:57:33.418: INFO: Got endpoints: latency-svc-s2dqd [347.194777ms]
Jun 17 15:57:33.433: INFO: Created: latency-svc-49cb5
Jun 17 15:57:33.467: INFO: Got endpoints: latency-svc-vjjm5 [388.002208ms]
Jun 17 15:57:33.484: INFO: Created: latency-svc-f8wbb
Jun 17 15:57:33.522: INFO: Got endpoints: latency-svc-cchr6 [430.837918ms]
Jun 17 15:57:33.538: INFO: Created: latency-svc-fgfhl
Jun 17 15:57:33.568: INFO: Got endpoints: latency-svc-t6qhc [471.419079ms]
Jun 17 15:57:33.583: INFO: Created: latency-svc-xmttd
Jun 17 15:57:33.618: INFO: Got endpoints: latency-svc-78j4x [514.469609ms]
Jun 17 15:57:33.634: INFO: Created: latency-svc-8kw9f
Jun 17 15:57:33.670: INFO: Got endpoints: latency-svc-5ndwg [555.743881ms]
Jun 17 15:57:33.687: INFO: Created: latency-svc-s6vkj
Jun 17 15:57:33.718: INFO: Got endpoints: latency-svc-jtmwf [593.662557ms]
Jun 17 15:57:33.734: INFO: Created: latency-svc-g6p6c
Jun 17 15:57:33.768: INFO: Got endpoints: latency-svc-7mhsg [638.82022ms]
Jun 17 15:57:33.786: INFO: Created: latency-svc-c692b
Jun 17 15:57:33.821: INFO: Got endpoints: latency-svc-9krk4 [676.777223ms]
Jun 17 15:57:33.838: INFO: Created: latency-svc-k6kw4
Jun 17 15:57:33.868: INFO: Got endpoints: latency-svc-4xmpd [723.106153ms]
Jun 17 15:57:33.884: INFO: Created: latency-svc-lhc9d
Jun 17 15:57:33.919: INFO: Got endpoints: latency-svc-z5xls [749.993852ms]
Jun 17 15:57:33.941: INFO: Created: latency-svc-nln9b
Jun 17 15:57:33.970: INFO: Got endpoints: latency-svc-txctt [748.748685ms]
Jun 17 15:57:33.987: INFO: Created: latency-svc-8lbq6
Jun 17 15:57:34.018: INFO: Got endpoints: latency-svc-dc6vp [748.968902ms]
Jun 17 15:57:34.034: INFO: Created: latency-svc-n2pvv
Jun 17 15:57:34.068: INFO: Got endpoints: latency-svc-dlzhw [744.151366ms]
Jun 17 15:57:34.083: INFO: Created: latency-svc-ftm7v
Jun 17 15:57:34.118: INFO: Got endpoints: latency-svc-5kzn9 [749.55246ms]
Jun 17 15:57:34.137: INFO: Created: latency-svc-vpxtj
Jun 17 15:57:34.169: INFO: Got endpoints: latency-svc-49cb5 [750.959931ms]
Jun 17 15:57:34.186: INFO: Created: latency-svc-5jpq6
Jun 17 15:57:34.218: INFO: Got endpoints: latency-svc-f8wbb [750.977656ms]
Jun 17 15:57:34.238: INFO: Created: latency-svc-ltfbs
Jun 17 15:57:34.269: INFO: Got endpoints: latency-svc-fgfhl [746.036768ms]
Jun 17 15:57:34.285: INFO: Created: latency-svc-zz2zx
Jun 17 15:57:34.317: INFO: Got endpoints: latency-svc-xmttd [749.403197ms]
Jun 17 15:57:34.334: INFO: Created: latency-svc-qpxvr
Jun 17 15:57:34.369: INFO: Got endpoints: latency-svc-8kw9f [750.534382ms]
Jun 17 15:57:34.386: INFO: Created: latency-svc-psnvz
Jun 17 15:57:34.418: INFO: Got endpoints: latency-svc-s6vkj [746.455826ms]
Jun 17 15:57:34.433: INFO: Created: latency-svc-pbkdz
Jun 17 15:57:34.468: INFO: Got endpoints: latency-svc-g6p6c [750.119078ms]
Jun 17 15:57:34.486: INFO: Created: latency-svc-pwczf
Jun 17 15:57:34.518: INFO: Got endpoints: latency-svc-c692b [749.189622ms]
Jun 17 15:57:34.533: INFO: Created: latency-svc-ncw5b
Jun 17 15:57:34.568: INFO: Got endpoints: latency-svc-k6kw4 [747.223535ms]
Jun 17 15:57:34.582: INFO: Created: latency-svc-hfl78
Jun 17 15:57:34.620: INFO: Got endpoints: latency-svc-lhc9d [751.649095ms]
Jun 17 15:57:34.633: INFO: Created: latency-svc-tq9qs
Jun 17 15:57:34.668: INFO: Got endpoints: latency-svc-nln9b [748.81809ms]
Jun 17 15:57:34.689: INFO: Created: latency-svc-qn4l7
Jun 17 15:57:34.719: INFO: Got endpoints: latency-svc-8lbq6 [749.251423ms]
Jun 17 15:57:34.740: INFO: Created: latency-svc-f8lf8
Jun 17 15:57:34.771: INFO: Got endpoints: latency-svc-n2pvv [753.444036ms]
Jun 17 15:57:34.787: INFO: Created: latency-svc-dnzrw
Jun 17 15:57:34.821: INFO: Got endpoints: latency-svc-ftm7v [752.154231ms]
Jun 17 15:57:34.836: INFO: Created: latency-svc-md8ft
Jun 17 15:57:34.868: INFO: Got endpoints: latency-svc-vpxtj [750.502837ms]
Jun 17 15:57:34.883: INFO: Created: latency-svc-6dp24
Jun 17 15:57:34.920: INFO: Got endpoints: latency-svc-5jpq6 [751.027346ms]
Jun 17 15:57:34.938: INFO: Created: latency-svc-65qwb
Jun 17 15:57:34.970: INFO: Got endpoints: latency-svc-ltfbs [751.701698ms]
Jun 17 15:57:34.985: INFO: Created: latency-svc-dlgdf
Jun 17 15:57:35.019: INFO: Got endpoints: latency-svc-zz2zx [750.392331ms]
Jun 17 15:57:35.034: INFO: Created: latency-svc-8psqs
Jun 17 15:57:35.070: INFO: Got endpoints: latency-svc-qpxvr [752.804175ms]
Jun 17 15:57:35.086: INFO: Created: latency-svc-gr2qp
Jun 17 15:57:35.120: INFO: Got endpoints: latency-svc-psnvz [747.812477ms]
Jun 17 15:57:35.135: INFO: Created: latency-svc-z62cr
Jun 17 15:57:35.172: INFO: Got endpoints: latency-svc-pbkdz [753.481107ms]
Jun 17 15:57:35.195: INFO: Created: latency-svc-j9vbj
Jun 17 15:57:35.220: INFO: Got endpoints: latency-svc-pwczf [751.46154ms]
Jun 17 15:57:35.234: INFO: Created: latency-svc-tg2lk
Jun 17 15:57:35.272: INFO: Got endpoints: latency-svc-ncw5b [753.455935ms]
Jun 17 15:57:35.285: INFO: Created: latency-svc-j2lfd
Jun 17 15:57:35.320: INFO: Got endpoints: latency-svc-hfl78 [751.422597ms]
Jun 17 15:57:35.335: INFO: Created: latency-svc-mjs6w
Jun 17 15:57:35.368: INFO: Got endpoints: latency-svc-tq9qs [748.308409ms]
Jun 17 15:57:35.388: INFO: Created: latency-svc-2vcq9
Jun 17 15:57:35.420: INFO: Got endpoints: latency-svc-qn4l7 [752.123379ms]
Jun 17 15:57:35.436: INFO: Created: latency-svc-85xgt
Jun 17 15:57:35.469: INFO: Got endpoints: latency-svc-f8lf8 [750.264876ms]
Jun 17 15:57:35.485: INFO: Created: latency-svc-nhcz8
Jun 17 15:57:35.518: INFO: Got endpoints: latency-svc-dnzrw [747.205753ms]
Jun 17 15:57:35.534: INFO: Created: latency-svc-5q7z2
Jun 17 15:57:35.568: INFO: Got endpoints: latency-svc-md8ft [747.218315ms]
Jun 17 15:57:35.588: INFO: Created: latency-svc-4lw88
Jun 17 15:57:35.617: INFO: Got endpoints: latency-svc-6dp24 [749.042538ms]
Jun 17 15:57:35.632: INFO: Created: latency-svc-pkfvd
Jun 17 15:57:35.669: INFO: Got endpoints: latency-svc-65qwb [749.178912ms]
Jun 17 15:57:35.686: INFO: Created: latency-svc-pzhtg
Jun 17 15:57:35.718: INFO: Got endpoints: latency-svc-dlgdf [747.239638ms]
Jun 17 15:57:35.736: INFO: Created: latency-svc-q6dns
Jun 17 15:57:35.770: INFO: Got endpoints: latency-svc-8psqs [751.158672ms]
Jun 17 15:57:35.787: INFO: Created: latency-svc-pdvlw
Jun 17 15:57:35.819: INFO: Got endpoints: latency-svc-gr2qp [748.231524ms]
Jun 17 15:57:35.834: INFO: Created: latency-svc-gx5k8
Jun 17 15:57:35.870: INFO: Got endpoints: latency-svc-z62cr [750.301096ms]
Jun 17 15:57:35.886: INFO: Created: latency-svc-f962d
Jun 17 15:57:35.919: INFO: Got endpoints: latency-svc-j9vbj [744.587359ms]
Jun 17 15:57:35.934: INFO: Created: latency-svc-sw6cs
Jun 17 15:57:35.969: INFO: Got endpoints: latency-svc-tg2lk [749.61494ms]
Jun 17 15:57:35.986: INFO: Created: latency-svc-ff7j8
Jun 17 15:57:36.021: INFO: Got endpoints: latency-svc-j2lfd [748.838878ms]
Jun 17 15:57:36.035: INFO: Created: latency-svc-p9svk
Jun 17 15:57:36.071: INFO: Got endpoints: latency-svc-mjs6w [750.549246ms]
Jun 17 15:57:36.086: INFO: Created: latency-svc-z4bgf
Jun 17 15:57:36.118: INFO: Got endpoints: latency-svc-2vcq9 [749.926683ms]
Jun 17 15:57:36.145: INFO: Created: latency-svc-8rh9k
Jun 17 15:57:36.168: INFO: Got endpoints: latency-svc-85xgt [747.881408ms]
Jun 17 15:57:36.188: INFO: Created: latency-svc-cd67h
Jun 17 15:57:36.219: INFO: Got endpoints: latency-svc-nhcz8 [749.967001ms]
Jun 17 15:57:36.238: INFO: Created: latency-svc-hvn52
Jun 17 15:57:36.269: INFO: Got endpoints: latency-svc-5q7z2 [750.529774ms]
Jun 17 15:57:36.286: INFO: Created: latency-svc-gz4lt
Jun 17 15:57:36.320: INFO: Got endpoints: latency-svc-4lw88 [752.244627ms]
Jun 17 15:57:36.336: INFO: Created: latency-svc-vmwgz
Jun 17 15:57:36.371: INFO: Got endpoints: latency-svc-pkfvd [753.325159ms]
Jun 17 15:57:36.388: INFO: Created: latency-svc-m9c82
Jun 17 15:57:36.421: INFO: Got endpoints: latency-svc-pzhtg [749.423262ms]
Jun 17 15:57:36.440: INFO: Created: latency-svc-vmngc
Jun 17 15:57:36.468: INFO: Got endpoints: latency-svc-q6dns [749.782508ms]
Jun 17 15:57:36.489: INFO: Created: latency-svc-6ctdb
Jun 17 15:57:36.520: INFO: Got endpoints: latency-svc-pdvlw [748.988251ms]
Jun 17 15:57:36.535: INFO: Created: latency-svc-rhg4x
Jun 17 15:57:36.568: INFO: Got endpoints: latency-svc-gx5k8 [748.912238ms]
Jun 17 15:57:36.583: INFO: Created: latency-svc-pgrcb
Jun 17 15:57:36.618: INFO: Got endpoints: latency-svc-f962d [747.388141ms]
Jun 17 15:57:36.631: INFO: Created: latency-svc-npr8v
Jun 17 15:57:36.671: INFO: Got endpoints: latency-svc-sw6cs [752.26612ms]
Jun 17 15:57:36.684: INFO: Created: latency-svc-qxqlk
Jun 17 15:57:36.718: INFO: Got endpoints: latency-svc-ff7j8 [748.193209ms]
Jun 17 15:57:36.734: INFO: Created: latency-svc-56bbd
Jun 17 15:57:36.770: INFO: Got endpoints: latency-svc-p9svk [749.550808ms]
Jun 17 15:57:36.792: INFO: Created: latency-svc-97lqq
Jun 17 15:57:36.819: INFO: Got endpoints: latency-svc-z4bgf [747.948427ms]
Jun 17 15:57:36.839: INFO: Created: latency-svc-cgrkr
Jun 17 15:57:36.868: INFO: Got endpoints: latency-svc-8rh9k [750.237918ms]
Jun 17 15:57:36.883: INFO: Created: latency-svc-8cvzh
Jun 17 15:57:36.918: INFO: Got endpoints: latency-svc-cd67h [749.826097ms]
Jun 17 15:57:36.934: INFO: Created: latency-svc-74f7n
Jun 17 15:57:36.968: INFO: Got endpoints: latency-svc-hvn52 [748.498437ms]
Jun 17 15:57:36.985: INFO: Created: latency-svc-fvctm
Jun 17 15:57:37.018: INFO: Got endpoints: latency-svc-gz4lt [748.730741ms]
Jun 17 15:57:37.031: INFO: Created: latency-svc-6264l
Jun 17 15:57:37.070: INFO: Got endpoints: latency-svc-vmwgz [750.14541ms]
Jun 17 15:57:37.084: INFO: Created: latency-svc-fjlfv
Jun 17 15:57:37.118: INFO: Got endpoints: latency-svc-m9c82 [745.206641ms]
Jun 17 15:57:37.134: INFO: Created: latency-svc-g4knp
Jun 17 15:57:37.169: INFO: Got endpoints: latency-svc-vmngc [745.192326ms]
Jun 17 15:57:37.185: INFO: Created: latency-svc-2wkbf
Jun 17 15:57:37.219: INFO: Got endpoints: latency-svc-6ctdb [750.218657ms]
Jun 17 15:57:37.235: INFO: Created: latency-svc-xrdrt
Jun 17 15:57:37.269: INFO: Got endpoints: latency-svc-rhg4x [749.419117ms]
Jun 17 15:57:37.285: INFO: Created: latency-svc-cbttf
Jun 17 15:57:37.319: INFO: Got endpoints: latency-svc-pgrcb [750.658161ms]
Jun 17 15:57:37.333: INFO: Created: latency-svc-rwtql
Jun 17 15:57:37.371: INFO: Got endpoints: latency-svc-npr8v [753.473888ms]
Jun 17 15:57:37.387: INFO: Created: latency-svc-wkl7p
Jun 17 15:57:37.421: INFO: Got endpoints: latency-svc-qxqlk [749.589064ms]
Jun 17 15:57:37.439: INFO: Created: latency-svc-9kgvb
Jun 17 15:57:37.469: INFO: Got endpoints: latency-svc-56bbd [750.874153ms]
Jun 17 15:57:37.484: INFO: Created: latency-svc-rzhct
Jun 17 15:57:37.518: INFO: Got endpoints: latency-svc-97lqq [748.13169ms]
Jun 17 15:57:37.534: INFO: Created: latency-svc-gzmxs
Jun 17 15:57:37.568: INFO: Got endpoints: latency-svc-cgrkr [745.501123ms]
Jun 17 15:57:37.585: INFO: Created: latency-svc-2ttrk
Jun 17 15:57:37.619: INFO: Got endpoints: latency-svc-8cvzh [750.200064ms]
Jun 17 15:57:37.635: INFO: Created: latency-svc-q48v9
Jun 17 15:57:37.669: INFO: Got endpoints: latency-svc-74f7n [750.299367ms]
Jun 17 15:57:37.684: INFO: Created: latency-svc-r6mrw
Jun 17 15:57:37.720: INFO: Got endpoints: latency-svc-fvctm [752.435171ms]
Jun 17 15:57:37.739: INFO: Created: latency-svc-vhmjr
Jun 17 15:57:37.769: INFO: Got endpoints: latency-svc-6264l [751.003849ms]
Jun 17 15:57:37.786: INFO: Created: latency-svc-d75ts
Jun 17 15:57:37.821: INFO: Got endpoints: latency-svc-fjlfv [750.548844ms]
Jun 17 15:57:37.838: INFO: Created: latency-svc-8l2r8
Jun 17 15:57:37.869: INFO: Got endpoints: latency-svc-g4knp [751.42526ms]
Jun 17 15:57:37.889: INFO: Created: latency-svc-sl85j
Jun 17 15:57:37.919: INFO: Got endpoints: latency-svc-2wkbf [749.635926ms]
Jun 17 15:57:37.936: INFO: Created: latency-svc-xkhwb
Jun 17 15:57:37.967: INFO: Got endpoints: latency-svc-xrdrt [748.562024ms]
Jun 17 15:57:37.990: INFO: Created: latency-svc-6n2j2
Jun 17 15:57:38.018: INFO: Got endpoints: latency-svc-cbttf [749.059087ms]
Jun 17 15:57:38.035: INFO: Created: latency-svc-t6h54
Jun 17 15:57:38.071: INFO: Got endpoints: latency-svc-rwtql [751.586485ms]
Jun 17 15:57:38.084: INFO: Created: latency-svc-gmrwr
Jun 17 15:57:38.121: INFO: Got endpoints: latency-svc-wkl7p [749.451389ms]
Jun 17 15:57:38.136: INFO: Created: latency-svc-9bxfz
Jun 17 15:57:38.172: INFO: Got endpoints: latency-svc-9kgvb [750.458432ms]
Jun 17 15:57:38.191: INFO: Created: latency-svc-jr9bs
Jun 17 15:57:38.219: INFO: Got endpoints: latency-svc-rzhct [749.933195ms]
Jun 17 15:57:38.241: INFO: Created: latency-svc-rplt4
Jun 17 15:57:38.268: INFO: Got endpoints: latency-svc-gzmxs [749.896412ms]
Jun 17 15:57:38.284: INFO: Created: latency-svc-cxkzf
Jun 17 15:57:38.318: INFO: Got endpoints: latency-svc-2ttrk [750.291156ms]
Jun 17 15:57:38.339: INFO: Created: latency-svc-cwb2z
Jun 17 15:57:38.369: INFO: Got endpoints: latency-svc-q48v9 [749.845308ms]
Jun 17 15:57:38.387: INFO: Created: latency-svc-xwnxw
Jun 17 15:57:38.422: INFO: Got endpoints: latency-svc-r6mrw [753.400091ms]
Jun 17 15:57:38.437: INFO: Created: latency-svc-kkxdx
Jun 17 15:57:38.471: INFO: Got endpoints: latency-svc-vhmjr [750.063571ms]
Jun 17 15:57:38.484: INFO: Created: latency-svc-hf44f
Jun 17 15:57:38.520: INFO: Got endpoints: latency-svc-d75ts [751.630551ms]
Jun 17 15:57:38.537: INFO: Created: latency-svc-fbpb8
Jun 17 15:57:38.569: INFO: Got endpoints: latency-svc-8l2r8 [748.225422ms]
Jun 17 15:57:38.588: INFO: Created: latency-svc-r8ftz
Jun 17 15:57:38.624: INFO: Got endpoints: latency-svc-sl85j [754.738281ms]
Jun 17 15:57:38.642: INFO: Created: latency-svc-qzkz6
Jun 17 15:57:38.668: INFO: Got endpoints: latency-svc-xkhwb [749.536364ms]
Jun 17 15:57:38.684: INFO: Created: latency-svc-hrwxg
Jun 17 15:57:38.718: INFO: Got endpoints: latency-svc-6n2j2 [746.973587ms]
Jun 17 15:57:38.732: INFO: Created: latency-svc-nlmn8
Jun 17 15:57:38.770: INFO: Got endpoints: latency-svc-t6h54 [751.765702ms]
Jun 17 15:57:38.785: INFO: Created: latency-svc-dk5p4
Jun 17 15:57:38.821: INFO: Got endpoints: latency-svc-gmrwr [749.71597ms]
Jun 17 15:57:38.837: INFO: Created: latency-svc-wbttt
Jun 17 15:57:38.869: INFO: Got endpoints: latency-svc-9bxfz [746.932525ms]
Jun 17 15:57:38.889: INFO: Created: latency-svc-vxcx4
Jun 17 15:57:38.921: INFO: Got endpoints: latency-svc-jr9bs [748.803643ms]
Jun 17 15:57:38.937: INFO: Created: latency-svc-hc6bj
Jun 17 15:57:38.970: INFO: Got endpoints: latency-svc-rplt4 [751.543617ms]
Jun 17 15:57:38.990: INFO: Created: latency-svc-pw7nm
Jun 17 15:57:39.018: INFO: Got endpoints: latency-svc-cxkzf [749.677307ms]
Jun 17 15:57:39.035: INFO: Created: latency-svc-l2bqs
Jun 17 15:57:39.068: INFO: Got endpoints: latency-svc-cwb2z [749.244447ms]
Jun 17 15:57:39.081: INFO: Created: latency-svc-hlnjp
Jun 17 15:57:39.118: INFO: Got endpoints: latency-svc-xwnxw [748.403139ms]
Jun 17 15:57:39.133: INFO: Created: latency-svc-fjv8f
Jun 17 15:57:39.169: INFO: Got endpoints: latency-svc-kkxdx [746.421003ms]
Jun 17 15:57:39.188: INFO: Created: latency-svc-bcv7h
Jun 17 15:57:39.219: INFO: Got endpoints: latency-svc-hf44f [748.355315ms]
Jun 17 15:57:39.237: INFO: Created: latency-svc-d8nn5
Jun 17 15:57:39.268: INFO: Got endpoints: latency-svc-fbpb8 [747.564373ms]
Jun 17 15:57:39.283: INFO: Created: latency-svc-xn2cn
Jun 17 15:57:39.319: INFO: Got endpoints: latency-svc-r8ftz [749.419052ms]
Jun 17 15:57:39.343: INFO: Created: latency-svc-72tnj
Jun 17 15:57:39.368: INFO: Got endpoints: latency-svc-qzkz6 [743.610967ms]
Jun 17 15:57:39.384: INFO: Created: latency-svc-7zjff
Jun 17 15:57:39.418: INFO: Got endpoints: latency-svc-hrwxg [749.171316ms]
Jun 17 15:57:39.430: INFO: Created: latency-svc-tls2p
Jun 17 15:57:39.470: INFO: Got endpoints: latency-svc-nlmn8 [751.12777ms]
Jun 17 15:57:39.482: INFO: Created: latency-svc-8zvd8
Jun 17 15:57:39.518: INFO: Got endpoints: latency-svc-dk5p4 [747.803773ms]
Jun 17 15:57:39.534: INFO: Created: latency-svc-qsgzm
Jun 17 15:57:39.568: INFO: Got endpoints: latency-svc-wbttt [747.074495ms]
Jun 17 15:57:39.584: INFO: Created: latency-svc-l5n79
Jun 17 15:57:39.621: INFO: Got endpoints: latency-svc-vxcx4 [751.951699ms]
Jun 17 15:57:39.638: INFO: Created: latency-svc-pvlbt
Jun 17 15:57:39.668: INFO: Got endpoints: latency-svc-hc6bj [747.792152ms]
Jun 17 15:57:39.687: INFO: Created: latency-svc-dth8l
Jun 17 15:57:39.718: INFO: Got endpoints: latency-svc-pw7nm [747.769007ms]
Jun 17 15:57:39.732: INFO: Created: latency-svc-4xmm6
Jun 17 15:57:39.771: INFO: Got endpoints: latency-svc-l2bqs [752.942906ms]
Jun 17 15:57:39.785: INFO: Created: latency-svc-mhrs7
Jun 17 15:57:39.818: INFO: Got endpoints: latency-svc-hlnjp [750.150725ms]
Jun 17 15:57:39.835: INFO: Created: latency-svc-r4sxr
Jun 17 15:57:39.869: INFO: Got endpoints: latency-svc-fjv8f [751.510722ms]
Jun 17 15:57:39.885: INFO: Created: latency-svc-bfntc
Jun 17 15:57:39.921: INFO: Got endpoints: latency-svc-bcv7h [751.209453ms]
Jun 17 15:57:39.942: INFO: Created: latency-svc-25bhz
Jun 17 15:57:39.969: INFO: Got endpoints: latency-svc-d8nn5 [749.78622ms]
Jun 17 15:57:39.988: INFO: Created: latency-svc-pmm22
Jun 17 15:57:40.018: INFO: Got endpoints: latency-svc-xn2cn [749.956778ms]
Jun 17 15:57:40.035: INFO: Created: latency-svc-xfvqj
Jun 17 15:57:40.068: INFO: Got endpoints: latency-svc-72tnj [749.110147ms]
Jun 17 15:57:40.084: INFO: Created: latency-svc-wcz9q
Jun 17 15:57:40.120: INFO: Got endpoints: latency-svc-7zjff [751.651816ms]
Jun 17 15:57:40.133: INFO: Created: latency-svc-7gpwf
Jun 17 15:57:40.168: INFO: Got endpoints: latency-svc-tls2p [750.386355ms]
Jun 17 15:57:40.186: INFO: Created: latency-svc-jglpx
Jun 17 15:57:40.221: INFO: Got endpoints: latency-svc-8zvd8 [751.875573ms]
Jun 17 15:57:40.236: INFO: Created: latency-svc-27cwj
Jun 17 15:57:40.271: INFO: Got endpoints: latency-svc-qsgzm [752.21111ms]
Jun 17 15:57:40.288: INFO: Created: latency-svc-8g4f9
Jun 17 15:57:40.318: INFO: Got endpoints: latency-svc-l5n79 [750.526533ms]
Jun 17 15:57:40.340: INFO: Created: latency-svc-qkm9r
Jun 17 15:57:40.369: INFO: Got endpoints: latency-svc-pvlbt [747.883989ms]
Jun 17 15:57:40.386: INFO: Created: latency-svc-d984s
Jun 17 15:57:40.420: INFO: Got endpoints: latency-svc-dth8l [751.239682ms]
Jun 17 15:57:40.434: INFO: Created: latency-svc-vwzcj
Jun 17 15:57:40.470: INFO: Got endpoints: latency-svc-4xmm6 [751.803972ms]
Jun 17 15:57:40.488: INFO: Created: latency-svc-ppm6b
Jun 17 15:57:40.521: INFO: Got endpoints: latency-svc-mhrs7 [749.151731ms]
Jun 17 15:57:40.535: INFO: Created: latency-svc-sfmvq
Jun 17 15:57:40.571: INFO: Got endpoints: latency-svc-r4sxr [752.704961ms]
Jun 17 15:57:40.595: INFO: Created: latency-svc-9ndp7
Jun 17 15:57:40.618: INFO: Got endpoints: latency-svc-bfntc [748.669747ms]
Jun 17 15:57:40.668: INFO: Got endpoints: latency-svc-25bhz [746.934431ms]
Jun 17 15:57:40.718: INFO: Got endpoints: latency-svc-pmm22 [749.343965ms]
Jun 17 15:57:40.770: INFO: Got endpoints: latency-svc-xfvqj [751.668821ms]
Jun 17 15:57:40.818: INFO: Got endpoints: latency-svc-wcz9q [749.378416ms]
Jun 17 15:57:40.869: INFO: Got endpoints: latency-svc-7gpwf [749.928224ms]
Jun 17 15:57:40.919: INFO: Got endpoints: latency-svc-jglpx [750.887719ms]
Jun 17 15:57:40.972: INFO: Got endpoints: latency-svc-27cwj [750.875246ms]
Jun 17 15:57:41.020: INFO: Got endpoints: latency-svc-8g4f9 [749.392313ms]
Jun 17 15:57:41.069: INFO: Got endpoints: latency-svc-qkm9r [750.957708ms]
Jun 17 15:57:41.118: INFO: Got endpoints: latency-svc-d984s [749.15621ms]
Jun 17 15:57:41.170: INFO: Got endpoints: latency-svc-vwzcj [749.896137ms]
Jun 17 15:57:41.225: INFO: Got endpoints: latency-svc-ppm6b [754.2174ms]
Jun 17 15:57:41.269: INFO: Got endpoints: latency-svc-sfmvq [748.426349ms]
Jun 17 15:57:41.318: INFO: Got endpoints: latency-svc-9ndp7 [747.18002ms]
Jun 17 15:57:41.318: INFO: Latencies: [22.599475ms 34.861106ms 47.612143ms 58.303744ms 73.97194ms 83.627281ms 95.636216ms 97.822311ms 109.2203ms 122.36558ms 135.163735ms 135.262042ms 138.276727ms 138.807457ms 138.934091ms 142.660518ms 143.015488ms 146.88168ms 149.007691ms 149.683531ms 149.854838ms 152.36488ms 153.857206ms 153.867621ms 154.605898ms 155.551581ms 160.559582ms 160.592399ms 160.636191ms 161.063557ms 162.31722ms 165.786206ms 166.551066ms 167.965261ms 168.103391ms 170.746555ms 178.239496ms 182.354643ms 226.952185ms 268.225866ms 305.703095ms 347.194777ms 388.002208ms 430.837918ms 471.419079ms 514.469609ms 555.743881ms 593.662557ms 638.82022ms 676.777223ms 723.106153ms 743.610967ms 744.151366ms 744.587359ms 745.192326ms 745.206641ms 745.501123ms 746.036768ms 746.421003ms 746.455826ms 746.932525ms 746.934431ms 746.973587ms 747.074495ms 747.18002ms 747.205753ms 747.218315ms 747.223535ms 747.239638ms 747.388141ms 747.564373ms 747.769007ms 747.792152ms 747.803773ms 747.812477ms 747.881408ms 747.883989ms 747.948427ms 748.13169ms 748.193209ms 748.225422ms 748.231524ms 748.308409ms 748.355315ms 748.403139ms 748.426349ms 748.498437ms 748.562024ms 748.669747ms 748.730741ms 748.748685ms 748.803643ms 748.81809ms 748.838878ms 748.912238ms 748.968902ms 748.988251ms 749.042538ms 749.059087ms 749.110147ms 749.151731ms 749.15621ms 749.171316ms 749.178912ms 749.189622ms 749.244447ms 749.251423ms 749.343965ms 749.378416ms 749.392313ms 749.403197ms 749.419052ms 749.419117ms 749.423262ms 749.451389ms 749.536364ms 749.550808ms 749.55246ms 749.589064ms 749.61494ms 749.635926ms 749.677307ms 749.71597ms 749.782508ms 749.78622ms 749.826097ms 749.845308ms 749.896137ms 749.896412ms 749.926683ms 749.928224ms 749.933195ms 749.956778ms 749.967001ms 749.993852ms 750.063571ms 750.119078ms 750.14541ms 750.150725ms 750.200064ms 750.218657ms 750.237918ms 750.264876ms 750.291156ms 750.299367ms 750.301096ms 750.386355ms 750.392331ms 750.458432ms 750.502837ms 750.526533ms 750.529774ms 750.534382ms 750.548844ms 750.549246ms 750.658161ms 750.874153ms 750.875246ms 750.887719ms 750.957708ms 750.959931ms 750.977656ms 751.003849ms 751.027346ms 751.12777ms 751.158672ms 751.209453ms 751.239682ms 751.422597ms 751.42526ms 751.46154ms 751.510722ms 751.543617ms 751.586485ms 751.630551ms 751.649095ms 751.651816ms 751.668821ms 751.701698ms 751.765702ms 751.803972ms 751.875573ms 751.951699ms 752.123379ms 752.154231ms 752.21111ms 752.244627ms 752.26612ms 752.435171ms 752.704961ms 752.804175ms 752.942906ms 753.325159ms 753.400091ms 753.444036ms 753.455935ms 753.473888ms 753.481107ms 754.2174ms 754.738281ms]
Jun 17 15:57:41.319: INFO: 50 %ile: 749.151731ms
Jun 17 15:57:41.319: INFO: 90 %ile: 751.803972ms
Jun 17 15:57:41.319: INFO: 99 %ile: 754.2174ms
Jun 17 15:57:41.319: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:57:41.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2164" for this suite.
Jun 17 15:57:53.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:57:53.500: INFO: namespace svc-latency-2164 deletion completed in 12.167287902s

• [SLOW TEST:23.082 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:57:53.501: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3946
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 15:57:53.668: INFO: (0) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.46234ms)
Jun 17 15:57:53.672: INFO: (1) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.114969ms)
Jun 17 15:57:53.677: INFO: (2) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.646161ms)
Jun 17 15:57:53.681: INFO: (3) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.030248ms)
Jun 17 15:57:53.685: INFO: (4) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.68712ms)
Jun 17 15:57:53.691: INFO: (5) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.201504ms)
Jun 17 15:57:53.700: INFO: (6) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.711063ms)
Jun 17 15:57:53.703: INFO: (7) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.733894ms)
Jun 17 15:57:53.707: INFO: (8) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.670499ms)
Jun 17 15:57:53.713: INFO: (9) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.955953ms)
Jun 17 15:57:53.717: INFO: (10) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.764244ms)
Jun 17 15:57:53.721: INFO: (11) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.335014ms)
Jun 17 15:57:53.726: INFO: (12) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.870356ms)
Jun 17 15:57:53.730: INFO: (13) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.30009ms)
Jun 17 15:57:53.734: INFO: (14) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.00618ms)
Jun 17 15:57:53.739: INFO: (15) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.880554ms)
Jun 17 15:57:53.744: INFO: (16) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.620972ms)
Jun 17 15:57:53.748: INFO: (17) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.308376ms)
Jun 17 15:57:53.753: INFO: (18) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.295808ms)
Jun 17 15:57:53.757: INFO: (19) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.408163ms)
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:57:53.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3946" for this suite.
Jun 17 15:57:59.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:57:59.910: INFO: namespace proxy-3946 deletion completed in 6.148655549s

• [SLOW TEST:6.409 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:57:59.912: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6008
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-bfzh
STEP: Creating a pod to test atomic-volume-subpath
Jun 17 15:58:00.100: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-bfzh" in namespace "subpath-6008" to be "success or failure"
Jun 17 15:58:00.107: INFO: Pod "pod-subpath-test-downwardapi-bfzh": Phase="Pending", Reason="", readiness=false. Elapsed: 7.665719ms
Jun 17 15:58:02.112: INFO: Pod "pod-subpath-test-downwardapi-bfzh": Phase="Running", Reason="", readiness=true. Elapsed: 2.012394931s
Jun 17 15:58:04.118: INFO: Pod "pod-subpath-test-downwardapi-bfzh": Phase="Running", Reason="", readiness=true. Elapsed: 4.018832208s
Jun 17 15:58:06.123: INFO: Pod "pod-subpath-test-downwardapi-bfzh": Phase="Running", Reason="", readiness=true. Elapsed: 6.023500603s
Jun 17 15:58:08.127: INFO: Pod "pod-subpath-test-downwardapi-bfzh": Phase="Running", Reason="", readiness=true. Elapsed: 8.027527816s
Jun 17 15:58:10.132: INFO: Pod "pod-subpath-test-downwardapi-bfzh": Phase="Running", Reason="", readiness=true. Elapsed: 10.032902533s
Jun 17 15:58:12.140: INFO: Pod "pod-subpath-test-downwardapi-bfzh": Phase="Running", Reason="", readiness=true. Elapsed: 12.040215944s
Jun 17 15:58:14.145: INFO: Pod "pod-subpath-test-downwardapi-bfzh": Phase="Running", Reason="", readiness=true. Elapsed: 14.045585881s
Jun 17 15:58:16.150: INFO: Pod "pod-subpath-test-downwardapi-bfzh": Phase="Running", Reason="", readiness=true. Elapsed: 16.050672638s
Jun 17 15:58:18.155: INFO: Pod "pod-subpath-test-downwardapi-bfzh": Phase="Running", Reason="", readiness=true. Elapsed: 18.055811747s
Jun 17 15:58:20.161: INFO: Pod "pod-subpath-test-downwardapi-bfzh": Phase="Running", Reason="", readiness=true. Elapsed: 20.061236008s
Jun 17 15:58:22.165: INFO: Pod "pod-subpath-test-downwardapi-bfzh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.065788046s
STEP: Saw pod success
Jun 17 15:58:22.165: INFO: Pod "pod-subpath-test-downwardapi-bfzh" satisfied condition "success or failure"
Jun 17 15:58:22.170: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-subpath-test-downwardapi-bfzh container test-container-subpath-downwardapi-bfzh: <nil>
STEP: delete the pod
Jun 17 15:58:22.204: INFO: Waiting for pod pod-subpath-test-downwardapi-bfzh to disappear
Jun 17 15:58:22.208: INFO: Pod pod-subpath-test-downwardapi-bfzh no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-bfzh
Jun 17 15:58:22.208: INFO: Deleting pod "pod-subpath-test-downwardapi-bfzh" in namespace "subpath-6008"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:58:22.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6008" for this suite.
Jun 17 15:58:28.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:58:28.366: INFO: namespace subpath-6008 deletion completed in 6.143735224s

• [SLOW TEST:28.455 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:58:28.370: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3848
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-3848
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 17 15:58:28.537: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 17 15:58:50.651: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.74.47 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3848 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 15:58:50.652: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 15:58:51.844: INFO: Found all expected endpoints: [netserver-0]
Jun 17 15:58:51.848: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.95.156 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3848 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 15:58:51.848: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 15:58:53.044: INFO: Found all expected endpoints: [netserver-1]
Jun 17 15:58:53.049: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.129 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3848 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 15:58:53.049: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 15:58:54.269: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:58:54.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3848" for this suite.
Jun 17 15:59:16.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:59:16.436: INFO: namespace pod-network-test-3848 deletion completed in 22.159532231s

• [SLOW TEST:48.067 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:59:16.439: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3802
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-dbe7b67f-9118-11e9-a8b9-dace53c98186
STEP: Creating a pod to test consume configMaps
Jun 17 15:59:16.635: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dbe8b504-9118-11e9-a8b9-dace53c98186" in namespace "projected-3802" to be "success or failure"
Jun 17 15:59:16.642: INFO: Pod "pod-projected-configmaps-dbe8b504-9118-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 6.777055ms
Jun 17 15:59:18.646: INFO: Pod "pod-projected-configmaps-dbe8b504-9118-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01150186s
Jun 17 15:59:20.651: INFO: Pod "pod-projected-configmaps-dbe8b504-9118-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0164813s
STEP: Saw pod success
Jun 17 15:59:20.652: INFO: Pod "pod-projected-configmaps-dbe8b504-9118-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 15:59:20.656: INFO: Trying to get logs from node lab1-k8s-node-2 pod pod-projected-configmaps-dbe8b504-9118-11e9-a8b9-dace53c98186 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 17 15:59:20.683: INFO: Waiting for pod pod-projected-configmaps-dbe8b504-9118-11e9-a8b9-dace53c98186 to disappear
Jun 17 15:59:20.688: INFO: Pod pod-projected-configmaps-dbe8b504-9118-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:59:20.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3802" for this suite.
Jun 17 15:59:26.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:59:26.850: INFO: namespace projected-3802 deletion completed in 6.156651786s

• [SLOW TEST:10.411 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:59:26.851: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7520
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 17 15:59:27.059: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 17 15:59:27.072: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 17 15:59:32.079: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 17 15:59:32.079: INFO: Creating deployment "test-rolling-update-deployment"
Jun 17 15:59:32.091: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 17 15:59:32.114: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Jun 17 15:59:34.123: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 17 15:59:34.127: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383972, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383972, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383972, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696383972, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 17 15:59:36.132: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jun 17 15:59:36.143: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-7520,SelfLink:/apis/apps/v1/namespaces/deployment-7520/deployments/test-rolling-update-deployment,UID:e520070a-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:34968,Generation:1,CreationTimestamp:2019-06-17 15:59:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-17 15:59:32 +0000 UTC 2019-06-17 15:59:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-17 15:59:34 +0000 UTC 2019-06-17 15:59:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 17 15:59:36.149: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-7520,SelfLink:/apis/apps/v1/namespaces/deployment-7520/replicasets/test-rolling-update-deployment-67599b4d9,UID:e523f04a-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:34957,Generation:1,CreationTimestamp:2019-06-17 15:59:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment e520070a-9118-11e9-bdd5-fa163e38c6bd 0xc0025d3940 0xc0025d3941}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 17 15:59:36.149: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 17 15:59:36.149: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-7520,SelfLink:/apis/apps/v1/namespaces/deployment-7520/replicasets/test-rolling-update-controller,UID:e221fb23-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:34967,Generation:2,CreationTimestamp:2019-06-17 15:59:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment e520070a-9118-11e9-bdd5-fa163e38c6bd 0xc0025d3877 0xc0025d3878}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 17 15:59:36.153: INFO: Pod "test-rolling-update-deployment-67599b4d9-vzmrc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-vzmrc,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-7520,SelfLink:/api/v1/namespaces/deployment-7520/pods/test-rolling-update-deployment-67599b4d9-vzmrc,UID:e5251202-9118-11e9-bdd5-fa163e38c6bd,ResourceVersion:34956,Generation:0,CreationTimestamp:2019-06-17 15:59:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 e523f04a-9118-11e9-bdd5-fa163e38c6bd 0xc00298a1c0 0xc00298a1c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nqpx7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nqpx7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-nqpx7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00298a230} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00298a250}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:59:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:59:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:59:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-17 15:59:32 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.7,PodIP:10.233.64.131,StartTime:2019-06-17 15:59:32 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-17 15:59:33 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://c39e37635252b77640f978670e097aa688567f04e33bfc11bcb97437e668ba15}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:59:36.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7520" for this suite.
Jun 17 15:59:42.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 15:59:42.307: INFO: namespace deployment-7520 deletion completed in 6.148384704s

• [SLOW TEST:15.456 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 15:59:42.308: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6796
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 17 15:59:46.530: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 17 15:59:46.534: INFO: Pod pod-with-poststart-http-hook still exists
Jun 17 15:59:48.534: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 17 15:59:48.538: INFO: Pod pod-with-poststart-http-hook still exists
Jun 17 15:59:50.534: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 17 15:59:50.538: INFO: Pod pod-with-poststart-http-hook still exists
Jun 17 15:59:52.534: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 17 15:59:52.538: INFO: Pod pod-with-poststart-http-hook still exists
Jun 17 15:59:54.534: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 17 15:59:54.538: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 15:59:54.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6796" for this suite.
Jun 17 16:00:16.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 16:00:16.692: INFO: namespace container-lifecycle-hook-6796 deletion completed in 22.147658657s

• [SLOW TEST:34.384 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 16:00:16.692: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1502
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jun 17 16:00:19.417: INFO: Successfully updated pod "annotationupdateffd0935a-9118-11e9-a8b9-dace53c98186"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 16:00:21.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1502" for this suite.
Jun 17 16:00:35.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 16:00:35.587: INFO: namespace downward-api-1502 deletion completed in 14.145163701s

• [SLOW TEST:18.895 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 16:00:35.587: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8654
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Jun 17 16:00:35.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 api-versions'
Jun 17 16:00:35.841: INFO: stderr: ""
Jun 17 16:00:35.841: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 16:00:35.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8654" for this suite.
Jun 17 16:00:41.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 16:00:41.995: INFO: namespace kubectl-8654 deletion completed in 6.14752955s

• [SLOW TEST:6.408 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 16:00:41.996: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7273
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Jun 17 16:00:42.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 create -f - --namespace=kubectl-7273'
Jun 17 16:00:42.429: INFO: stderr: ""
Jun 17 16:00:42.429: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 17 16:00:43.434: INFO: Selector matched 1 pods for map[app:redis]
Jun 17 16:00:43.434: INFO: Found 0 / 1
Jun 17 16:00:44.434: INFO: Selector matched 1 pods for map[app:redis]
Jun 17 16:00:44.434: INFO: Found 0 / 1
Jun 17 16:00:45.434: INFO: Selector matched 1 pods for map[app:redis]
Jun 17 16:00:45.434: INFO: Found 1 / 1
Jun 17 16:00:45.434: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 17 16:00:45.439: INFO: Selector matched 1 pods for map[app:redis]
Jun 17 16:00:45.439: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 17 16:00:45.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 patch pod redis-master-5tptz --namespace=kubectl-7273 -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 17 16:00:45.538: INFO: stderr: ""
Jun 17 16:00:45.538: INFO: stdout: "pod/redis-master-5tptz patched\n"
STEP: checking annotations
Jun 17 16:00:45.542: INFO: Selector matched 1 pods for map[app:redis]
Jun 17 16:00:45.542: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 16:00:45.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7273" for this suite.
Jun 17 16:01:07.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 16:01:07.699: INFO: namespace kubectl-7273 deletion completed in 22.150314898s

• [SLOW TEST:25.703 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 16:01:07.699: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3901
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-1e3719fe-9119-11e9-a8b9-dace53c98186
STEP: Creating secret with name s-test-opt-upd-1e371a4e-9119-11e9-a8b9-dace53c98186
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-1e3719fe-9119-11e9-a8b9-dace53c98186
STEP: Updating secret s-test-opt-upd-1e371a4e-9119-11e9-a8b9-dace53c98186
STEP: Creating secret with name s-test-opt-create-1e371a66-9119-11e9-a8b9-dace53c98186
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 16:02:22.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3901" for this suite.
Jun 17 16:02:44.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 16:02:44.505: INFO: namespace projected-3901 deletion completed in 22.148761077s

• [SLOW TEST:96.806 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 16:02:44.508: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6569
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-57e94b9b-9119-11e9-a8b9-dace53c98186
STEP: Creating secret with name secret-projected-all-test-volume-57e94b86-9119-11e9-a8b9-dace53c98186
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 17 16:02:44.686: INFO: Waiting up to 5m0s for pod "projected-volume-57e94b59-9119-11e9-a8b9-dace53c98186" in namespace "projected-6569" to be "success or failure"
Jun 17 16:02:44.690: INFO: Pod "projected-volume-57e94b59-9119-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 3.899918ms
Jun 17 16:02:46.695: INFO: Pod "projected-volume-57e94b59-9119-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009186587s
STEP: Saw pod success
Jun 17 16:02:46.695: INFO: Pod "projected-volume-57e94b59-9119-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 16:02:46.699: INFO: Trying to get logs from node lab1-k8s-node-2 pod projected-volume-57e94b59-9119-11e9-a8b9-dace53c98186 container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 17 16:02:46.741: INFO: Waiting for pod projected-volume-57e94b59-9119-11e9-a8b9-dace53c98186 to disappear
Jun 17 16:02:46.745: INFO: Pod projected-volume-57e94b59-9119-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 16:02:46.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6569" for this suite.
Jun 17 16:02:52.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 16:02:52.894: INFO: namespace projected-6569 deletion completed in 6.142155605s

• [SLOW TEST:8.387 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 16:02:52.896: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7535
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 17 16:02:53.069: INFO: Waiting up to 5m0s for pod "pod-5cea2aec-9119-11e9-a8b9-dace53c98186" in namespace "emptydir-7535" to be "success or failure"
Jun 17 16:02:53.076: INFO: Pod "pod-5cea2aec-9119-11e9-a8b9-dace53c98186": Phase="Pending", Reason="", readiness=false. Elapsed: 7.248288ms
Jun 17 16:02:55.082: INFO: Pod "pod-5cea2aec-9119-11e9-a8b9-dace53c98186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012826306s
STEP: Saw pod success
Jun 17 16:02:55.082: INFO: Pod "pod-5cea2aec-9119-11e9-a8b9-dace53c98186" satisfied condition "success or failure"
Jun 17 16:02:55.086: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-5cea2aec-9119-11e9-a8b9-dace53c98186 container test-container: <nil>
STEP: delete the pod
Jun 17 16:02:55.114: INFO: Waiting for pod pod-5cea2aec-9119-11e9-a8b9-dace53c98186 to disappear
Jun 17 16:02:55.119: INFO: Pod pod-5cea2aec-9119-11e9-a8b9-dace53c98186 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 16:02:55.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7535" for this suite.
Jun 17 16:03:01.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 16:03:01.271: INFO: namespace emptydir-7535 deletion completed in 6.145292711s

• [SLOW TEST:8.376 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 16:03:01.272: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4122
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-4122
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4122
STEP: Creating statefulset with conflicting port in namespace statefulset-4122
STEP: Waiting until pod test-pod will start running in namespace statefulset-4122
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4122
Jun 17 16:03:05.491: INFO: Observed stateful pod in namespace: statefulset-4122, name: ss-0, uid: 6452dda9-9119-11e9-bdd5-fa163e38c6bd, status phase: Pending. Waiting for statefulset controller to delete.
Jun 17 16:03:06.064: INFO: Observed stateful pod in namespace: statefulset-4122, name: ss-0, uid: 6452dda9-9119-11e9-bdd5-fa163e38c6bd, status phase: Failed. Waiting for statefulset controller to delete.
Jun 17 16:03:06.073: INFO: Observed stateful pod in namespace: statefulset-4122, name: ss-0, uid: 6452dda9-9119-11e9-bdd5-fa163e38c6bd, status phase: Failed. Waiting for statefulset controller to delete.
Jun 17 16:03:06.079: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4122
STEP: Removing pod with conflicting port in namespace statefulset-4122
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4122 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jun 17 16:03:10.112: INFO: Deleting all statefulset in ns statefulset-4122
Jun 17 16:03:10.116: INFO: Scaling statefulset ss to 0
Jun 17 16:03:30.135: INFO: Waiting for statefulset status.replicas updated to 0
Jun 17 16:03:30.139: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 16:03:30.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4122" for this suite.
Jun 17 16:03:36.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 16:03:36.307: INFO: namespace statefulset-4122 deletion completed in 6.141804223s

• [SLOW TEST:35.036 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 16:03:36.307: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1240
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
Jun 17 16:03:36.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 create -f - --namespace=kubectl-1240'
Jun 17 16:03:36.679: INFO: stderr: ""
Jun 17 16:03:36.679: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Jun 17 16:03:37.684: INFO: Selector matched 1 pods for map[app:redis]
Jun 17 16:03:37.684: INFO: Found 0 / 1
Jun 17 16:03:38.684: INFO: Selector matched 1 pods for map[app:redis]
Jun 17 16:03:38.684: INFO: Found 1 / 1
Jun 17 16:03:38.684: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 17 16:03:38.688: INFO: Selector matched 1 pods for map[app:redis]
Jun 17 16:03:38.688: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jun 17 16:03:38.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 logs redis-master-ll985 redis-master --namespace=kubectl-1240'
Jun 17 16:03:38.789: INFO: stderr: ""
Jun 17 16:03:38.789: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 17 Jun 16:03:38.093 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 17 Jun 16:03:38.093 # Server started, Redis version 3.2.12\n1:M 17 Jun 16:03:38.093 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 17 Jun 16:03:38.093 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jun 17 16:03:38.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 log redis-master-ll985 redis-master --namespace=kubectl-1240 --tail=1'
Jun 17 16:03:38.880: INFO: stderr: ""
Jun 17 16:03:38.880: INFO: stdout: "1:M 17 Jun 16:03:38.093 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jun 17 16:03:38.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 log redis-master-ll985 redis-master --namespace=kubectl-1240 --limit-bytes=1'
Jun 17 16:03:38.999: INFO: stderr: ""
Jun 17 16:03:38.999: INFO: stdout: " "
STEP: exposing timestamps
Jun 17 16:03:38.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 log redis-master-ll985 redis-master --namespace=kubectl-1240 --tail=1 --timestamps'
Jun 17 16:03:39.087: INFO: stderr: ""
Jun 17 16:03:39.087: INFO: stdout: "2019-06-17T16:03:38.094178674Z 1:M 17 Jun 16:03:38.093 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jun 17 16:03:41.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 log redis-master-ll985 redis-master --namespace=kubectl-1240 --since=1s'
Jun 17 16:03:41.689: INFO: stderr: ""
Jun 17 16:03:41.689: INFO: stdout: ""
Jun 17 16:03:41.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 log redis-master-ll985 redis-master --namespace=kubectl-1240 --since=24h'
Jun 17 16:03:41.780: INFO: stderr: ""
Jun 17 16:03:41.780: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 17 Jun 16:03:38.093 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 17 Jun 16:03:38.093 # Server started, Redis version 3.2.12\n1:M 17 Jun 16:03:38.093 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 17 Jun 16:03:38.093 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
Jun 17 16:03:41.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 delete --grace-period=0 --force -f - --namespace=kubectl-1240'
Jun 17 16:03:41.871: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 17 16:03:41.871: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jun 17 16:03:41.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get rc,svc -l name=nginx --no-headers --namespace=kubectl-1240'
Jun 17 16:03:41.960: INFO: stderr: "No resources found.\n"
Jun 17 16:03:41.960: INFO: stdout: ""
Jun 17 16:03:41.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-270483862 get pods -l name=nginx --namespace=kubectl-1240 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 17 16:03:42.037: INFO: stderr: ""
Jun 17 16:03:42.037: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 16:03:42.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1240" for this suite.
Jun 17 16:03:48.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 16:03:48.200: INFO: namespace kubectl-1240 deletion completed in 6.156244857s

• [SLOW TEST:11.892 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 16:03:48.200: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3722
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-3722
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3722 to expose endpoints map[]
Jun 17 16:03:48.380: INFO: Get endpoints failed (7.778751ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jun 17 16:03:49.384: INFO: successfully validated that service endpoint-test2 in namespace services-3722 exposes endpoints map[] (1.012409699s elapsed)
STEP: Creating pod pod1 in namespace services-3722
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3722 to expose endpoints map[pod1:[80]]
Jun 17 16:03:51.422: INFO: successfully validated that service endpoint-test2 in namespace services-3722 exposes endpoints map[pod1:[80]] (2.026967101s elapsed)
STEP: Creating pod pod2 in namespace services-3722
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3722 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 17 16:03:54.486: INFO: successfully validated that service endpoint-test2 in namespace services-3722 exposes endpoints map[pod1:[80] pod2:[80]] (3.056982995s elapsed)
STEP: Deleting pod pod1 in namespace services-3722
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3722 to expose endpoints map[pod2:[80]]
Jun 17 16:03:54.503: INFO: successfully validated that service endpoint-test2 in namespace services-3722 exposes endpoints map[pod2:[80]] (8.695601ms elapsed)
STEP: Deleting pod pod2 in namespace services-3722
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3722 to expose endpoints map[]
Jun 17 16:03:55.521: INFO: successfully validated that service endpoint-test2 in namespace services-3722 exposes endpoints map[] (1.008231125s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 16:03:55.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3722" for this suite.
Jun 17 16:04:17.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 16:04:17.709: INFO: namespace services-3722 deletion completed in 22.151474373s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:29.510 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 16:04:17.716: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-1558
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 17 16:04:25.933: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1558 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 16:04:25.933: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 16:04:26.079: INFO: Exec stderr: ""
Jun 17 16:04:26.079: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1558 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 16:04:26.079: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 16:04:26.240: INFO: Exec stderr: ""
Jun 17 16:04:26.241: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1558 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 16:04:26.241: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 16:04:26.388: INFO: Exec stderr: ""
Jun 17 16:04:26.388: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1558 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 16:04:26.388: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 16:04:26.549: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 17 16:04:26.549: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1558 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 16:04:26.549: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 16:04:26.693: INFO: Exec stderr: ""
Jun 17 16:04:26.693: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1558 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 16:04:26.693: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 16:04:26.855: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 17 16:04:26.855: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1558 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 16:04:26.855: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 16:04:27.049: INFO: Exec stderr: ""
Jun 17 16:04:27.049: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1558 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 16:04:27.049: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 16:04:27.252: INFO: Exec stderr: ""
Jun 17 16:04:27.252: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1558 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 16:04:27.252: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 16:04:27.463: INFO: Exec stderr: ""
Jun 17 16:04:27.463: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1558 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 16:04:27.463: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 16:04:27.677: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 16:04:27.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1558" for this suite.
Jun 17 16:05:19.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 16:05:19.841: INFO: namespace e2e-kubelet-etc-hosts-1558 deletion completed in 52.157565719s

• [SLOW TEST:62.125 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 16:05:19.841: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7791
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-98g7
STEP: Creating a pod to test atomic-volume-subpath
Jun 17 16:05:20.023: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-98g7" in namespace "subpath-7791" to be "success or failure"
Jun 17 16:05:20.028: INFO: Pod "pod-subpath-test-configmap-98g7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.005055ms
Jun 17 16:05:22.033: INFO: Pod "pod-subpath-test-configmap-98g7": Phase="Running", Reason="", readiness=true. Elapsed: 2.010146228s
Jun 17 16:05:24.038: INFO: Pod "pod-subpath-test-configmap-98g7": Phase="Running", Reason="", readiness=true. Elapsed: 4.014972269s
Jun 17 16:05:26.044: INFO: Pod "pod-subpath-test-configmap-98g7": Phase="Running", Reason="", readiness=true. Elapsed: 6.020500692s
Jun 17 16:05:28.048: INFO: Pod "pod-subpath-test-configmap-98g7": Phase="Running", Reason="", readiness=true. Elapsed: 8.025308526s
Jun 17 16:05:30.054: INFO: Pod "pod-subpath-test-configmap-98g7": Phase="Running", Reason="", readiness=true. Elapsed: 10.030769359s
Jun 17 16:05:32.059: INFO: Pod "pod-subpath-test-configmap-98g7": Phase="Running", Reason="", readiness=true. Elapsed: 12.036247426s
Jun 17 16:05:34.064: INFO: Pod "pod-subpath-test-configmap-98g7": Phase="Running", Reason="", readiness=true. Elapsed: 14.04077034s
Jun 17 16:05:36.069: INFO: Pod "pod-subpath-test-configmap-98g7": Phase="Running", Reason="", readiness=true. Elapsed: 16.046031224s
Jun 17 16:05:38.074: INFO: Pod "pod-subpath-test-configmap-98g7": Phase="Running", Reason="", readiness=true. Elapsed: 18.050665636s
Jun 17 16:05:40.079: INFO: Pod "pod-subpath-test-configmap-98g7": Phase="Running", Reason="", readiness=true. Elapsed: 20.055657548s
Jun 17 16:05:42.084: INFO: Pod "pod-subpath-test-configmap-98g7": Phase="Running", Reason="", readiness=true. Elapsed: 22.061144914s
Jun 17 16:05:44.089: INFO: Pod "pod-subpath-test-configmap-98g7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.065876287s
STEP: Saw pod success
Jun 17 16:05:44.089: INFO: Pod "pod-subpath-test-configmap-98g7" satisfied condition "success or failure"
Jun 17 16:05:44.093: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-subpath-test-configmap-98g7 container test-container-subpath-configmap-98g7: <nil>
STEP: delete the pod
Jun 17 16:05:44.124: INFO: Waiting for pod pod-subpath-test-configmap-98g7 to disappear
Jun 17 16:05:44.128: INFO: Pod pod-subpath-test-configmap-98g7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-98g7
Jun 17 16:05:44.128: INFO: Deleting pod "pod-subpath-test-configmap-98g7" in namespace "subpath-7791"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 16:05:44.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7791" for this suite.
Jun 17 16:05:50.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 16:05:50.298: INFO: namespace subpath-7791 deletion completed in 6.161327561s

• [SLOW TEST:30.457 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 16:05:50.298: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7332
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-7332
Jun 17 16:05:54.492: INFO: Started pod liveness-http in namespace container-probe-7332
STEP: checking the pod's current state and verifying that restartCount is present
Jun 17 16:05:54.496: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 16:09:55.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7332" for this suite.
Jun 17 16:10:01.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 16:10:01.285: INFO: namespace container-probe-7332 deletion completed in 6.154468946s

• [SLOW TEST:250.987 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 17 16:10:01.286: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1295
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-1295
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 17 16:10:01.445: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 17 16:10:25.572: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.95.169:8080/dial?request=hostName&protocol=udp&host=10.233.95.168&port=8081&tries=1'] Namespace:pod-network-test-1295 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 16:10:25.572: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 16:10:25.753: INFO: Waiting for endpoints: map[]
Jun 17 16:10:25.758: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.95.169:8080/dial?request=hostName&protocol=udp&host=10.233.64.138&port=8081&tries=1'] Namespace:pod-network-test-1295 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 16:10:25.758: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 16:10:26.001: INFO: Waiting for endpoints: map[]
Jun 17 16:10:26.006: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.95.169:8080/dial?request=hostName&protocol=udp&host=10.233.74.48&port=8081&tries=1'] Namespace:pod-network-test-1295 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 17 16:10:26.006: INFO: >>> kubeConfig: /tmp/kubeconfig-270483862
Jun 17 16:10:26.231: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 17 16:10:26.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1295" for this suite.
Jun 17 16:10:48.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 17 16:10:48.386: INFO: namespace pod-network-test-1295 deletion completed in 22.148850493s

• [SLOW TEST:47.100 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSJun 17 16:10:48.387: INFO: Running AfterSuite actions on all nodes
Jun 17 16:10:48.387: INFO: Running AfterSuite actions on node 1
Jun 17 16:10:48.387: INFO: Skipping dumping logs from cluster

Ran 204 of 3585 Specs in 5703.510 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3381 Skipped PASS

Ginkgo ran 1 suite in 1h35m4.95171857s
Test Suite Passed
