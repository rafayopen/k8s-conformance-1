Conformance test: not doing test setup.
I0410 09:32:55.627446    4420 e2e.go:240] Starting e2e run "9df826e3-5b73-11e9-bf19-c208aa3c4721" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1554888774 - Will randomize all specs
Will run 204 of 3584 specs

Apr 10 09:32:55.984: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 09:32:55.988: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr 10 09:32:56.097: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr 10 09:32:56.194: INFO: 16 / 16 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr 10 09:32:56.194: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Apr 10 09:32:56.194: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr 10 09:32:56.221: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'addons-kube2iam' (0 seconds elapsed)
Apr 10 09:32:56.221: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Apr 10 09:32:56.221: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Apr 10 09:32:56.221: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Apr 10 09:32:56.221: INFO: e2e test version: v1.14.0
Apr 10 09:32:56.240: INFO: kube-apiserver version: v1.14.0
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:32:56.240: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
Apr 10 09:32:56.356: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Apr 10 09:32:56.420: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4069
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr 10 09:32:56.602: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 10 09:32:56.642: INFO: Waiting for terminating namespaces to be deleted...
Apr 10 09:32:56.662: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-14-152.eu-west-1.compute.internal before test
Apr 10 09:32:56.786: INFO: kube-proxy-t9z66 from kube-system started at 2019-04-10 09:00:13 +0000 UTC (1 container statuses recorded)
Apr 10 09:32:56.786: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 10 09:32:56.786: INFO: addons-kube2iam-x6268 from kube-system started at 2019-04-10 09:00:33 +0000 UTC (1 container statuses recorded)
Apr 10 09:32:56.786: INFO: 	Container kube2iam ready: true, restart count 0
Apr 10 09:32:56.786: INFO: blackbox-exporter-6dc58dcffc-w6tfm from kube-system started at 2019-04-10 09:00:13 +0000 UTC (1 container statuses recorded)
Apr 10 09:32:56.786: INFO: 	Container blackbox-exporter ready: true, restart count 0
Apr 10 09:32:56.786: INFO: node-exporter-wsr4c from kube-system started at 2019-04-10 09:00:13 +0000 UTC (1 container statuses recorded)
Apr 10 09:32:56.786: INFO: 	Container node-exporter ready: true, restart count 0
Apr 10 09:32:56.786: INFO: calico-node-965bj from kube-system started at 2019-04-10 09:00:13 +0000 UTC (1 container statuses recorded)
Apr 10 09:32:56.786: INFO: 	Container calico-node ready: true, restart count 0
Apr 10 09:32:56.786: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-25-25.eu-west-1.compute.internal before test
Apr 10 09:32:56.848: INFO: calico-node-ftwpk from kube-system started at 2019-04-10 09:00:14 +0000 UTC (1 container statuses recorded)
Apr 10 09:32:56.848: INFO: 	Container calico-node ready: true, restart count 0
Apr 10 09:32:56.848: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-57c2j from kube-system started at 2019-04-10 09:00:29 +0000 UTC (1 container statuses recorded)
Apr 10 09:32:56.848: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Apr 10 09:32:56.848: INFO: kube-proxy-xk4pl from kube-system started at 2019-04-10 09:00:14 +0000 UTC (1 container statuses recorded)
Apr 10 09:32:56.848: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 10 09:32:56.848: INFO: node-exporter-n6hdz from kube-system started at 2019-04-10 09:00:14 +0000 UTC (1 container statuses recorded)
Apr 10 09:32:56.848: INFO: 	Container node-exporter ready: true, restart count 0
Apr 10 09:32:56.848: INFO: coredns-7f7f7978c8-qqrcn from kube-system started at 2019-04-10 09:00:24 +0000 UTC (1 container statuses recorded)
Apr 10 09:32:56.848: INFO: 	Container coredns ready: true, restart count 0
Apr 10 09:32:56.848: INFO: metrics-server-6b49cb886c-fjp6h from kube-system started at 2019-04-10 09:00:24 +0000 UTC (1 container statuses recorded)
Apr 10 09:32:56.848: INFO: 	Container metrics-server ready: true, restart count 0
Apr 10 09:32:56.848: INFO: coredns-7f7f7978c8-2nxbn from kube-system started at 2019-04-10 09:00:24 +0000 UTC (1 container statuses recorded)
Apr 10 09:32:56.848: INFO: 	Container coredns ready: true, restart count 0
Apr 10 09:32:56.848: INFO: addons-kubernetes-dashboard-665df4b66d-vlw97 from kube-system started at 2019-04-10 09:00:24 +0000 UTC (1 container statuses recorded)
Apr 10 09:32:56.848: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr 10 09:32:56.848: INFO: addons-kube2iam-zjj8r from kube-system started at 2019-04-10 09:00:24 +0000 UTC (1 container statuses recorded)
Apr 10 09:32:56.848: INFO: 	Container kube2iam ready: true, restart count 0
Apr 10 09:32:56.848: INFO: vpn-shoot-7b5f8c4995-mmd2b from kube-system started at 2019-04-10 09:00:29 +0000 UTC (1 container statuses recorded)
Apr 10 09:32:56.848: INFO: 	Container vpn-shoot ready: true, restart count 0
Apr 10 09:32:56.848: INFO: addons-nginx-ingress-controller-f88658d78-q6z6w from kube-system started at 2019-04-10 09:00:29 +0000 UTC (1 container statuses recorded)
Apr 10 09:32:56.848: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a20bc0e7-5b73-11e9-bf19-c208aa3c4721 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-a20bc0e7-5b73-11e9-bf19-c208aa3c4721 off the node ip-10-250-14-152.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a20bc0e7-5b73-11e9-bf19-c208aa3c4721
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:33:05.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4069" for this suite.
Apr 10 09:33:35.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:33:35.994: INFO: namespace sched-pred-4069 deletion completed in 30.820588375s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:33:35.994: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1290
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:33:40.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1290" for this suite.
Apr 10 09:33:46.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:33:47.307: INFO: namespace emptydir-wrapper-1290 deletion completed in 6.79734439s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:33:47.308: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-453
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 09:33:47.599: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config version --client'
Apr 10 09:33:47.681: INFO: stderr: ""
Apr 10 09:33:47.681: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"archive\", BuildDate:\"2019-04-10T09:26:54Z\", GoVersion:\"go1.12.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Apr 10 09:33:47.700: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config create -f - --namespace=kubectl-453'
Apr 10 09:33:48.060: INFO: stderr: ""
Apr 10 09:33:48.060: INFO: stdout: "replicationcontroller/redis-master created\n"
Apr 10 09:33:48.060: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config create -f - --namespace=kubectl-453'
Apr 10 09:33:48.421: INFO: stderr: ""
Apr 10 09:33:48.421: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 10 09:33:49.441: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 09:33:49.441: INFO: Found 0 / 1
Apr 10 09:33:50.441: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 09:33:50.441: INFO: Found 0 / 1
Apr 10 09:33:51.441: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 09:33:51.441: INFO: Found 1 / 1
Apr 10 09:33:51.441: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 10 09:33:51.461: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 09:33:51.461: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 10 09:33:51.462: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config describe pod redis-master-6b2dx --namespace=kubectl-453'
Apr 10 09:33:51.725: INFO: stderr: ""
Apr 10 09:33:51.725: INFO: stdout: "Name:               redis-master-6b2dx\nNamespace:          kubectl-453\nPriority:           0\nPriorityClassName:  <none>\nNode:               ip-10-250-14-152.eu-west-1.compute.internal/10.250.14.152\nStart Time:         Wed, 10 Apr 2019 09:33:48 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 100.96.0.6/32\n                    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 100.96.0.6\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://9837e24bfd1795fada6257c09394d5cdaf15c6803449c9fad6305d6bf680c937\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 10 Apr 2019 09:33:50 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-g7c7h (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-g7c7h:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-g7c7h\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                  Message\n  ----    ------     ----  ----                                                  -------\n  Normal  Scheduled  3s    default-scheduler                                     Successfully assigned kubectl-453/redis-master-6b2dx to ip-10-250-14-152.eu-west-1.compute.internal\n  Normal  Pulling    3s    kubelet, ip-10-250-14-152.eu-west-1.compute.internal  Pulling image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Pulled     1s    kubelet, ip-10-250-14-152.eu-west-1.compute.internal  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Created    1s    kubelet, ip-10-250-14-152.eu-west-1.compute.internal  Created container redis-master\n  Normal  Started    1s    kubelet, ip-10-250-14-152.eu-west-1.compute.internal  Started container redis-master\n"
Apr 10 09:33:51.726: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config describe rc redis-master --namespace=kubectl-453'
Apr 10 09:33:51.960: INFO: stderr: ""
Apr 10 09:33:51.960: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-453\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-6b2dx\n"
Apr 10 09:33:51.961: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config describe service redis-master --namespace=kubectl-453'
Apr 10 09:33:52.243: INFO: stderr: ""
Apr 10 09:33:52.243: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-453\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.65.157.183\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.96.0.6:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr 10 09:33:52.264: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config describe node ip-10-250-14-152.eu-west-1.compute.internal'
Apr 10 09:33:52.585: INFO: stderr: ""
Apr 10 09:33:52.585: INFO: stdout: "Name:               ip-10-250-14-152.eu-west-1.compute.internal\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m4.large\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-west-1\n                    failure-domain.beta.kubernetes.io/zone=eu-west-1c\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-250-14-152.eu-west-1.compute.internal\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=node\n                    node-role.kubernetes.io/node=\n                    worker.garden.sapcloud.io/group=cpu-worker\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.14.152/19\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 10 Apr 2019 09:00:13 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 10 Apr 2019 09:33:45 +0000   Wed, 10 Apr 2019 09:00:13 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 10 Apr 2019 09:33:45 +0000   Wed, 10 Apr 2019 09:00:13 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 10 Apr 2019 09:33:45 +0000   Wed, 10 Apr 2019 09:00:13 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 10 Apr 2019 09:33:45 +0000   Wed, 10 Apr 2019 09:00:33 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.250.14.152\n  InternalDNS:  ip-10-250-14-152.eu-west-1.compute.internal\n  Hostname:     ip-10-250-14-152.eu-west-1.compute.internal\nCapacity:\n attachable-volumes-aws-ebs:  39\n cpu:                         2\n ephemeral-storage:           17897500Ki\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      8167868Ki\n pods:                        110\nAllocatable:\n attachable-volumes-aws-ebs:  39\n cpu:                         1920m\n ephemeral-storage:           17410687987\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      6871960161\n pods:                        110\nSystem Info:\n Machine ID:                 1617df27256b4b45bb9ca1f27f4d0663\n System UUID:                ec2b064a-40fb-f1e3-afb4-ecb73751ff61\n Boot ID:                    73cea056-4883-41ff-b1a5-a8bbe97e54fd\n Kernel Version:             4.19.25-coreos\n OS Image:                   Container Linux by CoreOS 2023.5.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.14.0\n Kube-Proxy Version:         v1.14.0\nPodCIDR:                     100.96.0.0/24\nProviderID:                  aws:///eu-west-1c/i-0e34d478ce0896a0d\nNon-terminated Pods:         (6 in total)\n  Namespace                  Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                  ------------  ----------  ---------------  -------------  ---\n  kube-system                addons-kube2iam-x6268                 10m (0%)      80m (4%)    16Mi (0%)        128Mi (1%)     33m\n  kube-system                blackbox-exporter-6dc58dcffc-w6tfm    5m (0%)       10m (0%)    5Mi (0%)         35Mi (0%)      34m\n  kube-system                calico-node-965bj                     100m (5%)     500m (26%)  100Mi (1%)       700Mi (10%)    33m\n  kube-system                kube-proxy-t9z66                      20m (1%)      0 (0%)      64Mi (0%)        0 (0%)         33m\n  kube-system                node-exporter-wsr4c                   5m (0%)       15m (0%)    10Mi (0%)        100Mi (1%)     33m\n  kubectl-453                redis-master-6b2dx                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         140m (7%)   605m (31%)\n  memory                      195Mi (2%)  963Mi (14%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:\n  Type    Reason                   Age   From                                                     Message\n  ----    ------                   ----  ----                                                     -------\n  Normal  Starting                 33m   kubelet, ip-10-250-14-152.eu-west-1.compute.internal     Starting kubelet.\n  Normal  NodeHasSufficientMemory  33m   kubelet, ip-10-250-14-152.eu-west-1.compute.internal     Node ip-10-250-14-152.eu-west-1.compute.internal status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    33m   kubelet, ip-10-250-14-152.eu-west-1.compute.internal     Node ip-10-250-14-152.eu-west-1.compute.internal status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     33m   kubelet, ip-10-250-14-152.eu-west-1.compute.internal     Node ip-10-250-14-152.eu-west-1.compute.internal status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  33m   kubelet, ip-10-250-14-152.eu-west-1.compute.internal     Updated Node Allocatable limit across pods\n  Normal  Starting                 33m   kube-proxy, ip-10-250-14-152.eu-west-1.compute.internal  Starting kube-proxy.\n  Normal  NodeReady                33m   kubelet, ip-10-250-14-152.eu-west-1.compute.internal     Node ip-10-250-14-152.eu-west-1.compute.internal status is now: NodeReady\n"
Apr 10 09:33:52.585: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config describe namespace kubectl-453'
Apr 10 09:33:52.854: INFO: stderr: ""
Apr 10 09:33:52.854: INFO: stdout: "Name:         kubectl-453\nLabels:       e2e-framework=kubectl\n              e2e-run=9df826e3-5b73-11e9-bf19-c208aa3c4721\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:33:52.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-453" for this suite.
Apr 10 09:34:14.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:34:15.677: INFO: namespace kubectl-453 deletion completed in 22.802248917s
•SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:34:15.677: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6029
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-ceb75fab-5b73-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume configMaps
Apr 10 09:34:15.942: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ceba733e-5b73-11e9-bf19-c208aa3c4721" in namespace "projected-6029" to be "success or failure"
Apr 10 09:34:15.962: INFO: Pod "pod-projected-configmaps-ceba733e-5b73-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.561782ms
Apr 10 09:34:17.983: INFO: Pod "pod-projected-configmaps-ceba733e-5b73-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040202233s
Apr 10 09:34:20.003: INFO: Pod "pod-projected-configmaps-ceba733e-5b73-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060789875s
STEP: Saw pod success
Apr 10 09:34:20.003: INFO: Pod "pod-projected-configmaps-ceba733e-5b73-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 09:34:20.023: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-projected-configmaps-ceba733e-5b73-11e9-bf19-c208aa3c4721 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 09:34:20.074: INFO: Waiting for pod pod-projected-configmaps-ceba733e-5b73-11e9-bf19-c208aa3c4721 to disappear
Apr 10 09:34:20.094: INFO: Pod pod-projected-configmaps-ceba733e-5b73-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:34:20.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6029" for this suite.
Apr 10 09:34:26.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:34:26.913: INFO: namespace projected-6029 deletion completed in 6.798631063s
•SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:34:26.913: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2956
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-2956
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 10 09:34:27.400: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 10 09:34:49.771: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.0.9:8080/dial?request=hostName&protocol=http&host=100.96.0.8&port=8080&tries=1'] Namespace:pod-network-test-2956 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 09:34:49.771: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 09:34:50.332: INFO: Waiting for endpoints: map[]
Apr 10 09:34:50.352: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.0.9:8080/dial?request=hostName&protocol=http&host=100.96.1.9&port=8080&tries=1'] Namespace:pod-network-test-2956 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 09:34:50.352: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 09:34:50.875: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:34:50.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2956" for this suite.
Apr 10 09:35:12.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:35:13.699: INFO: namespace pod-network-test-2956 deletion completed in 22.803155694s
•SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:35:13.699: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-30
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-30
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-30 to expose endpoints map[]
Apr 10 09:35:14.043: INFO: successfully validated that service multi-endpoint-test in namespace services-30 exposes endpoints map[] (20.134766ms elapsed)
STEP: Creating pod pod1 in namespace services-30
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-30 to expose endpoints map[pod1:[100]]
Apr 10 09:35:16.243: INFO: successfully validated that service multi-endpoint-test in namespace services-30 exposes endpoints map[pod1:[100]] (2.177472218s elapsed)
STEP: Creating pod pod2 in namespace services-30
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-30 to expose endpoints map[pod1:[100] pod2:[101]]
Apr 10 09:35:18.449: INFO: successfully validated that service multi-endpoint-test in namespace services-30 exposes endpoints map[pod1:[100] pod2:[101]] (2.184469679s elapsed)
STEP: Deleting pod pod1 in namespace services-30
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-30 to expose endpoints map[pod2:[101]]
Apr 10 09:35:18.513: INFO: successfully validated that service multi-endpoint-test in namespace services-30 exposes endpoints map[pod2:[101]] (42.069882ms elapsed)
STEP: Deleting pod pod2 in namespace services-30
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-30 to expose endpoints map[]
Apr 10 09:35:18.554: INFO: successfully validated that service multi-endpoint-test in namespace services-30 exposes endpoints map[] (19.982836ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:35:18.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-30" for this suite.
Apr 10 09:35:40.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:35:41.418: INFO: namespace services-30 deletion completed in 22.81125866s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:35:41.418: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2164
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Apr 10 09:35:41.733: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Apr 10 09:35:52.872: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:35:52.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2164" for this suite.
Apr 10 09:35:58.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:35:59.724: INFO: namespace pods-2164 deletion completed in 6.811916357s
•SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:35:59.724: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3268
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 09:36:00.041: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 10 09:36:02.084: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 10 09:36:04.246: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-3268,SelfLink:/apis/apps/v1/namespaces/deployment-3268/deployments/test-cleanup-deployment,UID:0e0a13fa-5b74-11e9-a517-0202de25e2de,ResourceVersion:5471,Generation:1,CreationTimestamp:2019-04-10 09:36:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-04-10 09:36:02 +0000 UTC 2019-04-10 09:36:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-04-10 09:36:03 +0000 UTC 2019-04-10 09:36:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55cbfbc8f5" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Apr 10 09:36:04.266: INFO: New ReplicaSet "test-cleanup-deployment-55cbfbc8f5" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5,GenerateName:,Namespace:deployment-3268,SelfLink:/apis/apps/v1/namespaces/deployment-3268/replicasets/test-cleanup-deployment-55cbfbc8f5,UID:0e0b4f3a-5b74-11e9-a517-0202de25e2de,ResourceVersion:5464,Generation:1,CreationTimestamp:2019-04-10 09:36:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 0e0a13fa-5b74-11e9-a517-0202de25e2de 0xc001c297d7 0xc001c297d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr 10 09:36:04.287: INFO: Pod "test-cleanup-deployment-55cbfbc8f5-p4tv8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5-p4tv8,GenerateName:test-cleanup-deployment-55cbfbc8f5-,Namespace:deployment-3268,SelfLink:/api/v1/namespaces/deployment-3268/pods/test-cleanup-deployment-55cbfbc8f5-p4tv8,UID:0e0b9a3f-5b74-11e9-a517-0202de25e2de,ResourceVersion:5463,Generation:0,CreationTimestamp:2019-04-10 09:36:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.14/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55cbfbc8f5 0e0b4f3a-5b74-11e9-a517-0202de25e2de 0xc001c29d67 0xc001c29d68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-pkpgv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-pkpgv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-pkpgv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c29dd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c29df0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 09:36:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 09:36:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 09:36:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 09:36:02 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.152,PodIP:100.96.0.14,StartTime:2019-04-10 09:36:02 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-04-10 09:36:03 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://4361a006a802702898670439a33d826f322dde5e1584556a948f8885f037acc2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:36:04.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3268" for this suite.
Apr 10 09:36:10.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:36:11.133: INFO: namespace deployment-3268 deletion completed in 6.82344806s
•S
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:36:11.133: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3897
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 09:36:11.520: INFO: Waiting up to 5m0s for pod "downwardapi-volume-139de3a7-5b74-11e9-bf19-c208aa3c4721" in namespace "downward-api-3897" to be "success or failure"
Apr 10 09:36:11.541: INFO: Pod "downwardapi-volume-139de3a7-5b74-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.952142ms
Apr 10 09:36:13.562: INFO: Pod "downwardapi-volume-139de3a7-5b74-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041813338s
STEP: Saw pod success
Apr 10 09:36:13.562: INFO: Pod "downwardapi-volume-139de3a7-5b74-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 09:36:13.587: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downwardapi-volume-139de3a7-5b74-11e9-bf19-c208aa3c4721 container client-container: <nil>
STEP: delete the pod
Apr 10 09:36:13.636: INFO: Waiting for pod downwardapi-volume-139de3a7-5b74-11e9-bf19-c208aa3c4721 to disappear
Apr 10 09:36:13.656: INFO: Pod downwardapi-volume-139de3a7-5b74-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:36:13.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3897" for this suite.
Apr 10 09:36:19.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:36:20.479: INFO: namespace downward-api-3897 deletion completed in 6.801419221s
•SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:36:20.479: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4987
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-4987/secret-test-191a7d2d-5b74-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume secrets
Apr 10 09:36:20.744: INFO: Waiting up to 5m0s for pod "pod-configmaps-191d8d31-5b74-11e9-bf19-c208aa3c4721" in namespace "secrets-4987" to be "success or failure"
Apr 10 09:36:20.764: INFO: Pod "pod-configmaps-191d8d31-5b74-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.806818ms
Apr 10 09:36:22.784: INFO: Pod "pod-configmaps-191d8d31-5b74-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040022091s
Apr 10 09:36:24.807: INFO: Pod "pod-configmaps-191d8d31-5b74-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06305981s
STEP: Saw pod success
Apr 10 09:36:24.807: INFO: Pod "pod-configmaps-191d8d31-5b74-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 09:36:24.829: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-configmaps-191d8d31-5b74-11e9-bf19-c208aa3c4721 container env-test: <nil>
STEP: delete the pod
Apr 10 09:36:24.877: INFO: Waiting for pod pod-configmaps-191d8d31-5b74-11e9-bf19-c208aa3c4721 to disappear
Apr 10 09:36:24.897: INFO: Pod pod-configmaps-191d8d31-5b74-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:36:24.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4987" for this suite.
Apr 10 09:36:30.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:36:31.721: INFO: namespace secrets-4987 deletion completed in 6.802812341s
•SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:36:31.721: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6014
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 09:36:31.944: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6014'
Apr 10 09:36:32.289: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 10 09:36:32.289: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Apr 10 09:36:32.329: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-z6xz2]
Apr 10 09:36:32.329: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-z6xz2" in namespace "kubectl-6014" to be "running and ready"
Apr 10 09:36:32.349: INFO: Pod "e2e-test-nginx-rc-z6xz2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.166687ms
Apr 10 09:36:34.370: INFO: Pod "e2e-test-nginx-rc-z6xz2": Phase="Running", Reason="", readiness=true. Elapsed: 2.040958016s
Apr 10 09:36:34.370: INFO: Pod "e2e-test-nginx-rc-z6xz2" satisfied condition "running and ready"
Apr 10 09:36:34.370: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-z6xz2]
Apr 10 09:36:34.370: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config logs rc/e2e-test-nginx-rc --namespace=kubectl-6014'
Apr 10 09:36:34.732: INFO: stderr: ""
Apr 10 09:36:34.732: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Apr 10 09:36:34.732: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-6014'
Apr 10 09:36:34.997: INFO: stderr: ""
Apr 10 09:36:34.997: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:36:34.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6014" for this suite.
Apr 10 09:36:57.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:36:57.858: INFO: namespace kubectl-6014 deletion completed in 22.839432848s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:36:57.858: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6475
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 10 09:37:00.741: INFO: Successfully updated pod "pod-update-2f63e512-5b74-11e9-bf19-c208aa3c4721"
STEP: verifying the updated pod is in kubernetes
Apr 10 09:37:00.787: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:37:00.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6475" for this suite.
Apr 10 09:37:22.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:37:23.630: INFO: namespace pods-6475 deletion completed in 22.821878316s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:37:23.631: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-296
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-3ec82711-5b74-11e9-bf19-c208aa3c4721
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:37:26.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-296" for this suite.
Apr 10 09:37:48.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:37:48.955: INFO: namespace configmap-296 deletion completed in 22.803550406s
•S
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:37:48.956: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8187
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 09:37:53.330: INFO: Waiting up to 5m0s for pod "client-envvars-504d5da5-5b74-11e9-bf19-c208aa3c4721" in namespace "pods-8187" to be "success or failure"
Apr 10 09:37:53.351: INFO: Pod "client-envvars-504d5da5-5b74-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.390637ms
Apr 10 09:37:55.371: INFO: Pod "client-envvars-504d5da5-5b74-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040841917s
STEP: Saw pod success
Apr 10 09:37:55.371: INFO: Pod "client-envvars-504d5da5-5b74-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 09:37:55.391: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod client-envvars-504d5da5-5b74-11e9-bf19-c208aa3c4721 container env3cont: <nil>
STEP: delete the pod
Apr 10 09:37:55.440: INFO: Waiting for pod client-envvars-504d5da5-5b74-11e9-bf19-c208aa3c4721 to disappear
Apr 10 09:37:55.460: INFO: Pod client-envvars-504d5da5-5b74-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:37:55.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8187" for this suite.
Apr 10 09:38:45.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:38:46.343: INFO: namespace pods-8187 deletion completed in 50.859400562s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:38:46.343: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1739
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr 10 09:38:46.596: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:38:48.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1739" for this suite.
Apr 10 09:38:54.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:38:55.543: INFO: namespace init-container-1739 deletion completed in 6.852283899s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:38:55.543: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3180
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 10 09:38:55.821: INFO: Waiting up to 5m0s for pod "pod-758c3e6d-5b74-11e9-bf19-c208aa3c4721" in namespace "emptydir-3180" to be "success or failure"
Apr 10 09:38:55.841: INFO: Pod "pod-758c3e6d-5b74-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.920894ms
Apr 10 09:38:57.864: INFO: Pod "pod-758c3e6d-5b74-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042858045s
STEP: Saw pod success
Apr 10 09:38:57.864: INFO: Pod "pod-758c3e6d-5b74-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 09:38:57.884: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-758c3e6d-5b74-11e9-bf19-c208aa3c4721 container test-container: <nil>
STEP: delete the pod
Apr 10 09:38:57.935: INFO: Waiting for pod pod-758c3e6d-5b74-11e9-bf19-c208aa3c4721 to disappear
Apr 10 09:38:57.955: INFO: Pod pod-758c3e6d-5b74-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:38:57.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3180" for this suite.
Apr 10 09:39:04.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:39:04.806: INFO: namespace emptydir-3180 deletion completed in 6.828282521s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:39:04.807: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3798
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 09:39:05.118: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b172c86-5b74-11e9-bf19-c208aa3c4721" in namespace "projected-3798" to be "success or failure"
Apr 10 09:39:05.138: INFO: Pod "downwardapi-volume-7b172c86-5b74-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.516886ms
Apr 10 09:39:07.158: INFO: Pod "downwardapi-volume-7b172c86-5b74-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039984446s
STEP: Saw pod success
Apr 10 09:39:07.158: INFO: Pod "downwardapi-volume-7b172c86-5b74-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 09:39:07.178: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downwardapi-volume-7b172c86-5b74-11e9-bf19-c208aa3c4721 container client-container: <nil>
STEP: delete the pod
Apr 10 09:39:07.227: INFO: Waiting for pod downwardapi-volume-7b172c86-5b74-11e9-bf19-c208aa3c4721 to disappear
Apr 10 09:39:07.246: INFO: Pod downwardapi-volume-7b172c86-5b74-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:39:07.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3798" for this suite.
Apr 10 09:39:13.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:39:14.077: INFO: namespace projected-3798 deletion completed in 6.810821665s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:39:14.078: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8672
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 10 09:39:14.362: INFO: Waiting up to 5m0s for pod "pod-8099d87b-5b74-11e9-bf19-c208aa3c4721" in namespace "emptydir-8672" to be "success or failure"
Apr 10 09:39:14.382: INFO: Pod "pod-8099d87b-5b74-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.70794ms
Apr 10 09:39:16.403: INFO: Pod "pod-8099d87b-5b74-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040320801s
STEP: Saw pod success
Apr 10 09:39:16.403: INFO: Pod "pod-8099d87b-5b74-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 09:39:16.422: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-8099d87b-5b74-11e9-bf19-c208aa3c4721 container test-container: <nil>
STEP: delete the pod
Apr 10 09:39:16.471: INFO: Waiting for pod pod-8099d87b-5b74-11e9-bf19-c208aa3c4721 to disappear
Apr 10 09:39:16.491: INFO: Pod pod-8099d87b-5b74-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:39:16.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8672" for this suite.
Apr 10 09:39:22.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:39:23.311: INFO: namespace emptydir-8672 deletion completed in 6.798722336s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:39:23.311: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2859
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:39:25.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2859" for this suite.
Apr 10 09:39:47.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:39:48.539: INFO: namespace replication-controller-2859 deletion completed in 22.797292688s
•SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:39:48.539: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5327
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0410 09:39:54.938348    4420 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 10 09:39:54.938: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:39:54.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5327" for this suite.
Apr 10 09:40:01.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:40:01.773: INFO: namespace gc-5327 deletion completed in 6.814636668s
•SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:40:01.773: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7795
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-7795
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 10 09:40:02.000: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 10 09:40:16.361: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.1.14 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7795 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 09:40:16.361: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 09:40:17.875: INFO: Found all expected endpoints: [netserver-0]
Apr 10 09:40:17.900: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.0.33 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7795 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 09:40:17.900: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 09:40:19.427: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:40:19.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7795" for this suite.
Apr 10 09:40:41.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:40:42.282: INFO: namespace pod-network-test-7795 deletion completed in 22.834990926s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:40:42.283: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9932
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 10 09:40:42.617: INFO: Waiting up to 5m0s for pod "pod-b5345a22-5b74-11e9-bf19-c208aa3c4721" in namespace "emptydir-9932" to be "success or failure"
Apr 10 09:40:42.637: INFO: Pod "pod-b5345a22-5b74-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.189964ms
Apr 10 09:40:44.660: INFO: Pod "pod-b5345a22-5b74-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043127135s
STEP: Saw pod success
Apr 10 09:40:44.660: INFO: Pod "pod-b5345a22-5b74-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 09:40:44.681: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-b5345a22-5b74-11e9-bf19-c208aa3c4721 container test-container: <nil>
STEP: delete the pod
Apr 10 09:40:44.733: INFO: Waiting for pod pod-b5345a22-5b74-11e9-bf19-c208aa3c4721 to disappear
Apr 10 09:40:44.754: INFO: Pod pod-b5345a22-5b74-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:40:44.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9932" for this suite.
Apr 10 09:40:50.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:40:51.600: INFO: namespace emptydir-9932 deletion completed in 6.822347434s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:40:51.601: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2130
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-babfdb9f-5b74-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume configMaps
Apr 10 09:40:51.940: INFO: Waiting up to 5m0s for pod "pod-configmaps-bac2e9d2-5b74-11e9-bf19-c208aa3c4721" in namespace "configmap-2130" to be "success or failure"
Apr 10 09:40:51.960: INFO: Pod "pod-configmaps-bac2e9d2-5b74-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.572964ms
Apr 10 09:40:53.984: INFO: Pod "pod-configmaps-bac2e9d2-5b74-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043165358s
STEP: Saw pod success
Apr 10 09:40:53.984: INFO: Pod "pod-configmaps-bac2e9d2-5b74-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 09:40:54.008: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-configmaps-bac2e9d2-5b74-11e9-bf19-c208aa3c4721 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 09:40:54.058: INFO: Waiting for pod pod-configmaps-bac2e9d2-5b74-11e9-bf19-c208aa3c4721 to disappear
Apr 10 09:40:54.078: INFO: Pod pod-configmaps-bac2e9d2-5b74-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:40:54.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2130" for this suite.
Apr 10 09:41:00.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:41:00.943: INFO: namespace configmap-2130 deletion completed in 6.843978354s
•
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:41:00.943: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8065
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr 10 09:41:03.843: INFO: Successfully updated pod "annotationupdatec046a527-5b74-11e9-bf19-c208aa3c4721"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:41:05.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8065" for this suite.
Apr 10 09:41:27.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:41:28.732: INFO: namespace downward-api-8065 deletion completed in 22.813422584s
•SSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:41:28.732: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6728
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 10 09:41:28.979: INFO: Waiting up to 5m0s for pod "downward-api-d0d6b081-5b74-11e9-bf19-c208aa3c4721" in namespace "downward-api-6728" to be "success or failure"
Apr 10 09:41:29.009: INFO: Pod "downward-api-d0d6b081-5b74-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 29.948856ms
Apr 10 09:41:31.030: INFO: Pod "downward-api-d0d6b081-5b74-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.050839233s
STEP: Saw pod success
Apr 10 09:41:31.030: INFO: Pod "downward-api-d0d6b081-5b74-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 09:41:31.051: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downward-api-d0d6b081-5b74-11e9-bf19-c208aa3c4721 container dapi-container: <nil>
STEP: delete the pod
Apr 10 09:41:31.120: INFO: Waiting for pod downward-api-d0d6b081-5b74-11e9-bf19-c208aa3c4721 to disappear
Apr 10 09:41:31.140: INFO: Pod downward-api-d0d6b081-5b74-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:41:31.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6728" for this suite.
Apr 10 09:41:37.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:41:37.976: INFO: namespace downward-api-6728 deletion completed in 6.815226078s
•SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:41:37.976: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8807
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 09:41:38.194: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-8807'
Apr 10 09:41:38.412: INFO: stderr: ""
Apr 10 09:41:38.412: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Apr 10 09:41:43.463: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pod e2e-test-nginx-pod --namespace=kubectl-8807 -o json'
Apr 10 09:41:43.698: INFO: stderr: ""
Apr 10 09:41:43.698: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.0.39/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-04-10T09:41:38Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-8807\",\n        \"resourceVersion\": \"6638\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-8807/pods/e2e-test-nginx-pod\",\n        \"uid\": \"d677049f-5b74-11e9-a517-0202de25e2de\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-vpq9x\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-250-14-152.eu-west-1.compute.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-vpq9x\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-vpq9x\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-10T09:41:38Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-10T09:41:40Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-10T09:41:40Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-10T09:41:38Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://9ef5a9dce6a11e5344a6f9550b1db6e0adfdb2ca6b29ce3b4c1a9d00b10319f5\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:aed4a43d94cb1f8ad9addc55f0642a28e4f262061fffc1a447fac6e951d2e811\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-04-10T09:41:39Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.14.152\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.0.39\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-04-10T09:41:38Z\"\n    }\n}\n"
STEP: replace the image in the pod
Apr 10 09:41:43.698: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config replace -f - --namespace=kubectl-8807'
Apr 10 09:41:44.135: INFO: stderr: ""
Apr 10 09:41:44.135: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Apr 10 09:41:44.158: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-8807'
Apr 10 09:41:46.140: INFO: stderr: ""
Apr 10 09:41:46.140: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:41:46.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8807" for this suite.
Apr 10 09:41:52.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:41:52.960: INFO: namespace kubectl-8807 deletion completed in 6.799287652s
•
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:41:52.960: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-6359
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Apr 10 09:41:57.362: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6359 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 09:41:57.362: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 09:41:57.867: INFO: Exec stderr: ""
Apr 10 09:41:57.867: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6359 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 09:41:57.867: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 09:41:58.423: INFO: Exec stderr: ""
Apr 10 09:41:58.423: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6359 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 09:41:58.423: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 09:41:58.930: INFO: Exec stderr: ""
Apr 10 09:41:58.930: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6359 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 09:41:58.930: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 09:41:59.477: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Apr 10 09:41:59.477: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6359 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 09:41:59.477: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 09:42:00.037: INFO: Exec stderr: ""
Apr 10 09:42:00.037: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6359 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 09:42:00.037: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 09:42:00.516: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Apr 10 09:42:00.516: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6359 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 09:42:00.516: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 09:42:01.042: INFO: Exec stderr: ""
Apr 10 09:42:01.042: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6359 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 09:42:01.042: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 09:42:01.590: INFO: Exec stderr: ""
Apr 10 09:42:01.590: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6359 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 09:42:01.590: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 09:42:02.136: INFO: Exec stderr: ""
Apr 10 09:42:02.136: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6359 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 09:42:02.136: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 09:42:02.706: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:42:02.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6359" for this suite.
Apr 10 09:42:44.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:42:45.523: INFO: namespace e2e-kubelet-etc-hosts-6359 deletion completed in 42.796651136s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:42:45.524: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4921
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 09:42:45.822: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fea3bc63-5b74-11e9-bf19-c208aa3c4721" in namespace "projected-4921" to be "success or failure"
Apr 10 09:42:45.842: INFO: Pod "downwardapi-volume-fea3bc63-5b74-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.988363ms
Apr 10 09:42:47.862: INFO: Pod "downwardapi-volume-fea3bc63-5b74-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040690569s
STEP: Saw pod success
Apr 10 09:42:47.862: INFO: Pod "downwardapi-volume-fea3bc63-5b74-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 09:42:47.883: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downwardapi-volume-fea3bc63-5b74-11e9-bf19-c208aa3c4721 container client-container: <nil>
STEP: delete the pod
Apr 10 09:42:47.936: INFO: Waiting for pod downwardapi-volume-fea3bc63-5b74-11e9-bf19-c208aa3c4721 to disappear
Apr 10 09:42:47.956: INFO: Pod downwardapi-volume-fea3bc63-5b74-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:42:47.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4921" for this suite.
Apr 10 09:42:54.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:42:54.784: INFO: namespace projected-4921 deletion completed in 6.807933059s
•SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:42:54.785: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8584
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Apr 10 09:42:55.012: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config create -f - --namespace=kubectl-8584'
Apr 10 09:42:55.926: INFO: stderr: ""
Apr 10 09:42:55.926: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 10 09:42:56.947: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 09:42:56.947: INFO: Found 1 / 1
Apr 10 09:42:56.947: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Apr 10 09:42:56.971: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 09:42:56.971: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 10 09:42:56.971: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config patch pod redis-master-z4nrl --namespace=kubectl-8584 -p {"metadata":{"annotations":{"x":"y"}}}'
Apr 10 09:42:57.231: INFO: stderr: ""
Apr 10 09:42:57.231: INFO: stdout: "pod/redis-master-z4nrl patched\n"
STEP: checking annotations
Apr 10 09:42:57.254: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 09:42:57.254: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:42:57.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8584" for this suite.
Apr 10 09:43:19.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:43:20.095: INFO: namespace kubectl-8584 deletion completed in 22.820465134s
•SSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:43:20.095: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6965
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-133ad58c-5b75-11e9-bf19-c208aa3c4721
STEP: Creating secret with name s-test-opt-upd-133ad5df-5b75-11e9-bf19-c208aa3c4721
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-133ad58c-5b75-11e9-bf19-c208aa3c4721
STEP: Updating secret s-test-opt-upd-133ad5df-5b75-11e9-bf19-c208aa3c4721
STEP: Creating secret with name s-test-opt-create-133ad600-5b75-11e9-bf19-c208aa3c4721
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:43:24.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6965" for this suite.
Apr 10 09:43:47.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:43:47.774: INFO: namespace secrets-6965 deletion completed in 22.798565059s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:43:47.775: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6026
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Apr 10 09:43:50.099: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-23b63c33-5b75-11e9-bf19-c208aa3c4721,GenerateName:,Namespace:events-6026,SelfLink:/api/v1/namespaces/events-6026/pods/send-events-23b63c33-5b75-11e9-bf19-c208aa3c4721,UID:23b8961c-5b75-11e9-a517-0202de25e2de,ResourceVersion:7029,Generation:0,CreationTimestamp:2019-04-10 09:43:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 995761594,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.44/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ggrtg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ggrtg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-ggrtg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002713960} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002713980}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 09:43:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 09:43:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 09:43:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 09:43:48 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.152,PodIP:100.96.0.44,StartTime:2019-04-10 09:43:48 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-04-10 09:43:48 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://4246603a7cb5e33ae5b468b5e1ec983190e4f76ac0b2c52c05f48d747057e677}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Apr 10 09:43:52.120: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Apr 10 09:43:54.141: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:43:54.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6026" for this suite.
Apr 10 09:44:34.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:44:34.977: INFO: namespace events-6026 deletion completed in 40.794563915s
•SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:44:34.978: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9262
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-9262
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-9262
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9262
Apr 10 09:44:35.279: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Apr 10 09:44:45.301: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Apr 10 09:44:45.321: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 09:44:46.120: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 09:44:46.120: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 09:44:46.120: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 09:44:46.140: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 10 09:44:56.162: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 09:44:56.162: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 09:44:56.243: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999486s
Apr 10 09:44:57.264: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.978811052s
Apr 10 09:44:58.285: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.958200905s
Apr 10 09:44:59.306: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.936834641s
Apr 10 09:45:00.326: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.916243424s
Apr 10 09:45:01.347: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.895536653s
Apr 10 09:45:02.367: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.875368906s
Apr 10 09:45:03.388: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.854580055s
Apr 10 09:45:04.409: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.833799843s
Apr 10 09:45:05.430: INFO: Verifying statefulset ss doesn't scale past 1 for another 812.960927ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9262
Apr 10 09:45:06.451: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:45:07.249: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 09:45:07.249: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 09:45:07.249: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 09:45:07.272: INFO: Found 1 stateful pods, waiting for 3
Apr 10 09:45:17.293: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 09:45:17.294: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 09:45:17.294: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Apr 10 09:45:17.335: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 09:45:18.029: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 09:45:18.029: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 09:45:18.029: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 09:45:18.029: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 09:45:18.750: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 09:45:18.750: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 09:45:18.751: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 09:45:18.751: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 09:45:19.595: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 09:45:19.595: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 09:45:19.595: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 09:45:19.595: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 09:45:19.615: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Apr 10 09:45:29.657: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 09:45:29.658: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 09:45:29.658: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 09:45:29.717: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999459s
Apr 10 09:45:30.738: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.9798454s
Apr 10 09:45:31.761: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.958842603s
Apr 10 09:45:32.782: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.936343051s
Apr 10 09:45:33.803: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.915634387s
Apr 10 09:45:34.823: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.894713043s
Apr 10 09:45:35.844: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.874520331s
Apr 10 09:45:36.865: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.853534253s
Apr 10 09:45:37.885: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.832697134s
Apr 10 09:45:38.906: INFO: Verifying statefulset ss doesn't scale past 3 for another 812.222636ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9262
Apr 10 09:45:39.927: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:45:40.623: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 09:45:40.623: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 09:45:40.623: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 09:45:40.623: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:45:41.304: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 09:45:41.304: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 09:45:41.304: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 09:45:41.304: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:45:41.702: INFO: rc: 1
Apr 10 09:45:41.702: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0036f3b30 exit status 1 <nil> <nil> true [0xc001caa898 0xc001caa8b0 0xc001caa8c8] [0xc001caa898 0xc001caa8b0 0xc001caa8c8] [0xc001caa8a8 0xc001caa8c0] [0x9c0800 0x9c0800] 0xc002257860 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Apr 10 09:45:51.702: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:45:51.888: INFO: rc: 1
Apr 10 09:45:51.888: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000e946f0 exit status 1 <nil> <nil> true [0xc0000de3b8 0xc0000de818 0xc0000de898] [0xc0000de3b8 0xc0000de818 0xc0000de898] [0xc0000de700 0xc0000de860] [0x9c0800 0x9c0800] 0xc0021c24e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:46:01.889: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:46:02.081: INFO: rc: 1
Apr 10 09:46:02.081: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0017ce6f0 exit status 1 <nil> <nil> true [0xc00059c000 0xc00059c1c0 0xc00059c370] [0xc00059c000 0xc00059c1c0 0xc00059c370] [0xc00059c1a8 0xc00059c360] [0x9c0800 0x9c0800] 0xc001d6d2c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:46:12.082: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:46:12.307: INFO: rc: 1
Apr 10 09:46:12.307: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c148a0 exit status 1 <nil> <nil> true [0xc00264a000 0xc00264a018 0xc00264a030] [0xc00264a000 0xc00264a018 0xc00264a030] [0xc00264a010 0xc00264a028] [0x9c0800 0x9c0800] 0xc001dc9a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:46:22.308: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:46:22.499: INFO: rc: 1
Apr 10 09:46:22.499: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0017cee40 exit status 1 <nil> <nil> true [0xc00059c380 0xc00059c530 0xc00059c580] [0xc00059c380 0xc00059c530 0xc00059c580] [0xc00059c450 0xc00059c570] [0x9c0800 0x9c0800] 0xc001bb0c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:46:32.500: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:46:32.695: INFO: rc: 1
Apr 10 09:46:32.695: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c15170 exit status 1 <nil> <nil> true [0xc00264a038 0xc00264a050 0xc00264a068] [0xc00264a038 0xc00264a050 0xc00264a068] [0xc00264a048 0xc00264a060] [0x9c0800 0x9c0800] 0xc0020b63c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:46:42.695: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:46:42.890: INFO: rc: 1
Apr 10 09:46:42.891: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0017cf5c0 exit status 1 <nil> <nil> true [0xc00059c5b0 0xc00059c660 0xc00059c730] [0xc00059c5b0 0xc00059c660 0xc00059c730] [0xc00059c650 0xc00059c680] [0x9c0800 0x9c0800] 0xc001bb1920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:46:52.891: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:46:53.147: INFO: rc: 1
Apr 10 09:46:53.147: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002108330 exit status 1 <nil> <nil> true [0xc00059c778 0xc00059c8f8 0xc00059c9b0] [0xc00059c778 0xc00059c8f8 0xc00059c9b0] [0xc00059c858 0xc00059c970] [0x9c0800 0x9c0800] 0xc001abe540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:47:03.147: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:47:03.348: INFO: rc: 1
Apr 10 09:47:03.348: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c15920 exit status 1 <nil> <nil> true [0xc00264a070 0xc00264a088 0xc00264a0a0] [0xc00264a070 0xc00264a088 0xc00264a0a0] [0xc00264a080 0xc00264a098] [0x9c0800 0x9c0800] 0xc0020b68a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:47:13.348: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:47:13.582: INFO: rc: 1
Apr 10 09:47:13.582: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002386330 exit status 1 <nil> <nil> true [0xc00264a0a8 0xc00264a0c0 0xc00264a0d8] [0xc00264a0a8 0xc00264a0c0 0xc00264a0d8] [0xc00264a0b8 0xc00264a0d0] [0x9c0800 0x9c0800] 0xc0020b6de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:47:23.583: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:47:23.823: INFO: rc: 1
Apr 10 09:47:23.823: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002108a50 exit status 1 <nil> <nil> true [0xc00059c9f0 0xc00059cb68 0xc00059cc98] [0xc00059c9f0 0xc00059cb68 0xc00059cc98] [0xc00059cad8 0xc00059cc30] [0x9c0800 0x9c0800] 0xc001d1d1a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:47:33.824: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:47:34.000: INFO: rc: 1
Apr 10 09:47:34.000: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0021091d0 exit status 1 <nil> <nil> true [0xc00059ccc8 0xc00059cd20 0xc00059ce00] [0xc00059ccc8 0xc00059cd20 0xc00059ce00] [0xc00059cd08 0xc00059cd98] [0x9c0800 0x9c0800] 0xc00143eea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:47:44.000: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:47:44.201: INFO: rc: 1
Apr 10 09:47:44.201: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002658720 exit status 1 <nil> <nil> true [0xc0024f6000 0xc0024f6018 0xc0024f6030] [0xc0024f6000 0xc0024f6018 0xc0024f6030] [0xc0024f6010 0xc0024f6028] [0x9c0800 0x9c0800] 0xc0010365a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:47:54.201: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:47:54.428: INFO: rc: 1
Apr 10 09:47:54.428: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c148d0 exit status 1 <nil> <nil> true [0xc00264a008 0xc00264a020 0xc00264a038] [0xc00264a008 0xc00264a020 0xc00264a038] [0xc00264a018 0xc00264a030] [0x9c0800 0x9c0800] 0xc000cbdf20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:48:04.428: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:48:04.590: INFO: rc: 1
Apr 10 09:48:04.590: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0017ce750 exit status 1 <nil> <nil> true [0xc00059c000 0xc00059c1c0 0xc00059c370] [0xc00059c000 0xc00059c1c0 0xc00059c370] [0xc00059c1a8 0xc00059c360] [0x9c0800 0x9c0800] 0xc001abe360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:48:14.590: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:48:14.796: INFO: rc: 1
Apr 10 09:48:14.796: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c151a0 exit status 1 <nil> <nil> true [0xc00264a040 0xc00264a058 0xc00264a070] [0xc00264a040 0xc00264a058 0xc00264a070] [0xc00264a050 0xc00264a068] [0x9c0800 0x9c0800] 0xc001bb1440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:48:24.796: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:48:25.006: INFO: rc: 1
Apr 10 09:48:25.006: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c158c0 exit status 1 <nil> <nil> true [0xc00264a078 0xc00264a090 0xc00264a0a8] [0xc00264a078 0xc00264a090 0xc00264a0a8] [0xc00264a088 0xc00264a0a0] [0x9c0800 0x9c0800] 0xc001dc8000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:48:35.007: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:48:35.201: INFO: rc: 1
Apr 10 09:48:35.201: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023867e0 exit status 1 <nil> <nil> true [0xc0024f6000 0xc0024f6018 0xc0024f6030] [0xc0024f6000 0xc0024f6018 0xc0024f6030] [0xc0024f6010 0xc0024f6028] [0x9c0800 0x9c0800] 0xc001d6d2c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:48:45.201: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:48:45.382: INFO: rc: 1
Apr 10 09:48:45.382: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0021082a0 exit status 1 <nil> <nil> true [0xc00264a0b0 0xc00264a0c8 0xc00264a0e0] [0xc00264a0b0 0xc00264a0c8 0xc00264a0e0] [0xc00264a0c0 0xc00264a0d8] [0x9c0800 0x9c0800] 0xc001dc9ce0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:48:55.382: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:48:55.589: INFO: rc: 1
Apr 10 09:48:55.589: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0021089f0 exit status 1 <nil> <nil> true [0xc00264a0e8 0xc00264a100 0xc00264a118] [0xc00264a0e8 0xc00264a100 0xc00264a118] [0xc00264a0f8 0xc00264a110] [0x9c0800 0x9c0800] 0xc0020b64e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:49:05.590: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:49:05.787: INFO: rc: 1
Apr 10 09:49:05.787: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002386f60 exit status 1 <nil> <nil> true [0xc0024f6038 0xc0024f6050 0xc0024f6068] [0xc0024f6038 0xc0024f6050 0xc0024f6068] [0xc0024f6048 0xc0024f6060] [0x9c0800 0x9c0800] 0xc00105a960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:49:15.787: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:49:16.008: INFO: rc: 1
Apr 10 09:49:16.008: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023876b0 exit status 1 <nil> <nil> true [0xc0024f6070 0xc0024f6088 0xc0024f60a0] [0xc0024f6070 0xc0024f6088 0xc0024f60a0] [0xc0024f6080 0xc0024f6098] [0x9c0800 0x9c0800] 0xc0021c22a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:49:26.008: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:49:26.209: INFO: rc: 1
Apr 10 09:49:26.209: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002109200 exit status 1 <nil> <nil> true [0xc00264a120 0xc00264a138 0xc00264a150] [0xc00264a120 0xc00264a138 0xc00264a150] [0xc00264a130 0xc00264a148] [0x9c0800 0x9c0800] 0xc0020b6a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:49:36.209: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:49:36.402: INFO: rc: 1
Apr 10 09:49:36.402: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0017cef00 exit status 1 <nil> <nil> true [0xc00059c380 0xc00059c530 0xc00059c580] [0xc00059c380 0xc00059c530 0xc00059c580] [0xc00059c450 0xc00059c570] [0x9c0800 0x9c0800] 0xc001abef00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:49:46.402: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:49:46.586: INFO: rc: 1
Apr 10 09:49:46.586: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002109920 exit status 1 <nil> <nil> true [0xc00264a158 0xc00264a170 0xc00264a188] [0xc00264a158 0xc00264a170 0xc00264a188] [0xc00264a168 0xc00264a180] [0x9c0800 0x9c0800] 0xc0020b70e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:49:56.586: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:49:56.778: INFO: rc: 1
Apr 10 09:49:56.779: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c148a0 exit status 1 <nil> <nil> true [0xc00059c008 0xc00059c320 0xc00059c380] [0xc00059c008 0xc00059c320 0xc00059c380] [0xc00059c1c0 0xc00059c370] [0x9c0800 0x9c0800] 0xc001d6c720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:50:06.779: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:50:06.987: INFO: rc: 1
Apr 10 09:50:06.987: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0017ce720 exit status 1 <nil> <nil> true [0xc00264a000 0xc00264a018 0xc00264a030] [0xc00264a000 0xc00264a018 0xc00264a030] [0xc00264a010 0xc00264a028] [0x9c0800 0x9c0800] 0xc001dc9a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:50:16.988: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:50:17.223: INFO: rc: 1
Apr 10 09:50:17.223: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002108750 exit status 1 <nil> <nil> true [0xc0024f6000 0xc0024f6018 0xc0024f6030] [0xc0024f6000 0xc0024f6018 0xc0024f6030] [0xc0024f6010 0xc0024f6028] [0x9c0800 0x9c0800] 0xc001bb1440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:50:27.223: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:50:27.407: INFO: rc: 1
Apr 10 09:50:27.407: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002108ed0 exit status 1 <nil> <nil> true [0xc0024f6038 0xc0024f6050 0xc0024f6068] [0xc0024f6038 0xc0024f6050 0xc0024f6068] [0xc0024f6048 0xc0024f6060] [0x9c0800 0x9c0800] 0xc001d1d1a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:50:37.407: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:50:37.613: INFO: rc: 1
Apr 10 09:50:37.613: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002386780 exit status 1 <nil> <nil> true [0xc0000de078 0xc0000de700 0xc0000de860] [0xc0000de078 0xc0000de700 0xc0000de860] [0xc0000de460 0xc0000de828] [0x9c0800 0x9c0800] 0xc00143eea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 09:50:47.614: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-9262 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 09:50:47.794: INFO: rc: 1
Apr 10 09:50:47.794: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Apr 10 09:50:47.794: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 10 09:50:47.856: INFO: Deleting all statefulset in ns statefulset-9262
Apr 10 09:50:47.877: INFO: Scaling statefulset ss to 0
Apr 10 09:50:47.939: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 09:50:47.959: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:50:48.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9262" for this suite.
Apr 10 09:50:54.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:50:54.841: INFO: namespace statefulset-9262 deletion completed in 6.800354223s

• [SLOW TEST:379.864 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:50:54.842: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5541
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Apr 10 09:50:55.116: INFO: Waiting up to 5m0s for pod "pod-2248305a-5b76-11e9-bf19-c208aa3c4721" in namespace "emptydir-5541" to be "success or failure"
Apr 10 09:50:55.141: INFO: Pod "pod-2248305a-5b76-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 25.226256ms
Apr 10 09:50:57.162: INFO: Pod "pod-2248305a-5b76-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045951084s
STEP: Saw pod success
Apr 10 09:50:57.162: INFO: Pod "pod-2248305a-5b76-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 09:50:57.182: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-2248305a-5b76-11e9-bf19-c208aa3c4721 container test-container: <nil>
STEP: delete the pod
Apr 10 09:50:57.234: INFO: Waiting for pod pod-2248305a-5b76-11e9-bf19-c208aa3c4721 to disappear
Apr 10 09:50:57.254: INFO: Pod pod-2248305a-5b76-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:50:57.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5541" for this suite.
Apr 10 09:51:03.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:51:04.072: INFO: namespace emptydir-5541 deletion completed in 6.79736934s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:51:04.073: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8545
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 09:51:04.305: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8545'
Apr 10 09:51:04.505: INFO: stderr: ""
Apr 10 09:51:04.505: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Apr 10 09:51:04.525: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-8545'
Apr 10 09:51:13.340: INFO: stderr: ""
Apr 10 09:51:13.340: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:51:13.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8545" for this suite.
Apr 10 09:51:19.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:51:20.158: INFO: namespace kubectl-8545 deletion completed in 6.796254075s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:51:20.158: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6281
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-315d13a2-5b76-11e9-bf19-c208aa3c4721
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:51:20.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6281" for this suite.
Apr 10 09:51:26.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:51:27.255: INFO: namespace configmap-6281 deletion completed in 6.819981818s
•SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:51:27.256: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1596
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-d9vg
STEP: Creating a pod to test atomic-volume-subpath
Apr 10 09:51:27.561: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-d9vg" in namespace "subpath-1596" to be "success or failure"
Apr 10 09:51:27.581: INFO: Pod "pod-subpath-test-configmap-d9vg": Phase="Pending", Reason="", readiness=false. Elapsed: 20.259819ms
Apr 10 09:51:29.602: INFO: Pod "pod-subpath-test-configmap-d9vg": Phase="Running", Reason="", readiness=true. Elapsed: 2.04112401s
Apr 10 09:51:31.623: INFO: Pod "pod-subpath-test-configmap-d9vg": Phase="Running", Reason="", readiness=true. Elapsed: 4.0617874s
Apr 10 09:51:33.644: INFO: Pod "pod-subpath-test-configmap-d9vg": Phase="Running", Reason="", readiness=true. Elapsed: 6.082742499s
Apr 10 09:51:35.664: INFO: Pod "pod-subpath-test-configmap-d9vg": Phase="Running", Reason="", readiness=true. Elapsed: 8.103445822s
Apr 10 09:51:37.685: INFO: Pod "pod-subpath-test-configmap-d9vg": Phase="Running", Reason="", readiness=true. Elapsed: 10.124547599s
Apr 10 09:51:39.706: INFO: Pod "pod-subpath-test-configmap-d9vg": Phase="Running", Reason="", readiness=true. Elapsed: 12.145079103s
Apr 10 09:51:41.727: INFO: Pod "pod-subpath-test-configmap-d9vg": Phase="Running", Reason="", readiness=true. Elapsed: 14.165741667s
Apr 10 09:51:43.748: INFO: Pod "pod-subpath-test-configmap-d9vg": Phase="Running", Reason="", readiness=true. Elapsed: 16.186650651s
Apr 10 09:51:45.771: INFO: Pod "pod-subpath-test-configmap-d9vg": Phase="Running", Reason="", readiness=true. Elapsed: 18.209796179s
Apr 10 09:51:47.792: INFO: Pod "pod-subpath-test-configmap-d9vg": Phase="Running", Reason="", readiness=true. Elapsed: 20.230797392s
Apr 10 09:51:49.814: INFO: Pod "pod-subpath-test-configmap-d9vg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.252814661s
STEP: Saw pod success
Apr 10 09:51:49.814: INFO: Pod "pod-subpath-test-configmap-d9vg" satisfied condition "success or failure"
Apr 10 09:51:49.834: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-subpath-test-configmap-d9vg container test-container-subpath-configmap-d9vg: <nil>
STEP: delete the pod
Apr 10 09:51:49.882: INFO: Waiting for pod pod-subpath-test-configmap-d9vg to disappear
Apr 10 09:51:49.902: INFO: Pod pod-subpath-test-configmap-d9vg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-d9vg
Apr 10 09:51:49.902: INFO: Deleting pod "pod-subpath-test-configmap-d9vg" in namespace "subpath-1596"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:51:49.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1596" for this suite.
Apr 10 09:51:56.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:51:56.736: INFO: namespace subpath-1596 deletion completed in 6.793978425s
•SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:51:56.736: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6802
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0410 09:52:37.137559    4420 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 10 09:52:37.137: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:52:37.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6802" for this suite.
Apr 10 09:52:43.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:52:43.980: INFO: namespace gc-6802 deletion completed in 6.822907319s
•SS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:52:43.980: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4379
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-pj8cn in namespace proxy-4379
I0410 09:52:44.338292    4420 runners.go:184] Created replication controller with name: proxy-service-pj8cn, namespace: proxy-4379, replica count: 1
I0410 09:52:45.389972    4420 runners.go:184] proxy-service-pj8cn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0410 09:52:46.390251    4420 runners.go:184] proxy-service-pj8cn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0410 09:52:47.390498    4420 runners.go:184] proxy-service-pj8cn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0410 09:52:48.390673    4420 runners.go:184] proxy-service-pj8cn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0410 09:52:49.390902    4420 runners.go:184] proxy-service-pj8cn Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 10 09:52:49.411: INFO: setup took 5.117440985s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Apr 10 09:52:49.435: INFO: (0) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 24.703594ms)
Apr 10 09:52:49.435: INFO: (0) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 24.792806ms)
Apr 10 09:52:49.436: INFO: (0) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 24.796876ms)
Apr 10 09:52:49.436: INFO: (0) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 24.687273ms)
Apr 10 09:52:49.436: INFO: (0) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 24.73794ms)
Apr 10 09:52:49.442: INFO: (0) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 31.434949ms)
Apr 10 09:52:49.442: INFO: (0) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 31.343492ms)
Apr 10 09:52:49.442: INFO: (0) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 31.486215ms)
Apr 10 09:52:49.451: INFO: (0) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 39.770937ms)
Apr 10 09:52:49.451: INFO: (0) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 39.823135ms)
Apr 10 09:52:49.452: INFO: (0) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 41.020021ms)
Apr 10 09:52:49.453: INFO: (0) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 42.002674ms)
Apr 10 09:52:49.453: INFO: (0) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 41.871827ms)
Apr 10 09:52:49.466: INFO: (0) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 54.98097ms)
Apr 10 09:52:49.466: INFO: (0) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 55.407805ms)
Apr 10 09:52:49.475: INFO: (0) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 63.851527ms)
Apr 10 09:52:49.497: INFO: (1) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 21.678245ms)
Apr 10 09:52:49.498: INFO: (1) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 22.572387ms)
Apr 10 09:52:49.498: INFO: (1) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 22.64817ms)
Apr 10 09:52:49.498: INFO: (1) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 22.687053ms)
Apr 10 09:52:49.498: INFO: (1) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 22.712583ms)
Apr 10 09:52:49.498: INFO: (1) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 22.842703ms)
Apr 10 09:52:49.498: INFO: (1) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 22.691943ms)
Apr 10 09:52:49.498: INFO: (1) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 22.680724ms)
Apr 10 09:52:49.498: INFO: (1) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 22.662481ms)
Apr 10 09:52:49.498: INFO: (1) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 22.665408ms)
Apr 10 09:52:49.498: INFO: (1) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.667731ms)
Apr 10 09:52:49.498: INFO: (1) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.913626ms)
Apr 10 09:52:49.500: INFO: (1) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 24.841054ms)
Apr 10 09:52:49.500: INFO: (1) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 24.718774ms)
Apr 10 09:52:49.500: INFO: (1) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 24.788085ms)
Apr 10 09:52:49.500: INFO: (1) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 24.862093ms)
Apr 10 09:52:49.522: INFO: (2) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 21.719694ms)
Apr 10 09:52:49.522: INFO: (2) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 22.396365ms)
Apr 10 09:52:49.522: INFO: (2) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 22.349387ms)
Apr 10 09:52:49.522: INFO: (2) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 22.410656ms)
Apr 10 09:52:49.522: INFO: (2) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 22.360384ms)
Apr 10 09:52:49.522: INFO: (2) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 22.5664ms)
Apr 10 09:52:49.522: INFO: (2) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 22.471104ms)
Apr 10 09:52:49.522: INFO: (2) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 22.525695ms)
Apr 10 09:52:49.522: INFO: (2) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.387999ms)
Apr 10 09:52:49.522: INFO: (2) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 22.619811ms)
Apr 10 09:52:49.522: INFO: (2) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 22.424676ms)
Apr 10 09:52:49.522: INFO: (2) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 22.433308ms)
Apr 10 09:52:49.523: INFO: (2) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 23.349966ms)
Apr 10 09:52:49.524: INFO: (2) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 24.276437ms)
Apr 10 09:52:49.524: INFO: (2) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 24.422477ms)
Apr 10 09:52:49.525: INFO: (2) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 25.171219ms)
Apr 10 09:52:49.546: INFO: (3) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 21.162574ms)
Apr 10 09:52:49.547: INFO: (3) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 21.617048ms)
Apr 10 09:52:49.547: INFO: (3) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 21.664277ms)
Apr 10 09:52:49.547: INFO: (3) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 21.769853ms)
Apr 10 09:52:49.547: INFO: (3) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 21.685845ms)
Apr 10 09:52:49.547: INFO: (3) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 21.67787ms)
Apr 10 09:52:49.547: INFO: (3) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 21.918738ms)
Apr 10 09:52:49.547: INFO: (3) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 22.068639ms)
Apr 10 09:52:49.547: INFO: (3) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 22.020441ms)
Apr 10 09:52:49.547: INFO: (3) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 22.084177ms)
Apr 10 09:52:49.548: INFO: (3) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 22.684082ms)
Apr 10 09:52:49.548: INFO: (3) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 22.607695ms)
Apr 10 09:52:49.548: INFO: (3) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 22.773119ms)
Apr 10 09:52:49.549: INFO: (3) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 23.335705ms)
Apr 10 09:52:49.549: INFO: (3) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 23.363989ms)
Apr 10 09:52:49.549: INFO: (3) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 24.093229ms)
Apr 10 09:52:49.571: INFO: (4) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 21.839611ms)
Apr 10 09:52:49.572: INFO: (4) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 21.95626ms)
Apr 10 09:52:49.572: INFO: (4) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 22.118082ms)
Apr 10 09:52:49.572: INFO: (4) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 22.212628ms)
Apr 10 09:52:49.572: INFO: (4) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 22.233724ms)
Apr 10 09:52:49.572: INFO: (4) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 22.185072ms)
Apr 10 09:52:49.572: INFO: (4) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.257033ms)
Apr 10 09:52:49.573: INFO: (4) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 23.651019ms)
Apr 10 09:52:49.573: INFO: (4) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 23.517502ms)
Apr 10 09:52:49.573: INFO: (4) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 23.610636ms)
Apr 10 09:52:49.573: INFO: (4) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 23.776004ms)
Apr 10 09:52:49.573: INFO: (4) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 23.646204ms)
Apr 10 09:52:49.573: INFO: (4) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 23.77113ms)
Apr 10 09:52:49.573: INFO: (4) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 23.898748ms)
Apr 10 09:52:49.573: INFO: (4) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 23.821826ms)
Apr 10 09:52:49.573: INFO: (4) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 23.890599ms)
Apr 10 09:52:49.596: INFO: (5) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 21.870265ms)
Apr 10 09:52:49.596: INFO: (5) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 21.584217ms)
Apr 10 09:52:49.596: INFO: (5) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 21.902879ms)
Apr 10 09:52:49.596: INFO: (5) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 22.157653ms)
Apr 10 09:52:49.596: INFO: (5) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 22.315477ms)
Apr 10 09:52:49.596: INFO: (5) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 22.51705ms)
Apr 10 09:52:49.596: INFO: (5) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 22.676389ms)
Apr 10 09:52:49.596: INFO: (5) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 22.166801ms)
Apr 10 09:52:49.596: INFO: (5) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 22.217343ms)
Apr 10 09:52:49.596: INFO: (5) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 21.99204ms)
Apr 10 09:52:49.596: INFO: (5) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.411763ms)
Apr 10 09:52:49.597: INFO: (5) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 22.615619ms)
Apr 10 09:52:49.597: INFO: (5) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 22.988938ms)
Apr 10 09:52:49.597: INFO: (5) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 23.044406ms)
Apr 10 09:52:49.598: INFO: (5) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 23.761134ms)
Apr 10 09:52:49.598: INFO: (5) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 24.21248ms)
Apr 10 09:52:49.620: INFO: (6) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 21.594377ms)
Apr 10 09:52:49.620: INFO: (6) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 21.612673ms)
Apr 10 09:52:49.621: INFO: (6) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 22.131087ms)
Apr 10 09:52:49.621: INFO: (6) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.180604ms)
Apr 10 09:52:49.621: INFO: (6) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 22.257019ms)
Apr 10 09:52:49.622: INFO: (6) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 23.245867ms)
Apr 10 09:52:49.622: INFO: (6) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 23.267197ms)
Apr 10 09:52:49.622: INFO: (6) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 23.764855ms)
Apr 10 09:52:49.622: INFO: (6) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 23.169005ms)
Apr 10 09:52:49.622: INFO: (6) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 23.263329ms)
Apr 10 09:52:49.622: INFO: (6) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 23.615427ms)
Apr 10 09:52:49.622: INFO: (6) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 23.238639ms)
Apr 10 09:52:49.622: INFO: (6) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 23.565328ms)
Apr 10 09:52:49.623: INFO: (6) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 24.594486ms)
Apr 10 09:52:49.624: INFO: (6) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 25.531097ms)
Apr 10 09:52:49.624: INFO: (6) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 25.573867ms)
Apr 10 09:52:49.646: INFO: (7) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 21.61218ms)
Apr 10 09:52:49.646: INFO: (7) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 21.753345ms)
Apr 10 09:52:49.646: INFO: (7) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 21.931907ms)
Apr 10 09:52:49.647: INFO: (7) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 22.107998ms)
Apr 10 09:52:49.647: INFO: (7) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.163216ms)
Apr 10 09:52:49.647: INFO: (7) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 22.367755ms)
Apr 10 09:52:49.647: INFO: (7) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 22.202508ms)
Apr 10 09:52:49.647: INFO: (7) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.260153ms)
Apr 10 09:52:49.647: INFO: (7) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 22.298821ms)
Apr 10 09:52:49.647: INFO: (7) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 22.271694ms)
Apr 10 09:52:49.647: INFO: (7) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 22.731625ms)
Apr 10 09:52:49.647: INFO: (7) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 22.846315ms)
Apr 10 09:52:49.648: INFO: (7) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 23.268522ms)
Apr 10 09:52:49.648: INFO: (7) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 23.729452ms)
Apr 10 09:52:49.649: INFO: (7) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 24.484342ms)
Apr 10 09:52:49.649: INFO: (7) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 24.61176ms)
Apr 10 09:52:49.671: INFO: (8) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 22.146333ms)
Apr 10 09:52:49.672: INFO: (8) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 22.261595ms)
Apr 10 09:52:49.672: INFO: (8) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 22.254739ms)
Apr 10 09:52:49.672: INFO: (8) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.274402ms)
Apr 10 09:52:49.672: INFO: (8) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 22.368601ms)
Apr 10 09:52:49.672: INFO: (8) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 22.288282ms)
Apr 10 09:52:49.674: INFO: (8) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 24.48174ms)
Apr 10 09:52:49.674: INFO: (8) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 24.431849ms)
Apr 10 09:52:49.674: INFO: (8) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 24.411997ms)
Apr 10 09:52:49.674: INFO: (8) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 24.545252ms)
Apr 10 09:52:49.674: INFO: (8) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 24.639787ms)
Apr 10 09:52:49.674: INFO: (8) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 24.688879ms)
Apr 10 09:52:49.674: INFO: (8) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 24.5356ms)
Apr 10 09:52:49.674: INFO: (8) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 24.594416ms)
Apr 10 09:52:49.674: INFO: (8) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 24.686775ms)
Apr 10 09:52:49.674: INFO: (8) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 24.667851ms)
Apr 10 09:52:49.696: INFO: (9) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 21.434465ms)
Apr 10 09:52:49.696: INFO: (9) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.031115ms)
Apr 10 09:52:49.697: INFO: (9) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 21.943926ms)
Apr 10 09:52:49.697: INFO: (9) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 22.47233ms)
Apr 10 09:52:49.697: INFO: (9) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 22.105084ms)
Apr 10 09:52:49.697: INFO: (9) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 21.860234ms)
Apr 10 09:52:49.697: INFO: (9) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.033069ms)
Apr 10 09:52:49.697: INFO: (9) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 22.652574ms)
Apr 10 09:52:49.697: INFO: (9) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 22.143331ms)
Apr 10 09:52:49.697: INFO: (9) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 22.407925ms)
Apr 10 09:52:49.697: INFO: (9) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 22.298505ms)
Apr 10 09:52:49.697: INFO: (9) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 22.055726ms)
Apr 10 09:52:49.698: INFO: (9) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 22.727953ms)
Apr 10 09:52:49.698: INFO: (9) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 23.252816ms)
Apr 10 09:52:49.699: INFO: (9) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 23.994962ms)
Apr 10 09:52:49.699: INFO: (9) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 24.36819ms)
Apr 10 09:52:49.722: INFO: (10) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 22.735167ms)
Apr 10 09:52:49.723: INFO: (10) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.720817ms)
Apr 10 09:52:49.723: INFO: (10) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 23.430504ms)
Apr 10 09:52:49.723: INFO: (10) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 22.757723ms)
Apr 10 09:52:49.723: INFO: (10) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 22.894563ms)
Apr 10 09:52:49.724: INFO: (10) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 23.54765ms)
Apr 10 09:52:49.724: INFO: (10) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 23.510472ms)
Apr 10 09:52:49.724: INFO: (10) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 23.355662ms)
Apr 10 09:52:49.724: INFO: (10) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 23.812288ms)
Apr 10 09:52:49.725: INFO: (10) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 25.163649ms)
Apr 10 09:52:49.725: INFO: (10) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 25.073522ms)
Apr 10 09:52:49.725: INFO: (10) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 24.732864ms)
Apr 10 09:52:49.725: INFO: (10) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 24.793105ms)
Apr 10 09:52:49.725: INFO: (10) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 25.338595ms)
Apr 10 09:52:49.726: INFO: (10) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 25.935293ms)
Apr 10 09:52:49.726: INFO: (10) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 26.25277ms)
Apr 10 09:52:49.748: INFO: (11) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 21.922386ms)
Apr 10 09:52:49.748: INFO: (11) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 22.202745ms)
Apr 10 09:52:49.748: INFO: (11) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 22.20783ms)
Apr 10 09:52:49.748: INFO: (11) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 22.118608ms)
Apr 10 09:52:49.748: INFO: (11) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 22.212216ms)
Apr 10 09:52:49.748: INFO: (11) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 22.124283ms)
Apr 10 09:52:49.748: INFO: (11) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 22.199665ms)
Apr 10 09:52:49.748: INFO: (11) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 22.320025ms)
Apr 10 09:52:49.748: INFO: (11) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.154819ms)
Apr 10 09:52:49.748: INFO: (11) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 22.221135ms)
Apr 10 09:52:49.748: INFO: (11) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 22.191947ms)
Apr 10 09:52:49.749: INFO: (11) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 22.623434ms)
Apr 10 09:52:49.750: INFO: (11) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 23.639834ms)
Apr 10 09:52:49.751: INFO: (11) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 24.739527ms)
Apr 10 09:52:49.751: INFO: (11) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 24.617674ms)
Apr 10 09:52:49.751: INFO: (11) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 24.658939ms)
Apr 10 09:52:49.774: INFO: (12) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 22.781216ms)
Apr 10 09:52:49.774: INFO: (12) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.946498ms)
Apr 10 09:52:49.774: INFO: (12) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 23.075977ms)
Apr 10 09:52:49.774: INFO: (12) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 23.189306ms)
Apr 10 09:52:49.774: INFO: (12) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 23.092338ms)
Apr 10 09:52:49.774: INFO: (12) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 23.02788ms)
Apr 10 09:52:49.774: INFO: (12) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 23.160716ms)
Apr 10 09:52:49.774: INFO: (12) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 23.146326ms)
Apr 10 09:52:49.774: INFO: (12) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 23.260845ms)
Apr 10 09:52:49.774: INFO: (12) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 23.53031ms)
Apr 10 09:52:49.774: INFO: (12) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 23.59244ms)
Apr 10 09:52:49.775: INFO: (12) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 23.614182ms)
Apr 10 09:52:49.775: INFO: (12) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 24.000664ms)
Apr 10 09:52:49.776: INFO: (12) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 24.970977ms)
Apr 10 09:52:49.776: INFO: (12) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 25.052083ms)
Apr 10 09:52:49.776: INFO: (12) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 25.363851ms)
Apr 10 09:52:49.798: INFO: (13) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 21.375049ms)
Apr 10 09:52:49.798: INFO: (13) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 21.492935ms)
Apr 10 09:52:49.798: INFO: (13) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 21.67488ms)
Apr 10 09:52:49.798: INFO: (13) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 21.432084ms)
Apr 10 09:52:49.798: INFO: (13) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 21.493586ms)
Apr 10 09:52:49.798: INFO: (13) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 22.136217ms)
Apr 10 09:52:49.798: INFO: (13) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 22.081166ms)
Apr 10 09:52:49.798: INFO: (13) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 22.073176ms)
Apr 10 09:52:49.799: INFO: (13) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 22.643892ms)
Apr 10 09:52:49.799: INFO: (13) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 22.816268ms)
Apr 10 09:52:49.799: INFO: (13) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 22.719866ms)
Apr 10 09:52:49.799: INFO: (13) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 22.771293ms)
Apr 10 09:52:49.800: INFO: (13) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 23.28317ms)
Apr 10 09:52:49.800: INFO: (13) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 23.377293ms)
Apr 10 09:52:49.800: INFO: (13) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 23.398418ms)
Apr 10 09:52:49.800: INFO: (13) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 24.061917ms)
Apr 10 09:52:49.822: INFO: (14) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 21.830141ms)
Apr 10 09:52:49.822: INFO: (14) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 21.87634ms)
Apr 10 09:52:49.822: INFO: (14) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 22.153221ms)
Apr 10 09:52:49.822: INFO: (14) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 21.941188ms)
Apr 10 09:52:49.822: INFO: (14) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.176007ms)
Apr 10 09:52:49.822: INFO: (14) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.075799ms)
Apr 10 09:52:49.822: INFO: (14) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 22.023416ms)
Apr 10 09:52:49.822: INFO: (14) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 22.046395ms)
Apr 10 09:52:49.822: INFO: (14) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 21.997444ms)
Apr 10 09:52:49.823: INFO: (14) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 22.570412ms)
Apr 10 09:52:49.823: INFO: (14) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 22.723278ms)
Apr 10 09:52:49.823: INFO: (14) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 22.782399ms)
Apr 10 09:52:49.824: INFO: (14) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 23.420869ms)
Apr 10 09:52:49.824: INFO: (14) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 23.516118ms)
Apr 10 09:52:49.824: INFO: (14) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 24.06762ms)
Apr 10 09:52:49.829: INFO: (14) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 28.156841ms)
Apr 10 09:52:49.853: INFO: (15) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 24.130531ms)
Apr 10 09:52:49.853: INFO: (15) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 24.1938ms)
Apr 10 09:52:49.853: INFO: (15) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 24.112051ms)
Apr 10 09:52:49.853: INFO: (15) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 24.179949ms)
Apr 10 09:52:49.853: INFO: (15) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 24.218203ms)
Apr 10 09:52:49.853: INFO: (15) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 24.271499ms)
Apr 10 09:52:49.853: INFO: (15) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 24.230523ms)
Apr 10 09:52:49.853: INFO: (15) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 24.366563ms)
Apr 10 09:52:49.853: INFO: (15) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 24.335398ms)
Apr 10 09:52:49.853: INFO: (15) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 24.167392ms)
Apr 10 09:52:49.853: INFO: (15) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 24.410664ms)
Apr 10 09:52:49.853: INFO: (15) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 24.222781ms)
Apr 10 09:52:49.854: INFO: (15) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 24.816502ms)
Apr 10 09:52:49.854: INFO: (15) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 24.934811ms)
Apr 10 09:52:49.857: INFO: (15) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 28.165788ms)
Apr 10 09:52:49.857: INFO: (15) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 28.31778ms)
Apr 10 09:52:49.880: INFO: (16) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.20652ms)
Apr 10 09:52:49.880: INFO: (16) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 22.265632ms)
Apr 10 09:52:49.880: INFO: (16) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 22.490291ms)
Apr 10 09:52:49.880: INFO: (16) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 22.429359ms)
Apr 10 09:52:49.880: INFO: (16) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 22.63202ms)
Apr 10 09:52:49.880: INFO: (16) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 22.643493ms)
Apr 10 09:52:49.880: INFO: (16) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.560949ms)
Apr 10 09:52:49.880: INFO: (16) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 22.535323ms)
Apr 10 09:52:49.880: INFO: (16) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 23.034991ms)
Apr 10 09:52:49.880: INFO: (16) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 23.122054ms)
Apr 10 09:52:49.880: INFO: (16) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 23.149008ms)
Apr 10 09:52:49.880: INFO: (16) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 23.267415ms)
Apr 10 09:52:49.881: INFO: (16) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 24.243233ms)
Apr 10 09:52:49.882: INFO: (16) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 24.69688ms)
Apr 10 09:52:49.882: INFO: (16) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 24.676123ms)
Apr 10 09:52:49.883: INFO: (16) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 25.762947ms)
Apr 10 09:52:49.906: INFO: (17) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 22.826768ms)
Apr 10 09:52:49.906: INFO: (17) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.604063ms)
Apr 10 09:52:49.906: INFO: (17) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 22.783902ms)
Apr 10 09:52:49.906: INFO: (17) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 22.576954ms)
Apr 10 09:52:49.906: INFO: (17) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 22.998568ms)
Apr 10 09:52:49.906: INFO: (17) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 22.718932ms)
Apr 10 09:52:49.906: INFO: (17) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.951109ms)
Apr 10 09:52:49.906: INFO: (17) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 23.203264ms)
Apr 10 09:52:49.906: INFO: (17) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 22.356815ms)
Apr 10 09:52:49.907: INFO: (17) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 22.985228ms)
Apr 10 09:52:49.907: INFO: (17) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 23.172958ms)
Apr 10 09:52:49.907: INFO: (17) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 23.456154ms)
Apr 10 09:52:49.907: INFO: (17) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 23.413031ms)
Apr 10 09:52:49.909: INFO: (17) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 25.235274ms)
Apr 10 09:52:49.909: INFO: (17) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 25.587364ms)
Apr 10 09:52:49.909: INFO: (17) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 25.419552ms)
Apr 10 09:52:49.931: INFO: (18) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 21.616891ms)
Apr 10 09:52:49.931: INFO: (18) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 21.838425ms)
Apr 10 09:52:49.931: INFO: (18) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 21.768605ms)
Apr 10 09:52:49.931: INFO: (18) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 21.989088ms)
Apr 10 09:52:49.932: INFO: (18) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 22.160443ms)
Apr 10 09:52:49.932: INFO: (18) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 22.792173ms)
Apr 10 09:52:49.932: INFO: (18) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 22.734336ms)
Apr 10 09:52:49.932: INFO: (18) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 22.724035ms)
Apr 10 09:52:49.932: INFO: (18) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.76052ms)
Apr 10 09:52:49.932: INFO: (18) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 22.76316ms)
Apr 10 09:52:49.932: INFO: (18) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 22.982938ms)
Apr 10 09:52:49.933: INFO: (18) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 23.554685ms)
Apr 10 09:52:49.933: INFO: (18) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 23.58427ms)
Apr 10 09:52:49.934: INFO: (18) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 24.849756ms)
Apr 10 09:52:49.935: INFO: (18) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 25.135295ms)
Apr 10 09:52:49.935: INFO: (18) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 25.153877ms)
Apr 10 09:52:49.957: INFO: (19) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">test<... (200; 22.034251ms)
Apr 10 09:52:49.957: INFO: (19) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 22.134278ms)
Apr 10 09:52:49.957: INFO: (19) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 21.95208ms)
Apr 10 09:52:49.957: INFO: (19) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:1080/proxy/rewriteme">... (200; 22.034168ms)
Apr 10 09:52:49.957: INFO: (19) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:443/proxy/tlsrewritem... (200; 22.603948ms)
Apr 10 09:52:49.957: INFO: (19) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:160/proxy/: foo (200; 22.554733ms)
Apr 10 09:52:49.957: INFO: (19) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:460/proxy/: tls baz (200; 22.551745ms)
Apr 10 09:52:49.957: INFO: (19) /api/v1/namespaces/proxy-4379/pods/https:proxy-service-pj8cn-v62dw:462/proxy/: tls qux (200; 22.549809ms)
Apr 10 09:52:49.957: INFO: (19) /api/v1/namespaces/proxy-4379/pods/http:proxy-service-pj8cn-v62dw:162/proxy/: bar (200; 22.687752ms)
Apr 10 09:52:49.958: INFO: (19) /api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/: <a href="/api/v1/namespaces/proxy-4379/pods/proxy-service-pj8cn-v62dw/proxy/rewriteme">test</a> (200; 23.082316ms)
Apr 10 09:52:49.958: INFO: (19) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname1/proxy/: tls baz (200; 23.120358ms)
Apr 10 09:52:49.958: INFO: (19) /api/v1/namespaces/proxy-4379/services/https:proxy-service-pj8cn:tlsportname2/proxy/: tls qux (200; 23.108024ms)
Apr 10 09:52:49.959: INFO: (19) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname1/proxy/: foo (200; 23.745574ms)
Apr 10 09:52:49.959: INFO: (19) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname2/proxy/: bar (200; 23.724903ms)
Apr 10 09:52:49.960: INFO: (19) /api/v1/namespaces/proxy-4379/services/http:proxy-service-pj8cn:portname1/proxy/: foo (200; 24.664316ms)
Apr 10 09:52:49.960: INFO: (19) /api/v1/namespaces/proxy-4379/services/proxy-service-pj8cn:portname2/proxy/: bar (200; 24.730755ms)
STEP: deleting ReplicationController proxy-service-pj8cn in namespace proxy-4379, will wait for the garbage collector to delete the pods
Apr 10 09:52:50.051: INFO: Deleting ReplicationController proxy-service-pj8cn took: 21.516379ms
Apr 10 09:52:50.152: INFO: Terminating ReplicationController proxy-service-pj8cn pods took: 100.401748ms
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:53:03.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4379" for this suite.
Apr 10 09:53:09.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:53:10.172: INFO: namespace proxy-4379 deletion completed in 6.799499172s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:53:10.172: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5030
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 09:53:28.476: INFO: Container started at 2019-04-10 09:53:11 +0000 UTC, pod became ready at 2019-04-10 09:53:27 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:53:28.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5030" for this suite.
Apr 10 09:53:46.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:53:47.301: INFO: namespace container-probe-5030 deletion completed in 18.803266506s
•SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:53:47.302: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9727
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-qkcp
STEP: Creating a pod to test atomic-volume-subpath
Apr 10 09:53:47.663: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-qkcp" in namespace "subpath-9727" to be "success or failure"
Apr 10 09:53:47.683: INFO: Pod "pod-subpath-test-downwardapi-qkcp": Phase="Pending", Reason="", readiness=false. Elapsed: 19.502648ms
Apr 10 09:53:49.704: INFO: Pod "pod-subpath-test-downwardapi-qkcp": Phase="Running", Reason="", readiness=true. Elapsed: 2.040454111s
Apr 10 09:53:51.724: INFO: Pod "pod-subpath-test-downwardapi-qkcp": Phase="Running", Reason="", readiness=true. Elapsed: 4.061368316s
Apr 10 09:53:53.745: INFO: Pod "pod-subpath-test-downwardapi-qkcp": Phase="Running", Reason="", readiness=true. Elapsed: 6.082248532s
Apr 10 09:53:55.767: INFO: Pod "pod-subpath-test-downwardapi-qkcp": Phase="Running", Reason="", readiness=true. Elapsed: 8.103703593s
Apr 10 09:53:57.788: INFO: Pod "pod-subpath-test-downwardapi-qkcp": Phase="Running", Reason="", readiness=true. Elapsed: 10.12474121s
Apr 10 09:53:59.809: INFO: Pod "pod-subpath-test-downwardapi-qkcp": Phase="Running", Reason="", readiness=true. Elapsed: 12.14569925s
Apr 10 09:54:01.830: INFO: Pod "pod-subpath-test-downwardapi-qkcp": Phase="Running", Reason="", readiness=true. Elapsed: 14.166658014s
Apr 10 09:54:03.850: INFO: Pod "pod-subpath-test-downwardapi-qkcp": Phase="Running", Reason="", readiness=true. Elapsed: 16.187202492s
Apr 10 09:54:05.871: INFO: Pod "pod-subpath-test-downwardapi-qkcp": Phase="Running", Reason="", readiness=true. Elapsed: 18.207860674s
Apr 10 09:54:07.894: INFO: Pod "pod-subpath-test-downwardapi-qkcp": Phase="Running", Reason="", readiness=true. Elapsed: 20.230746866s
Apr 10 09:54:09.915: INFO: Pod "pod-subpath-test-downwardapi-qkcp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.251533978s
STEP: Saw pod success
Apr 10 09:54:09.915: INFO: Pod "pod-subpath-test-downwardapi-qkcp" satisfied condition "success or failure"
Apr 10 09:54:09.935: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-subpath-test-downwardapi-qkcp container test-container-subpath-downwardapi-qkcp: <nil>
STEP: delete the pod
Apr 10 09:54:09.985: INFO: Waiting for pod pod-subpath-test-downwardapi-qkcp to disappear
Apr 10 09:54:10.004: INFO: Pod pod-subpath-test-downwardapi-qkcp no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-qkcp
Apr 10 09:54:10.005: INFO: Deleting pod "pod-subpath-test-downwardapi-qkcp" in namespace "subpath-9727"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:54:10.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9727" for this suite.
Apr 10 09:54:16.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:54:16.831: INFO: namespace subpath-9727 deletion completed in 6.786579983s
•SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:54:16.831: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7737
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 10 09:54:17.333: INFO: Number of nodes with available pods: 0
Apr 10 09:54:17.333: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 09:54:18.374: INFO: Number of nodes with available pods: 0
Apr 10 09:54:18.374: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 09:54:19.374: INFO: Number of nodes with available pods: 2
Apr 10 09:54:19.374: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Apr 10 09:54:19.477: INFO: Number of nodes with available pods: 1
Apr 10 09:54:19.477: INFO: Node ip-10-250-25-25.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 09:54:20.518: INFO: Number of nodes with available pods: 1
Apr 10 09:54:20.518: INFO: Node ip-10-250-25-25.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 09:54:21.517: INFO: Number of nodes with available pods: 1
Apr 10 09:54:21.517: INFO: Node ip-10-250-25-25.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 09:54:22.518: INFO: Number of nodes with available pods: 1
Apr 10 09:54:22.518: INFO: Node ip-10-250-25-25.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 09:54:23.518: INFO: Number of nodes with available pods: 1
Apr 10 09:54:23.518: INFO: Node ip-10-250-25-25.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 09:54:24.517: INFO: Number of nodes with available pods: 1
Apr 10 09:54:24.517: INFO: Node ip-10-250-25-25.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 09:54:25.518: INFO: Number of nodes with available pods: 1
Apr 10 09:54:25.518: INFO: Node ip-10-250-25-25.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 09:54:26.518: INFO: Number of nodes with available pods: 1
Apr 10 09:54:26.518: INFO: Node ip-10-250-25-25.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 09:54:27.518: INFO: Number of nodes with available pods: 2
Apr 10 09:54:27.518: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7737, will wait for the garbage collector to delete the pods
Apr 10 09:54:27.631: INFO: Deleting DaemonSet.extensions daemon-set took: 22.776453ms
Apr 10 09:54:28.032: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.447467ms
Apr 10 09:54:34.652: INFO: Number of nodes with available pods: 0
Apr 10 09:54:34.653: INFO: Number of running nodes: 0, number of available pods: 0
Apr 10 09:54:34.675: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7737/daemonsets","resourceVersion":"8759"},"items":null}

Apr 10 09:54:34.695: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7737/pods","resourceVersion":"8759"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:54:34.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7737" for this suite.
Apr 10 09:54:40.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:54:41.573: INFO: namespace daemonsets-7737 deletion completed in 6.796280753s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:54:41.573: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 09:54:41.798: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:54:44.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6331" for this suite.
Apr 10 09:55:24.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:55:24.823: INFO: namespace pods-6331 deletion completed in 40.800430119s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:55:24.824: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3378
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 10 09:55:25.118: INFO: Waiting up to 5m0s for pod "pod-c33758f6-5b76-11e9-bf19-c208aa3c4721" in namespace "emptydir-3378" to be "success or failure"
Apr 10 09:55:25.138: INFO: Pod "pod-c33758f6-5b76-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.946436ms
Apr 10 09:55:27.159: INFO: Pod "pod-c33758f6-5b76-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040521375s
Apr 10 09:55:29.181: INFO: Pod "pod-c33758f6-5b76-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06253007s
STEP: Saw pod success
Apr 10 09:55:29.181: INFO: Pod "pod-c33758f6-5b76-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 09:55:29.201: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-c33758f6-5b76-11e9-bf19-c208aa3c4721 container test-container: <nil>
STEP: delete the pod
Apr 10 09:55:29.250: INFO: Waiting for pod pod-c33758f6-5b76-11e9-bf19-c208aa3c4721 to disappear
Apr 10 09:55:29.270: INFO: Pod pod-c33758f6-5b76-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:55:29.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3378" for this suite.
Apr 10 09:55:35.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:55:36.092: INFO: namespace emptydir-3378 deletion completed in 6.802126955s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:55:36.093: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5131
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Apr 10 09:55:36.344: INFO: Waiting up to 5m0s for pod "client-containers-c9e7e4ec-5b76-11e9-bf19-c208aa3c4721" in namespace "containers-5131" to be "success or failure"
Apr 10 09:55:36.364: INFO: Pod "client-containers-c9e7e4ec-5b76-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.649407ms
Apr 10 09:55:38.384: INFO: Pod "client-containers-c9e7e4ec-5b76-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040434318s
Apr 10 09:55:40.405: INFO: Pod "client-containers-c9e7e4ec-5b76-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060694413s
STEP: Saw pod success
Apr 10 09:55:40.405: INFO: Pod "client-containers-c9e7e4ec-5b76-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 09:55:40.425: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod client-containers-c9e7e4ec-5b76-11e9-bf19-c208aa3c4721 container test-container: <nil>
STEP: delete the pod
Apr 10 09:55:40.474: INFO: Waiting for pod client-containers-c9e7e4ec-5b76-11e9-bf19-c208aa3c4721 to disappear
Apr 10 09:55:40.497: INFO: Pod client-containers-c9e7e4ec-5b76-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:55:40.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5131" for this suite.
Apr 10 09:55:46.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:55:47.310: INFO: namespace containers-5131 deletion completed in 6.792920374s
•SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:55:47.310: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6764
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr 10 09:55:47.595: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:55:51.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6764" for this suite.
Apr 10 09:56:13.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:56:14.073: INFO: namespace init-container-6764 deletion completed in 22.796356498s
•SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:56:14.073: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6390
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 10 09:56:18.509: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 09:56:18.530: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 09:56:20.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 09:56:20.551: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 09:56:22.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 09:56:22.554: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 09:56:24.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 09:56:24.550: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 09:56:26.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 09:56:26.550: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 09:56:28.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 09:56:28.551: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 09:56:30.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 09:56:30.550: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 09:56:32.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 09:56:32.551: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 09:56:34.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 09:56:34.551: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 09:56:36.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 09:56:36.550: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 09:56:38.530: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 09:56:38.551: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:56:38.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6390" for this suite.
Apr 10 09:57:00.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:57:01.372: INFO: namespace container-lifecycle-hook-6390 deletion completed in 22.799940903s
•SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:57:01.372: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-996
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Apr 10 09:57:01.621: INFO: Waiting up to 5m0s for pod "client-containers-fcbc6400-5b76-11e9-bf19-c208aa3c4721" in namespace "containers-996" to be "success or failure"
Apr 10 09:57:01.642: INFO: Pod "client-containers-fcbc6400-5b76-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.217417ms
Apr 10 09:57:03.662: INFO: Pod "client-containers-fcbc6400-5b76-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.04111423s
STEP: Saw pod success
Apr 10 09:57:03.662: INFO: Pod "client-containers-fcbc6400-5b76-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 09:57:03.683: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod client-containers-fcbc6400-5b76-11e9-bf19-c208aa3c4721 container test-container: <nil>
STEP: delete the pod
Apr 10 09:57:03.735: INFO: Waiting for pod client-containers-fcbc6400-5b76-11e9-bf19-c208aa3c4721 to disappear
Apr 10 09:57:03.755: INFO: Pod client-containers-fcbc6400-5b76-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:57:03.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-996" for this suite.
Apr 10 09:57:09.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:57:10.572: INFO: namespace containers-996 deletion completed in 6.796694856s
•SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:57:10.572: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9659
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 10 09:57:10.821: INFO: Waiting up to 5m0s for pod "pod-02385a04-5b77-11e9-bf19-c208aa3c4721" in namespace "emptydir-9659" to be "success or failure"
Apr 10 09:57:10.842: INFO: Pod "pod-02385a04-5b77-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.650213ms
Apr 10 09:57:12.863: INFO: Pod "pod-02385a04-5b77-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041349461s
STEP: Saw pod success
Apr 10 09:57:12.863: INFO: Pod "pod-02385a04-5b77-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 09:57:12.887: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-02385a04-5b77-11e9-bf19-c208aa3c4721 container test-container: <nil>
STEP: delete the pod
Apr 10 09:57:12.936: INFO: Waiting for pod pod-02385a04-5b77-11e9-bf19-c208aa3c4721 to disappear
Apr 10 09:57:12.956: INFO: Pod pod-02385a04-5b77-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:57:12.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9659" for this suite.
Apr 10 09:57:19.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:57:19.803: INFO: namespace emptydir-9659 deletion completed in 6.825887695s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:57:19.803: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9692
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 09:57:20.097: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config version'
Apr 10 09:57:20.312: INFO: stderr: ""
Apr 10 09:57:20.312: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"archive\", BuildDate:\"2019-04-10T09:26:54Z\", GoVersion:\"go1.12.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:45:25Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:57:20.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9692" for this suite.
Apr 10 09:57:26.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:57:27.195: INFO: namespace kubectl-9692 deletion completed in 6.861272485s
•SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:57:27.195: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8706
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-1022
STEP: Creating secret with name secret-test-0c2be6ec-5b77-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume secrets
Apr 10 09:57:27.846: INFO: Waiting up to 5m0s for pod "pod-secrets-0c5e176b-5b77-11e9-bf19-c208aa3c4721" in namespace "secrets-8706" to be "success or failure"
Apr 10 09:57:27.869: INFO: Pod "pod-secrets-0c5e176b-5b77-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 22.973712ms
Apr 10 09:57:29.893: INFO: Pod "pod-secrets-0c5e176b-5b77-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.04670579s
STEP: Saw pod success
Apr 10 09:57:29.893: INFO: Pod "pod-secrets-0c5e176b-5b77-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 09:57:29.913: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-secrets-0c5e176b-5b77-11e9-bf19-c208aa3c4721 container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 09:57:29.965: INFO: Waiting for pod pod-secrets-0c5e176b-5b77-11e9-bf19-c208aa3c4721 to disappear
Apr 10 09:57:29.985: INFO: Pod pod-secrets-0c5e176b-5b77-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:57:29.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8706" for this suite.
Apr 10 09:57:36.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:57:36.815: INFO: namespace secrets-8706 deletion completed in 6.799773986s
STEP: Destroying namespace "secret-namespace-1022" for this suite.
Apr 10 09:57:42.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:57:43.607: INFO: namespace secret-namespace-1022 deletion completed in 6.79268654s
•SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:57:43.608: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9346
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 09:57:43.896: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr 10 09:57:43.939: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 10 09:57:45.980: INFO: Creating deployment "test-rolling-update-deployment"
Apr 10 09:57:46.001: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr 10 09:57:46.041: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr 10 09:57:46.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690487066, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690487066, loc:(*time.Location)(0x87c9f80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690487066, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690487066, loc:(*time.Location)(0x87c9f80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 09:57:48.081: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 10 09:57:48.144: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-9346,SelfLink:/apis/apps/v1/namespaces/deployment-9346/deployments/test-rolling-update-deployment,UID:1732a87a-5b77-11e9-a517-0202de25e2de,ResourceVersion:9388,Generation:1,CreationTimestamp:2019-04-10 09:57:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-04-10 09:57:46 +0000 UTC 2019-04-10 09:57:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-04-10 09:57:47 +0000 UTC 2019-04-10 09:57:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Apr 10 09:57:48.165: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-9346,SelfLink:/apis/apps/v1/namespaces/deployment-9346/replicasets/test-rolling-update-deployment-67599b4d9,UID:1733aa84-5b77-11e9-a517-0202de25e2de,ResourceVersion:9381,Generation:1,CreationTimestamp:2019-04-10 09:57:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 1732a87a-5b77-11e9-a517-0202de25e2de 0xc00386a920 0xc00386a921}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr 10 09:57:48.165: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr 10 09:57:48.165: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-9346,SelfLink:/apis/apps/v1/namespaces/deployment-9346/replicasets/test-rolling-update-controller,UID:15f4943a-5b77-11e9-a517-0202de25e2de,ResourceVersion:9387,Generation:2,CreationTimestamp:2019-04-10 09:57:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 1732a87a-5b77-11e9-a517-0202de25e2de 0xc00386a84f 0xc00386a860}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 09:57:48.186: INFO: Pod "test-rolling-update-deployment-67599b4d9-gwhrk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-gwhrk,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-9346,SelfLink:/api/v1/namespaces/deployment-9346/pods/test-rolling-update-deployment-67599b4d9-gwhrk,UID:1734065d-5b77-11e9-a517-0202de25e2de,ResourceVersion:9380,Generation:0,CreationTimestamp:2019-04-10 09:57:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.70/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 1733aa84-5b77-11e9-a517-0202de25e2de 0xc00386b190 0xc00386b191}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qwk78 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qwk78,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-qwk78 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00386b1f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00386b210}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 09:57:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 09:57:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 09:57:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 09:57:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.152,PodIP:100.96.0.70,StartTime:2019-04-10 09:57:46 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-04-10 09:57:46 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://6d3c5aef95c457fb06d126f3314b3de87d029b67ef31888dce8a03cd0f31c4eb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:57:48.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9346" for this suite.
Apr 10 09:57:54.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:57:55.011: INFO: namespace deployment-9346 deletion completed in 6.804361585s
•SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:57:55.011: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1428
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-94dz
STEP: Creating a pod to test atomic-volume-subpath
Apr 10 09:57:55.357: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-94dz" in namespace "subpath-1428" to be "success or failure"
Apr 10 09:57:55.377: INFO: Pod "pod-subpath-test-secret-94dz": Phase="Pending", Reason="", readiness=false. Elapsed: 19.650941ms
Apr 10 09:57:57.399: INFO: Pod "pod-subpath-test-secret-94dz": Phase="Running", Reason="", readiness=true. Elapsed: 2.041541424s
Apr 10 09:57:59.419: INFO: Pod "pod-subpath-test-secret-94dz": Phase="Running", Reason="", readiness=true. Elapsed: 4.062123207s
Apr 10 09:58:01.440: INFO: Pod "pod-subpath-test-secret-94dz": Phase="Running", Reason="", readiness=true. Elapsed: 6.082961015s
Apr 10 09:58:03.461: INFO: Pod "pod-subpath-test-secret-94dz": Phase="Running", Reason="", readiness=true. Elapsed: 8.103682186s
Apr 10 09:58:05.482: INFO: Pod "pod-subpath-test-secret-94dz": Phase="Running", Reason="", readiness=true. Elapsed: 10.124615616s
Apr 10 09:58:07.502: INFO: Pod "pod-subpath-test-secret-94dz": Phase="Running", Reason="", readiness=true. Elapsed: 12.145106513s
Apr 10 09:58:09.523: INFO: Pod "pod-subpath-test-secret-94dz": Phase="Running", Reason="", readiness=true. Elapsed: 14.165679584s
Apr 10 09:58:11.543: INFO: Pod "pod-subpath-test-secret-94dz": Phase="Running", Reason="", readiness=true. Elapsed: 16.1861651s
Apr 10 09:58:13.564: INFO: Pod "pod-subpath-test-secret-94dz": Phase="Running", Reason="", readiness=true. Elapsed: 18.20688917s
Apr 10 09:58:15.584: INFO: Pod "pod-subpath-test-secret-94dz": Phase="Running", Reason="", readiness=true. Elapsed: 20.22669377s
Apr 10 09:58:17.604: INFO: Pod "pod-subpath-test-secret-94dz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.246860986s
STEP: Saw pod success
Apr 10 09:58:17.604: INFO: Pod "pod-subpath-test-secret-94dz" satisfied condition "success or failure"
Apr 10 09:58:17.624: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-subpath-test-secret-94dz container test-container-subpath-secret-94dz: <nil>
STEP: delete the pod
Apr 10 09:58:17.671: INFO: Waiting for pod pod-subpath-test-secret-94dz to disappear
Apr 10 09:58:17.690: INFO: Pod pod-subpath-test-secret-94dz no longer exists
STEP: Deleting pod pod-subpath-test-secret-94dz
Apr 10 09:58:17.691: INFO: Deleting pod "pod-subpath-test-secret-94dz" in namespace "subpath-1428"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:58:17.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1428" for this suite.
Apr 10 09:58:23.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:58:24.531: INFO: namespace subpath-1428 deletion completed in 6.80053862s
•SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:58:24.532: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3283
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Apr 10 09:58:24.819: INFO: Waiting up to 5m0s for pod "var-expansion-2e53917e-5b77-11e9-bf19-c208aa3c4721" in namespace "var-expansion-3283" to be "success or failure"
Apr 10 09:58:24.839: INFO: Pod "var-expansion-2e53917e-5b77-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.488333ms
Apr 10 09:58:26.861: INFO: Pod "var-expansion-2e53917e-5b77-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041633975s
STEP: Saw pod success
Apr 10 09:58:26.861: INFO: Pod "var-expansion-2e53917e-5b77-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 09:58:26.881: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod var-expansion-2e53917e-5b77-11e9-bf19-c208aa3c4721 container dapi-container: <nil>
STEP: delete the pod
Apr 10 09:58:26.928: INFO: Waiting for pod var-expansion-2e53917e-5b77-11e9-bf19-c208aa3c4721 to disappear
Apr 10 09:58:26.948: INFO: Pod var-expansion-2e53917e-5b77-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:58:26.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3283" for this suite.
Apr 10 09:58:33.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:58:33.763: INFO: namespace var-expansion-3283 deletion completed in 6.791789796s
•SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:58:33.763: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4194
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Apr 10 09:58:33.981: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config create -f - --namespace=kubectl-4194'
Apr 10 09:58:34.628: INFO: stderr: ""
Apr 10 09:58:34.628: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 10 09:58:34.628: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4194'
Apr 10 09:58:34.828: INFO: stderr: ""
Apr 10 09:58:34.828: INFO: stdout: "update-demo-nautilus-sbsp8 update-demo-nautilus-t57tn "
Apr 10 09:58:34.828: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-sbsp8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4194'
Apr 10 09:58:34.998: INFO: stderr: ""
Apr 10 09:58:34.998: INFO: stdout: ""
Apr 10 09:58:34.998: INFO: update-demo-nautilus-sbsp8 is created but not running
Apr 10 09:58:39.998: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4194'
Apr 10 09:58:40.194: INFO: stderr: ""
Apr 10 09:58:40.194: INFO: stdout: "update-demo-nautilus-sbsp8 update-demo-nautilus-t57tn "
Apr 10 09:58:40.194: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-sbsp8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4194'
Apr 10 09:58:40.419: INFO: stderr: ""
Apr 10 09:58:40.419: INFO: stdout: "true"
Apr 10 09:58:40.419: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-sbsp8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4194'
Apr 10 09:58:40.600: INFO: stderr: ""
Apr 10 09:58:40.600: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 09:58:40.600: INFO: validating pod update-demo-nautilus-sbsp8
Apr 10 09:58:40.709: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 09:58:40.709: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 09:58:40.709: INFO: update-demo-nautilus-sbsp8 is verified up and running
Apr 10 09:58:40.709: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-t57tn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4194'
Apr 10 09:58:40.900: INFO: stderr: ""
Apr 10 09:58:40.900: INFO: stdout: "true"
Apr 10 09:58:40.900: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-t57tn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4194'
Apr 10 09:58:41.096: INFO: stderr: ""
Apr 10 09:58:41.096: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 09:58:41.096: INFO: validating pod update-demo-nautilus-t57tn
Apr 10 09:58:41.208: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 09:58:41.208: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 09:58:41.208: INFO: update-demo-nautilus-t57tn is verified up and running
STEP: scaling down the replication controller
Apr 10 09:58:41.210: INFO: scanned /root for discovery docs: <nil>
Apr 10 09:58:41.210: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-4194'
Apr 10 09:58:41.488: INFO: stderr: ""
Apr 10 09:58:41.488: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 10 09:58:41.488: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4194'
Apr 10 09:58:41.692: INFO: stderr: ""
Apr 10 09:58:41.692: INFO: stdout: "update-demo-nautilus-sbsp8 update-demo-nautilus-t57tn "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 10 09:58:46.695: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4194'
Apr 10 09:58:46.898: INFO: stderr: ""
Apr 10 09:58:46.898: INFO: stdout: "update-demo-nautilus-sbsp8 update-demo-nautilus-t57tn "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 10 09:58:51.899: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4194'
Apr 10 09:58:52.100: INFO: stderr: ""
Apr 10 09:58:52.100: INFO: stdout: "update-demo-nautilus-sbsp8 update-demo-nautilus-t57tn "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 10 09:58:57.101: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4194'
Apr 10 09:58:57.318: INFO: stderr: ""
Apr 10 09:58:57.318: INFO: stdout: "update-demo-nautilus-t57tn "
Apr 10 09:58:57.318: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-t57tn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4194'
Apr 10 09:58:57.567: INFO: stderr: ""
Apr 10 09:58:57.567: INFO: stdout: "true"
Apr 10 09:58:57.567: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-t57tn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4194'
Apr 10 09:58:57.800: INFO: stderr: ""
Apr 10 09:58:57.800: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 09:58:57.800: INFO: validating pod update-demo-nautilus-t57tn
Apr 10 09:58:57.826: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 09:58:57.826: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 09:58:57.826: INFO: update-demo-nautilus-t57tn is verified up and running
STEP: scaling up the replication controller
Apr 10 09:58:57.828: INFO: scanned /root for discovery docs: <nil>
Apr 10 09:58:57.828: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-4194'
Apr 10 09:58:58.086: INFO: stderr: ""
Apr 10 09:58:58.086: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 10 09:58:58.086: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4194'
Apr 10 09:58:58.298: INFO: stderr: ""
Apr 10 09:58:58.298: INFO: stdout: "update-demo-nautilus-bm29l update-demo-nautilus-t57tn "
Apr 10 09:58:58.298: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-bm29l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4194'
Apr 10 09:58:58.498: INFO: stderr: ""
Apr 10 09:58:58.498: INFO: stdout: ""
Apr 10 09:58:58.498: INFO: update-demo-nautilus-bm29l is created but not running
Apr 10 09:59:03.499: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4194'
Apr 10 09:59:03.778: INFO: stderr: ""
Apr 10 09:59:03.778: INFO: stdout: "update-demo-nautilus-bm29l update-demo-nautilus-t57tn "
Apr 10 09:59:03.778: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-bm29l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4194'
Apr 10 09:59:04.078: INFO: stderr: ""
Apr 10 09:59:04.078: INFO: stdout: "true"
Apr 10 09:59:04.078: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-bm29l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4194'
Apr 10 09:59:04.306: INFO: stderr: ""
Apr 10 09:59:04.306: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 09:59:04.306: INFO: validating pod update-demo-nautilus-bm29l
Apr 10 09:59:04.416: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 09:59:04.416: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 09:59:04.416: INFO: update-demo-nautilus-bm29l is verified up and running
Apr 10 09:59:04.416: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-t57tn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4194'
Apr 10 09:59:04.603: INFO: stderr: ""
Apr 10 09:59:04.604: INFO: stdout: "true"
Apr 10 09:59:04.604: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-t57tn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4194'
Apr 10 09:59:04.807: INFO: stderr: ""
Apr 10 09:59:04.807: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 09:59:04.807: INFO: validating pod update-demo-nautilus-t57tn
Apr 10 09:59:04.829: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 09:59:04.829: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 09:59:04.829: INFO: update-demo-nautilus-t57tn is verified up and running
STEP: using delete to clean up resources
Apr 10 09:59:04.830: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-4194'
Apr 10 09:59:05.028: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 09:59:05.028: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 10 09:59:05.028: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4194'
Apr 10 09:59:05.296: INFO: stderr: "No resources found.\n"
Apr 10 09:59:05.296: INFO: stdout: ""
Apr 10 09:59:05.296: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-4194 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 10 09:59:05.511: INFO: stderr: ""
Apr 10 09:59:05.511: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 09:59:05.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4194" for this suite.
Apr 10 09:59:27.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 09:59:28.346: INFO: namespace kubectl-4194 deletion completed in 22.814496563s
•SSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 09:59:28.346: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-694
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-694
Apr 10 09:59:32.665: INFO: Started pod liveness-http in namespace container-probe-694
STEP: checking the pod's current state and verifying that restartCount is present
Apr 10 09:59:32.689: INFO: Initial restart count of pod liveness-http is 0
Apr 10 09:59:52.931: INFO: Restart count of pod container-probe-694/liveness-http is now 1 (20.241450297s elapsed)
Apr 10 10:00:13.153: INFO: Restart count of pod container-probe-694/liveness-http is now 2 (40.463539575s elapsed)
Apr 10 10:00:33.374: INFO: Restart count of pod container-probe-694/liveness-http is now 3 (1m0.684749559s elapsed)
Apr 10 10:00:53.594: INFO: Restart count of pod container-probe-694/liveness-http is now 4 (1m20.904272406s elapsed)
Apr 10 10:01:54.238: INFO: Restart count of pod container-probe-694/liveness-http is now 5 (2m21.548902348s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:01:54.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-694" for this suite.
Apr 10 10:02:00.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:02:01.149: INFO: namespace container-probe-694 deletion completed in 6.866348763s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:02:01.150: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4790
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-af72b9db-5b77-11e9-bf19-c208aa3c4721
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-af72b9db-5b77-11e9-bf19-c208aa3c4721
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:02:07.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4790" for this suite.
Apr 10 10:02:29.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:02:30.543: INFO: namespace configmap-4790 deletion completed in 22.839047843s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:02:30.544: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4583
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 10 10:02:30.846: INFO: Waiting up to 5m0s for pod "downward-api-c0f83c3e-5b77-11e9-bf19-c208aa3c4721" in namespace "downward-api-4583" to be "success or failure"
Apr 10 10:02:30.866: INFO: Pod "downward-api-c0f83c3e-5b77-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.477043ms
Apr 10 10:02:32.888: INFO: Pod "downward-api-c0f83c3e-5b77-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041741922s
STEP: Saw pod success
Apr 10 10:02:32.888: INFO: Pod "downward-api-c0f83c3e-5b77-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:02:32.909: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downward-api-c0f83c3e-5b77-11e9-bf19-c208aa3c4721 container dapi-container: <nil>
STEP: delete the pod
Apr 10 10:02:32.959: INFO: Waiting for pod downward-api-c0f83c3e-5b77-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:02:32.978: INFO: Pod downward-api-c0f83c3e-5b77-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:02:32.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4583" for this suite.
Apr 10 10:02:39.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:02:39.913: INFO: namespace downward-api-4583 deletion completed in 6.913810433s
•SSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:02:39.914: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4164
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr 10 10:02:42.813: INFO: Successfully updated pod "labelsupdatec687e8f8-5b77-11e9-bf19-c208aa3c4721"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:02:44.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4164" for this suite.
Apr 10 10:03:06.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:03:07.726: INFO: namespace downward-api-4164 deletion completed in 22.841061313s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:03:07.727: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7458
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Apr 10 10:03:07.978: INFO: Waiting up to 5m0s for pod "var-expansion-d719bdf3-5b77-11e9-bf19-c208aa3c4721" in namespace "var-expansion-7458" to be "success or failure"
Apr 10 10:03:07.997: INFO: Pod "var-expansion-d719bdf3-5b77-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.781188ms
Apr 10 10:03:10.018: INFO: Pod "var-expansion-d719bdf3-5b77-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040780386s
STEP: Saw pod success
Apr 10 10:03:10.019: INFO: Pod "var-expansion-d719bdf3-5b77-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:03:10.039: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod var-expansion-d719bdf3-5b77-11e9-bf19-c208aa3c4721 container dapi-container: <nil>
STEP: delete the pod
Apr 10 10:03:10.091: INFO: Waiting for pod var-expansion-d719bdf3-5b77-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:03:10.111: INFO: Pod var-expansion-d719bdf3-5b77-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:03:10.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7458" for this suite.
Apr 10 10:03:16.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:03:16.966: INFO: namespace var-expansion-7458 deletion completed in 6.833639236s
•S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:03:16.966: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1551
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Apr 10 10:03:17.216: INFO: Waiting up to 5m0s for pod "client-containers-dc9bd762-5b77-11e9-bf19-c208aa3c4721" in namespace "containers-1551" to be "success or failure"
Apr 10 10:03:17.236: INFO: Pod "client-containers-dc9bd762-5b77-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.647194ms
Apr 10 10:03:19.256: INFO: Pod "client-containers-dc9bd762-5b77-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039886303s
STEP: Saw pod success
Apr 10 10:03:19.256: INFO: Pod "client-containers-dc9bd762-5b77-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:03:19.279: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod client-containers-dc9bd762-5b77-11e9-bf19-c208aa3c4721 container test-container: <nil>
STEP: delete the pod
Apr 10 10:03:19.376: INFO: Waiting for pod client-containers-dc9bd762-5b77-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:03:19.396: INFO: Pod client-containers-dc9bd762-5b77-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:03:19.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1551" for this suite.
Apr 10 10:03:25.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:03:26.244: INFO: namespace containers-1551 deletion completed in 6.825784814s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:03:26.244: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4007
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 10:03:26.501: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-4007'
Apr 10 10:03:26.749: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 10 10:03:26.749: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Apr 10 10:03:28.797: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-4007'
Apr 10 10:03:29.049: INFO: stderr: ""
Apr 10 10:03:29.049: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:03:29.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4007" for this suite.
Apr 10 10:03:47.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:03:47.920: INFO: namespace kubectl-4007 deletion completed in 18.847535025s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:03:47.920: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3753
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-3753
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3753 to expose endpoints map[]
Apr 10 10:03:48.244: INFO: successfully validated that service endpoint-test2 in namespace services-3753 exposes endpoints map[] (22.161227ms elapsed)
STEP: Creating pod pod1 in namespace services-3753
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3753 to expose endpoints map[pod1:[80]]
Apr 10 10:03:50.397: INFO: successfully validated that service endpoint-test2 in namespace services-3753 exposes endpoints map[pod1:[80]] (2.130717679s elapsed)
STEP: Creating pod pod2 in namespace services-3753
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3753 to expose endpoints map[pod1:[80] pod2:[80]]
Apr 10 10:03:51.548: INFO: successfully validated that service endpoint-test2 in namespace services-3753 exposes endpoints map[pod1:[80] pod2:[80]] (1.128931144s elapsed)
STEP: Deleting pod pod1 in namespace services-3753
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3753 to expose endpoints map[pod2:[80]]
Apr 10 10:03:51.612: INFO: successfully validated that service endpoint-test2 in namespace services-3753 exposes endpoints map[pod2:[80]] (41.371501ms elapsed)
STEP: Deleting pod pod2 in namespace services-3753
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3753 to expose endpoints map[]
Apr 10 10:03:51.654: INFO: successfully validated that service endpoint-test2 in namespace services-3753 exposes endpoints map[] (19.640739ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:03:51.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3753" for this suite.
Apr 10 10:04:13.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:04:14.547: INFO: namespace services-3753 deletion completed in 22.844377989s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
•SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:04:14.547: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3584
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:04:16.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3584" for this suite.
Apr 10 10:04:54.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:04:55.771: INFO: namespace kubelet-test-3584 deletion completed in 38.840335924s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:04:55.772: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9340
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Apr 10 10:04:56.042: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:04:56.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9340" for this suite.
Apr 10 10:05:02.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:05:02.996: INFO: namespace replication-controller-9340 deletion completed in 6.861853823s
•SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:05:02.996: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7695
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 10 10:05:07.503: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 10:05:07.524: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 10:05:09.524: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 10:05:09.545: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 10:05:11.524: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 10:05:11.555: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 10:05:13.524: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 10:05:13.546: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 10:05:15.524: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 10:05:15.545: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 10:05:17.524: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 10:05:17.545: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 10:05:19.524: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 10:05:19.545: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 10:05:21.524: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 10:05:21.545: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 10:05:23.524: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 10:05:23.545: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 10:05:25.524: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 10:05:25.546: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 10:05:27.524: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 10:05:27.545: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 10:05:29.524: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 10:05:29.634: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 10:05:31.524: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 10:05:31.547: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 10:05:33.524: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 10:05:33.547: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:05:33.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7695" for this suite.
Apr 10 10:05:55.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:05:56.447: INFO: namespace container-lifecycle-hook-7695 deletion completed in 22.849217668s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:05:56.447: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4633
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Apr 10 10:05:56.702: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config create -f - --namespace=kubectl-4633'
Apr 10 10:05:57.103: INFO: stderr: ""
Apr 10 10:05:57.103: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 10 10:05:57.103: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4633'
Apr 10 10:05:57.316: INFO: stderr: ""
Apr 10 10:05:57.316: INFO: stdout: "update-demo-nautilus-pzn54 update-demo-nautilus-qzkfb "
Apr 10 10:05:57.316: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-pzn54 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4633'
Apr 10 10:05:57.540: INFO: stderr: ""
Apr 10 10:05:57.540: INFO: stdout: ""
Apr 10 10:05:57.540: INFO: update-demo-nautilus-pzn54 is created but not running
Apr 10 10:06:02.541: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4633'
Apr 10 10:06:02.800: INFO: stderr: ""
Apr 10 10:06:02.800: INFO: stdout: "update-demo-nautilus-pzn54 update-demo-nautilus-qzkfb "
Apr 10 10:06:02.800: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-pzn54 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4633'
Apr 10 10:06:03.034: INFO: stderr: ""
Apr 10 10:06:03.034: INFO: stdout: "true"
Apr 10 10:06:03.034: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-pzn54 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4633'
Apr 10 10:06:03.220: INFO: stderr: ""
Apr 10 10:06:03.220: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 10:06:03.220: INFO: validating pod update-demo-nautilus-pzn54
Apr 10 10:06:03.330: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 10:06:03.330: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 10:06:03.330: INFO: update-demo-nautilus-pzn54 is verified up and running
Apr 10 10:06:03.330: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-qzkfb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4633'
Apr 10 10:06:03.591: INFO: stderr: ""
Apr 10 10:06:03.591: INFO: stdout: "true"
Apr 10 10:06:03.591: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-qzkfb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4633'
Apr 10 10:06:03.875: INFO: stderr: ""
Apr 10 10:06:03.875: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 10:06:03.875: INFO: validating pod update-demo-nautilus-qzkfb
Apr 10 10:06:03.983: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 10:06:03.983: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 10:06:03.983: INFO: update-demo-nautilus-qzkfb is verified up and running
STEP: rolling-update to new replication controller
Apr 10 10:06:03.986: INFO: scanned /root for discovery docs: <nil>
Apr 10 10:06:03.986: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-4633'
Apr 10 10:06:20.423: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Apr 10 10:06:20.423: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 10 10:06:20.423: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4633'
Apr 10 10:06:20.627: INFO: stderr: ""
Apr 10 10:06:20.627: INFO: stdout: "update-demo-kitten-bvxvc update-demo-kitten-ttjhl "
Apr 10 10:06:20.627: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-kitten-bvxvc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4633'
Apr 10 10:06:20.839: INFO: stderr: ""
Apr 10 10:06:20.839: INFO: stdout: "true"
Apr 10 10:06:20.839: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-kitten-bvxvc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4633'
Apr 10 10:06:21.021: INFO: stderr: ""
Apr 10 10:06:21.021: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Apr 10 10:06:21.021: INFO: validating pod update-demo-kitten-bvxvc
Apr 10 10:06:21.130: INFO: got data: {
  "image": "kitten.jpg"
}

Apr 10 10:06:21.130: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Apr 10 10:06:21.130: INFO: update-demo-kitten-bvxvc is verified up and running
Apr 10 10:06:21.130: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-kitten-ttjhl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4633'
Apr 10 10:06:21.438: INFO: stderr: ""
Apr 10 10:06:21.438: INFO: stdout: "true"
Apr 10 10:06:21.438: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-kitten-ttjhl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4633'
Apr 10 10:06:21.684: INFO: stderr: ""
Apr 10 10:06:21.685: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Apr 10 10:06:21.685: INFO: validating pod update-demo-kitten-ttjhl
Apr 10 10:06:21.796: INFO: got data: {
  "image": "kitten.jpg"
}

Apr 10 10:06:21.796: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Apr 10 10:06:21.796: INFO: update-demo-kitten-ttjhl is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:06:21.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4633" for this suite.
Apr 10 10:06:43.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:06:44.650: INFO: namespace kubectl-4633 deletion completed in 22.833456384s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:06:44.650: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3102
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
W0410 10:06:45.056673    4420 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 10 10:06:45.056: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:06:45.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3102" for this suite.
Apr 10 10:06:51.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:06:51.915: INFO: namespace gc-3102 deletion completed in 6.838763259s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:06:51.916: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7493
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 10:06:52.223: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5cc311a6-5b78-11e9-bf19-c208aa3c4721" in namespace "downward-api-7493" to be "success or failure"
Apr 10 10:06:52.247: INFO: Pod "downwardapi-volume-5cc311a6-5b78-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 24.351778ms
Apr 10 10:06:54.268: INFO: Pod "downwardapi-volume-5cc311a6-5b78-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045348766s
STEP: Saw pod success
Apr 10 10:06:54.268: INFO: Pod "downwardapi-volume-5cc311a6-5b78-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:06:54.289: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downwardapi-volume-5cc311a6-5b78-11e9-bf19-c208aa3c4721 container client-container: <nil>
STEP: delete the pod
Apr 10 10:06:54.339: INFO: Waiting for pod downwardapi-volume-5cc311a6-5b78-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:06:54.360: INFO: Pod downwardapi-volume-5cc311a6-5b78-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:06:54.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7493" for this suite.
Apr 10 10:07:00.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:07:01.235: INFO: namespace downward-api-7493 deletion completed in 6.854867414s
•SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:07:01.236: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-141
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 10 10:07:05.731: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 10 10:07:05.753: INFO: Pod pod-with-poststart-http-hook still exists
Apr 10 10:07:07.753: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 10 10:07:07.774: INFO: Pod pod-with-poststart-http-hook still exists
Apr 10 10:07:09.753: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 10 10:07:09.777: INFO: Pod pod-with-poststart-http-hook still exists
Apr 10 10:07:11.753: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 10 10:07:11.775: INFO: Pod pod-with-poststart-http-hook still exists
Apr 10 10:07:13.753: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 10 10:07:13.773: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:07:13.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-141" for this suite.
Apr 10 10:07:35.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:07:36.621: INFO: namespace container-lifecycle-hook-141 deletion completed in 22.826964436s
•SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:07:36.621: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9141
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 10:07:36.922: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77679fea-5b78-11e9-bf19-c208aa3c4721" in namespace "downward-api-9141" to be "success or failure"
Apr 10 10:07:36.942: INFO: Pod "downwardapi-volume-77679fea-5b78-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.74054ms
Apr 10 10:07:38.963: INFO: Pod "downwardapi-volume-77679fea-5b78-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041191964s
STEP: Saw pod success
Apr 10 10:07:38.963: INFO: Pod "downwardapi-volume-77679fea-5b78-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:07:38.983: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downwardapi-volume-77679fea-5b78-11e9-bf19-c208aa3c4721 container client-container: <nil>
STEP: delete the pod
Apr 10 10:07:39.053: INFO: Waiting for pod downwardapi-volume-77679fea-5b78-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:07:39.073: INFO: Pod downwardapi-volume-77679fea-5b78-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:07:39.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9141" for this suite.
Apr 10 10:07:45.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:07:45.955: INFO: namespace downward-api-9141 deletion completed in 6.861358413s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:07:45.956: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 10 10:07:46.222: INFO: Waiting up to 5m0s for pod "pod-7cf2c945-5b78-11e9-bf19-c208aa3c4721" in namespace "emptydir-5635" to be "success or failure"
Apr 10 10:07:46.243: INFO: Pod "pod-7cf2c945-5b78-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.331762ms
Apr 10 10:07:48.263: INFO: Pod "pod-7cf2c945-5b78-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040843328s
STEP: Saw pod success
Apr 10 10:07:48.263: INFO: Pod "pod-7cf2c945-5b78-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:07:48.283: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-7cf2c945-5b78-11e9-bf19-c208aa3c4721 container test-container: <nil>
STEP: delete the pod
Apr 10 10:07:48.334: INFO: Waiting for pod pod-7cf2c945-5b78-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:07:48.353: INFO: Pod pod-7cf2c945-5b78-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:07:48.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5635" for this suite.
Apr 10 10:07:54.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:07:55.205: INFO: namespace emptydir-5635 deletion completed in 6.83183359s
•SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:07:55.206: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1021
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 10:07:55.513: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:07:55.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1021" for this suite.
Apr 10 10:08:01.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:08:02.545: INFO: namespace custom-resource-definition-1021 deletion completed in 6.846128726s
•SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:08:02.546: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7019
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 10:08:02.898: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-7019'
Apr 10 10:08:03.133: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 10 10:08:03.133: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Apr 10 10:08:03.177: INFO: scanned /root for discovery docs: <nil>
Apr 10 10:08:03.177: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-7019'
Apr 10 10:08:14.517: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Apr 10 10:08:14.517: INFO: stdout: "Created e2e-test-nginx-rc-0eaf1eef480ed227178c75e87d7c4183\nScaling up e2e-test-nginx-rc-0eaf1eef480ed227178c75e87d7c4183 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-0eaf1eef480ed227178c75e87d7c4183 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-0eaf1eef480ed227178c75e87d7c4183 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Apr 10 10:08:14.517: INFO: stdout: "Created e2e-test-nginx-rc-0eaf1eef480ed227178c75e87d7c4183\nScaling up e2e-test-nginx-rc-0eaf1eef480ed227178c75e87d7c4183 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-0eaf1eef480ed227178c75e87d7c4183 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-0eaf1eef480ed227178c75e87d7c4183 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Apr 10 10:08:14.517: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-7019'
Apr 10 10:08:14.709: INFO: stderr: ""
Apr 10 10:08:14.709: INFO: stdout: "e2e-test-nginx-rc-0eaf1eef480ed227178c75e87d7c4183-9brwm "
Apr 10 10:08:14.709: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods e2e-test-nginx-rc-0eaf1eef480ed227178c75e87d7c4183-9brwm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7019'
Apr 10 10:08:14.906: INFO: stderr: ""
Apr 10 10:08:14.906: INFO: stdout: "true"
Apr 10 10:08:14.906: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods e2e-test-nginx-rc-0eaf1eef480ed227178c75e87d7c4183-9brwm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7019'
Apr 10 10:08:15.104: INFO: stderr: ""
Apr 10 10:08:15.104: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Apr 10 10:08:15.104: INFO: e2e-test-nginx-rc-0eaf1eef480ed227178c75e87d7c4183-9brwm is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Apr 10 10:08:15.104: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-7019'
Apr 10 10:08:15.310: INFO: stderr: ""
Apr 10 10:08:15.310: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:08:15.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7019" for this suite.
Apr 10 10:08:37.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:08:38.164: INFO: namespace kubectl-7019 deletion completed in 22.83221777s
•SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:08:38.164: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9878
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 10:08:38.430: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c109bd0-5b78-11e9-bf19-c208aa3c4721" in namespace "projected-9878" to be "success or failure"
Apr 10 10:08:38.453: INFO: Pod "downwardapi-volume-9c109bd0-5b78-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 23.502621ms
Apr 10 10:08:40.476: INFO: Pod "downwardapi-volume-9c109bd0-5b78-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046294138s
STEP: Saw pod success
Apr 10 10:08:40.476: INFO: Pod "downwardapi-volume-9c109bd0-5b78-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:08:40.497: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downwardapi-volume-9c109bd0-5b78-11e9-bf19-c208aa3c4721 container client-container: <nil>
STEP: delete the pod
Apr 10 10:08:40.552: INFO: Waiting for pod downwardapi-volume-9c109bd0-5b78-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:08:40.575: INFO: Pod downwardapi-volume-9c109bd0-5b78-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:08:40.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9878" for this suite.
Apr 10 10:08:46.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:08:47.439: INFO: namespace projected-9878 deletion completed in 6.842623773s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:08:47.440: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-a195ee61-5b78-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume configMaps
Apr 10 10:08:47.711: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a19902ad-5b78-11e9-bf19-c208aa3c4721" in namespace "projected-9119" to be "success or failure"
Apr 10 10:08:47.731: INFO: Pod "pod-projected-configmaps-a19902ad-5b78-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.552805ms
Apr 10 10:08:49.752: INFO: Pod "pod-projected-configmaps-a19902ad-5b78-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041175203s
STEP: Saw pod success
Apr 10 10:08:49.752: INFO: Pod "pod-projected-configmaps-a19902ad-5b78-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:08:49.772: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-projected-configmaps-a19902ad-5b78-11e9-bf19-c208aa3c4721 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 10:08:49.826: INFO: Waiting for pod pod-projected-configmaps-a19902ad-5b78-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:08:49.846: INFO: Pod pod-projected-configmaps-a19902ad-5b78-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:08:49.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9119" for this suite.
Apr 10 10:08:55.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:08:56.717: INFO: namespace projected-9119 deletion completed in 6.849425464s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:08:56.718: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-2469
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2469
I0410 10:08:57.019399    4420 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2469, replica count: 1
I0410 10:08:58.071105    4420 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0410 10:08:59.071293    4420 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 10 10:08:59.196: INFO: Created: latency-svc-b97qd
Apr 10 10:08:59.198: INFO: Got endpoints: latency-svc-b97qd [26.384211ms]
Apr 10 10:08:59.226: INFO: Created: latency-svc-btwqt
Apr 10 10:08:59.230: INFO: Created: latency-svc-f2dw6
Apr 10 10:08:59.234: INFO: Created: latency-svc-bx545
Apr 10 10:08:59.244: INFO: Created: latency-svc-8tttg
Apr 10 10:08:59.248: INFO: Created: latency-svc-8nqph
Apr 10 10:08:59.251: INFO: Created: latency-svc-l98g9
Apr 10 10:08:59.256: INFO: Created: latency-svc-99gzp
Apr 10 10:08:59.260: INFO: Created: latency-svc-d878d
Apr 10 10:08:59.272: INFO: Created: latency-svc-rb482
Apr 10 10:08:59.273: INFO: Created: latency-svc-vlwps
Apr 10 10:08:59.277: INFO: Created: latency-svc-5v6hb
Apr 10 10:08:59.283: INFO: Created: latency-svc-5clbn
Apr 10 10:08:59.286: INFO: Created: latency-svc-ntf6s
Apr 10 10:08:59.289: INFO: Created: latency-svc-bnx9x
Apr 10 10:08:59.293: INFO: Created: latency-svc-gm5rp
Apr 10 10:08:59.335: INFO: Got endpoints: latency-svc-bx545 [136.72101ms]
Apr 10 10:08:59.335: INFO: Got endpoints: latency-svc-f2dw6 [136.891779ms]
Apr 10 10:08:59.336: INFO: Got endpoints: latency-svc-8tttg [137.456623ms]
Apr 10 10:08:59.336: INFO: Got endpoints: latency-svc-8nqph [137.49565ms]
Apr 10 10:08:59.336: INFO: Got endpoints: latency-svc-btwqt [137.695074ms]
Apr 10 10:08:59.336: INFO: Got endpoints: latency-svc-l98g9 [138.04446ms]
Apr 10 10:08:59.336: INFO: Got endpoints: latency-svc-99gzp [138.089242ms]
Apr 10 10:08:59.339: INFO: Got endpoints: latency-svc-rb482 [140.37492ms]
Apr 10 10:08:59.339: INFO: Got endpoints: latency-svc-d878d [140.605352ms]
Apr 10 10:08:59.339: INFO: Got endpoints: latency-svc-5clbn [140.950336ms]
Apr 10 10:08:59.339: INFO: Got endpoints: latency-svc-vlwps [141.011772ms]
Apr 10 10:08:59.339: INFO: Got endpoints: latency-svc-5v6hb [140.95001ms]
Apr 10 10:08:59.348: INFO: Got endpoints: latency-svc-ntf6s [150.166569ms]
Apr 10 10:08:59.349: INFO: Got endpoints: latency-svc-bnx9x [150.631475ms]
Apr 10 10:08:59.350: INFO: Got endpoints: latency-svc-gm5rp [152.004345ms]
Apr 10 10:08:59.360: INFO: Created: latency-svc-ppf55
Apr 10 10:08:59.361: INFO: Got endpoints: latency-svc-ppf55 [25.831748ms]
Apr 10 10:08:59.365: INFO: Created: latency-svc-9xbzn
Apr 10 10:08:59.367: INFO: Got endpoints: latency-svc-9xbzn [31.778257ms]
Apr 10 10:08:59.376: INFO: Created: latency-svc-mwd7g
Apr 10 10:08:59.380: INFO: Got endpoints: latency-svc-mwd7g [44.408479ms]
Apr 10 10:08:59.382: INFO: Created: latency-svc-q9bdm
Apr 10 10:08:59.383: INFO: Got endpoints: latency-svc-q9bdm [47.073785ms]
Apr 10 10:08:59.385: INFO: Created: latency-svc-dvqn4
Apr 10 10:08:59.386: INFO: Got endpoints: latency-svc-dvqn4 [50.457218ms]
Apr 10 10:08:59.393: INFO: Created: latency-svc-fdngh
Apr 10 10:08:59.397: INFO: Created: latency-svc-6x9r4
Apr 10 10:08:59.399: INFO: Created: latency-svc-rml25
Apr 10 10:08:59.404: INFO: Created: latency-svc-b7825
Apr 10 10:08:59.408: INFO: Created: latency-svc-x578l
Apr 10 10:08:59.413: INFO: Created: latency-svc-m6mxf
Apr 10 10:08:59.418: INFO: Created: latency-svc-sndnf
Apr 10 10:08:59.421: INFO: Created: latency-svc-r2gsn
Apr 10 10:08:59.425: INFO: Created: latency-svc-fbhsk
Apr 10 10:08:59.434: INFO: Created: latency-svc-8wcm7
Apr 10 10:08:59.436: INFO: Got endpoints: latency-svc-fdngh [99.26056ms]
Apr 10 10:08:59.436: INFO: Got endpoints: latency-svc-rml25 [96.958592ms]
Apr 10 10:08:59.436: INFO: Got endpoints: latency-svc-6x9r4 [99.490369ms]
Apr 10 10:08:59.436: INFO: Got endpoints: latency-svc-b7825 [87.314865ms]
Apr 10 10:08:59.436: INFO: Got endpoints: latency-svc-x578l [96.55091ms]
Apr 10 10:08:59.436: INFO: Created: latency-svc-92ksl
Apr 10 10:08:59.442: INFO: Got endpoints: latency-svc-sndnf [91.724517ms]
Apr 10 10:08:59.442: INFO: Got endpoints: latency-svc-m6mxf [102.912896ms]
Apr 10 10:08:59.442: INFO: Created: latency-svc-dmbcp
Apr 10 10:08:59.442: INFO: Got endpoints: latency-svc-8wcm7 [103.119892ms]
Apr 10 10:08:59.442: INFO: Got endpoints: latency-svc-fbhsk [103.859709ms]
Apr 10 10:08:59.442: INFO: Got endpoints: latency-svc-r2gsn [93.563249ms]
Apr 10 10:08:59.443: INFO: Got endpoints: latency-svc-92ksl [81.752199ms]
Apr 10 10:08:59.445: INFO: Got endpoints: latency-svc-dmbcp [78.000374ms]
Apr 10 10:08:59.445: INFO: Created: latency-svc-qhz22
Apr 10 10:08:59.450: INFO: Created: latency-svc-b8g5h
Apr 10 10:08:59.458: INFO: Created: latency-svc-bm9l9
Apr 10 10:08:59.462: INFO: Created: latency-svc-tk5xj
Apr 10 10:08:59.495: INFO: Got endpoints: latency-svc-qhz22 [114.543955ms]
Apr 10 10:08:59.495: INFO: Created: latency-svc-74px6
Apr 10 10:08:59.500: INFO: Created: latency-svc-2zrdn
Apr 10 10:08:59.503: INFO: Created: latency-svc-k9nkb
Apr 10 10:08:59.508: INFO: Created: latency-svc-ksdpb
Apr 10 10:08:59.510: INFO: Created: latency-svc-ffhhp
Apr 10 10:08:59.514: INFO: Created: latency-svc-6scq8
Apr 10 10:08:59.517: INFO: Created: latency-svc-pr75f
Apr 10 10:08:59.521: INFO: Created: latency-svc-hszms
Apr 10 10:08:59.525: INFO: Created: latency-svc-kngxl
Apr 10 10:08:59.530: INFO: Created: latency-svc-6hdrz
Apr 10 10:08:59.534: INFO: Created: latency-svc-qkgsl
Apr 10 10:08:59.534: INFO: Got endpoints: latency-svc-b8g5h [151.508422ms]
Apr 10 10:08:59.538: INFO: Created: latency-svc-r9tgw
Apr 10 10:08:59.563: INFO: Created: latency-svc-bf9sf
Apr 10 10:08:59.585: INFO: Got endpoints: latency-svc-bm9l9 [198.702853ms]
Apr 10 10:08:59.611: INFO: Created: latency-svc-ndc8q
Apr 10 10:08:59.634: INFO: Got endpoints: latency-svc-tk5xj [198.467098ms]
Apr 10 10:08:59.660: INFO: Created: latency-svc-vr8pc
Apr 10 10:08:59.686: INFO: Got endpoints: latency-svc-74px6 [249.906606ms]
Apr 10 10:08:59.711: INFO: Created: latency-svc-hmv2k
Apr 10 10:08:59.737: INFO: Got endpoints: latency-svc-2zrdn [301.106498ms]
Apr 10 10:08:59.762: INFO: Created: latency-svc-vzvf7
Apr 10 10:08:59.787: INFO: Got endpoints: latency-svc-k9nkb [350.894142ms]
Apr 10 10:08:59.816: INFO: Created: latency-svc-m478n
Apr 10 10:08:59.834: INFO: Got endpoints: latency-svc-ksdpb [398.43978ms]
Apr 10 10:08:59.861: INFO: Created: latency-svc-fjssf
Apr 10 10:08:59.884: INFO: Got endpoints: latency-svc-ffhhp [441.758975ms]
Apr 10 10:08:59.909: INFO: Created: latency-svc-k8s6d
Apr 10 10:08:59.936: INFO: Got endpoints: latency-svc-6scq8 [493.380778ms]
Apr 10 10:08:59.964: INFO: Created: latency-svc-bt4dc
Apr 10 10:08:59.987: INFO: Got endpoints: latency-svc-pr75f [544.284712ms]
Apr 10 10:09:00.016: INFO: Created: latency-svc-kwnwd
Apr 10 10:09:00.034: INFO: Got endpoints: latency-svc-hszms [591.480717ms]
Apr 10 10:09:00.060: INFO: Created: latency-svc-v9prc
Apr 10 10:09:00.084: INFO: Got endpoints: latency-svc-kngxl [641.881598ms]
Apr 10 10:09:00.114: INFO: Created: latency-svc-rvkxp
Apr 10 10:09:00.134: INFO: Got endpoints: latency-svc-6hdrz [689.280876ms]
Apr 10 10:09:00.159: INFO: Created: latency-svc-675vb
Apr 10 10:09:00.187: INFO: Got endpoints: latency-svc-qkgsl [744.640747ms]
Apr 10 10:09:00.211: INFO: Created: latency-svc-dclqx
Apr 10 10:09:00.234: INFO: Got endpoints: latency-svc-r9tgw [739.785732ms]
Apr 10 10:09:00.262: INFO: Created: latency-svc-dd45b
Apr 10 10:09:00.285: INFO: Got endpoints: latency-svc-bf9sf [750.290738ms]
Apr 10 10:09:00.312: INFO: Created: latency-svc-vjbxs
Apr 10 10:09:00.334: INFO: Got endpoints: latency-svc-ndc8q [749.239708ms]
Apr 10 10:09:00.359: INFO: Created: latency-svc-msdsd
Apr 10 10:09:00.384: INFO: Got endpoints: latency-svc-vr8pc [749.56847ms]
Apr 10 10:09:00.410: INFO: Created: latency-svc-fbz9s
Apr 10 10:09:00.437: INFO: Got endpoints: latency-svc-hmv2k [751.457711ms]
Apr 10 10:09:00.464: INFO: Created: latency-svc-mm4sd
Apr 10 10:09:00.485: INFO: Got endpoints: latency-svc-vzvf7 [748.108658ms]
Apr 10 10:09:00.512: INFO: Created: latency-svc-f2m8w
Apr 10 10:09:00.537: INFO: Got endpoints: latency-svc-m478n [750.07386ms]
Apr 10 10:09:00.561: INFO: Created: latency-svc-wlqvl
Apr 10 10:09:00.585: INFO: Got endpoints: latency-svc-fjssf [750.197393ms]
Apr 10 10:09:00.612: INFO: Created: latency-svc-w6tgz
Apr 10 10:09:00.634: INFO: Got endpoints: latency-svc-k8s6d [749.932865ms]
Apr 10 10:09:00.663: INFO: Created: latency-svc-kncvq
Apr 10 10:09:00.684: INFO: Got endpoints: latency-svc-bt4dc [747.992518ms]
Apr 10 10:09:00.709: INFO: Created: latency-svc-555nj
Apr 10 10:09:00.738: INFO: Got endpoints: latency-svc-kwnwd [751.14442ms]
Apr 10 10:09:00.763: INFO: Created: latency-svc-rjsn2
Apr 10 10:09:00.784: INFO: Got endpoints: latency-svc-v9prc [750.159417ms]
Apr 10 10:09:00.809: INFO: Created: latency-svc-bjhmm
Apr 10 10:09:00.840: INFO: Got endpoints: latency-svc-rvkxp [755.190843ms]
Apr 10 10:09:00.865: INFO: Created: latency-svc-59jv8
Apr 10 10:09:00.884: INFO: Got endpoints: latency-svc-675vb [750.218827ms]
Apr 10 10:09:00.912: INFO: Created: latency-svc-x4prr
Apr 10 10:09:00.936: INFO: Got endpoints: latency-svc-dclqx [748.799896ms]
Apr 10 10:09:00.964: INFO: Created: latency-svc-7tthq
Apr 10 10:09:00.984: INFO: Got endpoints: latency-svc-dd45b [749.630705ms]
Apr 10 10:09:01.014: INFO: Created: latency-svc-fbhhq
Apr 10 10:09:01.034: INFO: Got endpoints: latency-svc-vjbxs [749.633741ms]
Apr 10 10:09:01.066: INFO: Created: latency-svc-rt45g
Apr 10 10:09:01.088: INFO: Got endpoints: latency-svc-msdsd [754.147252ms]
Apr 10 10:09:01.116: INFO: Created: latency-svc-xtq2t
Apr 10 10:09:01.134: INFO: Got endpoints: latency-svc-fbz9s [750.323591ms]
Apr 10 10:09:01.159: INFO: Created: latency-svc-j9jzg
Apr 10 10:09:01.184: INFO: Got endpoints: latency-svc-mm4sd [746.730456ms]
Apr 10 10:09:01.212: INFO: Created: latency-svc-b5l96
Apr 10 10:09:01.238: INFO: Got endpoints: latency-svc-f2m8w [752.954051ms]
Apr 10 10:09:01.267: INFO: Created: latency-svc-r2tfg
Apr 10 10:09:01.286: INFO: Got endpoints: latency-svc-wlqvl [748.643931ms]
Apr 10 10:09:01.312: INFO: Created: latency-svc-hh7cx
Apr 10 10:09:01.334: INFO: Got endpoints: latency-svc-w6tgz [749.395692ms]
Apr 10 10:09:01.364: INFO: Created: latency-svc-6tmks
Apr 10 10:09:01.384: INFO: Got endpoints: latency-svc-kncvq [750.202052ms]
Apr 10 10:09:01.413: INFO: Created: latency-svc-qmnfx
Apr 10 10:09:01.437: INFO: Got endpoints: latency-svc-555nj [752.986002ms]
Apr 10 10:09:01.463: INFO: Created: latency-svc-wczgl
Apr 10 10:09:01.484: INFO: Got endpoints: latency-svc-rjsn2 [746.160643ms]
Apr 10 10:09:01.509: INFO: Created: latency-svc-2zpqt
Apr 10 10:09:01.534: INFO: Got endpoints: latency-svc-bjhmm [749.329931ms]
Apr 10 10:09:01.559: INFO: Created: latency-svc-wccqj
Apr 10 10:09:01.587: INFO: Got endpoints: latency-svc-59jv8 [747.410263ms]
Apr 10 10:09:01.614: INFO: Created: latency-svc-wvhfd
Apr 10 10:09:01.636: INFO: Got endpoints: latency-svc-x4prr [751.003445ms]
Apr 10 10:09:01.663: INFO: Created: latency-svc-cx4rf
Apr 10 10:09:01.685: INFO: Got endpoints: latency-svc-7tthq [749.083894ms]
Apr 10 10:09:01.712: INFO: Created: latency-svc-jsm2g
Apr 10 10:09:01.737: INFO: Got endpoints: latency-svc-fbhhq [753.19857ms]
Apr 10 10:09:01.765: INFO: Created: latency-svc-wn2sm
Apr 10 10:09:01.784: INFO: Got endpoints: latency-svc-rt45g [749.43105ms]
Apr 10 10:09:01.812: INFO: Created: latency-svc-gq5p7
Apr 10 10:09:01.834: INFO: Got endpoints: latency-svc-xtq2t [745.738517ms]
Apr 10 10:09:01.863: INFO: Created: latency-svc-fxhhw
Apr 10 10:09:01.884: INFO: Got endpoints: latency-svc-j9jzg [749.789198ms]
Apr 10 10:09:02.033: INFO: Created: latency-svc-zxsr7
Apr 10 10:09:02.037: INFO: Got endpoints: latency-svc-hh7cx [751.376371ms]
Apr 10 10:09:02.037: INFO: Got endpoints: latency-svc-b5l96 [853.270615ms]
Apr 10 10:09:02.038: INFO: Got endpoints: latency-svc-r2tfg [799.507103ms]
Apr 10 10:09:02.062: INFO: Created: latency-svc-cmhwd
Apr 10 10:09:02.071: INFO: Created: latency-svc-k66cj
Apr 10 10:09:02.071: INFO: Created: latency-svc-rmg5c
Apr 10 10:09:02.084: INFO: Got endpoints: latency-svc-6tmks [749.998589ms]
Apr 10 10:09:02.117: INFO: Created: latency-svc-7sw8c
Apr 10 10:09:02.135: INFO: Got endpoints: latency-svc-qmnfx [750.735569ms]
Apr 10 10:09:02.164: INFO: Created: latency-svc-h6hzt
Apr 10 10:09:02.184: INFO: Got endpoints: latency-svc-wczgl [747.141012ms]
Apr 10 10:09:02.213: INFO: Created: latency-svc-dq2qf
Apr 10 10:09:02.237: INFO: Got endpoints: latency-svc-2zpqt [753.342594ms]
Apr 10 10:09:02.268: INFO: Created: latency-svc-lgtk8
Apr 10 10:09:02.284: INFO: Got endpoints: latency-svc-wccqj [750.147452ms]
Apr 10 10:09:02.311: INFO: Created: latency-svc-6rkbp
Apr 10 10:09:02.335: INFO: Got endpoints: latency-svc-wvhfd [747.452513ms]
Apr 10 10:09:02.360: INFO: Created: latency-svc-4bzdz
Apr 10 10:09:02.384: INFO: Got endpoints: latency-svc-cx4rf [748.616979ms]
Apr 10 10:09:02.413: INFO: Created: latency-svc-txskz
Apr 10 10:09:02.435: INFO: Got endpoints: latency-svc-jsm2g [749.725165ms]
Apr 10 10:09:02.459: INFO: Created: latency-svc-xxs96
Apr 10 10:09:02.484: INFO: Got endpoints: latency-svc-wn2sm [746.679937ms]
Apr 10 10:09:02.509: INFO: Created: latency-svc-d7jx7
Apr 10 10:09:02.537: INFO: Got endpoints: latency-svc-gq5p7 [752.866419ms]
Apr 10 10:09:02.562: INFO: Created: latency-svc-fs29b
Apr 10 10:09:02.584: INFO: Got endpoints: latency-svc-fxhhw [750.062838ms]
Apr 10 10:09:02.611: INFO: Created: latency-svc-2676v
Apr 10 10:09:02.635: INFO: Got endpoints: latency-svc-zxsr7 [750.404559ms]
Apr 10 10:09:02.662: INFO: Created: latency-svc-hgk2f
Apr 10 10:09:02.684: INFO: Got endpoints: latency-svc-cmhwd [646.212329ms]
Apr 10 10:09:02.712: INFO: Created: latency-svc-pclsl
Apr 10 10:09:02.734: INFO: Got endpoints: latency-svc-rmg5c [696.869096ms]
Apr 10 10:09:02.760: INFO: Created: latency-svc-jqwcq
Apr 10 10:09:02.785: INFO: Got endpoints: latency-svc-k66cj [747.513244ms]
Apr 10 10:09:02.814: INFO: Created: latency-svc-ch5lv
Apr 10 10:09:02.834: INFO: Got endpoints: latency-svc-7sw8c [750.011069ms]
Apr 10 10:09:02.862: INFO: Created: latency-svc-mgbqz
Apr 10 10:09:02.887: INFO: Got endpoints: latency-svc-h6hzt [752.047446ms]
Apr 10 10:09:02.913: INFO: Created: latency-svc-xhdlw
Apr 10 10:09:02.934: INFO: Got endpoints: latency-svc-dq2qf [750.212292ms]
Apr 10 10:09:02.964: INFO: Created: latency-svc-2hnn5
Apr 10 10:09:02.984: INFO: Got endpoints: latency-svc-lgtk8 [746.768305ms]
Apr 10 10:09:03.014: INFO: Created: latency-svc-nl8cq
Apr 10 10:09:03.035: INFO: Got endpoints: latency-svc-6rkbp [751.078359ms]
Apr 10 10:09:03.060: INFO: Created: latency-svc-75f69
Apr 10 10:09:03.084: INFO: Got endpoints: latency-svc-4bzdz [749.58305ms]
Apr 10 10:09:03.110: INFO: Created: latency-svc-l5j4l
Apr 10 10:09:03.134: INFO: Got endpoints: latency-svc-txskz [750.005293ms]
Apr 10 10:09:03.161: INFO: Created: latency-svc-557hn
Apr 10 10:09:03.184: INFO: Got endpoints: latency-svc-xxs96 [749.850552ms]
Apr 10 10:09:03.209: INFO: Created: latency-svc-7jg28
Apr 10 10:09:03.237: INFO: Got endpoints: latency-svc-d7jx7 [752.390721ms]
Apr 10 10:09:03.262: INFO: Created: latency-svc-lcxzv
Apr 10 10:09:03.284: INFO: Got endpoints: latency-svc-fs29b [747.127448ms]
Apr 10 10:09:03.308: INFO: Created: latency-svc-nx2w2
Apr 10 10:09:03.338: INFO: Got endpoints: latency-svc-2676v [753.168065ms]
Apr 10 10:09:03.366: INFO: Created: latency-svc-kwnd2
Apr 10 10:09:03.384: INFO: Got endpoints: latency-svc-hgk2f [749.734085ms]
Apr 10 10:09:03.412: INFO: Created: latency-svc-tvzzq
Apr 10 10:09:03.435: INFO: Got endpoints: latency-svc-pclsl [751.072471ms]
Apr 10 10:09:03.461: INFO: Created: latency-svc-2nszh
Apr 10 10:09:03.486: INFO: Got endpoints: latency-svc-jqwcq [751.578396ms]
Apr 10 10:09:03.513: INFO: Created: latency-svc-t6jcp
Apr 10 10:09:03.534: INFO: Got endpoints: latency-svc-ch5lv [749.810344ms]
Apr 10 10:09:03.559: INFO: Created: latency-svc-qzbvt
Apr 10 10:09:03.584: INFO: Got endpoints: latency-svc-mgbqz [749.629765ms]
Apr 10 10:09:03.678: INFO: Created: latency-svc-k6hf7
Apr 10 10:09:03.679: INFO: Got endpoints: latency-svc-xhdlw [791.532885ms]
Apr 10 10:09:03.684: INFO: Got endpoints: latency-svc-2hnn5 [749.377632ms]
Apr 10 10:09:03.719: INFO: Created: latency-svc-qwh7m
Apr 10 10:09:03.719: INFO: Created: latency-svc-69w75
Apr 10 10:09:03.736: INFO: Got endpoints: latency-svc-nl8cq [751.648511ms]
Apr 10 10:09:03.769: INFO: Created: latency-svc-zwnp8
Apr 10 10:09:03.785: INFO: Got endpoints: latency-svc-75f69 [749.35751ms]
Apr 10 10:09:03.810: INFO: Created: latency-svc-nbvsh
Apr 10 10:09:03.834: INFO: Got endpoints: latency-svc-l5j4l [749.814928ms]
Apr 10 10:09:03.862: INFO: Created: latency-svc-jxc5f
Apr 10 10:09:03.890: INFO: Got endpoints: latency-svc-557hn [755.987516ms]
Apr 10 10:09:03.919: INFO: Created: latency-svc-cmd66
Apr 10 10:09:03.934: INFO: Got endpoints: latency-svc-7jg28 [749.731012ms]
Apr 10 10:09:03.959: INFO: Created: latency-svc-vw25z
Apr 10 10:09:03.984: INFO: Got endpoints: latency-svc-lcxzv [747.149326ms]
Apr 10 10:09:04.141: INFO: Got endpoints: latency-svc-kwnd2 [803.278915ms]
Apr 10 10:09:04.141: INFO: Got endpoints: latency-svc-nx2w2 [856.750984ms]
Apr 10 10:09:04.141: INFO: Got endpoints: latency-svc-tvzzq [756.568076ms]
Apr 10 10:09:04.145: INFO: Created: latency-svc-ngblb
Apr 10 10:09:04.245: INFO: Created: latency-svc-46kws
Apr 10 10:09:04.245: INFO: Got endpoints: latency-svc-t6jcp [759.329692ms]
Apr 10 10:09:04.245: INFO: Got endpoints: latency-svc-2nszh [810.638868ms]
Apr 10 10:09:04.249: INFO: Created: latency-svc-tszgk
Apr 10 10:09:04.256: INFO: Created: latency-svc-gnwg7
Apr 10 10:09:04.270: INFO: Created: latency-svc-k8khd
Apr 10 10:09:04.276: INFO: Created: latency-svc-975bs
Apr 10 10:09:04.284: INFO: Got endpoints: latency-svc-k6hf7 [699.968606ms]
Apr 10 10:09:04.310: INFO: Created: latency-svc-7m4nx
Apr 10 10:09:04.339: INFO: Got endpoints: latency-svc-qzbvt [804.556941ms]
Apr 10 10:09:04.365: INFO: Created: latency-svc-hrfj9
Apr 10 10:09:04.385: INFO: Got endpoints: latency-svc-69w75 [706.085137ms]
Apr 10 10:09:04.413: INFO: Created: latency-svc-47cd2
Apr 10 10:09:04.435: INFO: Got endpoints: latency-svc-qwh7m [750.803621ms]
Apr 10 10:09:04.467: INFO: Created: latency-svc-zflwt
Apr 10 10:09:04.484: INFO: Got endpoints: latency-svc-zwnp8 [747.991061ms]
Apr 10 10:09:04.508: INFO: Created: latency-svc-cp2mh
Apr 10 10:09:04.534: INFO: Got endpoints: latency-svc-nbvsh [749.775319ms]
Apr 10 10:09:04.561: INFO: Created: latency-svc-kcbbs
Apr 10 10:09:04.584: INFO: Got endpoints: latency-svc-jxc5f [750.18692ms]
Apr 10 10:09:04.609: INFO: Created: latency-svc-cwsp7
Apr 10 10:09:04.635: INFO: Got endpoints: latency-svc-cmd66 [744.219474ms]
Apr 10 10:09:04.665: INFO: Created: latency-svc-qq9d5
Apr 10 10:09:04.685: INFO: Got endpoints: latency-svc-vw25z [750.703328ms]
Apr 10 10:09:04.709: INFO: Created: latency-svc-rv2rm
Apr 10 10:09:04.734: INFO: Got endpoints: latency-svc-ngblb [750.15626ms]
Apr 10 10:09:04.759: INFO: Created: latency-svc-7dcgx
Apr 10 10:09:04.785: INFO: Got endpoints: latency-svc-46kws [643.787122ms]
Apr 10 10:09:04.810: INFO: Created: latency-svc-xqjtj
Apr 10 10:09:04.834: INFO: Got endpoints: latency-svc-tszgk [692.738106ms]
Apr 10 10:09:04.862: INFO: Created: latency-svc-x6xjr
Apr 10 10:09:04.885: INFO: Got endpoints: latency-svc-gnwg7 [743.642663ms]
Apr 10 10:09:04.910: INFO: Created: latency-svc-8bxx6
Apr 10 10:09:04.934: INFO: Got endpoints: latency-svc-k8khd [688.938239ms]
Apr 10 10:09:04.964: INFO: Created: latency-svc-mpq8x
Apr 10 10:09:04.984: INFO: Got endpoints: latency-svc-975bs [738.647397ms]
Apr 10 10:09:05.010: INFO: Created: latency-svc-5hxtm
Apr 10 10:09:05.034: INFO: Got endpoints: latency-svc-7m4nx [750.330424ms]
Apr 10 10:09:05.059: INFO: Created: latency-svc-z7kkg
Apr 10 10:09:05.085: INFO: Got endpoints: latency-svc-hrfj9 [745.655127ms]
Apr 10 10:09:05.113: INFO: Created: latency-svc-rhrql
Apr 10 10:09:05.137: INFO: Got endpoints: latency-svc-47cd2 [751.772839ms]
Apr 10 10:09:05.161: INFO: Created: latency-svc-phdxg
Apr 10 10:09:05.187: INFO: Got endpoints: latency-svc-zflwt [752.634751ms]
Apr 10 10:09:05.212: INFO: Created: latency-svc-jxgd8
Apr 10 10:09:05.234: INFO: Got endpoints: latency-svc-cp2mh [750.163874ms]
Apr 10 10:09:05.261: INFO: Created: latency-svc-rd2x2
Apr 10 10:09:05.285: INFO: Got endpoints: latency-svc-kcbbs [750.381597ms]
Apr 10 10:09:05.317: INFO: Created: latency-svc-9gsns
Apr 10 10:09:05.334: INFO: Got endpoints: latency-svc-cwsp7 [749.586609ms]
Apr 10 10:09:05.360: INFO: Created: latency-svc-2ckp2
Apr 10 10:09:05.387: INFO: Got endpoints: latency-svc-qq9d5 [752.802041ms]
Apr 10 10:09:05.412: INFO: Created: latency-svc-8p5dc
Apr 10 10:09:05.435: INFO: Got endpoints: latency-svc-rv2rm [749.769541ms]
Apr 10 10:09:05.464: INFO: Created: latency-svc-k4lkn
Apr 10 10:09:05.484: INFO: Got endpoints: latency-svc-7dcgx [749.875969ms]
Apr 10 10:09:05.509: INFO: Created: latency-svc-jv2fj
Apr 10 10:09:05.535: INFO: Got endpoints: latency-svc-xqjtj [750.036264ms]
Apr 10 10:09:05.559: INFO: Created: latency-svc-zpscz
Apr 10 10:09:05.584: INFO: Got endpoints: latency-svc-x6xjr [750.135829ms]
Apr 10 10:09:05.610: INFO: Created: latency-svc-ltgjx
Apr 10 10:09:05.637: INFO: Got endpoints: latency-svc-8bxx6 [751.559047ms]
Apr 10 10:09:05.664: INFO: Created: latency-svc-h6bn2
Apr 10 10:09:05.684: INFO: Got endpoints: latency-svc-mpq8x [749.430492ms]
Apr 10 10:09:05.712: INFO: Created: latency-svc-2jz9q
Apr 10 10:09:05.734: INFO: Got endpoints: latency-svc-5hxtm [749.788947ms]
Apr 10 10:09:05.762: INFO: Created: latency-svc-97ndn
Apr 10 10:09:05.784: INFO: Got endpoints: latency-svc-z7kkg [749.546916ms]
Apr 10 10:09:05.810: INFO: Created: latency-svc-r6sqv
Apr 10 10:09:05.834: INFO: Got endpoints: latency-svc-rhrql [749.37518ms]
Apr 10 10:09:05.859: INFO: Created: latency-svc-vjctg
Apr 10 10:09:05.884: INFO: Got endpoints: latency-svc-phdxg [747.51026ms]
Apr 10 10:09:05.911: INFO: Created: latency-svc-k5x7m
Apr 10 10:09:05.934: INFO: Got endpoints: latency-svc-jxgd8 [746.506046ms]
Apr 10 10:09:05.961: INFO: Created: latency-svc-spf4f
Apr 10 10:09:05.984: INFO: Got endpoints: latency-svc-rd2x2 [749.986512ms]
Apr 10 10:09:06.009: INFO: Created: latency-svc-5nhtd
Apr 10 10:09:06.034: INFO: Got endpoints: latency-svc-9gsns [749.033412ms]
Apr 10 10:09:06.059: INFO: Created: latency-svc-qcw5n
Apr 10 10:09:06.084: INFO: Got endpoints: latency-svc-2ckp2 [749.933532ms]
Apr 10 10:09:06.111: INFO: Created: latency-svc-4r6sb
Apr 10 10:09:06.136: INFO: Got endpoints: latency-svc-8p5dc [748.208944ms]
Apr 10 10:09:06.162: INFO: Created: latency-svc-qjvj7
Apr 10 10:09:06.187: INFO: Got endpoints: latency-svc-k4lkn [751.772101ms]
Apr 10 10:09:06.211: INFO: Created: latency-svc-f5scp
Apr 10 10:09:06.236: INFO: Got endpoints: latency-svc-jv2fj [751.403064ms]
Apr 10 10:09:06.267: INFO: Created: latency-svc-ktnrc
Apr 10 10:09:06.284: INFO: Got endpoints: latency-svc-zpscz [749.316108ms]
Apr 10 10:09:06.311: INFO: Created: latency-svc-5zzdj
Apr 10 10:09:06.339: INFO: Got endpoints: latency-svc-ltgjx [754.787571ms]
Apr 10 10:09:06.364: INFO: Created: latency-svc-ncc89
Apr 10 10:09:06.385: INFO: Got endpoints: latency-svc-h6bn2 [748.172494ms]
Apr 10 10:09:06.409: INFO: Created: latency-svc-64sq6
Apr 10 10:09:06.434: INFO: Got endpoints: latency-svc-2jz9q [750.251219ms]
Apr 10 10:09:06.459: INFO: Created: latency-svc-lc6jv
Apr 10 10:09:06.484: INFO: Got endpoints: latency-svc-97ndn [750.189231ms]
Apr 10 10:09:06.510: INFO: Created: latency-svc-f58ls
Apr 10 10:09:06.535: INFO: Got endpoints: latency-svc-r6sqv [750.760823ms]
Apr 10 10:09:06.564: INFO: Created: latency-svc-b5bhb
Apr 10 10:09:06.584: INFO: Got endpoints: latency-svc-vjctg [749.960521ms]
Apr 10 10:09:06.610: INFO: Created: latency-svc-lm7mh
Apr 10 10:09:06.636: INFO: Got endpoints: latency-svc-k5x7m [751.987474ms]
Apr 10 10:09:06.663: INFO: Created: latency-svc-rxncv
Apr 10 10:09:06.684: INFO: Got endpoints: latency-svc-spf4f [749.777665ms]
Apr 10 10:09:06.710: INFO: Created: latency-svc-b2q7v
Apr 10 10:09:06.737: INFO: Got endpoints: latency-svc-5nhtd [752.518515ms]
Apr 10 10:09:06.766: INFO: Created: latency-svc-5qmvs
Apr 10 10:09:06.784: INFO: Got endpoints: latency-svc-qcw5n [750.207618ms]
Apr 10 10:09:06.812: INFO: Created: latency-svc-5bg88
Apr 10 10:09:06.838: INFO: Got endpoints: latency-svc-4r6sb [753.360825ms]
Apr 10 10:09:06.866: INFO: Created: latency-svc-czd8n
Apr 10 10:09:06.884: INFO: Got endpoints: latency-svc-qjvj7 [748.147697ms]
Apr 10 10:09:06.913: INFO: Created: latency-svc-ttgbk
Apr 10 10:09:06.934: INFO: Got endpoints: latency-svc-f5scp [747.68477ms]
Apr 10 10:09:06.961: INFO: Created: latency-svc-rcf6h
Apr 10 10:09:06.985: INFO: Got endpoints: latency-svc-ktnrc [749.758313ms]
Apr 10 10:09:07.011: INFO: Created: latency-svc-w8gd2
Apr 10 10:09:07.035: INFO: Got endpoints: latency-svc-5zzdj [750.310604ms]
Apr 10 10:09:07.062: INFO: Created: latency-svc-vjsp2
Apr 10 10:09:07.084: INFO: Got endpoints: latency-svc-ncc89 [745.389264ms]
Apr 10 10:09:07.112: INFO: Created: latency-svc-xprsk
Apr 10 10:09:07.134: INFO: Got endpoints: latency-svc-64sq6 [749.11002ms]
Apr 10 10:09:07.186: INFO: Got endpoints: latency-svc-lc6jv [751.742598ms]
Apr 10 10:09:07.235: INFO: Got endpoints: latency-svc-f58ls [750.33667ms]
Apr 10 10:09:07.284: INFO: Got endpoints: latency-svc-b5bhb [748.919534ms]
Apr 10 10:09:07.335: INFO: Got endpoints: latency-svc-lm7mh [750.34071ms]
Apr 10 10:09:07.387: INFO: Got endpoints: latency-svc-rxncv [750.523991ms]
Apr 10 10:09:07.435: INFO: Got endpoints: latency-svc-b2q7v [750.707354ms]
Apr 10 10:09:07.487: INFO: Got endpoints: latency-svc-5qmvs [749.974209ms]
Apr 10 10:09:07.535: INFO: Got endpoints: latency-svc-5bg88 [750.378181ms]
Apr 10 10:09:07.584: INFO: Got endpoints: latency-svc-czd8n [746.673336ms]
Apr 10 10:09:07.634: INFO: Got endpoints: latency-svc-ttgbk [750.197783ms]
Apr 10 10:09:07.686: INFO: Got endpoints: latency-svc-rcf6h [751.240609ms]
Apr 10 10:09:07.734: INFO: Got endpoints: latency-svc-w8gd2 [748.832919ms]
Apr 10 10:09:07.787: INFO: Got endpoints: latency-svc-vjsp2 [752.236338ms]
Apr 10 10:09:07.834: INFO: Got endpoints: latency-svc-xprsk [749.81557ms]
Apr 10 10:09:07.834: INFO: Latencies: [25.831748ms 31.778257ms 44.408479ms 47.073785ms 50.457218ms 78.000374ms 81.752199ms 87.314865ms 91.724517ms 93.563249ms 96.55091ms 96.958592ms 99.26056ms 99.490369ms 102.912896ms 103.119892ms 103.859709ms 114.543955ms 136.72101ms 136.891779ms 137.456623ms 137.49565ms 137.695074ms 138.04446ms 138.089242ms 140.37492ms 140.605352ms 140.95001ms 140.950336ms 141.011772ms 150.166569ms 150.631475ms 151.508422ms 152.004345ms 198.467098ms 198.702853ms 249.906606ms 301.106498ms 350.894142ms 398.43978ms 441.758975ms 493.380778ms 544.284712ms 591.480717ms 641.881598ms 643.787122ms 646.212329ms 688.938239ms 689.280876ms 692.738106ms 696.869096ms 699.968606ms 706.085137ms 738.647397ms 739.785732ms 743.642663ms 744.219474ms 744.640747ms 745.389264ms 745.655127ms 745.738517ms 746.160643ms 746.506046ms 746.673336ms 746.679937ms 746.730456ms 746.768305ms 747.127448ms 747.141012ms 747.149326ms 747.410263ms 747.452513ms 747.51026ms 747.513244ms 747.68477ms 747.991061ms 747.992518ms 748.108658ms 748.147697ms 748.172494ms 748.208944ms 748.616979ms 748.643931ms 748.799896ms 748.832919ms 748.919534ms 749.033412ms 749.083894ms 749.11002ms 749.239708ms 749.316108ms 749.329931ms 749.35751ms 749.37518ms 749.377632ms 749.395692ms 749.430492ms 749.43105ms 749.546916ms 749.56847ms 749.58305ms 749.586609ms 749.629765ms 749.630705ms 749.633741ms 749.725165ms 749.731012ms 749.734085ms 749.758313ms 749.769541ms 749.775319ms 749.777665ms 749.788947ms 749.789198ms 749.810344ms 749.814928ms 749.81557ms 749.850552ms 749.875969ms 749.932865ms 749.933532ms 749.960521ms 749.974209ms 749.986512ms 749.998589ms 750.005293ms 750.011069ms 750.036264ms 750.062838ms 750.07386ms 750.135829ms 750.147452ms 750.15626ms 750.159417ms 750.163874ms 750.18692ms 750.189231ms 750.197393ms 750.197783ms 750.202052ms 750.207618ms 750.212292ms 750.218827ms 750.251219ms 750.290738ms 750.310604ms 750.323591ms 750.330424ms 750.33667ms 750.34071ms 750.378181ms 750.381597ms 750.404559ms 750.523991ms 750.703328ms 750.707354ms 750.735569ms 750.760823ms 750.803621ms 751.003445ms 751.072471ms 751.078359ms 751.14442ms 751.240609ms 751.376371ms 751.403064ms 751.457711ms 751.559047ms 751.578396ms 751.648511ms 751.742598ms 751.772101ms 751.772839ms 751.987474ms 752.047446ms 752.236338ms 752.390721ms 752.518515ms 752.634751ms 752.802041ms 752.866419ms 752.954051ms 752.986002ms 753.168065ms 753.19857ms 753.342594ms 753.360825ms 754.147252ms 754.787571ms 755.190843ms 755.987516ms 756.568076ms 759.329692ms 791.532885ms 799.507103ms 803.278915ms 804.556941ms 810.638868ms 853.270615ms 856.750984ms]
Apr 10 10:09:07.835: INFO: 50 %ile: 749.58305ms
Apr 10 10:09:07.835: INFO: 90 %ile: 752.866419ms
Apr 10 10:09:07.835: INFO: 99 %ile: 853.270615ms
Apr 10 10:09:07.835: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:09:07.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2469" for this suite.
Apr 10 10:09:29.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:09:30.680: INFO: namespace svc-latency-2469 deletion completed in 22.819139123s
•SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:09:30.681: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6385
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
Apr 10 10:09:30.902: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config create -f - --namespace=kubectl-6385'
Apr 10 10:09:31.507: INFO: stderr: ""
Apr 10 10:09:31.507: INFO: stdout: "pod/pause created\n"
Apr 10 10:09:31.507: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr 10 10:09:31.507: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6385" to be "running and ready"
Apr 10 10:09:31.527: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 19.887773ms
Apr 10 10:09:33.548: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.040948014s
Apr 10 10:09:33.548: INFO: Pod "pause" satisfied condition "running and ready"
Apr 10 10:09:33.548: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Apr 10 10:09:33.548: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config label pods pause testing-label=testing-label-value --namespace=kubectl-6385'
Apr 10 10:09:33.809: INFO: stderr: ""
Apr 10 10:09:33.809: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Apr 10 10:09:33.810: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-6385'
Apr 10 10:09:34.017: INFO: stderr: ""
Apr 10 10:09:34.017: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Apr 10 10:09:34.017: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config label pods pause testing-label- --namespace=kubectl-6385'
Apr 10 10:09:34.219: INFO: stderr: ""
Apr 10 10:09:34.219: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Apr 10 10:09:34.219: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-6385'
Apr 10 10:09:34.399: INFO: stderr: ""
Apr 10 10:09:34.399: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
Apr 10 10:09:34.399: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-6385'
Apr 10 10:09:34.624: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 10:09:34.624: INFO: stdout: "pod \"pause\" force deleted\n"
Apr 10 10:09:34.624: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get rc,svc -l name=pause --no-headers --namespace=kubectl-6385'
Apr 10 10:09:34.822: INFO: stderr: "No resources found.\n"
Apr 10 10:09:34.822: INFO: stdout: ""
Apr 10 10:09:34.822: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods -l name=pause --namespace=kubectl-6385 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 10 10:09:35.008: INFO: stderr: ""
Apr 10 10:09:35.008: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:09:35.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6385" for this suite.
Apr 10 10:09:41.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:09:41.865: INFO: namespace kubectl-6385 deletion completed in 6.836953252s
•SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:09:41.865: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3399
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Apr 10 10:09:42.102: INFO: namespace kubectl-3399
Apr 10 10:09:42.102: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config create -f - --namespace=kubectl-3399'
Apr 10 10:09:42.519: INFO: stderr: ""
Apr 10 10:09:42.519: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 10 10:09:43.540: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 10:09:43.540: INFO: Found 0 / 1
Apr 10 10:09:44.540: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 10:09:44.540: INFO: Found 1 / 1
Apr 10 10:09:44.540: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 10 10:09:44.560: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 10:09:44.560: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 10 10:09:44.560: INFO: wait on redis-master startup in kubectl-3399 
Apr 10 10:09:44.560: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config logs redis-master-bqp24 redis-master --namespace=kubectl-3399'
Apr 10 10:09:44.816: INFO: stderr: ""
Apr 10 10:09:44.816: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 10 Apr 10:09:43.288 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 10 Apr 10:09:43.288 # Server started, Redis version 3.2.12\n1:M 10 Apr 10:09:43.288 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 10 Apr 10:09:43.288 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Apr 10 10:09:44.816: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-3399'
Apr 10 10:09:45.038: INFO: stderr: ""
Apr 10 10:09:45.038: INFO: stdout: "service/rm2 exposed\n"
Apr 10 10:09:45.058: INFO: Service rm2 in namespace kubectl-3399 found.
STEP: exposing service
Apr 10 10:09:47.098: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-3399'
Apr 10 10:09:47.333: INFO: stderr: ""
Apr 10 10:09:47.333: INFO: stdout: "service/rm3 exposed\n"
Apr 10 10:09:47.352: INFO: Service rm3 in namespace kubectl-3399 found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:09:49.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3399" for this suite.
Apr 10 10:10:11.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:10:12.254: INFO: namespace kubectl-3399 deletion completed in 22.840994998s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:10:12.254: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4783
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-d4266371-5b78-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume secrets
Apr 10 10:10:12.542: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d429833d-5b78-11e9-bf19-c208aa3c4721" in namespace "projected-4783" to be "success or failure"
Apr 10 10:10:12.563: INFO: Pod "pod-projected-secrets-d429833d-5b78-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.61138ms
Apr 10 10:10:14.584: INFO: Pod "pod-projected-secrets-d429833d-5b78-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041506495s
STEP: Saw pod success
Apr 10 10:10:14.584: INFO: Pod "pod-projected-secrets-d429833d-5b78-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:10:14.606: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-projected-secrets-d429833d-5b78-11e9-bf19-c208aa3c4721 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 10 10:10:14.659: INFO: Waiting for pod pod-projected-secrets-d429833d-5b78-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:10:14.683: INFO: Pod pod-projected-secrets-d429833d-5b78-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:10:14.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4783" for this suite.
Apr 10 10:10:20.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:10:21.547: INFO: namespace projected-4783 deletion completed in 6.84124909s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:10:21.547: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7356
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-d9b1d399-5b78-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume configMaps
Apr 10 10:10:21.849: INFO: Waiting up to 5m0s for pod "pod-configmaps-d9b4e744-5b78-11e9-bf19-c208aa3c4721" in namespace "configmap-7356" to be "success or failure"
Apr 10 10:10:21.870: INFO: Pod "pod-configmaps-d9b4e744-5b78-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.934143ms
Apr 10 10:10:23.891: INFO: Pod "pod-configmaps-d9b4e744-5b78-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041696033s
STEP: Saw pod success
Apr 10 10:10:23.891: INFO: Pod "pod-configmaps-d9b4e744-5b78-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:10:23.911: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-configmaps-d9b4e744-5b78-11e9-bf19-c208aa3c4721 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 10:10:23.962: INFO: Waiting for pod pod-configmaps-d9b4e744-5b78-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:10:23.982: INFO: Pod pod-configmaps-d9b4e744-5b78-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:10:23.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7356" for this suite.
Apr 10 10:10:30.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:10:30.849: INFO: namespace configmap-7356 deletion completed in 6.846575101s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:10:30.850: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5524
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5524.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5524.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5524.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5524.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5524.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5524.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5524.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5524.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5524.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5524.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5524.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5524.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5524.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 104.181.64.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.64.181.104_udp@PTR;check="$$(dig +tcp +noall +answer +search 104.181.64.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.64.181.104_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5524.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5524.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5524.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5524.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5524.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5524.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5524.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5524.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5524.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5524.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5524.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5524.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5524.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 104.181.64.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.64.181.104_udp@PTR;check="$$(dig +tcp +noall +answer +search 104.181.64.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.64.181.104_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 10 10:10:47.471: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5524.svc.cluster.local from pod dns-5524/dns-test-df436cf2-5b78-11e9-bf19-c208aa3c4721: the server could not find the requested resource (get pods dns-test-df436cf2-5b78-11e9-bf19-c208aa3c4721)
Apr 10 10:10:47.492: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5524.svc.cluster.local from pod dns-5524/dns-test-df436cf2-5b78-11e9-bf19-c208aa3c4721: the server could not find the requested resource (get pods dns-test-df436cf2-5b78-11e9-bf19-c208aa3c4721)
Apr 10 10:10:48.028: INFO: Unable to read jessie_udp@dns-test-service.dns-5524.svc.cluster.local from pod dns-5524/dns-test-df436cf2-5b78-11e9-bf19-c208aa3c4721: the server could not find the requested resource (get pods dns-test-df436cf2-5b78-11e9-bf19-c208aa3c4721)
Apr 10 10:10:48.054: INFO: Unable to read jessie_tcp@dns-test-service.dns-5524.svc.cluster.local from pod dns-5524/dns-test-df436cf2-5b78-11e9-bf19-c208aa3c4721: the server could not find the requested resource (get pods dns-test-df436cf2-5b78-11e9-bf19-c208aa3c4721)
Apr 10 10:10:48.076: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5524.svc.cluster.local from pod dns-5524/dns-test-df436cf2-5b78-11e9-bf19-c208aa3c4721: the server could not find the requested resource (get pods dns-test-df436cf2-5b78-11e9-bf19-c208aa3c4721)
Apr 10 10:10:48.097: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5524.svc.cluster.local from pod dns-5524/dns-test-df436cf2-5b78-11e9-bf19-c208aa3c4721: the server could not find the requested resource (get pods dns-test-df436cf2-5b78-11e9-bf19-c208aa3c4721)
Apr 10 10:10:48.585: INFO: Lookups using dns-5524/dns-test-df436cf2-5b78-11e9-bf19-c208aa3c4721 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-5524.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5524.svc.cluster.local jessie_udp@dns-test-service.dns-5524.svc.cluster.local jessie_tcp@dns-test-service.dns-5524.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5524.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5524.svc.cluster.local]

Apr 10 10:10:55.205: INFO: DNS probes using dns-5524/dns-test-df436cf2-5b78-11e9-bf19-c208aa3c4721 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:10:55.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5524" for this suite.
Apr 10 10:11:01.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:11:02.152: INFO: namespace dns-5524 deletion completed in 6.850791577s
•SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:11:02.152: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4167
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Apr 10 10:11:04.675: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:11:04.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4167" for this suite.
Apr 10 10:11:26.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:11:27.615: INFO: namespace replicaset-4167 deletion completed in 22.844077125s
•SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:11:27.615: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-4531/configmap-test-01175351-5b79-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume configMaps
Apr 10 10:11:27.940: INFO: Waiting up to 5m0s for pod "pod-configmaps-011a4b94-5b79-11e9-bf19-c208aa3c4721" in namespace "configmap-4531" to be "success or failure"
Apr 10 10:11:27.961: INFO: Pod "pod-configmaps-011a4b94-5b79-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.73719ms
Apr 10 10:11:29.982: INFO: Pod "pod-configmaps-011a4b94-5b79-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041632294s
STEP: Saw pod success
Apr 10 10:11:29.982: INFO: Pod "pod-configmaps-011a4b94-5b79-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:11:30.003: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-configmaps-011a4b94-5b79-11e9-bf19-c208aa3c4721 container env-test: <nil>
STEP: delete the pod
Apr 10 10:11:30.053: INFO: Waiting for pod pod-configmaps-011a4b94-5b79-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:11:30.072: INFO: Pod pod-configmaps-011a4b94-5b79-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:11:30.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4531" for this suite.
Apr 10 10:11:36.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:11:36.916: INFO: namespace configmap-4531 deletion completed in 6.822617939s
•SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:11:36.917: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6901
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Apr 10 10:11:37.223: INFO: Waiting up to 5m0s for pod "client-containers-06a25271-5b79-11e9-bf19-c208aa3c4721" in namespace "containers-6901" to be "success or failure"
Apr 10 10:11:37.244: INFO: Pod "client-containers-06a25271-5b79-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.743462ms
Apr 10 10:11:39.267: INFO: Pod "client-containers-06a25271-5b79-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043593793s
STEP: Saw pod success
Apr 10 10:11:39.267: INFO: Pod "client-containers-06a25271-5b79-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:11:39.287: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod client-containers-06a25271-5b79-11e9-bf19-c208aa3c4721 container test-container: <nil>
STEP: delete the pod
Apr 10 10:11:39.342: INFO: Waiting for pod client-containers-06a25271-5b79-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:11:39.362: INFO: Pod client-containers-06a25271-5b79-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:11:39.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6901" for this suite.
Apr 10 10:11:45.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:11:46.260: INFO: namespace containers-6901 deletion completed in 6.876187486s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:11:46.260: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3345
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 10 10:11:46.521: INFO: Waiting up to 5m0s for pod "pod-0c2d7061-5b79-11e9-bf19-c208aa3c4721" in namespace "emptydir-3345" to be "success or failure"
Apr 10 10:11:46.544: INFO: Pod "pod-0c2d7061-5b79-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 23.273655ms
Apr 10 10:11:48.565: INFO: Pod "pod-0c2d7061-5b79-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044067086s
STEP: Saw pod success
Apr 10 10:11:48.565: INFO: Pod "pod-0c2d7061-5b79-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:11:48.585: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-0c2d7061-5b79-11e9-bf19-c208aa3c4721 container test-container: <nil>
STEP: delete the pod
Apr 10 10:11:48.644: INFO: Waiting for pod pod-0c2d7061-5b79-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:11:48.664: INFO: Pod pod-0c2d7061-5b79-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:11:48.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3345" for this suite.
Apr 10 10:11:54.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:11:55.516: INFO: namespace emptydir-3345 deletion completed in 6.832138509s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:11:55.517: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-11b86ea8-5b79-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume secrets
Apr 10 10:11:55.840: INFO: Waiting up to 5m0s for pod "pod-secrets-11bb7aa4-5b79-11e9-bf19-c208aa3c4721" in namespace "secrets-3381" to be "success or failure"
Apr 10 10:11:55.863: INFO: Pod "pod-secrets-11bb7aa4-5b79-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 22.919924ms
Apr 10 10:11:57.885: INFO: Pod "pod-secrets-11bb7aa4-5b79-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044969588s
STEP: Saw pod success
Apr 10 10:11:57.885: INFO: Pod "pod-secrets-11bb7aa4-5b79-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:11:57.905: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-secrets-11bb7aa4-5b79-11e9-bf19-c208aa3c4721 container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 10:11:57.955: INFO: Waiting for pod pod-secrets-11bb7aa4-5b79-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:11:57.974: INFO: Pod pod-secrets-11bb7aa4-5b79-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:11:57.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3381" for this suite.
Apr 10 10:12:04.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:12:04.829: INFO: namespace secrets-3381 deletion completed in 6.834852884s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:12:04.830: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-492
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-8tj5
STEP: Creating a pod to test atomic-volume-subpath
Apr 10 10:12:05.124: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8tj5" in namespace "subpath-492" to be "success or failure"
Apr 10 10:12:05.144: INFO: Pod "pod-subpath-test-configmap-8tj5": Phase="Pending", Reason="", readiness=false. Elapsed: 19.540418ms
Apr 10 10:12:07.167: INFO: Pod "pod-subpath-test-configmap-8tj5": Phase="Running", Reason="", readiness=true. Elapsed: 2.042757772s
Apr 10 10:12:09.190: INFO: Pod "pod-subpath-test-configmap-8tj5": Phase="Running", Reason="", readiness=true. Elapsed: 4.065909891s
Apr 10 10:12:11.213: INFO: Pod "pod-subpath-test-configmap-8tj5": Phase="Running", Reason="", readiness=true. Elapsed: 6.088366243s
Apr 10 10:12:13.236: INFO: Pod "pod-subpath-test-configmap-8tj5": Phase="Running", Reason="", readiness=true. Elapsed: 8.111593293s
Apr 10 10:12:15.257: INFO: Pod "pod-subpath-test-configmap-8tj5": Phase="Running", Reason="", readiness=true. Elapsed: 10.132400342s
Apr 10 10:12:17.279: INFO: Pod "pod-subpath-test-configmap-8tj5": Phase="Running", Reason="", readiness=true. Elapsed: 12.155203059s
Apr 10 10:12:19.301: INFO: Pod "pod-subpath-test-configmap-8tj5": Phase="Running", Reason="", readiness=true. Elapsed: 14.176629248s
Apr 10 10:12:21.323: INFO: Pod "pod-subpath-test-configmap-8tj5": Phase="Running", Reason="", readiness=true. Elapsed: 16.198970294s
Apr 10 10:12:23.344: INFO: Pod "pod-subpath-test-configmap-8tj5": Phase="Running", Reason="", readiness=true. Elapsed: 18.219519731s
Apr 10 10:12:25.367: INFO: Pod "pod-subpath-test-configmap-8tj5": Phase="Running", Reason="", readiness=true. Elapsed: 20.243047049s
Apr 10 10:12:27.388: INFO: Pod "pod-subpath-test-configmap-8tj5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.263701697s
STEP: Saw pod success
Apr 10 10:12:27.388: INFO: Pod "pod-subpath-test-configmap-8tj5" satisfied condition "success or failure"
Apr 10 10:12:27.410: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-subpath-test-configmap-8tj5 container test-container-subpath-configmap-8tj5: <nil>
STEP: delete the pod
Apr 10 10:12:27.462: INFO: Waiting for pod pod-subpath-test-configmap-8tj5 to disappear
Apr 10 10:12:27.481: INFO: Pod pod-subpath-test-configmap-8tj5 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8tj5
Apr 10 10:12:27.481: INFO: Deleting pod "pod-subpath-test-configmap-8tj5" in namespace "subpath-492"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:12:27.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-492" for this suite.
Apr 10 10:12:33.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:12:34.344: INFO: namespace subpath-492 deletion completed in 6.816443679s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:12:34.345: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1385
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Apr 10 10:12:35.733: INFO: Pod name wrapped-volume-race-2975a74e-5b79-11e9-bf19-c208aa3c4721: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2975a74e-5b79-11e9-bf19-c208aa3c4721 in namespace emptydir-wrapper-1385, will wait for the garbage collector to delete the pods
Apr 10 10:12:39.959: INFO: Deleting ReplicationController wrapped-volume-race-2975a74e-5b79-11e9-bf19-c208aa3c4721 took: 21.946131ms
Apr 10 10:12:40.360: INFO: Terminating ReplicationController wrapped-volume-race-2975a74e-5b79-11e9-bf19-c208aa3c4721 pods took: 400.397213ms
STEP: Creating RC which spawns configmap-volume pods
Apr 10 10:13:23.567: INFO: Pod name wrapped-volume-race-45f8aec4-5b79-11e9-bf19-c208aa3c4721: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-45f8aec4-5b79-11e9-bf19-c208aa3c4721 in namespace emptydir-wrapper-1385, will wait for the garbage collector to delete the pods
Apr 10 10:13:27.793: INFO: Deleting ReplicationController wrapped-volume-race-45f8aec4-5b79-11e9-bf19-c208aa3c4721 took: 22.243838ms
Apr 10 10:13:28.193: INFO: Terminating ReplicationController wrapped-volume-race-45f8aec4-5b79-11e9-bf19-c208aa3c4721 pods took: 400.368937ms
STEP: Creating RC which spawns configmap-volume pods
Apr 10 10:14:13.641: INFO: Pod name wrapped-volume-race-63cb2ac3-5b79-11e9-bf19-c208aa3c4721: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-63cb2ac3-5b79-11e9-bf19-c208aa3c4721 in namespace emptydir-wrapper-1385, will wait for the garbage collector to delete the pods
Apr 10 10:14:17.943: INFO: Deleting ReplicationController wrapped-volume-race-63cb2ac3-5b79-11e9-bf19-c208aa3c4721 took: 24.998158ms
Apr 10 10:14:18.044: INFO: Terminating ReplicationController wrapped-volume-race-63cb2ac3-5b79-11e9-bf19-c208aa3c4721 pods took: 100.350935ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:15:04.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1385" for this suite.
Apr 10 10:15:10.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:15:11.429: INFO: namespace emptydir-wrapper-1385 deletion completed in 6.840544051s
•SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:15:11.429: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-232
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0410 10:15:22.133422    4420 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 10 10:15:22.133: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:15:22.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-232" for this suite.
Apr 10 10:15:28.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:15:29.180: INFO: namespace gc-232 deletion completed in 7.025750903s
•SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:15:29.180: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Apr 10 10:15:32.040: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl exec --namespace=svcaccounts-7635 pod-service-account-916084c8-5b79-11e9-bf19-c208aa3c4721 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Apr 10 10:15:32.867: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl exec --namespace=svcaccounts-7635 pod-service-account-916084c8-5b79-11e9-bf19-c208aa3c4721 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Apr 10 10:15:33.598: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl exec --namespace=svcaccounts-7635 pod-service-account-916084c8-5b79-11e9-bf19-c208aa3c4721 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:15:34.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7635" for this suite.
Apr 10 10:15:40.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:15:41.179: INFO: namespace svcaccounts-7635 deletion completed in 6.828362022s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:15:41.179: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-6317
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-6317
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6317
STEP: Deleting pre-stop pod
Apr 10 10:15:52.992: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:15:53.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6317" for this suite.
Apr 10 10:16:31.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:16:31.867: INFO: namespace prestop-6317 deletion completed in 38.831223178s
•SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:16:31.868: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2485
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-2485
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 10 10:16:32.098: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 10 10:16:54.468: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.0.137:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2485 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 10:16:54.468: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 10:16:55.034: INFO: Found all expected endpoints: [netserver-0]
Apr 10 10:16:55.055: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.29:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2485 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 10:16:55.055: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 10:16:55.661: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:16:55.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2485" for this suite.
Apr 10 10:17:17.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:17:18.523: INFO: namespace pod-network-test-2485 deletion completed in 22.837557871s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:17:18.524: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6461
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-6461/configmap-test-d23ec3da-5b79-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume configMaps
Apr 10 10:17:18.847: INFO: Waiting up to 5m0s for pod "pod-configmaps-d241f819-5b79-11e9-bf19-c208aa3c4721" in namespace "configmap-6461" to be "success or failure"
Apr 10 10:17:18.867: INFO: Pod "pod-configmaps-d241f819-5b79-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.904941ms
Apr 10 10:17:20.887: INFO: Pod "pod-configmaps-d241f819-5b79-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040227312s
STEP: Saw pod success
Apr 10 10:17:20.887: INFO: Pod "pod-configmaps-d241f819-5b79-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:17:20.911: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-configmaps-d241f819-5b79-11e9-bf19-c208aa3c4721 container env-test: <nil>
STEP: delete the pod
Apr 10 10:17:20.964: INFO: Waiting for pod pod-configmaps-d241f819-5b79-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:17:20.987: INFO: Pod pod-configmaps-d241f819-5b79-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:17:20.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6461" for this suite.
Apr 10 10:17:27.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:17:27.854: INFO: namespace configmap-6461 deletion completed in 6.845016572s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:17:27.854: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6574
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Apr 10 10:17:28.360: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6574,SelfLink:/api/v1/namespaces/watch-6574/configmaps/e2e-watch-test-label-changed,UID:d7e363fd-5b79-11e9-a517-0202de25e2de,ResourceVersion:14609,Generation:0,CreationTimestamp:2019-04-10 10:17:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 10 10:17:28.360: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6574,SelfLink:/api/v1/namespaces/watch-6574/configmaps/e2e-watch-test-label-changed,UID:d7e363fd-5b79-11e9-a517-0202de25e2de,ResourceVersion:14610,Generation:0,CreationTimestamp:2019-04-10 10:17:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Apr 10 10:17:28.360: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6574,SelfLink:/api/v1/namespaces/watch-6574/configmaps/e2e-watch-test-label-changed,UID:d7e363fd-5b79-11e9-a517-0202de25e2de,ResourceVersion:14611,Generation:0,CreationTimestamp:2019-04-10 10:17:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Apr 10 10:17:38.512: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6574,SelfLink:/api/v1/namespaces/watch-6574/configmaps/e2e-watch-test-label-changed,UID:d7e363fd-5b79-11e9-a517-0202de25e2de,ResourceVersion:14634,Generation:0,CreationTimestamp:2019-04-10 10:17:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 10 10:17:38.512: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6574,SelfLink:/api/v1/namespaces/watch-6574/configmaps/e2e-watch-test-label-changed,UID:d7e363fd-5b79-11e9-a517-0202de25e2de,ResourceVersion:14635,Generation:0,CreationTimestamp:2019-04-10 10:17:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Apr 10 10:17:38.512: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6574,SelfLink:/api/v1/namespaces/watch-6574/configmaps/e2e-watch-test-label-changed,UID:d7e363fd-5b79-11e9-a517-0202de25e2de,ResourceVersion:14636,Generation:0,CreationTimestamp:2019-04-10 10:17:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:17:38.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6574" for this suite.
Apr 10 10:17:44.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:17:45.401: INFO: namespace watch-6574 deletion completed in 6.862987113s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:17:45.401: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1139
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 10:17:45.730: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e2483e1a-5b79-11e9-bf19-c208aa3c4721" in namespace "projected-1139" to be "success or failure"
Apr 10 10:17:45.750: INFO: Pod "downwardapi-volume-e2483e1a-5b79-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.065788ms
Apr 10 10:17:47.770: INFO: Pod "downwardapi-volume-e2483e1a-5b79-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040692687s
STEP: Saw pod success
Apr 10 10:17:47.770: INFO: Pod "downwardapi-volume-e2483e1a-5b79-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:17:47.797: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downwardapi-volume-e2483e1a-5b79-11e9-bf19-c208aa3c4721 container client-container: <nil>
STEP: delete the pod
Apr 10 10:17:47.851: INFO: Waiting for pod downwardapi-volume-e2483e1a-5b79-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:17:47.871: INFO: Pod downwardapi-volume-e2483e1a-5b79-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:17:47.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1139" for this suite.
Apr 10 10:17:53.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:17:54.726: INFO: namespace projected-1139 deletion completed in 6.832197314s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:17:54.727: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-272
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 10:17:55.026: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e7d2c0d9-5b79-11e9-bf19-c208aa3c4721" in namespace "projected-272" to be "success or failure"
Apr 10 10:17:55.046: INFO: Pod "downwardapi-volume-e7d2c0d9-5b79-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.327878ms
Apr 10 10:17:57.070: INFO: Pod "downwardapi-volume-e7d2c0d9-5b79-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044912596s
STEP: Saw pod success
Apr 10 10:17:57.071: INFO: Pod "downwardapi-volume-e7d2c0d9-5b79-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:17:57.091: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downwardapi-volume-e7d2c0d9-5b79-11e9-bf19-c208aa3c4721 container client-container: <nil>
STEP: delete the pod
Apr 10 10:17:57.142: INFO: Waiting for pod downwardapi-volume-e7d2c0d9-5b79-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:17:57.163: INFO: Pod downwardapi-volume-e7d2c0d9-5b79-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:17:57.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-272" for this suite.
Apr 10 10:18:03.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:18:04.028: INFO: namespace projected-272 deletion completed in 6.844170427s
•
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:18:04.029: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2048
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-ed5d267c-5b79-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume secrets
Apr 10 10:18:04.343: INFO: Waiting up to 5m0s for pod "pod-secrets-ed608344-5b79-11e9-bf19-c208aa3c4721" in namespace "secrets-2048" to be "success or failure"
Apr 10 10:18:04.364: INFO: Pod "pod-secrets-ed608344-5b79-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 21.103227ms
Apr 10 10:18:06.385: INFO: Pod "pod-secrets-ed608344-5b79-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041990931s
STEP: Saw pod success
Apr 10 10:18:06.385: INFO: Pod "pod-secrets-ed608344-5b79-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:18:06.405: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-secrets-ed608344-5b79-11e9-bf19-c208aa3c4721 container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 10:18:06.455: INFO: Waiting for pod pod-secrets-ed608344-5b79-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:18:06.475: INFO: Pod pod-secrets-ed608344-5b79-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:18:06.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2048" for this suite.
Apr 10 10:18:12.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:18:13.340: INFO: namespace secrets-2048 deletion completed in 6.839886301s
•SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:18:13.341: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7237
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Apr 10 10:18:13.749: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7237,SelfLink:/api/v1/namespaces/watch-7237/configmaps/e2e-watch-test-resource-version,UID:f2e9d33b-5b79-11e9-a517-0202de25e2de,ResourceVersion:14780,Generation:0,CreationTimestamp:2019-04-10 10:18:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 10 10:18:13.750: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7237,SelfLink:/api/v1/namespaces/watch-7237/configmaps/e2e-watch-test-resource-version,UID:f2e9d33b-5b79-11e9-a517-0202de25e2de,ResourceVersion:14781,Generation:0,CreationTimestamp:2019-04-10 10:18:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:18:13.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7237" for this suite.
Apr 10 10:18:19.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:18:20.619: INFO: namespace watch-7237 deletion completed in 6.846747016s
•S
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:18:20.619: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1548
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:18:23.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1548" for this suite.
Apr 10 10:19:05.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:19:05.904: INFO: namespace kubelet-test-1548 deletion completed in 42.865666057s
•
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:19:05.904: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3004
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-124298ae-5b7a-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume secrets
Apr 10 10:19:06.246: INFO: Waiting up to 5m0s for pod "pod-secrets-1245e8fc-5b7a-11e9-bf19-c208aa3c4721" in namespace "secrets-3004" to be "success or failure"
Apr 10 10:19:06.273: INFO: Pod "pod-secrets-1245e8fc-5b7a-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 26.676418ms
Apr 10 10:19:08.295: INFO: Pod "pod-secrets-1245e8fc-5b7a-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.049132416s
STEP: Saw pod success
Apr 10 10:19:08.296: INFO: Pod "pod-secrets-1245e8fc-5b7a-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:19:08.316: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-secrets-1245e8fc-5b7a-11e9-bf19-c208aa3c4721 container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 10:19:08.373: INFO: Waiting for pod pod-secrets-1245e8fc-5b7a-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:19:08.396: INFO: Pod pod-secrets-1245e8fc-5b7a-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:19:08.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3004" for this suite.
Apr 10 10:19:14.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:19:15.359: INFO: namespace secrets-3004 deletion completed in 6.935301693s
•SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:19:15.359: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2932
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Apr 10 10:19:15.678: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2932,SelfLink:/api/v1/namespaces/watch-2932/configmaps/e2e-watch-test-configmap-a,UID:17e794d2-5b7a-11e9-a517-0202de25e2de,ResourceVersion:14962,Generation:0,CreationTimestamp:2019-04-10 10:19:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 10 10:19:15.678: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2932,SelfLink:/api/v1/namespaces/watch-2932/configmaps/e2e-watch-test-configmap-a,UID:17e794d2-5b7a-11e9-a517-0202de25e2de,ResourceVersion:14962,Generation:0,CreationTimestamp:2019-04-10 10:19:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Apr 10 10:19:25.722: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2932,SelfLink:/api/v1/namespaces/watch-2932/configmaps/e2e-watch-test-configmap-a,UID:17e794d2-5b7a-11e9-a517-0202de25e2de,ResourceVersion:14984,Generation:0,CreationTimestamp:2019-04-10 10:19:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Apr 10 10:19:25.722: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2932,SelfLink:/api/v1/namespaces/watch-2932/configmaps/e2e-watch-test-configmap-a,UID:17e794d2-5b7a-11e9-a517-0202de25e2de,ResourceVersion:14984,Generation:0,CreationTimestamp:2019-04-10 10:19:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Apr 10 10:19:35.768: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2932,SelfLink:/api/v1/namespaces/watch-2932/configmaps/e2e-watch-test-configmap-a,UID:17e794d2-5b7a-11e9-a517-0202de25e2de,ResourceVersion:15005,Generation:0,CreationTimestamp:2019-04-10 10:19:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 10 10:19:35.768: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2932,SelfLink:/api/v1/namespaces/watch-2932/configmaps/e2e-watch-test-configmap-a,UID:17e794d2-5b7a-11e9-a517-0202de25e2de,ResourceVersion:15005,Generation:0,CreationTimestamp:2019-04-10 10:19:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Apr 10 10:19:45.793: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2932,SelfLink:/api/v1/namespaces/watch-2932/configmaps/e2e-watch-test-configmap-a,UID:17e794d2-5b7a-11e9-a517-0202de25e2de,ResourceVersion:15026,Generation:0,CreationTimestamp:2019-04-10 10:19:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 10 10:19:45.793: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2932,SelfLink:/api/v1/namespaces/watch-2932/configmaps/e2e-watch-test-configmap-a,UID:17e794d2-5b7a-11e9-a517-0202de25e2de,ResourceVersion:15026,Generation:0,CreationTimestamp:2019-04-10 10:19:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Apr 10 10:19:55.819: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2932,SelfLink:/api/v1/namespaces/watch-2932/configmaps/e2e-watch-test-configmap-b,UID:2fd453ca-5b7a-11e9-a517-0202de25e2de,ResourceVersion:15049,Generation:0,CreationTimestamp:2019-04-10 10:19:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 10 10:19:55.819: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2932,SelfLink:/api/v1/namespaces/watch-2932/configmaps/e2e-watch-test-configmap-b,UID:2fd453ca-5b7a-11e9-a517-0202de25e2de,ResourceVersion:15049,Generation:0,CreationTimestamp:2019-04-10 10:19:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Apr 10 10:20:05.841: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2932,SelfLink:/api/v1/namespaces/watch-2932/configmaps/e2e-watch-test-configmap-b,UID:2fd453ca-5b7a-11e9-a517-0202de25e2de,ResourceVersion:15070,Generation:0,CreationTimestamp:2019-04-10 10:19:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 10 10:20:05.842: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2932,SelfLink:/api/v1/namespaces/watch-2932/configmaps/e2e-watch-test-configmap-b,UID:2fd453ca-5b7a-11e9-a517-0202de25e2de,ResourceVersion:15070,Generation:0,CreationTimestamp:2019-04-10 10:19:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:20:15.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2932" for this suite.
Apr 10 10:20:21.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:20:22.687: INFO: namespace watch-2932 deletion completed in 6.822581521s
•
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:20:22.687: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3011
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 10:20:23.028: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Apr 10 10:20:23.068: INFO: Number of nodes with available pods: 0
Apr 10 10:20:23.068: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Apr 10 10:20:23.155: INFO: Number of nodes with available pods: 0
Apr 10 10:20:23.155: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:20:24.176: INFO: Number of nodes with available pods: 0
Apr 10 10:20:24.176: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:20:25.176: INFO: Number of nodes with available pods: 1
Apr 10 10:20:25.176: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Apr 10 10:20:25.262: INFO: Number of nodes with available pods: 0
Apr 10 10:20:25.262: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Apr 10 10:20:25.304: INFO: Number of nodes with available pods: 0
Apr 10 10:20:25.304: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:20:26.325: INFO: Number of nodes with available pods: 0
Apr 10 10:20:26.326: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:20:27.327: INFO: Number of nodes with available pods: 0
Apr 10 10:20:27.327: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:20:28.325: INFO: Number of nodes with available pods: 0
Apr 10 10:20:28.325: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:20:29.325: INFO: Number of nodes with available pods: 0
Apr 10 10:20:29.325: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:20:30.325: INFO: Number of nodes with available pods: 0
Apr 10 10:20:30.325: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:20:31.329: INFO: Number of nodes with available pods: 0
Apr 10 10:20:31.329: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:20:32.325: INFO: Number of nodes with available pods: 0
Apr 10 10:20:32.325: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:20:33.327: INFO: Number of nodes with available pods: 0
Apr 10 10:20:33.327: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:20:34.325: INFO: Number of nodes with available pods: 0
Apr 10 10:20:34.325: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:20:35.325: INFO: Number of nodes with available pods: 1
Apr 10 10:20:35.325: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3011, will wait for the garbage collector to delete the pods
Apr 10 10:20:35.466: INFO: Deleting DaemonSet.extensions daemon-set took: 22.768581ms
Apr 10 10:20:35.866: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.321277ms
Apr 10 10:20:43.389: INFO: Number of nodes with available pods: 0
Apr 10 10:20:43.389: INFO: Number of running nodes: 0, number of available pods: 0
Apr 10 10:20:43.413: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3011/daemonsets","resourceVersion":"15195"},"items":null}

Apr 10 10:20:43.438: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3011/pods","resourceVersion":"15195"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:20:43.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3011" for this suite.
Apr 10 10:20:49.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:20:50.395: INFO: namespace daemonsets-3011 deletion completed in 6.834884332s
•S
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:20:50.395: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6301
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 10 10:20:50.640: INFO: Waiting up to 5m0s for pod "downward-api-507f71a3-5b7a-11e9-bf19-c208aa3c4721" in namespace "downward-api-6301" to be "success or failure"
Apr 10 10:20:50.664: INFO: Pod "downward-api-507f71a3-5b7a-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 23.549667ms
Apr 10 10:20:52.684: INFO: Pod "downward-api-507f71a3-5b7a-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044107613s
STEP: Saw pod success
Apr 10 10:20:52.684: INFO: Pod "downward-api-507f71a3-5b7a-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:20:52.706: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downward-api-507f71a3-5b7a-11e9-bf19-c208aa3c4721 container dapi-container: <nil>
STEP: delete the pod
Apr 10 10:20:52.761: INFO: Waiting for pod downward-api-507f71a3-5b7a-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:20:52.781: INFO: Pod downward-api-507f71a3-5b7a-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:20:52.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6301" for this suite.
Apr 10 10:20:58.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:20:59.622: INFO: namespace downward-api-6301 deletion completed in 6.820667288s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:20:59.622: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7371
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Apr 10 10:20:59.905: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:21:00.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7371" for this suite.
Apr 10 10:21:06.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:21:06.959: INFO: namespace kubectl-7371 deletion completed in 6.851804162s
•S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:21:06.959: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3952
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr 10 10:21:09.904: INFO: Successfully updated pod "annotationupdate5a6177e2-5b7a-11e9-bf19-c208aa3c4721"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:21:13.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3952" for this suite.
Apr 10 10:21:36.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:21:36.839: INFO: namespace projected-3952 deletion completed in 22.835923893s
•SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:21:36.839: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6850
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 10:21:37.097: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6850'
Apr 10 10:21:37.539: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 10 10:21:37.539: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Apr 10 10:21:37.559: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-6850'
Apr 10 10:21:37.807: INFO: stderr: ""
Apr 10 10:21:37.807: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:21:37.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6850" for this suite.
Apr 10 10:21:43.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:21:44.655: INFO: namespace kubectl-6850 deletion completed in 6.82808254s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:21:44.655: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2085
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0410 10:21:55.035967    4420 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 10 10:21:55.036: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:21:55.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2085" for this suite.
Apr 10 10:22:01.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:22:01.910: INFO: namespace gc-2085 deletion completed in 6.851065754s
•
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:22:01.910: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9342
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 10:22:02.242: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 10 10:22:04.294: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr 10 10:22:06.315: INFO: Creating deployment "test-rollover-deployment"
Apr 10 10:22:06.358: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr 10 10:22:06.380: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr 10 10:22:06.421: INFO: Ensure that both replica sets have 1 created replica
Apr 10 10:22:06.461: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr 10 10:22:06.505: INFO: Updating deployment test-rollover-deployment
Apr 10 10:22:06.505: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr 10 10:22:06.531: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr 10 10:22:06.572: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr 10 10:22:06.615: INFO: all replica sets need to contain the pod-template-hash label
Apr 10 10:22:06.616: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 10:22:08.661: INFO: all replica sets need to contain the pod-template-hash label
Apr 10 10:22:08.661: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488528, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 10:22:10.656: INFO: all replica sets need to contain the pod-template-hash label
Apr 10 10:22:10.656: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488528, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 10:22:12.662: INFO: all replica sets need to contain the pod-template-hash label
Apr 10 10:22:12.662: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488528, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 10:22:14.658: INFO: all replica sets need to contain the pod-template-hash label
Apr 10 10:22:14.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488528, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 10:22:16.657: INFO: all replica sets need to contain the pod-template-hash label
Apr 10 10:22:16.657: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488528, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488526, loc:(*time.Location)(0x87c9f80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 10:22:18.661: INFO: 
Apr 10 10:22:18.661: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 10 10:22:18.721: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-9342,SelfLink:/apis/apps/v1/namespaces/deployment-9342/deployments/test-rollover-deployment,UID:7da03481-5b7a-11e9-a517-0202de25e2de,ResourceVersion:15568,Generation:2,CreationTimestamp:2019-04-10 10:22:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-04-10 10:22:06 +0000 UTC 2019-04-10 10:22:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-04-10 10:22:18 +0000 UTC 2019-04-10 10:22:06 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Apr 10 10:22:18.742: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-9342,SelfLink:/apis/apps/v1/namespaces/deployment-9342/replicasets/test-rollover-deployment-766b4d6c9d,UID:7db966fe-5b7a-11e9-a517-0202de25e2de,ResourceVersion:15561,Generation:2,CreationTimestamp:2019-04-10 10:22:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 7da03481-5b7a-11e9-a517-0202de25e2de 0xc002529877 0xc002529878}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr 10 10:22:18.742: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr 10 10:22:18.742: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-9342,SelfLink:/apis/apps/v1/namespaces/deployment-9342/replicasets/test-rollover-controller,UID:7b2c17cd-5b7a-11e9-a517-0202de25e2de,ResourceVersion:15567,Generation:2,CreationTimestamp:2019-04-10 10:22:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 7da03481-5b7a-11e9-a517-0202de25e2de 0xc0025296c7 0xc0025296c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 10:22:18.742: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-9342,SelfLink:/apis/apps/v1/namespaces/deployment-9342/replicasets/test-rollover-deployment-6455657675,UID:7da1243a-5b7a-11e9-a517-0202de25e2de,ResourceVersion:15526,Generation:2,CreationTimestamp:2019-04-10 10:22:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 7da03481-5b7a-11e9-a517-0202de25e2de 0xc002529797 0xc002529798}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 10:22:18.762: INFO: Pod "test-rollover-deployment-766b4d6c9d-7ntd5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-7ntd5,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-9342,SelfLink:/api/v1/namespaces/deployment-9342/pods/test-rollover-deployment-766b4d6c9d-7ntd5,UID:7dbae888-5b7a-11e9-a517-0202de25e2de,ResourceVersion:15537,Generation:0,CreationTimestamp:2019-04-10 10:22:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.152/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d 7db966fe-5b7a-11e9-a517-0202de25e2de 0xc00253ca17 0xc00253ca18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sffh2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sffh2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-sffh2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00253cbd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00253cbf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:22:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:22:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:22:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:22:06 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.152,PodIP:100.96.0.152,StartTime:2019-04-10 10:22:06 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-04-10 10:22:07 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://68d0a6d41cee63ad81520a623785a2c265b1794198c289be0c2d6b5fed46a2b6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:22:18.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9342" for this suite.
Apr 10 10:22:24.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:22:25.608: INFO: namespace deployment-9342 deletion completed in 6.825597062s
•SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:22:25.608: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1545
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 10:22:25.985: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8952c769-5b7a-11e9-a517-0202de25e2de", Controller:(*bool)(0xc00034bd3a), BlockOwnerDeletion:(*bool)(0xc00034bd3b)}}
Apr 10 10:22:26.006: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"894c7cec-5b7a-11e9-a517-0202de25e2de", Controller:(*bool)(0xc0004b4446), BlockOwnerDeletion:(*bool)(0xc0004b4447)}}
Apr 10 10:22:26.028: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"894fa3d1-5b7a-11e9-a517-0202de25e2de", Controller:(*bool)(0xc00005bfb6), BlockOwnerDeletion:(*bool)(0xc00005bfb7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:22:31.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1545" for this suite.
Apr 10 10:22:37.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:22:37.919: INFO: namespace gc-1545 deletion completed in 6.82699142s
•
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:22:37.919: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3029
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-3029
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Apr 10 10:22:38.264: INFO: Found 1 stateful pods, waiting for 3
Apr 10 10:22:48.285: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 10:22:48.285: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 10:22:48.285: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 10:22:48.350: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-3029 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 10:22:49.204: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 10:22:49.204: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 10:22:49.204: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Apr 10 10:22:59.339: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Apr 10 10:22:59.400: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-3029 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 10:23:00.254: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 10:23:00.254: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 10:23:00.254: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 10:23:00.342: INFO: Waiting for StatefulSet statefulset-3029/ss2 to complete update
Apr 10 10:23:00.342: INFO: Waiting for Pod statefulset-3029/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 10:23:00.342: INFO: Waiting for Pod statefulset-3029/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 10:23:00.342: INFO: Waiting for Pod statefulset-3029/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 10:23:10.395: INFO: Waiting for StatefulSet statefulset-3029/ss2 to complete update
Apr 10 10:23:10.395: INFO: Waiting for Pod statefulset-3029/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 10:23:10.395: INFO: Waiting for Pod statefulset-3029/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 10:23:10.395: INFO: Waiting for Pod statefulset-3029/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 10:23:20.384: INFO: Waiting for StatefulSet statefulset-3029/ss2 to complete update
Apr 10 10:23:20.384: INFO: Waiting for Pod statefulset-3029/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 10:23:30.390: INFO: Waiting for StatefulSet statefulset-3029/ss2 to complete update
Apr 10 10:23:30.390: INFO: Waiting for Pod statefulset-3029/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Apr 10 10:23:40.385: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-3029 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 10:23:41.114: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 10:23:41.114: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 10:23:41.114: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 10:23:51.250: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Apr 10 10:23:51.310: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-3029 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 10:23:52.043: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 10:23:52.043: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 10:23:52.043: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 10:24:02.172: INFO: Waiting for StatefulSet statefulset-3029/ss2 to complete update
Apr 10 10:24:02.172: INFO: Waiting for Pod statefulset-3029/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 10 10:24:02.172: INFO: Waiting for Pod statefulset-3029/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 10 10:24:02.172: INFO: Waiting for Pod statefulset-3029/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 10 10:24:12.216: INFO: Waiting for StatefulSet statefulset-3029/ss2 to complete update
Apr 10 10:24:12.216: INFO: Waiting for Pod statefulset-3029/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 10 10:24:22.220: INFO: Deleting all statefulset in ns statefulset-3029
Apr 10 10:24:22.240: INFO: Scaling statefulset ss2 to 0
Apr 10 10:25:02.325: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 10:25:02.346: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:25:02.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3029" for this suite.
Apr 10 10:25:08.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:25:09.319: INFO: namespace statefulset-3029 deletion completed in 6.888382989s
•SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:25:09.319: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2291
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 10:25:09.600: INFO: Creating ReplicaSet my-hostname-basic-eadd0af4-5b7a-11e9-bf19-c208aa3c4721
Apr 10 10:25:09.640: INFO: Pod name my-hostname-basic-eadd0af4-5b7a-11e9-bf19-c208aa3c4721: Found 1 pods out of 1
Apr 10 10:25:09.640: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-eadd0af4-5b7a-11e9-bf19-c208aa3c4721" is running
Apr 10 10:25:11.682: INFO: Pod "my-hostname-basic-eadd0af4-5b7a-11e9-bf19-c208aa3c4721-7gtjg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 10:25:09 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 10:25:09 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-eadd0af4-5b7a-11e9-bf19-c208aa3c4721]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 10:25:09 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-eadd0af4-5b7a-11e9-bf19-c208aa3c4721]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 10:25:09 +0000 UTC Reason: Message:}])
Apr 10 10:25:11.682: INFO: Trying to dial the pod
Apr 10 10:25:16.842: INFO: Controller my-hostname-basic-eadd0af4-5b7a-11e9-bf19-c208aa3c4721: Got expected result from replica 1 [my-hostname-basic-eadd0af4-5b7a-11e9-bf19-c208aa3c4721-7gtjg]: "my-hostname-basic-eadd0af4-5b7a-11e9-bf19-c208aa3c4721-7gtjg", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:25:16.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2291" for this suite.
Apr 10 10:25:22.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:25:23.727: INFO: namespace replicaset-2291 deletion completed in 6.864020051s
•SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:25:23.728: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4285
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 10:25:24.024: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3724371-5b7a-11e9-bf19-c208aa3c4721" in namespace "downward-api-4285" to be "success or failure"
Apr 10 10:25:24.046: INFO: Pod "downwardapi-volume-f3724371-5b7a-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 21.74303ms
Apr 10 10:25:26.066: INFO: Pod "downwardapi-volume-f3724371-5b7a-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042482749s
STEP: Saw pod success
Apr 10 10:25:26.066: INFO: Pod "downwardapi-volume-f3724371-5b7a-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:25:26.086: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downwardapi-volume-f3724371-5b7a-11e9-bf19-c208aa3c4721 container client-container: <nil>
STEP: delete the pod
Apr 10 10:25:26.145: INFO: Waiting for pod downwardapi-volume-f3724371-5b7a-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:25:26.165: INFO: Pod downwardapi-volume-f3724371-5b7a-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:25:26.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4285" for this suite.
Apr 10 10:25:32.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:25:33.034: INFO: namespace downward-api-4285 deletion completed in 6.849057484s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:25:33.034: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-808
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-808.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-808.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-808.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-808.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-808.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-808.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 10 10:25:38.088: INFO: DNS probes using dns-808/dns-test-f8fe0fc2-5b7a-11e9-bf19-c208aa3c4721 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:25:38.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-808" for this suite.
Apr 10 10:25:44.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:25:44.982: INFO: namespace dns-808 deletion completed in 6.84162349s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:25:44.982: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8335
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Apr 10 10:25:45.381: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8335,SelfLink:/api/v1/namespaces/watch-8335/configmaps/e2e-watch-test-watch-closed,UID:00292b4b-5b7b-11e9-a517-0202de25e2de,ResourceVersion:16296,Generation:0,CreationTimestamp:2019-04-10 10:25:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 10 10:25:45.382: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8335,SelfLink:/api/v1/namespaces/watch-8335/configmaps/e2e-watch-test-watch-closed,UID:00292b4b-5b7b-11e9-a517-0202de25e2de,ResourceVersion:16297,Generation:0,CreationTimestamp:2019-04-10 10:25:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Apr 10 10:25:45.462: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8335,SelfLink:/api/v1/namespaces/watch-8335/configmaps/e2e-watch-test-watch-closed,UID:00292b4b-5b7b-11e9-a517-0202de25e2de,ResourceVersion:16298,Generation:0,CreationTimestamp:2019-04-10 10:25:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 10 10:25:45.462: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8335,SelfLink:/api/v1/namespaces/watch-8335/configmaps/e2e-watch-test-watch-closed,UID:00292b4b-5b7b-11e9-a517-0202de25e2de,ResourceVersion:16299,Generation:0,CreationTimestamp:2019-04-10 10:25:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:25:45.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8335" for this suite.
Apr 10 10:25:51.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:25:52.357: INFO: namespace watch-8335 deletion completed in 6.874685777s
•SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:25:52.358: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3691
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-047e9ff6-5b7b-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume secrets
Apr 10 10:25:52.648: INFO: Waiting up to 5m0s for pod "pod-secrets-0481be1f-5b7b-11e9-bf19-c208aa3c4721" in namespace "secrets-3691" to be "success or failure"
Apr 10 10:25:52.668: INFO: Pod "pod-secrets-0481be1f-5b7b-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.757637ms
Apr 10 10:25:54.691: INFO: Pod "pod-secrets-0481be1f-5b7b-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042700099s
STEP: Saw pod success
Apr 10 10:25:54.691: INFO: Pod "pod-secrets-0481be1f-5b7b-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:25:54.715: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-secrets-0481be1f-5b7b-11e9-bf19-c208aa3c4721 container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 10:25:54.764: INFO: Waiting for pod pod-secrets-0481be1f-5b7b-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:25:54.784: INFO: Pod pod-secrets-0481be1f-5b7b-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:25:54.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3691" for this suite.
Apr 10 10:26:00.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:26:01.659: INFO: namespace secrets-3691 deletion completed in 6.855265542s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:26:01.660: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6633
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 10:26:01.971: INFO: (0) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 27.938636ms)
Apr 10 10:26:02.011: INFO: (1) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 39.791714ms)
Apr 10 10:26:02.034: INFO: (2) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 22.881291ms)
Apr 10 10:26:02.057: INFO: (3) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 22.574721ms)
Apr 10 10:26:02.081: INFO: (4) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 23.924547ms)
Apr 10 10:26:02.103: INFO: (5) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 22.181486ms)
Apr 10 10:26:02.125: INFO: (6) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 22.269371ms)
Apr 10 10:26:02.148: INFO: (7) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 22.687374ms)
Apr 10 10:26:02.170: INFO: (8) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 21.836044ms)
Apr 10 10:26:02.195: INFO: (9) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.253798ms)
Apr 10 10:26:02.218: INFO: (10) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 22.745255ms)
Apr 10 10:26:02.240: INFO: (11) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 21.91278ms)
Apr 10 10:26:02.265: INFO: (12) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.319076ms)
Apr 10 10:26:02.287: INFO: (13) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 22.002538ms)
Apr 10 10:26:02.310: INFO: (14) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 22.198484ms)
Apr 10 10:26:02.333: INFO: (15) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 22.786505ms)
Apr 10 10:26:02.357: INFO: (16) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 24.788036ms)
Apr 10 10:26:02.382: INFO: (17) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 24.563861ms)
Apr 10 10:26:02.405: INFO: (18) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 22.871434ms)
Apr 10 10:26:02.428: INFO: (19) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 22.603273ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:26:02.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6633" for this suite.
Apr 10 10:26:08.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:26:09.517: INFO: namespace proxy-6633 deletion completed in 7.066645578s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:26:09.517: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8926
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-0ebf8161-5b7b-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume configMaps
Apr 10 10:26:09.850: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0ec31f80-5b7b-11e9-bf19-c208aa3c4721" in namespace "projected-8926" to be "success or failure"
Apr 10 10:26:09.872: INFO: Pod "pod-projected-configmaps-0ec31f80-5b7b-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 21.417244ms
Apr 10 10:26:11.893: INFO: Pod "pod-projected-configmaps-0ec31f80-5b7b-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042239638s
STEP: Saw pod success
Apr 10 10:26:11.893: INFO: Pod "pod-projected-configmaps-0ec31f80-5b7b-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:26:11.915: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-projected-configmaps-0ec31f80-5b7b-11e9-bf19-c208aa3c4721 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 10:26:11.965: INFO: Waiting for pod pod-projected-configmaps-0ec31f80-5b7b-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:26:11.987: INFO: Pod pod-projected-configmaps-0ec31f80-5b7b-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:26:11.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8926" for this suite.
Apr 10 10:26:18.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:26:18.847: INFO: namespace projected-8926 deletion completed in 6.833541861s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:26:18.847: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3283
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 10:26:19.125: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1449cf36-5b7b-11e9-bf19-c208aa3c4721" in namespace "projected-3283" to be "success or failure"
Apr 10 10:26:19.145: INFO: Pod "downwardapi-volume-1449cf36-5b7b-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.519271ms
Apr 10 10:26:21.166: INFO: Pod "downwardapi-volume-1449cf36-5b7b-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041458961s
STEP: Saw pod success
Apr 10 10:26:21.166: INFO: Pod "downwardapi-volume-1449cf36-5b7b-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:26:21.186: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downwardapi-volume-1449cf36-5b7b-11e9-bf19-c208aa3c4721 container client-container: <nil>
STEP: delete the pod
Apr 10 10:26:21.237: INFO: Waiting for pod downwardapi-volume-1449cf36-5b7b-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:26:21.262: INFO: Pod downwardapi-volume-1449cf36-5b7b-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:26:21.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3283" for this suite.
Apr 10 10:26:27.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:26:28.127: INFO: namespace projected-3283 deletion completed in 6.842061969s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:26:28.127: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9751
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-19d59dcf-5b7b-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume configMaps
Apr 10 10:26:28.447: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-19d8adc5-5b7b-11e9-bf19-c208aa3c4721" in namespace "projected-9751" to be "success or failure"
Apr 10 10:26:28.467: INFO: Pod "pod-projected-configmaps-19d8adc5-5b7b-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.822495ms
Apr 10 10:26:30.487: INFO: Pod "pod-projected-configmaps-19d8adc5-5b7b-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040191638s
STEP: Saw pod success
Apr 10 10:26:30.487: INFO: Pod "pod-projected-configmaps-19d8adc5-5b7b-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:26:30.508: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-projected-configmaps-19d8adc5-5b7b-11e9-bf19-c208aa3c4721 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 10:26:30.560: INFO: Waiting for pod pod-projected-configmaps-19d8adc5-5b7b-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:26:30.582: INFO: Pod pod-projected-configmaps-19d8adc5-5b7b-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:26:30.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9751" for this suite.
Apr 10 10:26:36.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:26:37.434: INFO: namespace projected-9751 deletion completed in 6.83073442s
•SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:26:37.435: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9694
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 10:26:37.721: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f5fb4aa-5b7b-11e9-bf19-c208aa3c4721" in namespace "downward-api-9694" to be "success or failure"
Apr 10 10:26:37.743: INFO: Pod "downwardapi-volume-1f5fb4aa-5b7b-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 21.826271ms
Apr 10 10:26:39.766: INFO: Pod "downwardapi-volume-1f5fb4aa-5b7b-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044271541s
STEP: Saw pod success
Apr 10 10:26:39.766: INFO: Pod "downwardapi-volume-1f5fb4aa-5b7b-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:26:39.786: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downwardapi-volume-1f5fb4aa-5b7b-11e9-bf19-c208aa3c4721 container client-container: <nil>
STEP: delete the pod
Apr 10 10:26:39.839: INFO: Waiting for pod downwardapi-volume-1f5fb4aa-5b7b-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:26:39.859: INFO: Pod downwardapi-volume-1f5fb4aa-5b7b-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:26:39.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9694" for this suite.
Apr 10 10:26:45.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:26:46.709: INFO: namespace downward-api-9694 deletion completed in 6.824416843s
•SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:26:46.709: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2789
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Apr 10 10:26:47.549: INFO: created pod pod-service-account-defaultsa
Apr 10 10:26:47.549: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr 10 10:26:47.570: INFO: created pod pod-service-account-mountsa
Apr 10 10:26:47.570: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr 10 10:26:47.590: INFO: created pod pod-service-account-nomountsa
Apr 10 10:26:47.590: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr 10 10:26:47.611: INFO: created pod pod-service-account-defaultsa-mountspec
Apr 10 10:26:47.611: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr 10 10:26:47.632: INFO: created pod pod-service-account-mountsa-mountspec
Apr 10 10:26:47.632: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr 10 10:26:47.653: INFO: created pod pod-service-account-nomountsa-mountspec
Apr 10 10:26:47.653: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr 10 10:26:47.673: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr 10 10:26:47.673: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr 10 10:26:47.697: INFO: created pod pod-service-account-mountsa-nomountspec
Apr 10 10:26:47.697: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr 10 10:26:47.717: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr 10 10:26:47.717: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:26:47.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2789" for this suite.
Apr 10 10:26:53.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:26:54.568: INFO: namespace svcaccounts-2789 deletion completed in 6.830416153s
•SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:26:54.568: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3194
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 10 10:26:55.127: INFO: Waiting up to 5m0s for pod "pod-29bf550d-5b7b-11e9-bf19-c208aa3c4721" in namespace "emptydir-3194" to be "success or failure"
Apr 10 10:26:55.147: INFO: Pod "pod-29bf550d-5b7b-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.551124ms
Apr 10 10:26:57.176: INFO: Pod "pod-29bf550d-5b7b-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.048171464s
STEP: Saw pod success
Apr 10 10:26:57.176: INFO: Pod "pod-29bf550d-5b7b-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:26:57.248: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-29bf550d-5b7b-11e9-bf19-c208aa3c4721 container test-container: <nil>
STEP: delete the pod
Apr 10 10:26:57.313: INFO: Waiting for pod pod-29bf550d-5b7b-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:26:57.333: INFO: Pod pod-29bf550d-5b7b-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:26:57.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3194" for this suite.
Apr 10 10:27:03.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:27:04.238: INFO: namespace emptydir-3194 deletion completed in 6.88384857s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:27:04.239: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7039
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr 10 10:27:04.503: INFO: PodSpec: initContainers in spec.initContainers
Apr 10 10:27:52.847: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-2f59ec6a-5b7b-11e9-bf19-c208aa3c4721", GenerateName:"", Namespace:"init-container-7039", SelfLink:"/api/v1/namespaces/init-container-7039/pods/pod-init-2f59ec6a-5b7b-11e9-bf19-c208aa3c4721", UID:"2f5bffc5-5b7b-11e9-a517-0202de25e2de", ResourceVersion:"16797", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63690488824, loc:(*time.Location)(0x87c9f80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"503824292"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.0.175/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-zs4cl", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc003ec2240), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-zs4cl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-zs4cl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-zs4cl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003eb05e8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-250-14-152.eu-west-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003ea0360), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003eb0660)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003eb0680)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003eb0688), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003eb068c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488824, loc:(*time.Location)(0x87c9f80)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488824, loc:(*time.Location)(0x87c9f80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488824, loc:(*time.Location)(0x87c9f80)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690488824, loc:(*time.Location)(0x87c9f80)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.14.152", PodIP:"100.96.0.175", StartTime:(*v1.Time)(0xc003eac2e0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000aa6fc0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000aa7030)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://3b25273fc54a1d494f3661a1579629967b36650dac8601b3cc2b59f127381d32"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003eac320), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003eac300), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:27:52.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7039" for this suite.
Apr 10 10:28:14.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:28:15.723: INFO: namespace init-container-7039 deletion completed in 22.854380243s
•SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:28:15.723: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7064
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Apr 10 10:28:15.996: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config --namespace=kubectl-7064 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Apr 10 10:28:17.844: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Apr 10 10:28:17.844: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:28:19.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7064" for this suite.
Apr 10 10:28:25.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:28:26.741: INFO: namespace kubectl-7064 deletion completed in 6.829409691s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:28:26.742: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3067
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-60858eed-5b7b-11e9-bf19-c208aa3c4721
STEP: Creating secret with name secret-projected-all-test-volume-60858ece-5b7b-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test Check all projections for projected volume plugin
Apr 10 10:28:27.064: INFO: Waiting up to 5m0s for pod "projected-volume-60858e87-5b7b-11e9-bf19-c208aa3c4721" in namespace "projected-3067" to be "success or failure"
Apr 10 10:28:27.083: INFO: Pod "projected-volume-60858e87-5b7b-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.699814ms
Apr 10 10:28:29.104: INFO: Pod "projected-volume-60858e87-5b7b-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040337066s
STEP: Saw pod success
Apr 10 10:28:29.104: INFO: Pod "projected-volume-60858e87-5b7b-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:28:29.124: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod projected-volume-60858e87-5b7b-11e9-bf19-c208aa3c4721 container projected-all-volume-test: <nil>
STEP: delete the pod
Apr 10 10:28:29.173: INFO: Waiting for pod projected-volume-60858e87-5b7b-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:28:29.192: INFO: Pod projected-volume-60858e87-5b7b-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:28:29.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3067" for this suite.
Apr 10 10:28:35.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:28:36.063: INFO: namespace projected-3067 deletion completed in 6.847710183s
•SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:28:36.063: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3633
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 10:28:36.407: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Apr 10 10:28:36.471: INFO: Number of nodes with available pods: 0
Apr 10 10:28:36.471: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:28:37.519: INFO: Number of nodes with available pods: 0
Apr 10 10:28:37.519: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:28:38.515: INFO: Number of nodes with available pods: 2
Apr 10 10:28:38.515: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Apr 10 10:28:38.663: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:38.663: INFO: Wrong image for pod: daemon-set-rgg5x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:39.703: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:39.703: INFO: Wrong image for pod: daemon-set-rgg5x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:40.706: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:40.706: INFO: Wrong image for pod: daemon-set-rgg5x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:41.704: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:41.704: INFO: Wrong image for pod: daemon-set-rgg5x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:41.704: INFO: Pod daemon-set-rgg5x is not available
Apr 10 10:28:42.704: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:42.704: INFO: Wrong image for pod: daemon-set-rgg5x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:42.704: INFO: Pod daemon-set-rgg5x is not available
Apr 10 10:28:43.704: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:43.704: INFO: Wrong image for pod: daemon-set-rgg5x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:43.704: INFO: Pod daemon-set-rgg5x is not available
Apr 10 10:28:44.706: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:44.706: INFO: Wrong image for pod: daemon-set-rgg5x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:44.706: INFO: Pod daemon-set-rgg5x is not available
Apr 10 10:28:45.704: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:45.704: INFO: Wrong image for pod: daemon-set-rgg5x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:45.704: INFO: Pod daemon-set-rgg5x is not available
Apr 10 10:28:46.705: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:46.705: INFO: Wrong image for pod: daemon-set-rgg5x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:46.705: INFO: Pod daemon-set-rgg5x is not available
Apr 10 10:28:47.707: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:47.707: INFO: Wrong image for pod: daemon-set-rgg5x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:47.707: INFO: Pod daemon-set-rgg5x is not available
Apr 10 10:28:48.704: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:48.704: INFO: Wrong image for pod: daemon-set-rgg5x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:48.704: INFO: Pod daemon-set-rgg5x is not available
Apr 10 10:28:49.706: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:49.706: INFO: Wrong image for pod: daemon-set-rgg5x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:49.706: INFO: Pod daemon-set-rgg5x is not available
Apr 10 10:28:50.704: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:50.704: INFO: Wrong image for pod: daemon-set-rgg5x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:50.704: INFO: Pod daemon-set-rgg5x is not available
Apr 10 10:28:51.704: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:51.704: INFO: Wrong image for pod: daemon-set-rgg5x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:51.704: INFO: Pod daemon-set-rgg5x is not available
Apr 10 10:28:52.708: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:52.708: INFO: Wrong image for pod: daemon-set-rgg5x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:52.708: INFO: Pod daemon-set-rgg5x is not available
Apr 10 10:28:53.705: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:53.706: INFO: Pod daemon-set-ssl9q is not available
Apr 10 10:28:54.706: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:55.708: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:55.708: INFO: Pod daemon-set-hjfrl is not available
Apr 10 10:28:56.704: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:56.704: INFO: Pod daemon-set-hjfrl is not available
Apr 10 10:28:57.705: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:57.705: INFO: Pod daemon-set-hjfrl is not available
Apr 10 10:28:58.704: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:58.704: INFO: Pod daemon-set-hjfrl is not available
Apr 10 10:28:59.705: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:28:59.705: INFO: Pod daemon-set-hjfrl is not available
Apr 10 10:29:00.706: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:29:00.706: INFO: Pod daemon-set-hjfrl is not available
Apr 10 10:29:01.711: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:29:01.711: INFO: Pod daemon-set-hjfrl is not available
Apr 10 10:29:02.704: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:29:02.704: INFO: Pod daemon-set-hjfrl is not available
Apr 10 10:29:03.707: INFO: Wrong image for pod: daemon-set-hjfrl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 10:29:03.707: INFO: Pod daemon-set-hjfrl is not available
Apr 10 10:29:04.707: INFO: Pod daemon-set-txsxl is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Apr 10 10:29:04.776: INFO: Number of nodes with available pods: 1
Apr 10 10:29:04.776: INFO: Node ip-10-250-25-25.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:29:05.822: INFO: Number of nodes with available pods: 1
Apr 10 10:29:05.822: INFO: Node ip-10-250-25-25.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:29:06.821: INFO: Number of nodes with available pods: 1
Apr 10 10:29:06.821: INFO: Node ip-10-250-25-25.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:29:07.827: INFO: Number of nodes with available pods: 2
Apr 10 10:29:07.827: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3633, will wait for the garbage collector to delete the pods
Apr 10 10:29:08.024: INFO: Deleting DaemonSet.extensions daemon-set took: 24.162142ms
Apr 10 10:29:08.426: INFO: Terminating DaemonSet.extensions daemon-set pods took: 401.520617ms
Apr 10 10:29:14.734: INFO: Number of nodes with available pods: 0
Apr 10 10:29:14.734: INFO: Number of running nodes: 0, number of available pods: 0
Apr 10 10:29:14.834: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3633/daemonsets","resourceVersion":"17076"},"items":null}

Apr 10 10:29:14.853: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3633/pods","resourceVersion":"17076"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:29:14.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3633" for this suite.
Apr 10 10:29:21.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:29:21.793: INFO: namespace daemonsets-3633 deletion completed in 6.851898193s
•SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:29:21.793: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9583
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr 10 10:29:22.098: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:29:25.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9583" for this suite.
Apr 10 10:29:31.838: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:29:32.612: INFO: namespace init-container-9583 deletion completed in 6.840271282s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:29:32.613: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-339
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 10:29:32.922: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87cd6c66-5b7b-11e9-bf19-c208aa3c4721" in namespace "downward-api-339" to be "success or failure"
Apr 10 10:29:32.941: INFO: Pod "downwardapi-volume-87cd6c66-5b7b-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.468452ms
Apr 10 10:29:34.962: INFO: Pod "downwardapi-volume-87cd6c66-5b7b-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03999136s
STEP: Saw pod success
Apr 10 10:29:34.962: INFO: Pod "downwardapi-volume-87cd6c66-5b7b-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:29:34.982: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downwardapi-volume-87cd6c66-5b7b-11e9-bf19-c208aa3c4721 container client-container: <nil>
STEP: delete the pod
Apr 10 10:29:35.032: INFO: Waiting for pod downwardapi-volume-87cd6c66-5b7b-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:29:35.051: INFO: Pod downwardapi-volume-87cd6c66-5b7b-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:29:35.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-339" for this suite.
Apr 10 10:29:41.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:29:41.917: INFO: namespace downward-api-339 deletion completed in 6.842198062s
•SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:29:41.917: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9178
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-8d57fbf4-5b7b-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume secrets
Apr 10 10:29:42.241: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8d5b0dcb-5b7b-11e9-bf19-c208aa3c4721" in namespace "projected-9178" to be "success or failure"
Apr 10 10:29:42.260: INFO: Pod "pod-projected-secrets-8d5b0dcb-5b7b-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.576493ms
Apr 10 10:29:44.281: INFO: Pod "pod-projected-secrets-8d5b0dcb-5b7b-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039971944s
STEP: Saw pod success
Apr 10 10:29:44.281: INFO: Pod "pod-projected-secrets-8d5b0dcb-5b7b-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:29:44.303: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-projected-secrets-8d5b0dcb-5b7b-11e9-bf19-c208aa3c4721 container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 10:29:44.356: INFO: Waiting for pod pod-projected-secrets-8d5b0dcb-5b7b-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:29:44.378: INFO: Pod pod-projected-secrets-8d5b0dcb-5b7b-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:29:44.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9178" for this suite.
Apr 10 10:29:50.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:29:51.274: INFO: namespace projected-9178 deletion completed in 6.875116217s
•
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:29:51.274: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3448
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-3448
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-3448
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3448
Apr 10 10:29:51.559: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Apr 10 10:30:01.581: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Apr 10 10:30:01.601: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-3448 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 10:30:02.415: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 10:30:02.415: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 10:30:02.415: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 10:30:02.437: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 10 10:30:12.459: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 10:30:12.459: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 10:30:12.546: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999313s
Apr 10 10:30:13.567: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.976929633s
Apr 10 10:30:14.598: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.949812801s
Apr 10 10:30:15.618: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.92542348s
Apr 10 10:30:16.642: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.904782097s
Apr 10 10:30:17.662: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.881153577s
Apr 10 10:30:18.686: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.860576755s
Apr 10 10:30:19.706: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.837409113s
Apr 10 10:30:20.727: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.816627746s
Apr 10 10:30:21.748: INFO: Verifying statefulset ss doesn't scale past 3 for another 796.008265ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3448
Apr 10 10:30:22.769: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-3448 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 10:30:23.503: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 10:30:23.503: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 10:30:23.503: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 10:30:23.503: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-3448 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 10:30:24.238: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 10 10:30:24.238: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 10:30:24.238: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 10:30:24.238: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-3448 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 10:30:24.935: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 10 10:30:24.935: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 10:30:24.935: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 10:30:24.956: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 10:30:24.956: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 10:30:24.956: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Apr 10 10:30:24.979: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-3448 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 10:30:25.702: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 10:30:25.702: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 10:30:25.702: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 10:30:25.702: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-3448 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 10:30:26.413: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 10:30:26.413: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 10:30:26.413: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 10:30:26.413: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config exec --namespace=statefulset-3448 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 10:30:27.126: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 10:30:27.126: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 10:30:27.126: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 10:30:27.126: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 10:30:27.146: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr 10 10:30:37.188: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 10:30:37.188: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 10:30:37.188: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 10:30:37.255: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Apr 10 10:30:37.255: INFO: ss-0  ip-10-250-14-152.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:29:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:29:51 +0000 UTC  }]
Apr 10 10:30:37.255: INFO: ss-1  ip-10-250-14-152.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  }]
Apr 10 10:30:37.255: INFO: ss-2  ip-10-250-25-25.eu-west-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  }]
Apr 10 10:30:37.255: INFO: 
Apr 10 10:30:37.256: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 10 10:30:38.277: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Apr 10 10:30:38.277: INFO: ss-0  ip-10-250-14-152.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:29:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:29:51 +0000 UTC  }]
Apr 10 10:30:38.277: INFO: ss-1  ip-10-250-14-152.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  }]
Apr 10 10:30:38.277: INFO: ss-2  ip-10-250-25-25.eu-west-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  }]
Apr 10 10:30:38.277: INFO: 
Apr 10 10:30:38.277: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 10 10:30:39.303: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Apr 10 10:30:39.303: INFO: ss-0  ip-10-250-14-152.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:29:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:29:51 +0000 UTC  }]
Apr 10 10:30:39.303: INFO: ss-1  ip-10-250-14-152.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  }]
Apr 10 10:30:39.303: INFO: ss-2  ip-10-250-25-25.eu-west-1.compute.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  }]
Apr 10 10:30:39.303: INFO: 
Apr 10 10:30:39.303: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 10 10:30:40.324: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Apr 10 10:30:40.324: INFO: ss-2  ip-10-250-25-25.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  }]
Apr 10 10:30:40.324: INFO: 
Apr 10 10:30:40.324: INFO: StatefulSet ss has not reached scale 0, at 1
Apr 10 10:30:41.345: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Apr 10 10:30:41.345: INFO: ss-2  ip-10-250-25-25.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  }]
Apr 10 10:30:41.345: INFO: 
Apr 10 10:30:41.345: INFO: StatefulSet ss has not reached scale 0, at 1
Apr 10 10:30:42.366: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Apr 10 10:30:42.366: INFO: ss-2  ip-10-250-25-25.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  }]
Apr 10 10:30:42.366: INFO: 
Apr 10 10:30:42.366: INFO: StatefulSet ss has not reached scale 0, at 1
Apr 10 10:30:43.396: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Apr 10 10:30:43.396: INFO: ss-2  ip-10-250-25-25.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  }]
Apr 10 10:30:43.396: INFO: 
Apr 10 10:30:43.396: INFO: StatefulSet ss has not reached scale 0, at 1
Apr 10 10:30:44.418: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Apr 10 10:30:44.418: INFO: ss-2  ip-10-250-25-25.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:30:12 +0000 UTC  }]
Apr 10 10:30:44.418: INFO: 
Apr 10 10:30:44.418: INFO: StatefulSet ss has not reached scale 0, at 1
Apr 10 10:30:45.438: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.811691404s
Apr 10 10:30:46.463: INFO: Verifying statefulset ss doesn't scale past 0 for another 791.49443ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3448
Apr 10 10:30:47.483: INFO: Scaling statefulset ss to 0
Apr 10 10:30:47.546: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 10 10:30:47.566: INFO: Deleting all statefulset in ns statefulset-3448
Apr 10 10:30:47.586: INFO: Scaling statefulset ss to 0
Apr 10 10:30:47.654: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 10:30:47.675: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:30:47.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3448" for this suite.
Apr 10 10:30:53.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:30:54.615: INFO: namespace statefulset-3448 deletion completed in 6.85161331s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:30:54.615: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7544
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-b8aeb3fc-5b7b-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume configMaps
Apr 10 10:30:54.950: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b8b1cc3d-5b7b-11e9-bf19-c208aa3c4721" in namespace "projected-7544" to be "success or failure"
Apr 10 10:30:54.971: INFO: Pod "pod-projected-configmaps-b8b1cc3d-5b7b-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 21.171606ms
Apr 10 10:30:56.993: INFO: Pod "pod-projected-configmaps-b8b1cc3d-5b7b-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043072503s
STEP: Saw pod success
Apr 10 10:30:56.993: INFO: Pod "pod-projected-configmaps-b8b1cc3d-5b7b-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:30:57.014: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-projected-configmaps-b8b1cc3d-5b7b-11e9-bf19-c208aa3c4721 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 10:30:57.076: INFO: Waiting for pod pod-projected-configmaps-b8b1cc3d-5b7b-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:30:57.096: INFO: Pod pod-projected-configmaps-b8b1cc3d-5b7b-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:30:57.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7544" for this suite.
Apr 10 10:31:03.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:31:03.959: INFO: namespace projected-7544 deletion completed in 6.841751319s
•SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:31:03.959: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8160
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-8160
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 10 10:31:04.207: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 10 10:31:24.626: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.0.187:8080/dial?request=hostName&protocol=udp&host=100.96.1.39&port=8081&tries=1'] Namespace:pod-network-test-8160 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 10:31:24.626: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 10:31:25.168: INFO: Waiting for endpoints: map[]
Apr 10 10:31:25.191: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.0.187:8080/dial?request=hostName&protocol=udp&host=100.96.0.186&port=8081&tries=1'] Namespace:pod-network-test-8160 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 10:31:25.191: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
Apr 10 10:31:25.760: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:31:25.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8160" for this suite.
Apr 10 10:31:47.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:31:48.601: INFO: namespace pod-network-test-8160 deletion completed in 22.819083352s
•S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:31:48.601: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4421
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-d8de768a-5b7b-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume secrets
Apr 10 10:31:48.950: INFO: Waiting up to 5m0s for pod "pod-secrets-d8e187e5-5b7b-11e9-bf19-c208aa3c4721" in namespace "secrets-4421" to be "success or failure"
Apr 10 10:31:48.970: INFO: Pod "pod-secrets-d8e187e5-5b7b-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.870469ms
Apr 10 10:31:50.990: INFO: Pod "pod-secrets-d8e187e5-5b7b-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040321001s
STEP: Saw pod success
Apr 10 10:31:50.990: INFO: Pod "pod-secrets-d8e187e5-5b7b-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:31:51.015: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-secrets-d8e187e5-5b7b-11e9-bf19-c208aa3c4721 container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 10:31:51.070: INFO: Waiting for pod pod-secrets-d8e187e5-5b7b-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:31:51.089: INFO: Pod pod-secrets-d8e187e5-5b7b-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:31:51.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4421" for this suite.
Apr 10 10:31:57.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:31:57.946: INFO: namespace secrets-4421 deletion completed in 6.835682992s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:31:57.946: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2371
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
Apr 10 10:31:58.199: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config create -f - --namespace=kubectl-2371'
Apr 10 10:31:58.825: INFO: stderr: ""
Apr 10 10:31:58.825: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Apr 10 10:31:59.847: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 10:31:59.847: INFO: Found 0 / 1
Apr 10 10:32:00.853: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 10:32:00.853: INFO: Found 1 / 1
Apr 10 10:32:00.853: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 10 10:32:00.873: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 10:32:00.873: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Apr 10 10:32:00.873: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config logs redis-master-qnnbn redis-master --namespace=kubectl-2371'
Apr 10 10:32:01.211: INFO: stderr: ""
Apr 10 10:32:01.211: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 10 Apr 10:31:59.620 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 10 Apr 10:31:59.620 # Server started, Redis version 3.2.12\n1:M 10 Apr 10:31:59.620 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 10 Apr 10:31:59.620 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Apr 10 10:32:01.211: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config log redis-master-qnnbn redis-master --namespace=kubectl-2371 --tail=1'
Apr 10 10:32:01.445: INFO: stderr: ""
Apr 10 10:32:01.445: INFO: stdout: "1:M 10 Apr 10:31:59.620 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Apr 10 10:32:01.445: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config log redis-master-qnnbn redis-master --namespace=kubectl-2371 --limit-bytes=1'
Apr 10 10:32:01.692: INFO: stderr: ""
Apr 10 10:32:01.692: INFO: stdout: " "
STEP: exposing timestamps
Apr 10 10:32:01.692: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config log redis-master-qnnbn redis-master --namespace=kubectl-2371 --tail=1 --timestamps'
Apr 10 10:32:01.916: INFO: stderr: ""
Apr 10 10:32:01.916: INFO: stdout: "2019-04-10T10:31:59.621111767Z 1:M 10 Apr 10:31:59.620 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Apr 10 10:32:04.416: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config log redis-master-qnnbn redis-master --namespace=kubectl-2371 --since=1s'
Apr 10 10:32:04.620: INFO: stderr: ""
Apr 10 10:32:04.620: INFO: stdout: ""
Apr 10 10:32:04.621: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config log redis-master-qnnbn redis-master --namespace=kubectl-2371 --since=24h'
Apr 10 10:32:04.839: INFO: stderr: ""
Apr 10 10:32:04.839: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 10 Apr 10:31:59.620 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 10 Apr 10:31:59.620 # Server started, Redis version 3.2.12\n1:M 10 Apr 10:31:59.620 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 10 Apr 10:31:59.620 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
Apr 10 10:32:04.839: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-2371'
Apr 10 10:32:05.093: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 10:32:05.093: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Apr 10 10:32:05.093: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get rc,svc -l name=nginx --no-headers --namespace=kubectl-2371'
Apr 10 10:32:05.309: INFO: stderr: "No resources found.\n"
Apr 10 10:32:05.309: INFO: stdout: ""
Apr 10 10:32:05.309: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods -l name=nginx --namespace=kubectl-2371 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 10 10:32:05.487: INFO: stderr: ""
Apr 10 10:32:05.487: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:32:05.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2371" for this suite.
Apr 10 10:32:27.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:32:28.368: INFO: namespace kubectl-2371 deletion completed in 22.850016197s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:32:28.368: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8047
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Apr 10 10:32:28.603: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config cluster-info'
Apr 10 10:32:28.873: INFO: stderr: ""
Apr 10 10:32:28.873: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.tm-s27nv.it.internal.dev.k8s.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.tm-s27nv.it.internal.dev.k8s.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.tm-s27nv.it.internal.dev.k8s.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:32:28.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8047" for this suite.
Apr 10 10:32:34.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:32:35.780: INFO: namespace kubectl-8047 deletion completed in 6.882104488s
•
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:32:35.780: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5106
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-f4f29741-5b7b-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume configMaps
Apr 10 10:32:36.062: INFO: Waiting up to 5m0s for pod "pod-configmaps-f4f61af1-5b7b-11e9-bf19-c208aa3c4721" in namespace "configmap-5106" to be "success or failure"
Apr 10 10:32:36.082: INFO: Pod "pod-configmaps-f4f61af1-5b7b-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.974173ms
Apr 10 10:32:38.103: INFO: Pod "pod-configmaps-f4f61af1-5b7b-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040779808s
STEP: Saw pod success
Apr 10 10:32:38.103: INFO: Pod "pod-configmaps-f4f61af1-5b7b-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:32:38.123: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-configmaps-f4f61af1-5b7b-11e9-bf19-c208aa3c4721 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 10:32:38.173: INFO: Waiting for pod pod-configmaps-f4f61af1-5b7b-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:32:38.193: INFO: Pod pod-configmaps-f4f61af1-5b7b-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:32:38.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5106" for this suite.
Apr 10 10:32:44.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:32:45.040: INFO: namespace configmap-5106 deletion completed in 6.825854556s
•SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:32:45.040: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2302
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-2302
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Apr 10 10:32:45.361: INFO: Found 1 stateful pods, waiting for 3
Apr 10 10:32:55.383: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 10:32:55.383: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 10:32:55.383: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Apr 10 10:32:55.497: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Apr 10 10:32:55.593: INFO: Updating stateful set ss2
Apr 10 10:32:55.637: INFO: Waiting for Pod statefulset-2302/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Apr 10 10:33:05.758: INFO: Found 2 stateful pods, waiting for 3
Apr 10 10:33:15.780: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 10:33:15.780: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 10:33:15.781: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Apr 10 10:33:15.878: INFO: Updating stateful set ss2
Apr 10 10:33:16.041: INFO: Waiting for Pod statefulset-2302/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 10:33:26.134: INFO: Updating stateful set ss2
Apr 10 10:33:26.179: INFO: Waiting for StatefulSet statefulset-2302/ss2 to complete update
Apr 10 10:33:26.180: INFO: Waiting for Pod statefulset-2302/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 10 10:33:36.225: INFO: Deleting all statefulset in ns statefulset-2302
Apr 10 10:33:36.245: INFO: Scaling statefulset ss2 to 0
Apr 10 10:33:56.331: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 10:33:56.356: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:33:56.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2302" for this suite.
Apr 10 10:34:02.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:34:03.293: INFO: namespace statefulset-2302 deletion completed in 6.853755501s
•SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:34:03.293: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5551
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:34:25.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5551" for this suite.
Apr 10 10:34:31.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:34:32.491: INFO: namespace container-runtime-5551 deletion completed in 6.831084403s
•SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:34:32.491: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4494
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4494.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4494.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 10 10:34:35.543: INFO: DNS probes using dns-4494/dns-test-3a8e12d7-5b7c-11e9-bf19-c208aa3c4721 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:34:35.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4494" for this suite.
Apr 10 10:34:41.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:34:42.446: INFO: namespace dns-4494 deletion completed in 6.851378726s
•SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:34:42.446: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1415
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:34:44.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1415" for this suite.
Apr 10 10:35:22.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:35:23.685: INFO: namespace kubelet-test-1415 deletion completed in 38.879625891s
•SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:35:23.685: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1234
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-m8h8
STEP: Creating a pod to test atomic-volume-subpath
Apr 10 10:35:24.072: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-m8h8" in namespace "subpath-1234" to be "success or failure"
Apr 10 10:35:24.097: INFO: Pod "pod-subpath-test-projected-m8h8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.835445ms
Apr 10 10:35:26.134: INFO: Pod "pod-subpath-test-projected-m8h8": Phase="Running", Reason="", readiness=true. Elapsed: 2.062160266s
Apr 10 10:35:28.155: INFO: Pod "pod-subpath-test-projected-m8h8": Phase="Running", Reason="", readiness=true. Elapsed: 4.082919821s
Apr 10 10:35:30.176: INFO: Pod "pod-subpath-test-projected-m8h8": Phase="Running", Reason="", readiness=true. Elapsed: 6.103888461s
Apr 10 10:35:32.197: INFO: Pod "pod-subpath-test-projected-m8h8": Phase="Running", Reason="", readiness=true. Elapsed: 8.124765732s
Apr 10 10:35:34.218: INFO: Pod "pod-subpath-test-projected-m8h8": Phase="Running", Reason="", readiness=true. Elapsed: 10.146466875s
Apr 10 10:35:36.240: INFO: Pod "pod-subpath-test-projected-m8h8": Phase="Running", Reason="", readiness=true. Elapsed: 12.168326083s
Apr 10 10:35:38.263: INFO: Pod "pod-subpath-test-projected-m8h8": Phase="Running", Reason="", readiness=true. Elapsed: 14.191215489s
Apr 10 10:35:40.284: INFO: Pod "pod-subpath-test-projected-m8h8": Phase="Running", Reason="", readiness=true. Elapsed: 16.21219898s
Apr 10 10:35:42.311: INFO: Pod "pod-subpath-test-projected-m8h8": Phase="Running", Reason="", readiness=true. Elapsed: 18.239158411s
Apr 10 10:35:44.332: INFO: Pod "pod-subpath-test-projected-m8h8": Phase="Running", Reason="", readiness=true. Elapsed: 20.260363042s
Apr 10 10:35:46.356: INFO: Pod "pod-subpath-test-projected-m8h8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.283992494s
STEP: Saw pod success
Apr 10 10:35:46.356: INFO: Pod "pod-subpath-test-projected-m8h8" satisfied condition "success or failure"
Apr 10 10:35:46.376: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-subpath-test-projected-m8h8 container test-container-subpath-projected-m8h8: <nil>
STEP: delete the pod
Apr 10 10:35:46.431: INFO: Waiting for pod pod-subpath-test-projected-m8h8 to disappear
Apr 10 10:35:46.452: INFO: Pod pod-subpath-test-projected-m8h8 no longer exists
STEP: Deleting pod pod-subpath-test-projected-m8h8
Apr 10 10:35:46.452: INFO: Deleting pod "pod-subpath-test-projected-m8h8" in namespace "subpath-1234"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:35:46.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1234" for this suite.
Apr 10 10:35:52.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:35:53.326: INFO: namespace subpath-1234 deletion completed in 6.833565487s
•SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:35:53.327: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5825
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 10 10:35:53.623: INFO: Waiting up to 5m0s for pod "pod-6ab77cfc-5b7c-11e9-bf19-c208aa3c4721" in namespace "emptydir-5825" to be "success or failure"
Apr 10 10:35:53.646: INFO: Pod "pod-6ab77cfc-5b7c-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 22.713608ms
Apr 10 10:35:55.666: INFO: Pod "pod-6ab77cfc-5b7c-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043409923s
STEP: Saw pod success
Apr 10 10:35:55.666: INFO: Pod "pod-6ab77cfc-5b7c-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:35:55.686: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-6ab77cfc-5b7c-11e9-bf19-c208aa3c4721 container test-container: <nil>
STEP: delete the pod
Apr 10 10:35:55.737: INFO: Waiting for pod pod-6ab77cfc-5b7c-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:35:55.757: INFO: Pod pod-6ab77cfc-5b7c-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:35:55.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5825" for this suite.
Apr 10 10:36:01.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:36:02.631: INFO: namespace emptydir-5825 deletion completed in 6.853217s
•SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:36:02.631: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6579
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-703c86e9-5b7c-11e9-bf19-c208aa3c4721
Apr 10 10:36:02.902: INFO: Pod name my-hostname-basic-703c86e9-5b7c-11e9-bf19-c208aa3c4721: Found 1 pods out of 1
Apr 10 10:36:02.903: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-703c86e9-5b7c-11e9-bf19-c208aa3c4721" are running
Apr 10 10:36:04.951: INFO: Pod "my-hostname-basic-703c86e9-5b7c-11e9-bf19-c208aa3c4721-jtxls" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 10:36:02 +0000 UTC Reason: Message:}])
Apr 10 10:36:04.952: INFO: Trying to dial the pod
Apr 10 10:36:10.069: INFO: Controller my-hostname-basic-703c86e9-5b7c-11e9-bf19-c208aa3c4721: Got expected result from replica 1 [my-hostname-basic-703c86e9-5b7c-11e9-bf19-c208aa3c4721-jtxls]: "my-hostname-basic-703c86e9-5b7c-11e9-bf19-c208aa3c4721-jtxls", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:36:10.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6579" for this suite.
Apr 10 10:36:16.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:36:16.923: INFO: namespace replication-controller-6579 deletion completed in 6.833124864s
•SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:36:16.923: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7481
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 10:36:17.225: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78c8e7d5-5b7c-11e9-bf19-c208aa3c4721" in namespace "projected-7481" to be "success or failure"
Apr 10 10:36:17.245: INFO: Pod "downwardapi-volume-78c8e7d5-5b7c-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.68892ms
Apr 10 10:36:19.268: INFO: Pod "downwardapi-volume-78c8e7d5-5b7c-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042454691s
STEP: Saw pod success
Apr 10 10:36:19.268: INFO: Pod "downwardapi-volume-78c8e7d5-5b7c-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:36:19.288: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downwardapi-volume-78c8e7d5-5b7c-11e9-bf19-c208aa3c4721 container client-container: <nil>
STEP: delete the pod
Apr 10 10:36:19.342: INFO: Waiting for pod downwardapi-volume-78c8e7d5-5b7c-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:36:19.362: INFO: Pod downwardapi-volume-78c8e7d5-5b7c-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:36:19.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7481" for this suite.
Apr 10 10:36:25.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:36:26.215: INFO: namespace projected-7481 deletion completed in 6.825987194s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:36:26.216: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8405
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-7e53ba30-5b7c-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume secrets
Apr 10 10:36:26.546: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7e56ef93-5b7c-11e9-bf19-c208aa3c4721" in namespace "projected-8405" to be "success or failure"
Apr 10 10:36:26.565: INFO: Pod "pod-projected-secrets-7e56ef93-5b7c-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.34045ms
Apr 10 10:36:28.586: INFO: Pod "pod-projected-secrets-7e56ef93-5b7c-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039905546s
STEP: Saw pod success
Apr 10 10:36:28.586: INFO: Pod "pod-projected-secrets-7e56ef93-5b7c-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:36:28.609: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-projected-secrets-7e56ef93-5b7c-11e9-bf19-c208aa3c4721 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 10 10:36:28.661: INFO: Waiting for pod pod-projected-secrets-7e56ef93-5b7c-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:36:28.680: INFO: Pod pod-projected-secrets-7e56ef93-5b7c-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:36:28.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8405" for this suite.
Apr 10 10:36:34.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:36:35.549: INFO: namespace projected-8405 deletion completed in 6.84745725s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:36:35.549: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:37:35.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5647" for this suite.
Apr 10 10:37:57.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:37:58.724: INFO: namespace container-probe-5647 deletion completed in 22.847390389s
•SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:37:58.725: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9800
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Apr 10 10:37:58.947: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Apr 10 10:37:58.947: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config create -f - --namespace=kubectl-9800'
Apr 10 10:37:59.351: INFO: stderr: ""
Apr 10 10:37:59.351: INFO: stdout: "service/redis-slave created\n"
Apr 10 10:37:59.351: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Apr 10 10:37:59.351: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config create -f - --namespace=kubectl-9800'
Apr 10 10:37:59.747: INFO: stderr: ""
Apr 10 10:37:59.747: INFO: stdout: "service/redis-master created\n"
Apr 10 10:37:59.747: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr 10 10:37:59.747: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config create -f - --namespace=kubectl-9800'
Apr 10 10:38:00.140: INFO: stderr: ""
Apr 10 10:38:00.140: INFO: stdout: "service/frontend created\n"
Apr 10 10:38:00.140: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Apr 10 10:38:00.140: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config create -f - --namespace=kubectl-9800'
Apr 10 10:38:00.540: INFO: stderr: ""
Apr 10 10:38:00.540: INFO: stdout: "deployment.apps/frontend created\n"
Apr 10 10:38:00.541: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 10 10:38:00.541: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config create -f - --namespace=kubectl-9800'
Apr 10 10:38:00.937: INFO: stderr: ""
Apr 10 10:38:00.938: INFO: stdout: "deployment.apps/redis-master created\n"
Apr 10 10:38:00.938: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Apr 10 10:38:00.938: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config create -f - --namespace=kubectl-9800'
Apr 10 10:38:01.349: INFO: stderr: ""
Apr 10 10:38:01.349: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Apr 10 10:38:01.349: INFO: Waiting for all frontend pods to be Running.
Apr 10 10:38:26.401: INFO: Waiting for frontend to serve content.
Apr 10 10:38:26.511: INFO: Trying to add a new entry to the guestbook.
Apr 10 10:38:26.560: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Apr 10 10:38:26.671: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-9800'
Apr 10 10:38:26.926: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 10:38:26.926: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Apr 10 10:38:26.926: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-9800'
Apr 10 10:38:27.125: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 10:38:27.125: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Apr 10 10:38:27.125: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-9800'
Apr 10 10:38:27.327: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 10:38:27.327: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 10 10:38:27.328: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-9800'
Apr 10 10:38:27.524: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 10:38:27.524: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 10 10:38:27.524: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-9800'
Apr 10 10:38:27.727: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 10:38:27.727: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Apr 10 10:38:27.727: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-9800'
Apr 10 10:38:27.948: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 10:38:27.948: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:38:27.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9800" for this suite.
Apr 10 10:39:06.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:39:06.817: INFO: namespace kubectl-9800 deletion completed in 38.846752577s
•SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:39:06.817: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8522
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-de0e1973-5b7c-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume secrets
Apr 10 10:39:07.150: INFO: Waiting up to 5m0s for pod "pod-secrets-de118b21-5b7c-11e9-bf19-c208aa3c4721" in namespace "secrets-8522" to be "success or failure"
Apr 10 10:39:07.170: INFO: Pod "pod-secrets-de118b21-5b7c-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.804281ms
Apr 10 10:39:09.192: INFO: Pod "pod-secrets-de118b21-5b7c-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042139296s
STEP: Saw pod success
Apr 10 10:39:09.193: INFO: Pod "pod-secrets-de118b21-5b7c-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:39:09.213: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-secrets-de118b21-5b7c-11e9-bf19-c208aa3c4721 container secret-env-test: <nil>
STEP: delete the pod
Apr 10 10:39:09.263: INFO: Waiting for pod pod-secrets-de118b21-5b7c-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:39:09.283: INFO: Pod pod-secrets-de118b21-5b7c-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:39:09.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8522" for this suite.
Apr 10 10:39:15.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:39:16.150: INFO: namespace secrets-8522 deletion completed in 6.843475006s
•SSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:39:16.150: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1120
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 10:39:16.406: INFO: Creating deployment "test-recreate-deployment"
Apr 10 10:39:16.426: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr 10 10:39:16.468: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr 10 10:39:16.488: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690489556, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690489556, loc:(*time.Location)(0x87c9f80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690489556, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690489556, loc:(*time.Location)(0x87c9f80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 10:39:18.514: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr 10 10:39:18.558: INFO: Updating deployment test-recreate-deployment
Apr 10 10:39:18.558: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 10 10:39:18.601: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-1120,SelfLink:/apis/apps/v1/namespaces/deployment-1120/deployments/test-recreate-deployment,UID:e39b1ee3-5b7c-11e9-a517-0202de25e2de,ResourceVersion:19148,Generation:2,CreationTimestamp:2019-04-10 10:39:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-04-10 10:39:18 +0000 UTC 2019-04-10 10:39:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-04-10 10:39:18 +0000 UTC 2019-04-10 10:39:16 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Apr 10 10:39:18.622: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-1120,SelfLink:/apis/apps/v1/namespaces/deployment-1120/replicasets/test-recreate-deployment-c9cbd8684,UID:e4e22a16-5b7c-11e9-a517-0202de25e2de,ResourceVersion:19147,Generation:1,CreationTimestamp:2019-04-10 10:39:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment e39b1ee3-5b7c-11e9-a517-0202de25e2de 0xc0039da2b0 0xc0039da2b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 10:39:18.622: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr 10 10:39:18.622: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-1120,SelfLink:/apis/apps/v1/namespaces/deployment-1120/replicasets/test-recreate-deployment-7d57d5ff7c,UID:e39b826a-5b7c-11e9-a517-0202de25e2de,ResourceVersion:19140,Generation:2,CreationTimestamp:2019-04-10 10:39:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment e39b1ee3-5b7c-11e9-a517-0202de25e2de 0xc0039da1e7 0xc0039da1e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 10:39:18.643: INFO: Pod "test-recreate-deployment-c9cbd8684-gkqj2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-gkqj2,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-1120,SelfLink:/api/v1/namespaces/deployment-1120/pods/test-recreate-deployment-c9cbd8684-gkqj2,UID:e4e2b094-5b7c-11e9-a517-0202de25e2de,ResourceVersion:19149,Generation:0,CreationTimestamp:2019-04-10 10:39:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 e4e22a16-5b7c-11e9-a517-0202de25e2de 0xc0039daad0 0xc0039daad1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-v48xn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v48xn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-v48xn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0039dab30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0039dab50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:39:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:39:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:39:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:39:18 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.152,PodIP:,StartTime:2019-04-10 10:39:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:39:18.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1120" for this suite.
Apr 10 10:39:24.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:39:25.491: INFO: namespace deployment-1120 deletion completed in 6.824718464s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:39:25.491: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4368
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-e932adf6-5b7c-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume configMaps
Apr 10 10:39:25.842: INFO: Waiting up to 5m0s for pod "pod-configmaps-e935bf4c-5b7c-11e9-bf19-c208aa3c4721" in namespace "configmap-4368" to be "success or failure"
Apr 10 10:39:25.862: INFO: Pod "pod-configmaps-e935bf4c-5b7c-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.740902ms
Apr 10 10:39:27.885: INFO: Pod "pod-configmaps-e935bf4c-5b7c-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042960055s
STEP: Saw pod success
Apr 10 10:39:27.885: INFO: Pod "pod-configmaps-e935bf4c-5b7c-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:39:27.906: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-configmaps-e935bf4c-5b7c-11e9-bf19-c208aa3c4721 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 10:39:27.959: INFO: Waiting for pod pod-configmaps-e935bf4c-5b7c-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:39:27.979: INFO: Pod pod-configmaps-e935bf4c-5b7c-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:39:27.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4368" for this suite.
Apr 10 10:39:34.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:39:34.826: INFO: namespace configmap-4368 deletion completed in 6.82466471s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:39:34.827: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7417
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 10:39:35.210: INFO: Create a RollingUpdate DaemonSet
Apr 10 10:39:35.230: INFO: Check that daemon pods launch on every node of the cluster
Apr 10 10:39:35.272: INFO: Number of nodes with available pods: 0
Apr 10 10:39:35.272: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:39:36.313: INFO: Number of nodes with available pods: 0
Apr 10 10:39:36.313: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:39:37.316: INFO: Number of nodes with available pods: 2
Apr 10 10:39:37.316: INFO: Number of running nodes: 2, number of available pods: 2
Apr 10 10:39:37.316: INFO: Update the DaemonSet to trigger a rollout
Apr 10 10:39:37.357: INFO: Updating DaemonSet daemon-set
Apr 10 10:39:43.427: INFO: Roll back the DaemonSet before rollout is complete
Apr 10 10:39:43.470: INFO: Updating DaemonSet daemon-set
Apr 10 10:39:43.470: INFO: Make sure DaemonSet rollback is complete
Apr 10 10:39:43.492: INFO: Wrong image for pod: daemon-set-frfbc. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr 10 10:39:43.492: INFO: Pod daemon-set-frfbc is not available
Apr 10 10:39:44.538: INFO: Wrong image for pod: daemon-set-frfbc. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr 10 10:39:44.538: INFO: Pod daemon-set-frfbc is not available
Apr 10 10:39:45.541: INFO: Wrong image for pod: daemon-set-frfbc. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr 10 10:39:45.541: INFO: Pod daemon-set-frfbc is not available
Apr 10 10:39:46.539: INFO: Wrong image for pod: daemon-set-frfbc. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr 10 10:39:46.539: INFO: Pod daemon-set-frfbc is not available
Apr 10 10:39:47.536: INFO: Wrong image for pod: daemon-set-frfbc. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr 10 10:39:47.536: INFO: Pod daemon-set-frfbc is not available
Apr 10 10:39:48.540: INFO: Wrong image for pod: daemon-set-frfbc. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr 10 10:39:48.540: INFO: Pod daemon-set-frfbc is not available
Apr 10 10:39:49.536: INFO: Wrong image for pod: daemon-set-frfbc. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr 10 10:39:49.536: INFO: Pod daemon-set-frfbc is not available
Apr 10 10:39:50.540: INFO: Wrong image for pod: daemon-set-frfbc. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr 10 10:39:50.540: INFO: Pod daemon-set-frfbc is not available
Apr 10 10:39:51.540: INFO: Wrong image for pod: daemon-set-frfbc. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr 10 10:39:51.540: INFO: Pod daemon-set-frfbc is not available
Apr 10 10:39:52.537: INFO: Wrong image for pod: daemon-set-frfbc. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr 10 10:39:52.537: INFO: Pod daemon-set-frfbc is not available
Apr 10 10:39:53.541: INFO: Pod daemon-set-r9lhw is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7417, will wait for the garbage collector to delete the pods
Apr 10 10:39:53.695: INFO: Deleting DaemonSet.extensions daemon-set took: 21.178046ms
Apr 10 10:39:53.796: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.382728ms
Apr 10 10:40:04.718: INFO: Number of nodes with available pods: 0
Apr 10 10:40:04.718: INFO: Number of running nodes: 0, number of available pods: 0
Apr 10 10:40:04.743: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7417/daemonsets","resourceVersion":"19335"},"items":null}

Apr 10 10:40:04.766: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7417/pods","resourceVersion":"19335"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:40:04.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7417" for this suite.
Apr 10 10:40:10.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:40:11.714: INFO: namespace daemonsets-7417 deletion completed in 6.860609021s
•SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:40:11.715: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8062
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-04bbb6d5-5b7d-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume secrets
Apr 10 10:40:12.040: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-04bee4c9-5b7d-11e9-bf19-c208aa3c4721" in namespace "projected-8062" to be "success or failure"
Apr 10 10:40:12.063: INFO: Pod "pod-projected-secrets-04bee4c9-5b7d-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 22.835352ms
Apr 10 10:40:14.084: INFO: Pod "pod-projected-secrets-04bee4c9-5b7d-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044044234s
STEP: Saw pod success
Apr 10 10:40:14.084: INFO: Pod "pod-projected-secrets-04bee4c9-5b7d-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:40:14.108: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-projected-secrets-04bee4c9-5b7d-11e9-bf19-c208aa3c4721 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 10 10:40:14.156: INFO: Waiting for pod pod-projected-secrets-04bee4c9-5b7d-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:40:14.175: INFO: Pod pod-projected-secrets-04bee4c9-5b7d-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:40:14.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8062" for this suite.
Apr 10 10:40:20.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:40:21.037: INFO: namespace projected-8062 deletion completed in 6.84050885s
•SSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:40:21.037: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-118
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-118
Apr 10 10:40:23.370: INFO: Started pod liveness-exec in namespace container-probe-118
STEP: checking the pod's current state and verifying that restartCount is present
Apr 10 10:40:23.391: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:44:24.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-118" for this suite.
Apr 10 10:44:30.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:44:30.958: INFO: namespace container-probe-118 deletion completed in 6.829108137s
•SS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:44:30.959: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6524
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Apr 10 10:44:33.300: INFO: Pod pod-hostip-9f3a55a0-5b7d-11e9-bf19-c208aa3c4721 has hostIP: 10.250.14.152
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:44:33.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6524" for this suite.
Apr 10 10:44:55.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:44:56.154: INFO: namespace pods-6524 deletion completed in 22.833125349s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:44:56.155: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8931
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 10:44:56.430: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ae418b26-5b7d-11e9-bf19-c208aa3c4721" in namespace "downward-api-8931" to be "success or failure"
Apr 10 10:44:56.450: INFO: Pod "downwardapi-volume-ae418b26-5b7d-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016572ms
Apr 10 10:44:58.471: INFO: Pod "downwardapi-volume-ae418b26-5b7d-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040477142s
STEP: Saw pod success
Apr 10 10:44:58.471: INFO: Pod "downwardapi-volume-ae418b26-5b7d-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:44:58.491: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downwardapi-volume-ae418b26-5b7d-11e9-bf19-c208aa3c4721 container client-container: <nil>
STEP: delete the pod
Apr 10 10:44:58.544: INFO: Waiting for pod downwardapi-volume-ae418b26-5b7d-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:44:58.568: INFO: Pod downwardapi-volume-ae418b26-5b7d-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:44:58.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8931" for this suite.
Apr 10 10:45:04.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:45:05.450: INFO: namespace downward-api-8931 deletion completed in 6.858237578s
•SSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:45:05.450: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2073
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:45:05.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2073" for this suite.
Apr 10 10:45:11.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:45:12.595: INFO: namespace services-2073 deletion completed in 6.852788161s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:45:12.595: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2894
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 10:45:12.926: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b81687ae-5b7d-11e9-bf19-c208aa3c4721" in namespace "downward-api-2894" to be "success or failure"
Apr 10 10:45:12.946: INFO: Pod "downwardapi-volume-b81687ae-5b7d-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.568012ms
Apr 10 10:45:14.970: INFO: Pod "downwardapi-volume-b81687ae-5b7d-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.04396317s
STEP: Saw pod success
Apr 10 10:45:14.970: INFO: Pod "downwardapi-volume-b81687ae-5b7d-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:45:14.992: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downwardapi-volume-b81687ae-5b7d-11e9-bf19-c208aa3c4721 container client-container: <nil>
STEP: delete the pod
Apr 10 10:45:15.045: INFO: Waiting for pod downwardapi-volume-b81687ae-5b7d-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:45:15.069: INFO: Pod downwardapi-volume-b81687ae-5b7d-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:45:15.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2894" for this suite.
Apr 10 10:45:21.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:45:21.946: INFO: namespace downward-api-2894 deletion completed in 6.853712979s
•SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:45:21.947: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3380
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 10 10:45:22.219: INFO: Waiting up to 5m0s for pod "pod-bda076c6-5b7d-11e9-bf19-c208aa3c4721" in namespace "emptydir-3380" to be "success or failure"
Apr 10 10:45:22.242: INFO: Pod "pod-bda076c6-5b7d-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 22.321659ms
Apr 10 10:45:24.263: INFO: Pod "pod-bda076c6-5b7d-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043461398s
STEP: Saw pod success
Apr 10 10:45:24.263: INFO: Pod "pod-bda076c6-5b7d-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:45:24.288: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-bda076c6-5b7d-11e9-bf19-c208aa3c4721 container test-container: <nil>
STEP: delete the pod
Apr 10 10:45:24.350: INFO: Waiting for pod pod-bda076c6-5b7d-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:45:24.373: INFO: Pod pod-bda076c6-5b7d-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:45:24.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3380" for this suite.
Apr 10 10:45:30.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:45:31.232: INFO: namespace emptydir-3380 deletion completed in 6.831298829s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:45:31.233: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7877
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-c32643a5-5b7d-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume configMaps
Apr 10 10:45:31.505: INFO: Waiting up to 5m0s for pod "pod-configmaps-c3295540-5b7d-11e9-bf19-c208aa3c4721" in namespace "configmap-7877" to be "success or failure"
Apr 10 10:45:31.524: INFO: Pod "pod-configmaps-c3295540-5b7d-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.610856ms
Apr 10 10:45:33.545: INFO: Pod "pod-configmaps-c3295540-5b7d-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040336714s
STEP: Saw pod success
Apr 10 10:45:33.545: INFO: Pod "pod-configmaps-c3295540-5b7d-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:45:33.566: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-configmaps-c3295540-5b7d-11e9-bf19-c208aa3c4721 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 10:45:33.624: INFO: Waiting for pod pod-configmaps-c3295540-5b7d-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:45:33.645: INFO: Pod pod-configmaps-c3295540-5b7d-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:45:33.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7877" for this suite.
Apr 10 10:45:39.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:45:40.502: INFO: namespace configmap-7877 deletion completed in 6.836569038s
•
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:45:40.502: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3338
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6902
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-606
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:46:05.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3338" for this suite.
Apr 10 10:46:11.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:46:12.373: INFO: namespace namespaces-3338 deletion completed in 6.849174332s
STEP: Destroying namespace "nsdeletetest-6902" for this suite.
Apr 10 10:46:12.393: INFO: Namespace nsdeletetest-6902 was already deleted
STEP: Destroying namespace "nsdeletetest-606" for this suite.
Apr 10 10:46:18.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:46:19.229: INFO: namespace nsdeletetest-606 deletion completed in 6.835653251s
•SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:46:19.229: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:46:23.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9112" for this suite.
Apr 10 10:46:29.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:46:30.418: INFO: namespace kubelet-test-9112 deletion completed in 6.835731688s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:46:30.419: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1384
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-e66fcd81-5b7d-11e9-bf19-c208aa3c4721
STEP: Creating configMap with name cm-test-opt-upd-e66fcdc4-5b7d-11e9-bf19-c208aa3c4721
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e66fcd81-5b7d-11e9-bf19-c208aa3c4721
STEP: Updating configmap cm-test-opt-upd-e66fcdc4-5b7d-11e9-bf19-c208aa3c4721
STEP: Creating configMap with name cm-test-opt-create-e66fcdd7-5b7d-11e9-bf19-c208aa3c4721
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:47:42.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1384" for this suite.
Apr 10 10:48:04.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:48:05.184: INFO: namespace configmap-1384 deletion completed in 22.848146416s
•SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:48:05.184: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-947
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-1ee925ab-5b7e-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume secrets
Apr 10 10:48:05.458: INFO: Waiting up to 5m0s for pod "pod-secrets-1eecba50-5b7e-11e9-bf19-c208aa3c4721" in namespace "secrets-947" to be "success or failure"
Apr 10 10:48:05.477: INFO: Pod "pod-secrets-1eecba50-5b7e-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.491315ms
Apr 10 10:48:07.498: INFO: Pod "pod-secrets-1eecba50-5b7e-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039891186s
STEP: Saw pod success
Apr 10 10:48:07.498: INFO: Pod "pod-secrets-1eecba50-5b7e-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:48:07.518: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-secrets-1eecba50-5b7e-11e9-bf19-c208aa3c4721 container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 10:48:07.568: INFO: Waiting for pod pod-secrets-1eecba50-5b7e-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:48:07.589: INFO: Pod pod-secrets-1eecba50-5b7e-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:48:07.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-947" for this suite.
Apr 10 10:48:13.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:48:14.449: INFO: namespace secrets-947 deletion completed in 6.837106363s
•SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:48:14.450: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9106
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-2472cbc2-5b7e-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume configMaps
Apr 10 10:48:14.744: INFO: Waiting up to 5m0s for pod "pod-configmaps-2475d6d9-5b7e-11e9-bf19-c208aa3c4721" in namespace "configmap-9106" to be "success or failure"
Apr 10 10:48:14.764: INFO: Pod "pod-configmaps-2475d6d9-5b7e-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.493251ms
Apr 10 10:48:16.787: INFO: Pod "pod-configmaps-2475d6d9-5b7e-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043112509s
STEP: Saw pod success
Apr 10 10:48:16.787: INFO: Pod "pod-configmaps-2475d6d9-5b7e-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:48:16.807: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-configmaps-2475d6d9-5b7e-11e9-bf19-c208aa3c4721 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 10:48:16.861: INFO: Waiting for pod pod-configmaps-2475d6d9-5b7e-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:48:16.881: INFO: Pod pod-configmaps-2475d6d9-5b7e-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:48:16.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9106" for this suite.
Apr 10 10:48:22.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:48:23.724: INFO: namespace configmap-9106 deletion completed in 6.823439433s
•SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:48:23.725: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1446
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr 10 10:48:23.997: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 10 10:48:24.038: INFO: Waiting for terminating namespaces to be deleted...
Apr 10 10:48:24.061: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-14-152.eu-west-1.compute.internal before test
Apr 10 10:48:24.087: INFO: addons-kube2iam-x6268 from kube-system started at 2019-04-10 09:00:33 +0000 UTC (1 container statuses recorded)
Apr 10 10:48:24.087: INFO: 	Container kube2iam ready: true, restart count 0
Apr 10 10:48:24.087: INFO: blackbox-exporter-6dc58dcffc-w6tfm from kube-system started at 2019-04-10 09:00:13 +0000 UTC (1 container statuses recorded)
Apr 10 10:48:24.087: INFO: 	Container blackbox-exporter ready: true, restart count 0
Apr 10 10:48:24.087: INFO: calico-node-965bj from kube-system started at 2019-04-10 09:00:13 +0000 UTC (1 container statuses recorded)
Apr 10 10:48:24.087: INFO: 	Container calico-node ready: true, restart count 0
Apr 10 10:48:24.087: INFO: node-exporter-wsr4c from kube-system started at 2019-04-10 09:00:13 +0000 UTC (1 container statuses recorded)
Apr 10 10:48:24.087: INFO: 	Container node-exporter ready: true, restart count 0
Apr 10 10:48:24.087: INFO: kube-proxy-t9z66 from kube-system started at 2019-04-10 09:00:13 +0000 UTC (1 container statuses recorded)
Apr 10 10:48:24.087: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 10 10:48:24.087: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-25-25.eu-west-1.compute.internal before test
Apr 10 10:48:24.144: INFO: calico-node-ftwpk from kube-system started at 2019-04-10 09:00:14 +0000 UTC (1 container statuses recorded)
Apr 10 10:48:24.144: INFO: 	Container calico-node ready: true, restart count 0
Apr 10 10:48:24.144: INFO: metrics-server-6b49cb886c-fjp6h from kube-system started at 2019-04-10 09:00:24 +0000 UTC (1 container statuses recorded)
Apr 10 10:48:24.144: INFO: 	Container metrics-server ready: true, restart count 0
Apr 10 10:48:24.144: INFO: vpn-shoot-7b5f8c4995-mmd2b from kube-system started at 2019-04-10 09:00:29 +0000 UTC (1 container statuses recorded)
Apr 10 10:48:24.144: INFO: 	Container vpn-shoot ready: true, restart count 0
Apr 10 10:48:24.144: INFO: addons-nginx-ingress-controller-f88658d78-q6z6w from kube-system started at 2019-04-10 09:00:29 +0000 UTC (1 container statuses recorded)
Apr 10 10:48:24.144: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Apr 10 10:48:24.144: INFO: kube-proxy-xk4pl from kube-system started at 2019-04-10 09:00:14 +0000 UTC (1 container statuses recorded)
Apr 10 10:48:24.144: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 10 10:48:24.144: INFO: node-exporter-n6hdz from kube-system started at 2019-04-10 09:00:14 +0000 UTC (1 container statuses recorded)
Apr 10 10:48:24.144: INFO: 	Container node-exporter ready: true, restart count 0
Apr 10 10:48:24.144: INFO: coredns-7f7f7978c8-qqrcn from kube-system started at 2019-04-10 09:00:24 +0000 UTC (1 container statuses recorded)
Apr 10 10:48:24.144: INFO: 	Container coredns ready: true, restart count 0
Apr 10 10:48:24.144: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-57c2j from kube-system started at 2019-04-10 09:00:29 +0000 UTC (1 container statuses recorded)
Apr 10 10:48:24.144: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Apr 10 10:48:24.144: INFO: coredns-7f7f7978c8-2nxbn from kube-system started at 2019-04-10 09:00:24 +0000 UTC (1 container statuses recorded)
Apr 10 10:48:24.144: INFO: 	Container coredns ready: true, restart count 0
Apr 10 10:48:24.144: INFO: addons-kubernetes-dashboard-665df4b66d-vlw97 from kube-system started at 2019-04-10 09:00:24 +0000 UTC (1 container statuses recorded)
Apr 10 10:48:24.144: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr 10 10:48:24.144: INFO: addons-kube2iam-zjj8r from kube-system started at 2019-04-10 09:00:24 +0000 UTC (1 container statuses recorded)
Apr 10 10:48:24.144: INFO: 	Container kube2iam ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node ip-10-250-14-152.eu-west-1.compute.internal
STEP: verifying the node has the label node ip-10-250-25-25.eu-west-1.compute.internal
Apr 10 10:48:24.289: INFO: Pod addons-kube2iam-x6268 requesting resource cpu=10m on Node ip-10-250-14-152.eu-west-1.compute.internal
Apr 10 10:48:24.289: INFO: Pod addons-kube2iam-zjj8r requesting resource cpu=10m on Node ip-10-250-25-25.eu-west-1.compute.internal
Apr 10 10:48:24.289: INFO: Pod addons-kubernetes-dashboard-665df4b66d-vlw97 requesting resource cpu=50m on Node ip-10-250-25-25.eu-west-1.compute.internal
Apr 10 10:48:24.289: INFO: Pod addons-nginx-ingress-controller-f88658d78-q6z6w requesting resource cpu=100m on Node ip-10-250-25-25.eu-west-1.compute.internal
Apr 10 10:48:24.289: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-57c2j requesting resource cpu=0m on Node ip-10-250-25-25.eu-west-1.compute.internal
Apr 10 10:48:24.289: INFO: Pod blackbox-exporter-6dc58dcffc-w6tfm requesting resource cpu=5m on Node ip-10-250-14-152.eu-west-1.compute.internal
Apr 10 10:48:24.289: INFO: Pod calico-node-965bj requesting resource cpu=100m on Node ip-10-250-14-152.eu-west-1.compute.internal
Apr 10 10:48:24.289: INFO: Pod calico-node-ftwpk requesting resource cpu=100m on Node ip-10-250-25-25.eu-west-1.compute.internal
Apr 10 10:48:24.289: INFO: Pod coredns-7f7f7978c8-2nxbn requesting resource cpu=50m on Node ip-10-250-25-25.eu-west-1.compute.internal
Apr 10 10:48:24.289: INFO: Pod coredns-7f7f7978c8-qqrcn requesting resource cpu=50m on Node ip-10-250-25-25.eu-west-1.compute.internal
Apr 10 10:48:24.289: INFO: Pod kube-proxy-t9z66 requesting resource cpu=20m on Node ip-10-250-14-152.eu-west-1.compute.internal
Apr 10 10:48:24.289: INFO: Pod kube-proxy-xk4pl requesting resource cpu=20m on Node ip-10-250-25-25.eu-west-1.compute.internal
Apr 10 10:48:24.289: INFO: Pod metrics-server-6b49cb886c-fjp6h requesting resource cpu=20m on Node ip-10-250-25-25.eu-west-1.compute.internal
Apr 10 10:48:24.289: INFO: Pod node-exporter-n6hdz requesting resource cpu=5m on Node ip-10-250-25-25.eu-west-1.compute.internal
Apr 10 10:48:24.289: INFO: Pod node-exporter-wsr4c requesting resource cpu=5m on Node ip-10-250-14-152.eu-west-1.compute.internal
Apr 10 10:48:24.289: INFO: Pod vpn-shoot-7b5f8c4995-mmd2b requesting resource cpu=50m on Node ip-10-250-25-25.eu-west-1.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2a29b9ee-5b7e-11e9-bf19-c208aa3c4721.15941738dad65cab], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1446/filler-pod-2a29b9ee-5b7e-11e9-bf19-c208aa3c4721 to ip-10-250-14-152.eu-west-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2a29b9ee-5b7e-11e9-bf19-c208aa3c4721.15941739025000cb], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2a29b9ee-5b7e-11e9-bf19-c208aa3c4721.1594173904cb64af], Reason = [Created], Message = [Created container filler-pod-2a29b9ee-5b7e-11e9-bf19-c208aa3c4721]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2a29b9ee-5b7e-11e9-bf19-c208aa3c4721.159417390bd291cf], Reason = [Started], Message = [Started container filler-pod-2a29b9ee-5b7e-11e9-bf19-c208aa3c4721]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2a2d52bb-5b7e-11e9-bf19-c208aa3c4721.15941738dc27c9b9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1446/filler-pod-2a2d52bb-5b7e-11e9-bf19-c208aa3c4721 to ip-10-250-25-25.eu-west-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2a2d52bb-5b7e-11e9-bf19-c208aa3c4721.1594173903c04b05], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2a2d52bb-5b7e-11e9-bf19-c208aa3c4721.1594173925cba1ed], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2a2d52bb-5b7e-11e9-bf19-c208aa3c4721.1594173928972657], Reason = [Created], Message = [Created container filler-pod-2a2d52bb-5b7e-11e9-bf19-c208aa3c4721]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2a2d52bb-5b7e-11e9-bf19-c208aa3c4721.159417392f939992], Reason = [Started], Message = [Started container filler-pod-2a2d52bb-5b7e-11e9-bf19-c208aa3c4721]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15941739d3369b68], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node ip-10-250-14-152.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-250-25-25.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:48:29.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1446" for this suite.
Apr 10 10:48:35.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:48:36.495: INFO: namespace sched-pred-1446 deletion completed in 6.831166422s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:48:36.496: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6835
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 10 10:48:36.974: INFO: Number of nodes with available pods: 0
Apr 10 10:48:36.974: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:48:38.018: INFO: Number of nodes with available pods: 0
Apr 10 10:48:38.018: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:48:39.017: INFO: Number of nodes with available pods: 2
Apr 10 10:48:39.017: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Apr 10 10:48:39.234: INFO: Number of nodes with available pods: 1
Apr 10 10:48:39.234: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:48:40.282: INFO: Number of nodes with available pods: 1
Apr 10 10:48:40.282: INFO: Node ip-10-250-14-152.eu-west-1.compute.internal is running more than one daemon pod
Apr 10 10:48:41.276: INFO: Number of nodes with available pods: 2
Apr 10 10:48:41.276: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6835, will wait for the garbage collector to delete the pods
Apr 10 10:48:41.415: INFO: Deleting DaemonSet.extensions daemon-set took: 21.070456ms
Apr 10 10:48:41.515: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.319158ms
Apr 10 10:48:53.436: INFO: Number of nodes with available pods: 0
Apr 10 10:48:53.436: INFO: Number of running nodes: 0, number of available pods: 0
Apr 10 10:48:53.458: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6835/daemonsets","resourceVersion":"20769"},"items":null}

Apr 10 10:48:53.477: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6835/pods","resourceVersion":"20769"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:48:53.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6835" for this suite.
Apr 10 10:48:59.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:49:00.405: INFO: namespace daemonsets-6835 deletion completed in 6.840881911s
•SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:49:00.405: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6736
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Apr 10 10:49:00.701: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config api-versions'
Apr 10 10:49:00.922: INFO: stderr: ""
Apr 10 10:49:00.922: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:49:00.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6736" for this suite.
Apr 10 10:49:07.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:49:07.772: INFO: namespace kubectl-6736 deletion completed in 6.82869137s
•SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:49:07.772: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7765
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 10:49:08.023: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44379624-5b7e-11e9-bf19-c208aa3c4721" in namespace "projected-7765" to be "success or failure"
Apr 10 10:49:08.043: INFO: Pod "downwardapi-volume-44379624-5b7e-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.59542ms
Apr 10 10:49:10.064: INFO: Pod "downwardapi-volume-44379624-5b7e-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040296616s
STEP: Saw pod success
Apr 10 10:49:10.064: INFO: Pod "downwardapi-volume-44379624-5b7e-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:49:10.086: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downwardapi-volume-44379624-5b7e-11e9-bf19-c208aa3c4721 container client-container: <nil>
STEP: delete the pod
Apr 10 10:49:10.144: INFO: Waiting for pod downwardapi-volume-44379624-5b7e-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:49:10.167: INFO: Pod downwardapi-volume-44379624-5b7e-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:49:10.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7765" for this suite.
Apr 10 10:49:16.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:49:17.019: INFO: namespace projected-7765 deletion completed in 6.828635572s
•SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:49:17.019: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3946
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 10:49:17.327: INFO: Waiting up to 5m0s for pod "downwardapi-volume-49c2ed9b-5b7e-11e9-bf19-c208aa3c4721" in namespace "downward-api-3946" to be "success or failure"
Apr 10 10:49:17.349: INFO: Pod "downwardapi-volume-49c2ed9b-5b7e-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004899ms
Apr 10 10:49:19.371: INFO: Pod "downwardapi-volume-49c2ed9b-5b7e-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043186929s
STEP: Saw pod success
Apr 10 10:49:19.371: INFO: Pod "downwardapi-volume-49c2ed9b-5b7e-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:49:19.392: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downwardapi-volume-49c2ed9b-5b7e-11e9-bf19-c208aa3c4721 container client-container: <nil>
STEP: delete the pod
Apr 10 10:49:19.443: INFO: Waiting for pod downwardapi-volume-49c2ed9b-5b7e-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:49:19.463: INFO: Pod downwardapi-volume-49c2ed9b-5b7e-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:49:19.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3946" for this suite.
Apr 10 10:49:25.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:49:26.307: INFO: namespace downward-api-3946 deletion completed in 6.818653071s
•SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:49:26.308: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4376
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 10 10:49:26.627: INFO: Waiting up to 5m0s for pod "pod-4f4e11cf-5b7e-11e9-bf19-c208aa3c4721" in namespace "emptydir-4376" to be "success or failure"
Apr 10 10:49:26.647: INFO: Pod "pod-4f4e11cf-5b7e-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.071136ms
Apr 10 10:49:28.668: INFO: Pod "pod-4f4e11cf-5b7e-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040467464s
STEP: Saw pod success
Apr 10 10:49:28.668: INFO: Pod "pod-4f4e11cf-5b7e-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:49:28.688: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-4f4e11cf-5b7e-11e9-bf19-c208aa3c4721 container test-container: <nil>
STEP: delete the pod
Apr 10 10:49:28.739: INFO: Waiting for pod pod-4f4e11cf-5b7e-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:49:28.764: INFO: Pod pod-4f4e11cf-5b7e-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:49:28.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4376" for this suite.
Apr 10 10:49:34.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:49:35.616: INFO: namespace emptydir-4376 deletion completed in 6.831984744s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:49:35.617: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7939
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 10:49:35.950: INFO: (0) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 24.007199ms)
Apr 10 10:49:35.994: INFO: (1) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 43.223372ms)
Apr 10 10:49:36.018: INFO: (2) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 24.497665ms)
Apr 10 10:49:36.040: INFO: (3) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 21.408398ms)
Apr 10 10:49:36.063: INFO: (4) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 23.52298ms)
Apr 10 10:49:36.087: INFO: (5) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 23.410042ms)
Apr 10 10:49:36.108: INFO: (6) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 21.314369ms)
Apr 10 10:49:36.137: INFO: (7) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 28.501166ms)
Apr 10 10:49:36.159: INFO: (8) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 22.004646ms)
Apr 10 10:49:36.180: INFO: (9) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 21.556539ms)
Apr 10 10:49:36.201: INFO: (10) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 21.032329ms)
Apr 10 10:49:36.224: INFO: (11) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 22.716706ms)
Apr 10 10:49:36.249: INFO: (12) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 24.924206ms)
Apr 10 10:49:36.271: INFO: (13) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 21.598642ms)
Apr 10 10:49:36.296: INFO: (14) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 24.81211ms)
Apr 10 10:49:36.317: INFO: (15) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 21.692477ms)
Apr 10 10:49:36.341: INFO: (16) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 24.056418ms)
Apr 10 10:49:36.363: INFO: (17) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 21.79346ms)
Apr 10 10:49:36.385: INFO: (18) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 21.17077ms)
Apr 10 10:49:36.406: INFO: (19) /api/v1/nodes/ip-10-250-14-152.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 21.339411ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:49:36.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7939" for this suite.
Apr 10 10:49:42.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:49:43.284: INFO: namespace proxy-7939 deletion completed in 6.85730289s
•SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:49:43.284: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1580
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 10 10:49:43.538: INFO: Waiting up to 5m0s for pod "pod-5962536d-5b7e-11e9-bf19-c208aa3c4721" in namespace "emptydir-1580" to be "success or failure"
Apr 10 10:49:43.558: INFO: Pod "pod-5962536d-5b7e-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.561393ms
Apr 10 10:49:45.580: INFO: Pod "pod-5962536d-5b7e-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042089547s
STEP: Saw pod success
Apr 10 10:49:45.580: INFO: Pod "pod-5962536d-5b7e-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:49:45.600: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-5962536d-5b7e-11e9-bf19-c208aa3c4721 container test-container: <nil>
STEP: delete the pod
Apr 10 10:49:45.656: INFO: Waiting for pod pod-5962536d-5b7e-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:49:45.675: INFO: Pod pod-5962536d-5b7e-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:49:45.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1580" for this suite.
Apr 10 10:49:51.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:49:52.524: INFO: namespace emptydir-1580 deletion completed in 6.826406953s
•SSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:49:52.524: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 10:49:52.800: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:49:55.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-109" for this suite.
Apr 10 10:50:47.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:50:48.011: INFO: namespace pods-109 deletion completed in 52.818814251s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:50:48.011: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-214
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 10 10:50:48.341: INFO: Waiting up to 5m0s for pod "downward-api-8002c7c9-5b7e-11e9-bf19-c208aa3c4721" in namespace "downward-api-214" to be "success or failure"
Apr 10 10:50:48.362: INFO: Pod "downward-api-8002c7c9-5b7e-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.736568ms
Apr 10 10:50:50.382: INFO: Pod "downward-api-8002c7c9-5b7e-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041318436s
STEP: Saw pod success
Apr 10 10:50:50.382: INFO: Pod "downward-api-8002c7c9-5b7e-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:50:50.403: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downward-api-8002c7c9-5b7e-11e9-bf19-c208aa3c4721 container dapi-container: <nil>
STEP: delete the pod
Apr 10 10:50:50.457: INFO: Waiting for pod downward-api-8002c7c9-5b7e-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:50:50.477: INFO: Pod downward-api-8002c7c9-5b7e-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:50:50.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-214" for this suite.
Apr 10 10:50:56.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:50:57.317: INFO: namespace downward-api-214 deletion completed in 6.820891944s
•SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:50:57.318: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3542
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr 10 10:51:00.254: INFO: Successfully updated pod "labelsupdate858b13d4-5b7e-11e9-bf19-c208aa3c4721"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:51:04.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3542" for this suite.
Apr 10 10:51:26.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:51:27.182: INFO: namespace projected-3542 deletion completed in 22.820326489s
•SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:51:27.182: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1118
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Apr 10 10:51:27.518: INFO: Waiting up to 5m0s for pod "pod-975cb5b6-5b7e-11e9-bf19-c208aa3c4721" in namespace "emptydir-1118" to be "success or failure"
Apr 10 10:51:27.544: INFO: Pod "pod-975cb5b6-5b7e-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 25.607208ms
Apr 10 10:51:29.564: INFO: Pod "pod-975cb5b6-5b7e-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.04615196s
STEP: Saw pod success
Apr 10 10:51:29.564: INFO: Pod "pod-975cb5b6-5b7e-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:51:29.584: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-975cb5b6-5b7e-11e9-bf19-c208aa3c4721 container test-container: <nil>
STEP: delete the pod
Apr 10 10:51:29.640: INFO: Waiting for pod pod-975cb5b6-5b7e-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:51:29.661: INFO: Pod pod-975cb5b6-5b7e-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:51:29.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1118" for this suite.
Apr 10 10:51:35.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:51:36.512: INFO: namespace emptydir-1118 deletion completed in 6.817482904s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:51:36.512: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-116
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Apr 10 10:51:36.824: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-116" to be "success or failure"
Apr 10 10:51:36.844: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 19.862255ms
Apr 10 10:51:38.865: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040920256s
STEP: Saw pod success
Apr 10 10:51:38.865: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Apr 10 10:51:38.888: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Apr 10 10:51:38.972: INFO: Waiting for pod pod-host-path-test to disappear
Apr 10 10:51:38.992: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:51:38.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-116" for this suite.
Apr 10 10:51:45.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:51:45.903: INFO: namespace hostpath-116 deletion completed in 6.844462897s
•SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:51:45.904: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6248
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:51:46.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6248" for this suite.
Apr 10 10:52:08.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:52:09.158: INFO: namespace kubelet-test-6248 deletion completed in 22.889833308s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:52:09.158: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2538
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-b05abeff-5b7e-11e9-bf19-c208aa3c4721
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-b05abeff-5b7e-11e9-bf19-c208aa3c4721
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:52:13.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2538" for this suite.
Apr 10 10:52:35.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:52:36.479: INFO: namespace projected-2538 deletion completed in 22.822952155s
•SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:52:36.479: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5843
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-5843
Apr 10 10:52:38.777: INFO: Started pod liveness-exec in namespace container-probe-5843
STEP: checking the pod's current state and verifying that restartCount is present
Apr 10 10:52:38.797: INFO: Initial restart count of pod liveness-exec is 0
Apr 10 10:53:27.353: INFO: Restart count of pod container-probe-5843/liveness-exec is now 1 (48.555967414s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:53:27.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5843" for this suite.
Apr 10 10:53:33.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:53:34.238: INFO: namespace container-probe-5843 deletion completed in 6.839399242s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:53:34.239: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3230
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-e30f7b8c-5b7e-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume secrets
Apr 10 10:53:34.540: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e312b26a-5b7e-11e9-bf19-c208aa3c4721" in namespace "projected-3230" to be "success or failure"
Apr 10 10:53:34.561: INFO: Pod "pod-projected-secrets-e312b26a-5b7e-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.944223ms
Apr 10 10:53:36.582: INFO: Pod "pod-projected-secrets-e312b26a-5b7e-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041840032s
STEP: Saw pod success
Apr 10 10:53:36.582: INFO: Pod "pod-projected-secrets-e312b26a-5b7e-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:53:36.606: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-projected-secrets-e312b26a-5b7e-11e9-bf19-c208aa3c4721 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 10 10:53:36.666: INFO: Waiting for pod pod-projected-secrets-e312b26a-5b7e-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:53:36.688: INFO: Pod pod-projected-secrets-e312b26a-5b7e-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:53:36.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3230" for this suite.
Apr 10 10:53:42.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:53:43.507: INFO: namespace projected-3230 deletion completed in 6.797351444s
•SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:53:43.507: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-6828
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Apr 10 10:53:45.290: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 10:53:47.312: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 10:53:49.310: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 10:53:51.311: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 10:53:53.334: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690490425, loc:(*time.Location)(0x87c9f80)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 10:53:56.430: INFO: Waited 1.098915168s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:53:57.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6828" for this suite.
Apr 10 10:54:03.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:54:04.559: INFO: namespace aggregator-6828 deletion completed in 6.899150298s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:54:04.560: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8927
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 10 10:54:04.824: INFO: Waiting up to 5m0s for pod "downward-api-f51f40a4-5b7e-11e9-bf19-c208aa3c4721" in namespace "downward-api-8927" to be "success or failure"
Apr 10 10:54:04.846: INFO: Pod "downward-api-f51f40a4-5b7e-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 21.472046ms
Apr 10 10:54:06.870: INFO: Pod "downward-api-f51f40a4-5b7e-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045426616s
STEP: Saw pod success
Apr 10 10:54:06.870: INFO: Pod "downward-api-f51f40a4-5b7e-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:54:06.890: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downward-api-f51f40a4-5b7e-11e9-bf19-c208aa3c4721 container dapi-container: <nil>
STEP: delete the pod
Apr 10 10:54:06.939: INFO: Waiting for pod downward-api-f51f40a4-5b7e-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:54:06.959: INFO: Pod downward-api-f51f40a4-5b7e-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:54:06.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8927" for this suite.
Apr 10 10:54:13.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:54:13.801: INFO: namespace downward-api-8927 deletion completed in 6.821497113s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:54:13.801: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5519
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-faad27c6-5b7e-11e9-bf19-c208aa3c4721
STEP: Creating secret with name s-test-opt-upd-faad2841-5b7e-11e9-bf19-c208aa3c4721
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-faad27c6-5b7e-11e9-bf19-c208aa3c4721
STEP: Updating secret s-test-opt-upd-faad2841-5b7e-11e9-bf19-c208aa3c4721
STEP: Creating secret with name s-test-opt-create-faad2856-5b7e-11e9-bf19-c208aa3c4721
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:54:18.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5519" for this suite.
Apr 10 10:54:40.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:54:41.563: INFO: namespace projected-5519 deletion completed in 22.802805707s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:54:41.563: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7303
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-0b2d7667-5b7f-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume configMaps
Apr 10 10:54:41.846: INFO: Waiting up to 5m0s for pod "pod-configmaps-0b30e210-5b7f-11e9-bf19-c208aa3c4721" in namespace "configmap-7303" to be "success or failure"
Apr 10 10:54:41.866: INFO: Pod "pod-configmaps-0b30e210-5b7f-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.875687ms
Apr 10 10:54:43.886: INFO: Pod "pod-configmaps-0b30e210-5b7f-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.04017864s
STEP: Saw pod success
Apr 10 10:54:43.886: INFO: Pod "pod-configmaps-0b30e210-5b7f-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:54:43.906: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-configmaps-0b30e210-5b7f-11e9-bf19-c208aa3c4721 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 10:54:43.955: INFO: Waiting for pod pod-configmaps-0b30e210-5b7f-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:54:43.975: INFO: Pod pod-configmaps-0b30e210-5b7f-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:54:43.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7303" for this suite.
Apr 10 10:54:50.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:54:50.793: INFO: namespace configmap-7303 deletion completed in 6.796429903s
•SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:54:50.793: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4970
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-10bbf731-5b7f-11e9-bf19-c208aa3c4721
STEP: Creating configMap with name cm-test-opt-upd-10bbf782-5b7f-11e9-bf19-c208aa3c4721
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-10bbf731-5b7f-11e9-bf19-c208aa3c4721
STEP: Updating configmap cm-test-opt-upd-10bbf782-5b7f-11e9-bf19-c208aa3c4721
STEP: Creating configMap with name cm-test-opt-create-10bbf796-5b7f-11e9-bf19-c208aa3c4721
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:54:57.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4970" for this suite.
Apr 10 10:55:19.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:55:20.627: INFO: namespace projected-4970 deletion completed in 22.842698605s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:55:20.627: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6547
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Apr 10 10:55:20.929: INFO: Waiting up to 5m0s for pod "var-expansion-227c84e6-5b7f-11e9-bf19-c208aa3c4721" in namespace "var-expansion-6547" to be "success or failure"
Apr 10 10:55:20.948: INFO: Pod "var-expansion-227c84e6-5b7f-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.499716ms
Apr 10 10:55:22.972: INFO: Pod "var-expansion-227c84e6-5b7f-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043261791s
STEP: Saw pod success
Apr 10 10:55:22.972: INFO: Pod "var-expansion-227c84e6-5b7f-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:55:22.999: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod var-expansion-227c84e6-5b7f-11e9-bf19-c208aa3c4721 container dapi-container: <nil>
STEP: delete the pod
Apr 10 10:55:23.055: INFO: Waiting for pod var-expansion-227c84e6-5b7f-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:55:23.078: INFO: Pod var-expansion-227c84e6-5b7f-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:55:23.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6547" for this suite.
Apr 10 10:55:29.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:55:29.919: INFO: namespace var-expansion-6547 deletion completed in 6.818560262s
•SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:55:29.919: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4684
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr 10 10:55:30.147: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 10 10:55:30.192: INFO: Waiting for terminating namespaces to be deleted...
Apr 10 10:55:30.212: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-14-152.eu-west-1.compute.internal before test
Apr 10 10:55:30.239: INFO: addons-kube2iam-x6268 from kube-system started at 2019-04-10 09:00:33 +0000 UTC (1 container statuses recorded)
Apr 10 10:55:30.239: INFO: 	Container kube2iam ready: true, restart count 0
Apr 10 10:55:30.239: INFO: blackbox-exporter-6dc58dcffc-w6tfm from kube-system started at 2019-04-10 09:00:13 +0000 UTC (1 container statuses recorded)
Apr 10 10:55:30.239: INFO: 	Container blackbox-exporter ready: true, restart count 0
Apr 10 10:55:30.239: INFO: calico-node-965bj from kube-system started at 2019-04-10 09:00:13 +0000 UTC (1 container statuses recorded)
Apr 10 10:55:30.239: INFO: 	Container calico-node ready: true, restart count 0
Apr 10 10:55:30.239: INFO: node-exporter-wsr4c from kube-system started at 2019-04-10 09:00:13 +0000 UTC (1 container statuses recorded)
Apr 10 10:55:30.239: INFO: 	Container node-exporter ready: true, restart count 0
Apr 10 10:55:30.239: INFO: kube-proxy-t9z66 from kube-system started at 2019-04-10 09:00:13 +0000 UTC (1 container statuses recorded)
Apr 10 10:55:30.239: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 10 10:55:30.239: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-25-25.eu-west-1.compute.internal before test
Apr 10 10:55:30.302: INFO: coredns-7f7f7978c8-2nxbn from kube-system started at 2019-04-10 09:00:24 +0000 UTC (1 container statuses recorded)
Apr 10 10:55:30.302: INFO: 	Container coredns ready: true, restart count 0
Apr 10 10:55:30.302: INFO: addons-kubernetes-dashboard-665df4b66d-vlw97 from kube-system started at 2019-04-10 09:00:24 +0000 UTC (1 container statuses recorded)
Apr 10 10:55:30.302: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr 10 10:55:30.302: INFO: addons-kube2iam-zjj8r from kube-system started at 2019-04-10 09:00:24 +0000 UTC (1 container statuses recorded)
Apr 10 10:55:30.302: INFO: 	Container kube2iam ready: true, restart count 0
Apr 10 10:55:30.302: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-57c2j from kube-system started at 2019-04-10 09:00:29 +0000 UTC (1 container statuses recorded)
Apr 10 10:55:30.302: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Apr 10 10:55:30.302: INFO: calico-node-ftwpk from kube-system started at 2019-04-10 09:00:14 +0000 UTC (1 container statuses recorded)
Apr 10 10:55:30.302: INFO: 	Container calico-node ready: true, restart count 0
Apr 10 10:55:30.302: INFO: addons-nginx-ingress-controller-f88658d78-q6z6w from kube-system started at 2019-04-10 09:00:29 +0000 UTC (1 container statuses recorded)
Apr 10 10:55:30.302: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Apr 10 10:55:30.302: INFO: kube-proxy-xk4pl from kube-system started at 2019-04-10 09:00:14 +0000 UTC (1 container statuses recorded)
Apr 10 10:55:30.302: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 10 10:55:30.302: INFO: node-exporter-n6hdz from kube-system started at 2019-04-10 09:00:14 +0000 UTC (1 container statuses recorded)
Apr 10 10:55:30.302: INFO: 	Container node-exporter ready: true, restart count 0
Apr 10 10:55:30.302: INFO: coredns-7f7f7978c8-qqrcn from kube-system started at 2019-04-10 09:00:24 +0000 UTC (1 container statuses recorded)
Apr 10 10:55:30.302: INFO: 	Container coredns ready: true, restart count 0
Apr 10 10:55:30.302: INFO: metrics-server-6b49cb886c-fjp6h from kube-system started at 2019-04-10 09:00:24 +0000 UTC (1 container statuses recorded)
Apr 10 10:55:30.302: INFO: 	Container metrics-server ready: true, restart count 0
Apr 10 10:55:30.302: INFO: vpn-shoot-7b5f8c4995-mmd2b from kube-system started at 2019-04-10 09:00:29 +0000 UTC (1 container statuses recorded)
Apr 10 10:55:30.302: INFO: 	Container vpn-shoot ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1594179c0f3d2b76], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:55:31.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4684" for this suite.
Apr 10 10:55:37.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:55:38.261: INFO: namespace sched-pred-4684 deletion completed in 6.825210415s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70
•SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:55:38.261: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3659
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6777
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8146
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:55:45.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3659" for this suite.
Apr 10 10:55:51.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:55:51.964: INFO: namespace namespaces-3659 deletion completed in 6.839023604s
STEP: Destroying namespace "nsdeletetest-6777" for this suite.
Apr 10 10:55:51.984: INFO: Namespace nsdeletetest-6777 was already deleted
STEP: Destroying namespace "nsdeletetest-8146" for this suite.
Apr 10 10:55:58.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:55:58.830: INFO: namespace nsdeletetest-8146 deletion completed in 6.84591423s
•SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:55:58.830: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4575
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-39407557-5b7f-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume secrets
Apr 10 10:55:59.147: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3943cb0d-5b7f-11e9-bf19-c208aa3c4721" in namespace "projected-4575" to be "success or failure"
Apr 10 10:55:59.173: INFO: Pod "pod-projected-secrets-3943cb0d-5b7f-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 26.071553ms
Apr 10 10:56:01.194: INFO: Pod "pod-projected-secrets-3943cb0d-5b7f-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046729616s
STEP: Saw pod success
Apr 10 10:56:01.194: INFO: Pod "pod-projected-secrets-3943cb0d-5b7f-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:56:01.214: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-projected-secrets-3943cb0d-5b7f-11e9-bf19-c208aa3c4721 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 10 10:56:01.267: INFO: Waiting for pod pod-projected-secrets-3943cb0d-5b7f-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:56:01.286: INFO: Pod pod-projected-secrets-3943cb0d-5b7f-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:56:01.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4575" for this suite.
Apr 10 10:56:07.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:56:08.149: INFO: namespace projected-4575 deletion completed in 6.841922126s
•SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:56:08.150: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1781
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1781
[It] Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1781
STEP: Creating statefulset with conflicting port in namespace statefulset-1781
STEP: Waiting until pod test-pod will start running in namespace statefulset-1781
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1781
Apr 10 10:56:10.581: INFO: Observed stateful pod in namespace: statefulset-1781, name: ss-0, uid: 4016992a-5b7f-11e9-a517-0202de25e2de, status phase: Pending. Waiting for statefulset controller to delete.
Apr 10 10:56:10.593: INFO: Observed stateful pod in namespace: statefulset-1781, name: ss-0, uid: 4016992a-5b7f-11e9-a517-0202de25e2de, status phase: Failed. Waiting for statefulset controller to delete.
Apr 10 10:56:10.633: INFO: Observed stateful pod in namespace: statefulset-1781, name: ss-0, uid: 4016992a-5b7f-11e9-a517-0202de25e2de, status phase: Failed. Waiting for statefulset controller to delete.
Apr 10 10:56:10.633: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1781
STEP: Removing pod with conflicting port in namespace statefulset-1781
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1781 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 10 10:56:12.704: INFO: Deleting all statefulset in ns statefulset-1781
Apr 10 10:56:12.726: INFO: Scaling statefulset ss to 0
Apr 10 10:56:22.810: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 10:56:22.830: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:56:22.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1781" for this suite.
Apr 10 10:56:28.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:56:29.758: INFO: namespace statefulset-1781 deletion completed in 6.836569047s
•SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:56:29.758: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5677
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:56:30.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5677" for this suite.
Apr 10 10:56:52.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:56:53.050: INFO: namespace pods-5677 deletion completed in 22.995969214s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:56:53.050: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2175
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 10:56:53.300: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2175'
Apr 10 10:56:53.906: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 10 10:56:53.906: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Apr 10 10:56:53.926: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config delete jobs e2e-test-nginx-job --namespace=kubectl-2175'
Apr 10 10:56:54.143: INFO: stderr: ""
Apr 10 10:56:54.143: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:56:54.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2175" for this suite.
Apr 10 10:57:00.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:57:01.004: INFO: namespace kubectl-2175 deletion completed in 6.827773678s
•SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:57:01.005: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4206
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Apr 10 10:57:01.314: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config proxy --unix-socket=/tmp/kubectl-proxy-unix637811638/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:57:01.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4206" for this suite.
Apr 10 10:57:07.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:57:08.247: INFO: namespace kubectl-4206 deletion completed in 6.832078372s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:57:08.248: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3296
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 10:57:08.520: INFO: Waiting up to 5m0s for pod "downwardapi-volume-629d1542-5b7f-11e9-bf19-c208aa3c4721" in namespace "projected-3296" to be "success or failure"
Apr 10 10:57:08.540: INFO: Pod "downwardapi-volume-629d1542-5b7f-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.077219ms
Apr 10 10:57:10.560: INFO: Pod "downwardapi-volume-629d1542-5b7f-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040740117s
STEP: Saw pod success
Apr 10 10:57:10.561: INFO: Pod "downwardapi-volume-629d1542-5b7f-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:57:10.581: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod downwardapi-volume-629d1542-5b7f-11e9-bf19-c208aa3c4721 container client-container: <nil>
STEP: delete the pod
Apr 10 10:57:10.632: INFO: Waiting for pod downwardapi-volume-629d1542-5b7f-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:57:10.657: INFO: Pod downwardapi-volume-629d1542-5b7f-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:57:10.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3296" for this suite.
Apr 10 10:57:16.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:57:17.510: INFO: namespace projected-3296 deletion completed in 6.832183942s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:57:17.510: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6464
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-682895c2-5b7f-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume configMaps
Apr 10 10:57:17.841: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-682babea-5b7f-11e9-bf19-c208aa3c4721" in namespace "projected-6464" to be "success or failure"
Apr 10 10:57:17.860: INFO: Pod "pod-projected-configmaps-682babea-5b7f-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 19.653221ms
Apr 10 10:57:19.883: INFO: Pod "pod-projected-configmaps-682babea-5b7f-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042055439s
STEP: Saw pod success
Apr 10 10:57:19.883: INFO: Pod "pod-projected-configmaps-682babea-5b7f-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 10:57:19.906: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-projected-configmaps-682babea-5b7f-11e9-bf19-c208aa3c4721 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 10:57:19.958: INFO: Waiting for pod pod-projected-configmaps-682babea-5b7f-11e9-bf19-c208aa3c4721 to disappear
Apr 10 10:57:19.977: INFO: Pod pod-projected-configmaps-682babea-5b7f-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:57:19.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6464" for this suite.
Apr 10 10:57:26.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:57:26.859: INFO: namespace projected-6464 deletion completed in 6.860794148s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:57:26.860: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7319
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 10:57:27.108: INFO: Creating deployment "nginx-deployment"
Apr 10 10:57:27.131: INFO: Waiting for observed generation 1
Apr 10 10:57:27.156: INFO: Waiting for all required pods to come up
Apr 10 10:57:27.176: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Apr 10 10:57:31.253: INFO: Waiting for deployment "nginx-deployment" to complete
Apr 10 10:57:31.295: INFO: Updating deployment "nginx-deployment" with a non-existent image
Apr 10 10:57:31.336: INFO: Updating deployment nginx-deployment
Apr 10 10:57:31.336: INFO: Waiting for observed generation 2
Apr 10 10:57:33.380: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr 10 10:57:33.400: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr 10 10:57:33.419: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Apr 10 10:57:33.482: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr 10 10:57:33.482: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr 10 10:57:33.503: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Apr 10 10:57:33.543: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Apr 10 10:57:33.543: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Apr 10 10:57:33.589: INFO: Updating deployment nginx-deployment
Apr 10 10:57:33.589: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Apr 10 10:57:33.736: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr 10 10:57:33.834: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 10 10:57:34.033: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-7319,SelfLink:/apis/apps/v1/namespaces/deployment-7319/deployments/nginx-deployment,UID:6db76172-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22764,Generation:3,CreationTimestamp:2019-04-10 10:57:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-04-10 10:57:33 +0000 UTC 2019-04-10 10:57:33 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-04-10 10:57:33 +0000 UTC 2019-04-10 10:57:27 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Apr 10 10:57:34.056: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-7319,SelfLink:/apis/apps/v1/namespaces/deployment-7319/replicasets/nginx-deployment-5f9595f595,UID:70396c87-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22760,Generation:3,CreationTimestamp:2019-04-10 10:57:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 6db76172-5b7f-11e9-a517-0202de25e2de 0xc000f95ee7 0xc000f95ee8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 10:57:34.056: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Apr 10 10:57:34.056: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-7319,SelfLink:/apis/apps/v1/namespaces/deployment-7319/replicasets/nginx-deployment-6f478d8d8,UID:6db7ce4b-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22762,Generation:3,CreationTimestamp:2019-04-10 10:57:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 6db76172-5b7f-11e9-a517-0202de25e2de 0xc000f95fb7 0xc000f95fb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Apr 10 10:57:34.080: INFO: Pod "nginx-deployment-5f9595f595-2kvdk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-2kvdk,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-5f9595f595-2kvdk,UID:7191899a-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22754,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 70396c87-5b7f-11e9-a517-0202de25e2de 0xc003bc68c7 0xc003bc68c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003bc6930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003bc6950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.152,PodIP:,StartTime:2019-04-10 10:57:33 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.080: INFO: Pod "nginx-deployment-5f9595f595-4t7r8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-4t7r8,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-5f9595f595-4t7r8,UID:71925b99-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22732,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 70396c87-5b7f-11e9-a517-0202de25e2de 0xc003bc6a20 0xc003bc6a21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003bc6a90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003bc6ab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.080: INFO: Pod "nginx-deployment-5f9595f595-5qztp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-5qztp,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-5f9595f595-5qztp,UID:7192f38a-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22742,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 70396c87-5b7f-11e9-a517-0202de25e2de 0xc003bc6b30 0xc003bc6b31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003bc6ba0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003bc6bc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.080: INFO: Pod "nginx-deployment-5f9595f595-7j5wv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-7j5wv,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-5f9595f595-7j5wv,UID:70486f4e-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22701,Generation:0,CreationTimestamp:2019-04-10 10:57:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.12/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 70396c87-5b7f-11e9-a517-0202de25e2de 0xc003bc6c50 0xc003bc6c51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003bc6cc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003bc6ce0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.152,PodIP:,StartTime:2019-04-10 10:57:31 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.080: INFO: Pod "nginx-deployment-5f9595f595-9d9b9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-9d9b9,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-5f9595f595-9d9b9,UID:703a5ab2-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22705,Generation:0,CreationTimestamp:2019-04-10 10:57:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.51/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 70396c87-5b7f-11e9-a517-0202de25e2de 0xc003bc6dc0 0xc003bc6dc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-25-25.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003bc6e30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003bc6e50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC  }],Message:,Reason:,HostIP:10.250.25.25,PodIP:100.96.1.51,StartTime:2019-04-10 10:57:31 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.081: INFO: Pod "nginx-deployment-5f9595f595-bdnvq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-bdnvq,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-5f9595f595-bdnvq,UID:7039b764-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22702,Generation:0,CreationTimestamp:2019-04-10 10:57:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.13/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 70396c87-5b7f-11e9-a517-0202de25e2de 0xc003bc6f50 0xc003bc6f51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003bc6fc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003bc6fe0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.152,PodIP:,StartTime:2019-04-10 10:57:31 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.081: INFO: Pod "nginx-deployment-5f9595f595-dfw7n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-dfw7n,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-5f9595f595-dfw7n,UID:7192eac4-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22739,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 70396c87-5b7f-11e9-a517-0202de25e2de 0xc003bc70b0 0xc003bc70b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003bc7120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003bc7140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.081: INFO: Pod "nginx-deployment-5f9595f595-hrrlq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-hrrlq,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-5f9595f595-hrrlq,UID:7192e663-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22766,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 70396c87-5b7f-11e9-a517-0202de25e2de 0xc003bc71c0 0xc003bc71c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-25-25.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003bc7230} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003bc7250}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.25.25,PodIP:,StartTime:2019-04-10 10:57:33 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.081: INFO: Pod "nginx-deployment-5f9595f595-lvqkb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-lvqkb,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-5f9595f595-lvqkb,UID:7192fccc-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22768,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 70396c87-5b7f-11e9-a517-0202de25e2de 0xc003bc7330 0xc003bc7331}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-25-25.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003bc73a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003bc73c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.25.25,PodIP:,StartTime:2019-04-10 10:57:33 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.081: INFO: Pod "nginx-deployment-5f9595f595-p8cq8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-p8cq8,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-5f9595f595-p8cq8,UID:719258d5-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22767,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 70396c87-5b7f-11e9-a517-0202de25e2de 0xc003bc7490 0xc003bc7491}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-25-25.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003bc7500} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003bc7520}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.25.25,PodIP:,StartTime:2019-04-10 10:57:33 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.081: INFO: Pod "nginx-deployment-5f9595f595-pgfwg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-pgfwg,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-5f9595f595-pgfwg,UID:719852e3-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22769,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 70396c87-5b7f-11e9-a517-0202de25e2de 0xc003bc75f0 0xc003bc75f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-25-25.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003bc7660} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003bc7680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.25.25,PodIP:,StartTime:2019-04-10 10:57:33 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.081: INFO: Pod "nginx-deployment-5f9595f595-r4whs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-r4whs,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-5f9595f595-r4whs,UID:703a65e1-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22703,Generation:0,CreationTimestamp:2019-04-10 10:57:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.14/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 70396c87-5b7f-11e9-a517-0202de25e2de 0xc003bc7760 0xc003bc7761}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003bc77d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003bc77f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.152,PodIP:,StartTime:2019-04-10 10:57:31 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.081: INFO: Pod "nginx-deployment-5f9595f595-xv92t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-xv92t,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-5f9595f595-xv92t,UID:704905b6-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22699,Generation:0,CreationTimestamp:2019-04-10 10:57:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.52/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 70396c87-5b7f-11e9-a517-0202de25e2de 0xc003bc78d0 0xc003bc78d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-25-25.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003bc7940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003bc7960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:31 +0000 UTC  }],Message:,Reason:,HostIP:10.250.25.25,PodIP:,StartTime:2019-04-10 10:57:31 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.081: INFO: Pod "nginx-deployment-6f478d8d8-2x7ds" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-2x7ds,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-2x7ds,UID:71987adc-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22759,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc003bc7a30 0xc003bc7a31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-25-25.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003bc7a90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003bc7ab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.082: INFO: Pod "nginx-deployment-6f478d8d8-4pwnd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-4pwnd,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-4pwnd,UID:71924add-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22727,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc003bc7b30 0xc003bc7b31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003bc7b90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003bc7bb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.082: INFO: Pod "nginx-deployment-6f478d8d8-4xkdv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-4xkdv,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-4xkdv,UID:719139de-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22750,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc003bc7c30 0xc003bc7c31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003bc7c90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003bc7cb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.152,PodIP:,StartTime:2019-04-10 10:57:33 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.082: INFO: Pod "nginx-deployment-6f478d8d8-67j72" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-67j72,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-67j72,UID:6dba3410-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22629,Generation:0,CreationTimestamp:2019-04-10 10:57:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.49/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc003bc7d87 0xc003bc7d88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-25-25.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003bc7df0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003bc7e10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.25.25,PodIP:100.96.1.49,StartTime:2019-04-10 10:57:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 10:57:28 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:aed4a43d94cb1f8ad9addc55f0642a28e4f262061fffc1a447fac6e951d2e811 docker://a50955a1fc3d97d1257952cea22c4280a59fa3dcb34410107035683d50e2a29d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.082: INFO: Pod "nginx-deployment-6f478d8d8-7glgx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7glgx,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-7glgx,UID:7191b131-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22748,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc003bc7ee0 0xc003bc7ee1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-25-25.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003bc7f40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003bc7f60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.25.25,PodIP:,StartTime:2019-04-10 10:57:33 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.082: INFO: Pod "nginx-deployment-6f478d8d8-7scpn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7scpn,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-7scpn,UID:71985f7d-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22757,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc00243e027 0xc00243e028}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-25-25.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243e090} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243e0b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.082: INFO: Pod "nginx-deployment-6f478d8d8-85qqb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-85qqb,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-85qqb,UID:7192fe3a-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22741,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc00243e130 0xc00243e131}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243e190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243e1b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.082: INFO: Pod "nginx-deployment-6f478d8d8-c7bz4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-c7bz4,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-c7bz4,UID:6dba2ea8-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22635,Generation:0,CreationTimestamp:2019-04-10 10:57:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.50/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc00243e240 0xc00243e241}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-25-25.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243e2a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243e2c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.25.25,PodIP:100.96.1.50,StartTime:2019-04-10 10:57:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 10:57:28 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:aed4a43d94cb1f8ad9addc55f0642a28e4f262061fffc1a447fac6e951d2e811 docker://d764d347d48b0e0a278af29476b5b9965d17c589043fe2ae3c74c39c37f2ff0f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.082: INFO: Pod "nginx-deployment-6f478d8d8-ch24t" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ch24t,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-ch24t,UID:6db93fe1-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22648,Generation:0,CreationTimestamp:2019-04-10 10:57:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.11/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc00243e3a0 0xc00243e3a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243e400} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243e420}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.152,PodIP:100.96.0.11,StartTime:2019-04-10 10:57:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 10:57:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:aed4a43d94cb1f8ad9addc55f0642a28e4f262061fffc1a447fac6e951d2e811 docker://da55e17867f7fc14d0cf3f1b3db3c5506e26ac0009cf466a9718acd0edb930b4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.082: INFO: Pod "nginx-deployment-6f478d8d8-cnpdr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-cnpdr,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-cnpdr,UID:6db8d6b4-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22638,Generation:0,CreationTimestamp:2019-04-10 10:57:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.47/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc00243e500 0xc00243e501}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-25-25.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243e560} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243e580}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.25.25,PodIP:100.96.1.47,StartTime:2019-04-10 10:57:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 10:57:28 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:aed4a43d94cb1f8ad9addc55f0642a28e4f262061fffc1a447fac6e951d2e811 docker://d7cc63ad575532a64d7780094a645ddeff11383af91185d41b918d8b452c4e05}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.083: INFO: Pod "nginx-deployment-6f478d8d8-h5lc2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-h5lc2,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-h5lc2,UID:6db96eeb-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22658,Generation:0,CreationTimestamp:2019-04-10 10:57:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.9/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc00243e660 0xc00243e661}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243e6c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243e6e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.152,PodIP:100.96.0.9,StartTime:2019-04-10 10:57:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 10:57:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:aed4a43d94cb1f8ad9addc55f0642a28e4f262061fffc1a447fac6e951d2e811 docker://51f9cc2a36aba853f7605fd2babb09d39d59cfeb9952bf4c78cf5b157ab72573}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.083: INFO: Pod "nginx-deployment-6f478d8d8-hfkdl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-hfkdl,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-hfkdl,UID:6db9553d-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22651,Generation:0,CreationTimestamp:2019-04-10 10:57:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.10/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc00243e7c0 0xc00243e7c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243e820} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243e840}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.152,PodIP:100.96.0.10,StartTime:2019-04-10 10:57:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 10:57:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:aed4a43d94cb1f8ad9addc55f0642a28e4f262061fffc1a447fac6e951d2e811 docker://c6a79b21f79cc4f236632844683f8f8f1f710ee5fe703973c50a707284d4a442}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.083: INFO: Pod "nginx-deployment-6f478d8d8-k7w2g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-k7w2g,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-k7w2g,UID:7191b237-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22765,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc00243e930 0xc00243e931}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243e990} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243e9c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.152,PodIP:,StartTime:2019-04-10 10:57:33 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.083: INFO: Pod "nginx-deployment-6f478d8d8-ksptf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ksptf,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-ksptf,UID:6db94756-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22632,Generation:0,CreationTimestamp:2019-04-10 10:57:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.48/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc00243eac7 0xc00243eac8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-25-25.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243eb40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243eb60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.25.25,PodIP:100.96.1.48,StartTime:2019-04-10 10:57:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 10:57:28 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:aed4a43d94cb1f8ad9addc55f0642a28e4f262061fffc1a447fac6e951d2e811 docker://cb367b1f172a913563c6f63f14cf35c11c9460a6012d667c937da267e3f1dfb4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.083: INFO: Pod "nginx-deployment-6f478d8d8-mn9t6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-mn9t6,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-mn9t6,UID:7192fa14-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22743,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc00243ec80 0xc00243ec81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243ece0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243ed00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.083: INFO: Pod "nginx-deployment-6f478d8d8-n45qq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-n45qq,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-n45qq,UID:719253b4-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22763,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc00243ed80 0xc00243ed81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-25-25.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243ede0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243ee00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.25.25,PodIP:,StartTime:2019-04-10 10:57:33 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.083: INFO: Pod "nginx-deployment-6f478d8d8-s27h2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-s27h2,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-s27h2,UID:71924c02-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22756,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc00243eec7 0xc00243eec8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-25-25.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243ef30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243ef50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.25.25,PodIP:,StartTime:2019-04-10 10:57:33 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.083: INFO: Pod "nginx-deployment-6f478d8d8-tmsdm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-tmsdm,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-tmsdm,UID:719861c4-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22758,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc00243f017 0xc00243f018}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243f080} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243f0a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.083: INFO: Pod "nginx-deployment-6f478d8d8-x9d4k" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-x9d4k,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-x9d4k,UID:6dba536b-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22642,Generation:0,CreationTimestamp:2019-04-10 10:57:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.8/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc00243f130 0xc00243f131}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243f190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243f1b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.152,PodIP:100.96.0.8,StartTime:2019-04-10 10:57:27 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 10:57:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:aed4a43d94cb1f8ad9addc55f0642a28e4f262061fffc1a447fac6e951d2e811 docker://565beac8aa70c1b1d5c83cae12efbc27eddfd658bca3c7c545a1e4f4b0512573}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 10:57:34.083: INFO: Pod "nginx-deployment-6f478d8d8-z5xsh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-z5xsh,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-7319,SelfLink:/api/v1/namespaces/deployment-7319/pods/nginx-deployment-6f478d8d8-z5xsh,UID:71924fea-5b7f-11e9-a517-0202de25e2de,ResourceVersion:22729,Generation:0,CreationTimestamp:2019-04-10 10:57:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 6db7ce4b-5b7f-11e9-a517-0202de25e2de 0xc00243f280 0xc00243f281}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wkxj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wkxj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wkxj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-152.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243f2e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243f300}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 10:57:33 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:57:34.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7319" for this suite.
Apr 10 10:57:40.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:57:40.942: INFO: namespace deployment-7319 deletion completed in 6.835828384s
•SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:57:40.942: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9406
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-9406
Apr 10 10:57:45.261: INFO: Started pod liveness-http in namespace container-probe-9406
STEP: checking the pod's current state and verifying that restartCount is present
Apr 10 10:57:45.281: INFO: Initial restart count of pod liveness-http is 0
Apr 10 10:58:09.564: INFO: Restart count of pod container-probe-9406/liveness-http is now 1 (24.282523477s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 10:58:09.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9406" for this suite.
Apr 10 10:58:15.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 10:58:16.430: INFO: namespace container-probe-9406 deletion completed in 6.820355993s
•SSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 10:58:16.430: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3393
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-3393
Apr 10 10:58:18.768: INFO: Started pod liveness-http in namespace container-probe-3393
STEP: checking the pod's current state and verifying that restartCount is present
Apr 10 10:58:18.788: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 11:02:19.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3393" for this suite.
Apr 10 11:02:25.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 11:02:26.263: INFO: namespace container-probe-3393 deletion completed in 6.833971208s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 11:02:26.264: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4322
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0410 11:02:56.663315    4420 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 10 11:02:56.663: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 11:02:56.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4322" for this suite.
Apr 10 11:03:02.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 11:03:03.533: INFO: namespace gc-4322 deletion completed in 6.848988705s
•SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 11:03:03.533: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9839
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Apr 10 11:03:03.759: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config create -f - --namespace=kubectl-9839'
Apr 10 11:03:04.413: INFO: stderr: ""
Apr 10 11:03:04.413: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 10 11:03:04.414: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9839'
Apr 10 11:03:04.613: INFO: stderr: ""
Apr 10 11:03:04.613: INFO: stdout: "update-demo-nautilus-5sqkn update-demo-nautilus-zjbgz "
Apr 10 11:03:04.613: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-5sqkn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9839'
Apr 10 11:03:04.797: INFO: stderr: ""
Apr 10 11:03:04.797: INFO: stdout: ""
Apr 10 11:03:04.797: INFO: update-demo-nautilus-5sqkn is created but not running
Apr 10 11:03:09.797: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9839'
Apr 10 11:03:09.992: INFO: stderr: ""
Apr 10 11:03:09.992: INFO: stdout: "update-demo-nautilus-5sqkn update-demo-nautilus-zjbgz "
Apr 10 11:03:09.992: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-5sqkn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9839'
Apr 10 11:03:10.218: INFO: stderr: ""
Apr 10 11:03:10.218: INFO: stdout: "true"
Apr 10 11:03:10.218: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-5sqkn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9839'
Apr 10 11:03:10.393: INFO: stderr: ""
Apr 10 11:03:10.393: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 11:03:10.393: INFO: validating pod update-demo-nautilus-5sqkn
Apr 10 11:03:10.500: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 11:03:10.500: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 11:03:10.500: INFO: update-demo-nautilus-5sqkn is verified up and running
Apr 10 11:03:10.500: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-zjbgz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9839'
Apr 10 11:03:10.754: INFO: stderr: ""
Apr 10 11:03:10.755: INFO: stdout: "true"
Apr 10 11:03:10.755: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods update-demo-nautilus-zjbgz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9839'
Apr 10 11:03:10.978: INFO: stderr: ""
Apr 10 11:03:11.001: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 11:03:11.001: INFO: validating pod update-demo-nautilus-zjbgz
Apr 10 11:03:11.112: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 11:03:11.112: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 11:03:11.112: INFO: update-demo-nautilus-zjbgz is verified up and running
STEP: using delete to clean up resources
Apr 10 11:03:11.112: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-9839'
Apr 10 11:03:11.309: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 11:03:11.309: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 10 11:03:11.309: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9839'
Apr 10 11:03:11.521: INFO: stderr: "No resources found.\n"
Apr 10 11:03:11.521: INFO: stdout: ""
Apr 10 11:03:11.521: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.tm-s27nv.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/env/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-9839 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 10 11:03:11.704: INFO: stderr: ""
Apr 10 11:03:11.704: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 11:03:11.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9839" for this suite.
Apr 10 11:03:17.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 11:03:18.555: INFO: namespace kubectl-9839 deletion completed in 6.827400883s
•SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 11:03:18.555: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9511
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-3f549b8c-5b80-11e9-bf19-c208aa3c4721
STEP: Creating a pod to test consume configMaps
Apr 10 11:03:18.843: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3f584ad4-5b80-11e9-bf19-c208aa3c4721" in namespace "projected-9511" to be "success or failure"
Apr 10 11:03:18.864: INFO: Pod "pod-projected-configmaps-3f584ad4-5b80-11e9-bf19-c208aa3c4721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.476591ms
Apr 10 11:03:20.884: INFO: Pod "pod-projected-configmaps-3f584ad4-5b80-11e9-bf19-c208aa3c4721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041113931s
STEP: Saw pod success
Apr 10 11:03:20.884: INFO: Pod "pod-projected-configmaps-3f584ad4-5b80-11e9-bf19-c208aa3c4721" satisfied condition "success or failure"
Apr 10 11:03:20.904: INFO: Trying to get logs from node ip-10-250-14-152.eu-west-1.compute.internal pod pod-projected-configmaps-3f584ad4-5b80-11e9-bf19-c208aa3c4721 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 11:03:20.956: INFO: Waiting for pod pod-projected-configmaps-3f584ad4-5b80-11e9-bf19-c208aa3c4721 to disappear
Apr 10 11:03:20.979: INFO: Pod pod-projected-configmaps-3f584ad4-5b80-11e9-bf19-c208aa3c4721 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 11:03:20.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9511" for this suite.
Apr 10 11:03:27.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 11:03:27.843: INFO: namespace projected-9511 deletion completed in 6.840829835s
•SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 11:03:27.844: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4462
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 10 11:03:30.769: INFO: Successfully updated pod "pod-update-activedeadlineseconds-44e0ee95-5b80-11e9-bf19-c208aa3c4721"
Apr 10 11:03:30.769: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-44e0ee95-5b80-11e9-bf19-c208aa3c4721" in namespace "pods-4462" to be "terminated due to deadline exceeded"
Apr 10 11:03:30.789: INFO: Pod "pod-update-activedeadlineseconds-44e0ee95-5b80-11e9-bf19-c208aa3c4721": Phase="Running", Reason="", readiness=true. Elapsed: 20.072613ms
Apr 10 11:03:32.812: INFO: Pod "pod-update-activedeadlineseconds-44e0ee95-5b80-11e9-bf19-c208aa3c4721": Phase="Running", Reason="", readiness=true. Elapsed: 2.042562993s
Apr 10 11:03:34.833: INFO: Pod "pod-update-activedeadlineseconds-44e0ee95-5b80-11e9-bf19-c208aa3c4721": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.063331105s
Apr 10 11:03:34.833: INFO: Pod "pod-update-activedeadlineseconds-44e0ee95-5b80-11e9-bf19-c208aa3c4721" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 11:03:34.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4462" for this suite.
Apr 10 11:03:40.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 11:03:41.709: INFO: namespace pods-4462 deletion completed in 6.853334514s
•SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 11:03:41.710: INFO: >>> kubeConfig: /tmp/env/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4655
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 10 11:03:46.196: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 10 11:03:46.217: INFO: Pod pod-with-prestop-http-hook still exists
Apr 10 11:03:48.217: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 10 11:03:48.240: INFO: Pod pod-with-prestop-http-hook still exists
Apr 10 11:03:50.217: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 10 11:03:50.243: INFO: Pod pod-with-prestop-http-hook still exists
Apr 10 11:03:52.217: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 10 11:03:52.239: INFO: Pod pod-with-prestop-http-hook still exists
Apr 10 11:03:54.217: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 10 11:03:54.240: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 11:03:54.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4655" for this suite.
Apr 10 11:04:16.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 11:04:17.134: INFO: namespace container-lifecycle-hook-4655 deletion completed in 22.848331729s
•SSSSSSSSSSSSApr 10 11:04:17.134: INFO: Running AfterSuite actions on all nodes
Apr 10 11:04:17.134: INFO: Running AfterSuite actions on node 1
Apr 10 11:04:17.134: INFO: Skipping dumping logs from cluster

Ran 204 of 3584 Specs in 5481.150 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Flaked | 0 Pending | 3380 Skipped PASS

Ginkgo ran 1 suite in 1h31m23.15090413s
Test Suite Passed
