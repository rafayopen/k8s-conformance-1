Conformance test: not doing test setup.
I0410 12:50:57.795209    3207 e2e.go:240] Starting e2e run "480187d1-5b8f-11e9-8d1d-a6f828030f0b" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1554900655 - Will randomize all specs
Will run 204 of 3584 specs

Apr 10 12:50:58.293: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 12:50:58.297: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr 10 12:50:58.447: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr 10 12:50:58.584: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr 10 12:50:58.584: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Apr 10 12:50:58.584: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr 10 12:50:58.614: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Apr 10 12:50:58.614: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Apr 10 12:50:58.614: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Apr 10 12:50:58.614: INFO: e2e test version: v1.14.0
Apr 10 12:50:58.637: INFO: kube-apiserver version: v1.14.0
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 12:50:58.638: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
Apr 10 12:50:58.819: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Apr 10 12:50:58.906: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-274
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 12:50:59.110: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-274'
Apr 10 12:50:59.384: INFO: stderr: ""
Apr 10 12:50:59.384: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Apr 10 12:50:59.410: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-274'
Apr 10 12:51:00.959: INFO: stderr: ""
Apr 10 12:51:00.959: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 12:51:00.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-274" for this suite.
Apr 10 12:51:07.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 12:51:07.970: INFO: namespace kubectl-274 deletion completed in 6.985515449s
•SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 12:51:07.970: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7712
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-4f70cab9-5b8f-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume configMaps
Apr 10 12:51:08.328: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4f749b24-5b8f-11e9-8d1d-a6f828030f0b" in namespace "projected-7712" to be "success or failure"
Apr 10 12:51:08.363: INFO: Pod "pod-projected-configmaps-4f749b24-5b8f-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 35.130016ms
Apr 10 12:51:10.388: INFO: Pod "pod-projected-configmaps-4f749b24-5b8f-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060210671s
Apr 10 12:51:12.414: INFO: Pod "pod-projected-configmaps-4f749b24-5b8f-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.085831932s
STEP: Saw pod success
Apr 10 12:51:12.414: INFO: Pod "pod-projected-configmaps-4f749b24-5b8f-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 12:51:12.452: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-projected-configmaps-4f749b24-5b8f-11e9-8d1d-a6f828030f0b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 12:51:12.651: INFO: Waiting for pod pod-projected-configmaps-4f749b24-5b8f-11e9-8d1d-a6f828030f0b to disappear
Apr 10 12:51:12.676: INFO: Pod pod-projected-configmaps-4f749b24-5b8f-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 12:51:12.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7712" for this suite.
Apr 10 12:51:18.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 12:51:19.703: INFO: namespace projected-7712 deletion completed in 7.001377936s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 12:51:19.703: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1736
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 12:51:19.967: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1736'
Apr 10 12:51:20.244: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 10 12:51:20.244: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Apr 10 12:51:20.298: INFO: scanned /root for discovery docs: <nil>
Apr 10 12:51:20.299: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-1736'
Apr 10 12:51:32.760: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Apr 10 12:51:32.760: INFO: stdout: "Created e2e-test-nginx-rc-35d2ebbeeefc9376eb418349d73559fb\nScaling up e2e-test-nginx-rc-35d2ebbeeefc9376eb418349d73559fb from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-35d2ebbeeefc9376eb418349d73559fb up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-35d2ebbeeefc9376eb418349d73559fb to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Apr 10 12:51:32.760: INFO: stdout: "Created e2e-test-nginx-rc-35d2ebbeeefc9376eb418349d73559fb\nScaling up e2e-test-nginx-rc-35d2ebbeeefc9376eb418349d73559fb from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-35d2ebbeeefc9376eb418349d73559fb up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-35d2ebbeeefc9376eb418349d73559fb to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Apr 10 12:51:32.761: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-1736'
Apr 10 12:51:33.015: INFO: stderr: ""
Apr 10 12:51:33.015: INFO: stdout: "e2e-test-nginx-rc-35d2ebbeeefc9376eb418349d73559fb-r9v64 "
Apr 10 12:51:33.015: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-nginx-rc-35d2ebbeeefc9376eb418349d73559fb-r9v64 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1736'
Apr 10 12:51:33.218: INFO: stderr: ""
Apr 10 12:51:33.218: INFO: stdout: "true"
Apr 10 12:51:33.218: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-nginx-rc-35d2ebbeeefc9376eb418349d73559fb-r9v64 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1736'
Apr 10 12:51:33.426: INFO: stderr: ""
Apr 10 12:51:33.426: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Apr 10 12:51:33.426: INFO: e2e-test-nginx-rc-35d2ebbeeefc9376eb418349d73559fb-r9v64 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Apr 10 12:51:33.426: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-1736'
Apr 10 12:51:33.644: INFO: stderr: ""
Apr 10 12:51:33.644: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 12:51:33.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1736" for this suite.
Apr 10 12:51:55.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 12:51:56.683: INFO: namespace kubectl-1736 deletion completed in 23.000498617s
•
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 12:51:56.683: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4764
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3036
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8043
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 12:52:03.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4764" for this suite.
Apr 10 12:52:09.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 12:52:10.684: INFO: namespace namespaces-4764 deletion completed in 6.993079305s
STEP: Destroying namespace "nsdeletetest-3036" for this suite.
Apr 10 12:52:10.708: INFO: Namespace nsdeletetest-3036 was already deleted
STEP: Destroying namespace "nsdeletetest-8043" for this suite.
Apr 10 12:52:16.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 12:52:17.726: INFO: namespace nsdeletetest-8043 deletion completed in 7.017641096s
•SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 12:52:17.726: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5091
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr 10 12:52:18.062: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 12:52:24.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5091" for this suite.
Apr 10 12:52:31.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 12:52:32.005: INFO: namespace init-container-5091 deletion completed in 6.990899635s
•SSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 12:52:32.005: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-3008
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3008
I0410 12:52:32.370229    3207 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3008, replica count: 1
I0410 12:52:33.420929    3207 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0410 12:52:34.421187    3207 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0410 12:52:35.421470    3207 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0410 12:52:36.421793    3207 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 10 12:52:36.574: INFO: Created: latency-svc-58w4s
Apr 10 12:52:36.579: INFO: Got endpoints: latency-svc-58w4s [57.359254ms]
Apr 10 12:52:36.648: INFO: Created: latency-svc-9gqkb
Apr 10 12:52:36.648: INFO: Got endpoints: latency-svc-9gqkb [68.813895ms]
Apr 10 12:52:36.664: INFO: Created: latency-svc-g847k
Apr 10 12:52:36.668: INFO: Got endpoints: latency-svc-g847k [87.931866ms]
Apr 10 12:52:36.683: INFO: Created: latency-svc-llcs9
Apr 10 12:52:36.688: INFO: Got endpoints: latency-svc-llcs9 [107.985201ms]
Apr 10 12:52:36.703: INFO: Created: latency-svc-488ps
Apr 10 12:52:36.740: INFO: Got endpoints: latency-svc-488ps [160.502628ms]
Apr 10 12:52:36.741: INFO: Created: latency-svc-flmk7
Apr 10 12:52:36.758: INFO: Got endpoints: latency-svc-flmk7 [177.712346ms]
Apr 10 12:52:36.773: INFO: Created: latency-svc-82gzq
Apr 10 12:52:36.786: INFO: Got endpoints: latency-svc-82gzq [206.29541ms]
Apr 10 12:52:36.800: INFO: Created: latency-svc-xp276
Apr 10 12:52:36.805: INFO: Got endpoints: latency-svc-xp276 [224.864521ms]
Apr 10 12:52:36.822: INFO: Created: latency-svc-5xrdw
Apr 10 12:52:36.827: INFO: Got endpoints: latency-svc-5xrdw [246.801039ms]
Apr 10 12:52:36.864: INFO: Created: latency-svc-7dqfz
Apr 10 12:52:36.884: INFO: Got endpoints: latency-svc-7dqfz [303.763248ms]
Apr 10 12:52:36.884: INFO: Created: latency-svc-2psdt
Apr 10 12:52:36.889: INFO: Got endpoints: latency-svc-2psdt [308.134297ms]
Apr 10 12:52:36.939: INFO: Created: latency-svc-k76xd
Apr 10 12:52:36.943: INFO: Got endpoints: latency-svc-k76xd [362.870725ms]
Apr 10 12:52:36.991: INFO: Created: latency-svc-7v828
Apr 10 12:52:37.037: INFO: Got endpoints: latency-svc-7v828 [456.156936ms]
Apr 10 12:52:37.037: INFO: Created: latency-svc-29pp2
Apr 10 12:52:37.054: INFO: Got endpoints: latency-svc-29pp2 [473.283987ms]
Apr 10 12:52:37.054: INFO: Created: latency-svc-mgtqj
Apr 10 12:52:37.072: INFO: Created: latency-svc-qh5nb
Apr 10 12:52:37.072: INFO: Got endpoints: latency-svc-mgtqj [492.768728ms]
Apr 10 12:52:37.076: INFO: Got endpoints: latency-svc-qh5nb [495.800397ms]
Apr 10 12:52:37.120: INFO: Created: latency-svc-5x9sm
Apr 10 12:52:37.121: INFO: Created: latency-svc-xrtmr
Apr 10 12:52:37.126: INFO: Got endpoints: latency-svc-5x9sm [477.828428ms]
Apr 10 12:52:37.126: INFO: Got endpoints: latency-svc-xrtmr [458.278715ms]
Apr 10 12:52:37.140: INFO: Created: latency-svc-ms8bx
Apr 10 12:52:37.145: INFO: Got endpoints: latency-svc-ms8bx [457.144716ms]
Apr 10 12:52:37.204: INFO: Created: latency-svc-9t6qb
Apr 10 12:52:37.208: INFO: Got endpoints: latency-svc-9t6qb [467.783268ms]
Apr 10 12:52:37.242: INFO: Created: latency-svc-6kfm8
Apr 10 12:52:37.258: INFO: Created: latency-svc-vn5l8
Apr 10 12:52:37.258: INFO: Got endpoints: latency-svc-6kfm8 [499.728562ms]
Apr 10 12:52:37.272: INFO: Got endpoints: latency-svc-vn5l8 [485.511645ms]
Apr 10 12:52:37.285: INFO: Created: latency-svc-2zcp4
Apr 10 12:52:37.302: INFO: Created: latency-svc-hkw5g
Apr 10 12:52:37.303: INFO: Got endpoints: latency-svc-2zcp4 [497.449218ms]
Apr 10 12:52:37.306: INFO: Got endpoints: latency-svc-hkw5g [479.110634ms]
Apr 10 12:52:37.319: INFO: Created: latency-svc-h4549
Apr 10 12:52:37.324: INFO: Got endpoints: latency-svc-h4549 [435.889807ms]
Apr 10 12:52:37.337: INFO: Created: latency-svc-fcztd
Apr 10 12:52:37.341: INFO: Got endpoints: latency-svc-fcztd [456.452996ms]
Apr 10 12:52:37.392: INFO: Created: latency-svc-b8rmc
Apr 10 12:52:37.394: INFO: Got endpoints: latency-svc-b8rmc [451.042365ms]
Apr 10 12:52:37.416: INFO: Created: latency-svc-c5rqn
Apr 10 12:52:37.429: INFO: Got endpoints: latency-svc-c5rqn [392.515422ms]
Apr 10 12:52:37.443: INFO: Created: latency-svc-gcz8b
Apr 10 12:52:37.447: INFO: Got endpoints: latency-svc-gcz8b [392.85882ms]
Apr 10 12:52:37.461: INFO: Created: latency-svc-qv4c6
Apr 10 12:52:37.465: INFO: Got endpoints: latency-svc-qv4c6 [393.061156ms]
Apr 10 12:52:37.478: INFO: Created: latency-svc-mmcll
Apr 10 12:52:37.482: INFO: Got endpoints: latency-svc-mmcll [405.616518ms]
Apr 10 12:52:37.527: INFO: Created: latency-svc-6xc9w
Apr 10 12:52:37.543: INFO: Got endpoints: latency-svc-6xc9w [417.125547ms]
Apr 10 12:52:37.543: INFO: Created: latency-svc-xfxwv
Apr 10 12:52:37.557: INFO: Got endpoints: latency-svc-xfxwv [430.604544ms]
Apr 10 12:52:37.571: INFO: Created: latency-svc-dhw7r
Apr 10 12:52:37.575: INFO: Got endpoints: latency-svc-dhw7r [430.256881ms]
Apr 10 12:52:37.590: INFO: Created: latency-svc-gxfng
Apr 10 12:52:37.594: INFO: Got endpoints: latency-svc-gxfng [385.649565ms]
Apr 10 12:52:37.615: INFO: Created: latency-svc-gkjcn
Apr 10 12:52:37.621: INFO: Got endpoints: latency-svc-gkjcn [363.502837ms]
Apr 10 12:52:37.662: INFO: Created: latency-svc-c6kcd
Apr 10 12:52:37.678: INFO: Got endpoints: latency-svc-c6kcd [406.432181ms]
Apr 10 12:52:37.679: INFO: Created: latency-svc-gwk2n
Apr 10 12:52:37.691: INFO: Got endpoints: latency-svc-gwk2n [388.561214ms]
Apr 10 12:52:37.705: INFO: Created: latency-svc-wzgsm
Apr 10 12:52:37.709: INFO: Got endpoints: latency-svc-wzgsm [402.488513ms]
Apr 10 12:52:37.723: INFO: Created: latency-svc-4n95r
Apr 10 12:52:37.726: INFO: Got endpoints: latency-svc-4n95r [401.777321ms]
Apr 10 12:52:37.740: INFO: Created: latency-svc-s75vg
Apr 10 12:52:37.746: INFO: Got endpoints: latency-svc-s75vg [405.43277ms]
Apr 10 12:52:37.757: INFO: Created: latency-svc-dqth4
Apr 10 12:52:37.761: INFO: Got endpoints: latency-svc-dqth4 [366.774723ms]
Apr 10 12:52:37.793: INFO: Created: latency-svc-wvww6
Apr 10 12:52:37.797: INFO: Got endpoints: latency-svc-wvww6 [368.048132ms]
Apr 10 12:52:37.811: INFO: Created: latency-svc-hlvcs
Apr 10 12:52:37.815: INFO: Got endpoints: latency-svc-hlvcs [368.548752ms]
Apr 10 12:52:37.831: INFO: Created: latency-svc-sjtp7
Apr 10 12:52:37.835: INFO: Got endpoints: latency-svc-sjtp7 [370.039355ms]
Apr 10 12:52:37.850: INFO: Created: latency-svc-ppdsl
Apr 10 12:52:37.855: INFO: Got endpoints: latency-svc-ppdsl [372.985372ms]
Apr 10 12:52:37.868: INFO: Created: latency-svc-9lths
Apr 10 12:52:37.873: INFO: Got endpoints: latency-svc-9lths [329.131469ms]
Apr 10 12:52:37.887: INFO: Created: latency-svc-tz6s7
Apr 10 12:52:37.891: INFO: Got endpoints: latency-svc-tz6s7 [334.379042ms]
Apr 10 12:52:37.930: INFO: Created: latency-svc-ns9hc
Apr 10 12:52:37.948: INFO: Created: latency-svc-mx5h8
Apr 10 12:52:37.948: INFO: Got endpoints: latency-svc-ns9hc [373.155848ms]
Apr 10 12:52:37.953: INFO: Got endpoints: latency-svc-mx5h8 [358.500733ms]
Apr 10 12:52:37.978: INFO: Created: latency-svc-4w7b8
Apr 10 12:52:37.983: INFO: Got endpoints: latency-svc-4w7b8 [361.435931ms]
Apr 10 12:52:37.998: INFO: Created: latency-svc-wblnq
Apr 10 12:52:38.010: INFO: Got endpoints: latency-svc-wblnq [331.036489ms]
Apr 10 12:52:38.025: INFO: Created: latency-svc-psjw6
Apr 10 12:52:38.030: INFO: Got endpoints: latency-svc-psjw6 [338.429264ms]
Apr 10 12:52:38.064: INFO: Created: latency-svc-gn92x
Apr 10 12:52:38.086: INFO: Created: latency-svc-hcvnc
Apr 10 12:52:38.086: INFO: Got endpoints: latency-svc-gn92x [376.873804ms]
Apr 10 12:52:38.090: INFO: Got endpoints: latency-svc-hcvnc [363.61576ms]
Apr 10 12:52:38.107: INFO: Created: latency-svc-95s87
Apr 10 12:52:38.110: INFO: Got endpoints: latency-svc-95s87 [364.269028ms]
Apr 10 12:52:38.126: INFO: Created: latency-svc-n6khs
Apr 10 12:52:38.130: INFO: Got endpoints: latency-svc-n6khs [368.804297ms]
Apr 10 12:52:38.144: INFO: Created: latency-svc-xnnzp
Apr 10 12:52:38.149: INFO: Got endpoints: latency-svc-xnnzp [350.844798ms]
Apr 10 12:52:38.162: INFO: Created: latency-svc-84f4r
Apr 10 12:52:38.198: INFO: Got endpoints: latency-svc-84f4r [382.799334ms]
Apr 10 12:52:38.240: INFO: Created: latency-svc-4hswt
Apr 10 12:52:38.244: INFO: Got endpoints: latency-svc-4hswt [408.723251ms]
Apr 10 12:52:38.262: INFO: Created: latency-svc-52zvq
Apr 10 12:52:38.266: INFO: Got endpoints: latency-svc-52zvq [410.62956ms]
Apr 10 12:52:38.279: INFO: Created: latency-svc-qw2st
Apr 10 12:52:38.283: INFO: Got endpoints: latency-svc-qw2st [410.598972ms]
Apr 10 12:52:38.296: INFO: Created: latency-svc-wtwfh
Apr 10 12:52:38.325: INFO: Got endpoints: latency-svc-wtwfh [434.061186ms]
Apr 10 12:52:38.326: INFO: Created: latency-svc-m8vk2
Apr 10 12:52:38.331: INFO: Got endpoints: latency-svc-m8vk2 [382.30363ms]
Apr 10 12:52:38.349: INFO: Created: latency-svc-ngtkp
Apr 10 12:52:38.361: INFO: Got endpoints: latency-svc-ngtkp [408.443323ms]
Apr 10 12:52:38.374: INFO: Created: latency-svc-vb7p7
Apr 10 12:52:38.391: INFO: Created: latency-svc-b9jtm
Apr 10 12:52:38.396: INFO: Got endpoints: latency-svc-vb7p7 [412.933394ms]
Apr 10 12:52:38.424: INFO: Created: latency-svc-vnfkp
Apr 10 12:52:38.459: INFO: Got endpoints: latency-svc-b9jtm [449.471402ms]
Apr 10 12:52:38.460: INFO: Created: latency-svc-r4hw5
Apr 10 12:52:38.490: INFO: Created: latency-svc-75vwv
Apr 10 12:52:38.495: INFO: Got endpoints: latency-svc-vnfkp [465.23071ms]
Apr 10 12:52:38.511: INFO: Created: latency-svc-tpznq
Apr 10 12:52:38.530: INFO: Created: latency-svc-6prmd
Apr 10 12:52:38.549: INFO: Got endpoints: latency-svc-r4hw5 [463.507577ms]
Apr 10 12:52:38.550: INFO: Created: latency-svc-2v6jv
Apr 10 12:52:38.594: INFO: Got endpoints: latency-svc-75vwv [504.298891ms]
Apr 10 12:52:38.594: INFO: Created: latency-svc-46xn2
Apr 10 12:52:38.622: INFO: Created: latency-svc-gjr89
Apr 10 12:52:38.642: INFO: Created: latency-svc-wsjrf
Apr 10 12:52:38.645: INFO: Got endpoints: latency-svc-tpznq [534.664255ms]
Apr 10 12:52:38.682: INFO: Created: latency-svc-rph89
Apr 10 12:52:38.723: INFO: Got endpoints: latency-svc-6prmd [592.368957ms]
Apr 10 12:52:38.723: INFO: Created: latency-svc-lsm9m
Apr 10 12:52:38.741: INFO: Created: latency-svc-pt5cc
Apr 10 12:52:38.760: INFO: Got endpoints: latency-svc-2v6jv [611.123419ms]
Apr 10 12:52:38.802: INFO: Created: latency-svc-kdj54
Apr 10 12:52:38.802: INFO: Got endpoints: latency-svc-46xn2 [603.654405ms]
Apr 10 12:52:38.859: INFO: Created: latency-svc-ftt8b
Apr 10 12:52:38.859: INFO: Got endpoints: latency-svc-gjr89 [614.772353ms]
Apr 10 12:52:38.878: INFO: Created: latency-svc-nd8r2
Apr 10 12:52:38.909: INFO: Got endpoints: latency-svc-wsjrf [643.258211ms]
Apr 10 12:52:38.909: INFO: Created: latency-svc-2tf2m
Apr 10 12:52:38.931: INFO: Created: latency-svc-wrfbn
Apr 10 12:52:38.953: INFO: Created: latency-svc-4fgg5
Apr 10 12:52:38.954: INFO: Got endpoints: latency-svc-rph89 [670.298844ms]
Apr 10 12:52:38.985: INFO: Created: latency-svc-tlsgw
Apr 10 12:52:38.994: INFO: Got endpoints: latency-svc-lsm9m [668.06476ms]
Apr 10 12:52:39.008: INFO: Created: latency-svc-hz6wg
Apr 10 12:52:39.025: INFO: Created: latency-svc-cf85z
Apr 10 12:52:39.042: INFO: Created: latency-svc-69lcd
Apr 10 12:52:39.042: INFO: Got endpoints: latency-svc-pt5cc [711.194452ms]
Apr 10 12:52:39.061: INFO: Created: latency-svc-rl44n
Apr 10 12:52:39.078: INFO: Created: latency-svc-q9427
Apr 10 12:52:39.115: INFO: Got endpoints: latency-svc-kdj54 [753.811547ms]
Apr 10 12:52:39.115: INFO: Created: latency-svc-xhnxp
Apr 10 12:52:39.137: INFO: Created: latency-svc-bg889
Apr 10 12:52:39.142: INFO: Got endpoints: latency-svc-ftt8b [746.692701ms]
Apr 10 12:52:39.162: INFO: Created: latency-svc-9br6c
Apr 10 12:52:39.201: INFO: Created: latency-svc-wtgk2
Apr 10 12:52:39.201: INFO: Got endpoints: latency-svc-nd8r2 [741.977769ms]
Apr 10 12:52:39.242: INFO: Got endpoints: latency-svc-2tf2m [746.782269ms]
Apr 10 12:52:39.246: INFO: Created: latency-svc-d8pkg
Apr 10 12:52:39.264: INFO: Created: latency-svc-7fx8b
Apr 10 12:52:39.285: INFO: Created: latency-svc-zpnjc
Apr 10 12:52:39.292: INFO: Got endpoints: latency-svc-wrfbn [742.509932ms]
Apr 10 12:52:39.333: INFO: Created: latency-svc-2ssxv
Apr 10 12:52:39.363: INFO: Got endpoints: latency-svc-4fgg5 [768.85975ms]
Apr 10 12:52:39.408: INFO: Created: latency-svc-hs25b
Apr 10 12:52:39.408: INFO: Got endpoints: latency-svc-tlsgw [762.619732ms]
Apr 10 12:52:39.451: INFO: Got endpoints: latency-svc-hz6wg [728.454037ms]
Apr 10 12:52:39.451: INFO: Created: latency-svc-8fvm2
Apr 10 12:52:39.492: INFO: Got endpoints: latency-svc-cf85z [732.388417ms]
Apr 10 12:52:39.511: INFO: Created: latency-svc-mvfrb
Apr 10 12:52:39.553: INFO: Got endpoints: latency-svc-69lcd [750.633517ms]
Apr 10 12:52:39.553: INFO: Created: latency-svc-6frb6
Apr 10 12:52:39.645: INFO: Got endpoints: latency-svc-rl44n [785.909932ms]
Apr 10 12:52:39.645: INFO: Got endpoints: latency-svc-q9427 [736.147135ms]
Apr 10 12:52:39.665: INFO: Created: latency-svc-8c28v
Apr 10 12:52:39.689: INFO: Created: latency-svc-cqv4x
Apr 10 12:52:39.693: INFO: Got endpoints: latency-svc-xhnxp [739.268215ms]
Apr 10 12:52:39.707: INFO: Created: latency-svc-v9xrl
Apr 10 12:52:39.743: INFO: Got endpoints: latency-svc-bg889 [749.675204ms]
Apr 10 12:52:39.762: INFO: Created: latency-svc-zdmg4
Apr 10 12:52:39.808: INFO: Created: latency-svc-td56d
Apr 10 12:52:39.808: INFO: Got endpoints: latency-svc-9br6c [765.529893ms]
Apr 10 12:52:39.876: INFO: Created: latency-svc-k98ws
Apr 10 12:52:39.876: INFO: Got endpoints: latency-svc-wtgk2 [761.030148ms]
Apr 10 12:52:39.892: INFO: Got endpoints: latency-svc-d8pkg [749.531809ms]
Apr 10 12:52:39.923: INFO: Created: latency-svc-gd7ng
Apr 10 12:52:39.946: INFO: Created: latency-svc-5mk5k
Apr 10 12:52:39.946: INFO: Got endpoints: latency-svc-7fx8b [744.596318ms]
Apr 10 12:52:40.003: INFO: Created: latency-svc-tfjxp
Apr 10 12:52:40.003: INFO: Got endpoints: latency-svc-zpnjc [761.623658ms]
Apr 10 12:52:40.047: INFO: Created: latency-svc-qd4gx
Apr 10 12:52:40.048: INFO: Got endpoints: latency-svc-2ssxv [755.659376ms]
Apr 10 12:52:40.090: INFO: Created: latency-svc-zstbm
Apr 10 12:52:40.095: INFO: Got endpoints: latency-svc-hs25b [731.16404ms]
Apr 10 12:52:40.155: INFO: Created: latency-svc-bp27q
Apr 10 12:52:40.155: INFO: Got endpoints: latency-svc-8fvm2 [747.400541ms]
Apr 10 12:52:40.199: INFO: Got endpoints: latency-svc-mvfrb [747.863235ms]
Apr 10 12:52:40.199: INFO: Created: latency-svc-7bn8s
Apr 10 12:52:40.266: INFO: Got endpoints: latency-svc-6frb6 [773.634179ms]
Apr 10 12:52:40.266: INFO: Created: latency-svc-f6l4x
Apr 10 12:52:40.291: INFO: Got endpoints: latency-svc-8c28v [738.477426ms]
Apr 10 12:52:40.310: INFO: Created: latency-svc-nth96
Apr 10 12:52:40.332: INFO: Created: latency-svc-d6vjz
Apr 10 12:52:40.342: INFO: Got endpoints: latency-svc-cqv4x [697.147672ms]
Apr 10 12:52:40.395: INFO: Got endpoints: latency-svc-v9xrl [749.937767ms]
Apr 10 12:52:40.415: INFO: Created: latency-svc-2scq2
Apr 10 12:52:40.441: INFO: Created: latency-svc-q4d5z
Apr 10 12:52:40.445: INFO: Got endpoints: latency-svc-zdmg4 [752.177364ms]
Apr 10 12:52:40.489: INFO: Created: latency-svc-gbc4z
Apr 10 12:52:40.493: INFO: Got endpoints: latency-svc-td56d [749.884512ms]
Apr 10 12:52:40.547: INFO: Created: latency-svc-27vs8
Apr 10 12:52:40.547: INFO: Got endpoints: latency-svc-k98ws [738.872176ms]
Apr 10 12:52:40.589: INFO: Created: latency-svc-sbc8g
Apr 10 12:52:40.593: INFO: Got endpoints: latency-svc-gd7ng [717.254912ms]
Apr 10 12:52:40.664: INFO: Created: latency-svc-nmz2b
Apr 10 12:52:40.665: INFO: Got endpoints: latency-svc-5mk5k [772.475552ms]
Apr 10 12:52:40.692: INFO: Got endpoints: latency-svc-tfjxp [746.076705ms]
Apr 10 12:52:40.738: INFO: Created: latency-svc-6pg5c
Apr 10 12:52:40.754: INFO: Got endpoints: latency-svc-qd4gx [750.198ms]
Apr 10 12:52:40.791: INFO: Created: latency-svc-57rfl
Apr 10 12:52:40.791: INFO: Got endpoints: latency-svc-zstbm [743.595546ms]
Apr 10 12:52:40.816: INFO: Created: latency-svc-w8rss
Apr 10 12:52:40.839: INFO: Created: latency-svc-fb92n
Apr 10 12:52:40.842: INFO: Got endpoints: latency-svc-bp27q [747.510871ms]
Apr 10 12:52:40.884: INFO: Created: latency-svc-sc2gg
Apr 10 12:52:40.926: INFO: Got endpoints: latency-svc-7bn8s [770.668636ms]
Apr 10 12:52:40.942: INFO: Got endpoints: latency-svc-f6l4x [743.011731ms]
Apr 10 12:52:40.968: INFO: Created: latency-svc-wpbfd
Apr 10 12:52:40.987: INFO: Created: latency-svc-hjc2s
Apr 10 12:52:40.991: INFO: Got endpoints: latency-svc-nth96 [725.471665ms]
Apr 10 12:52:41.078: INFO: Created: latency-svc-z26jp
Apr 10 12:52:41.078: INFO: Got endpoints: latency-svc-d6vjz [786.892398ms]
Apr 10 12:52:41.092: INFO: Got endpoints: latency-svc-2scq2 [750.075904ms]
Apr 10 12:52:41.131: INFO: Created: latency-svc-2wqd6
Apr 10 12:52:41.145: INFO: Got endpoints: latency-svc-q4d5z [749.744732ms]
Apr 10 12:52:41.145: INFO: Created: latency-svc-5cbx2
Apr 10 12:52:41.223: INFO: Created: latency-svc-jrn4b
Apr 10 12:52:41.223: INFO: Got endpoints: latency-svc-gbc4z [778.279442ms]
Apr 10 12:52:41.242: INFO: Got endpoints: latency-svc-27vs8 [748.838997ms]
Apr 10 12:52:41.265: INFO: Created: latency-svc-mxgnz
Apr 10 12:52:41.285: INFO: Created: latency-svc-r495c
Apr 10 12:52:41.292: INFO: Got endpoints: latency-svc-sbc8g [745.236059ms]
Apr 10 12:52:41.351: INFO: Created: latency-svc-r6ggj
Apr 10 12:52:41.351: INFO: Got endpoints: latency-svc-nmz2b [757.181274ms]
Apr 10 12:52:41.395: INFO: Created: latency-svc-rgl2x
Apr 10 12:52:41.395: INFO: Got endpoints: latency-svc-6pg5c [730.614739ms]
Apr 10 12:52:41.438: INFO: Created: latency-svc-w58b6
Apr 10 12:52:41.442: INFO: Got endpoints: latency-svc-57rfl [750.550391ms]
Apr 10 12:52:41.498: INFO: Created: latency-svc-tw2n5
Apr 10 12:52:41.498: INFO: Got endpoints: latency-svc-w8rss [744.295231ms]
Apr 10 12:52:41.548: INFO: Created: latency-svc-kqgj6
Apr 10 12:52:41.549: INFO: Got endpoints: latency-svc-fb92n [757.339547ms]
Apr 10 12:52:41.603: INFO: Created: latency-svc-9sjhk
Apr 10 12:52:41.603: INFO: Got endpoints: latency-svc-sc2gg [760.885977ms]
Apr 10 12:52:41.652: INFO: Created: latency-svc-p2tv9
Apr 10 12:52:41.652: INFO: Got endpoints: latency-svc-wpbfd [725.581139ms]
Apr 10 12:52:41.696: INFO: Created: latency-svc-d44gs
Apr 10 12:52:41.696: INFO: Got endpoints: latency-svc-hjc2s [753.360837ms]
Apr 10 12:52:41.753: INFO: Created: latency-svc-2m7dg
Apr 10 12:52:41.753: INFO: Got endpoints: latency-svc-z26jp [761.204052ms]
Apr 10 12:52:41.795: INFO: Created: latency-svc-r8d9w
Apr 10 12:52:41.795: INFO: Got endpoints: latency-svc-2wqd6 [716.43454ms]
Apr 10 12:52:41.859: INFO: Got endpoints: latency-svc-5cbx2 [767.037251ms]
Apr 10 12:52:41.859: INFO: Created: latency-svc-5pstv
Apr 10 12:52:41.905: INFO: Created: latency-svc-56pgh
Apr 10 12:52:41.905: INFO: Got endpoints: latency-svc-jrn4b [759.883539ms]
Apr 10 12:52:41.950: INFO: Created: latency-svc-f6md6
Apr 10 12:52:41.950: INFO: Got endpoints: latency-svc-mxgnz [726.249421ms]
Apr 10 12:52:42.001: INFO: Got endpoints: latency-svc-r495c [758.963041ms]
Apr 10 12:52:42.002: INFO: Created: latency-svc-rtshm
Apr 10 12:52:42.041: INFO: Created: latency-svc-mnzm8
Apr 10 12:52:42.045: INFO: Got endpoints: latency-svc-r6ggj [752.64795ms]
Apr 10 12:52:42.116: INFO: Got endpoints: latency-svc-rgl2x [765.605729ms]
Apr 10 12:52:42.116: INFO: Created: latency-svc-226gl
Apr 10 12:52:42.142: INFO: Got endpoints: latency-svc-w58b6 [746.597172ms]
Apr 10 12:52:42.164: INFO: Created: latency-svc-wcblm
Apr 10 12:52:42.184: INFO: Created: latency-svc-jw59h
Apr 10 12:52:42.198: INFO: Got endpoints: latency-svc-tw2n5 [755.646275ms]
Apr 10 12:52:42.246: INFO: Got endpoints: latency-svc-kqgj6 [747.513499ms]
Apr 10 12:52:42.265: INFO: Created: latency-svc-qvnm5
Apr 10 12:52:42.288: INFO: Created: latency-svc-9skwd
Apr 10 12:52:42.292: INFO: Got endpoints: latency-svc-9sjhk [743.53383ms]
Apr 10 12:52:42.336: INFO: Created: latency-svc-8n2xq
Apr 10 12:52:42.342: INFO: Got endpoints: latency-svc-p2tv9 [738.370652ms]
Apr 10 12:52:42.385: INFO: Created: latency-svc-sk4hz
Apr 10 12:52:42.392: INFO: Got endpoints: latency-svc-d44gs [740.056859ms]
Apr 10 12:52:42.495: INFO: Created: latency-svc-wfp2f
Apr 10 12:52:42.496: INFO: Got endpoints: latency-svc-r8d9w [742.72253ms]
Apr 10 12:52:42.496: INFO: Got endpoints: latency-svc-2m7dg [799.931351ms]
Apr 10 12:52:42.540: INFO: Created: latency-svc-c9769
Apr 10 12:52:42.544: INFO: Got endpoints: latency-svc-5pstv [749.023934ms]
Apr 10 12:52:42.560: INFO: Created: latency-svc-cdsxd
Apr 10 12:52:42.631: INFO: Got endpoints: latency-svc-56pgh [771.514433ms]
Apr 10 12:52:42.631: INFO: Created: latency-svc-8vnmp
Apr 10 12:52:42.642: INFO: Got endpoints: latency-svc-f6md6 [736.832994ms]
Apr 10 12:52:42.675: INFO: Created: latency-svc-ss5c5
Apr 10 12:52:42.713: INFO: Created: latency-svc-bkkf6
Apr 10 12:52:42.713: INFO: Got endpoints: latency-svc-mnzm8 [711.93054ms]
Apr 10 12:52:42.761: INFO: Got endpoints: latency-svc-rtshm [810.948873ms]
Apr 10 12:52:42.802: INFO: Created: latency-svc-7x5lb
Apr 10 12:52:42.802: INFO: Got endpoints: latency-svc-226gl [757.333917ms]
Apr 10 12:52:42.823: INFO: Created: latency-svc-rcszc
Apr 10 12:52:42.845: INFO: Created: latency-svc-dpbhv
Apr 10 12:52:42.845: INFO: Got endpoints: latency-svc-wcblm [728.283965ms]
Apr 10 12:52:42.893: INFO: Got endpoints: latency-svc-jw59h [751.099654ms]
Apr 10 12:52:42.919: INFO: Created: latency-svc-sk284
Apr 10 12:52:42.937: INFO: Created: latency-svc-fd585
Apr 10 12:52:42.946: INFO: Got endpoints: latency-svc-qvnm5 [747.742757ms]
Apr 10 12:52:43.020: INFO: Created: latency-svc-vb4v8
Apr 10 12:52:43.020: INFO: Got endpoints: latency-svc-9skwd [774.647889ms]
Apr 10 12:52:43.042: INFO: Got endpoints: latency-svc-8n2xq [749.90945ms]
Apr 10 12:52:43.095: INFO: Created: latency-svc-9drg6
Apr 10 12:52:43.095: INFO: Got endpoints: latency-svc-sk4hz [753.467065ms]
Apr 10 12:52:43.117: INFO: Created: latency-svc-nlsbs
Apr 10 12:52:43.153: INFO: Got endpoints: latency-svc-wfp2f [760.910935ms]
Apr 10 12:52:43.176: INFO: Created: latency-svc-t528s
Apr 10 12:52:43.192: INFO: Got endpoints: latency-svc-c9769 [696.528814ms]
Apr 10 12:52:43.211: INFO: Created: latency-svc-f78f5
Apr 10 12:52:43.234: INFO: Created: latency-svc-cvncc
Apr 10 12:52:43.242: INFO: Got endpoints: latency-svc-cdsxd [746.176953ms]
Apr 10 12:52:43.311: INFO: Got endpoints: latency-svc-8vnmp [767.593402ms]
Apr 10 12:52:43.339: INFO: Created: latency-svc-s7crv
Apr 10 12:52:43.343: INFO: Got endpoints: latency-svc-ss5c5 [711.669788ms]
Apr 10 12:52:43.358: INFO: Created: latency-svc-czl24
Apr 10 12:52:43.384: INFO: Created: latency-svc-dpkcd
Apr 10 12:52:43.392: INFO: Got endpoints: latency-svc-bkkf6 [749.458442ms]
Apr 10 12:52:43.457: INFO: Got endpoints: latency-svc-7x5lb [744.135412ms]
Apr 10 12:52:43.476: INFO: Created: latency-svc-t527v
Apr 10 12:52:43.499: INFO: Got endpoints: latency-svc-rcszc [738.471227ms]
Apr 10 12:52:43.499: INFO: Created: latency-svc-cwdbl
Apr 10 12:52:43.543: INFO: Created: latency-svc-48kdh
Apr 10 12:52:43.546: INFO: Got endpoints: latency-svc-dpbhv [743.747112ms]
Apr 10 12:52:43.603: INFO: Got endpoints: latency-svc-sk284 [758.355166ms]
Apr 10 12:52:43.603: INFO: Created: latency-svc-tl7c7
Apr 10 12:52:43.645: INFO: Created: latency-svc-dgg4f
Apr 10 12:52:43.645: INFO: Got endpoints: latency-svc-fd585 [751.546266ms]
Apr 10 12:52:43.685: INFO: Created: latency-svc-mpbc4
Apr 10 12:52:43.712: INFO: Got endpoints: latency-svc-vb4v8 [765.876124ms]
Apr 10 12:52:43.757: INFO: Got endpoints: latency-svc-9drg6 [736.367916ms]
Apr 10 12:52:43.757: INFO: Created: latency-svc-hpk4c
Apr 10 12:52:43.799: INFO: Created: latency-svc-7xr8g
Apr 10 12:52:43.799: INFO: Got endpoints: latency-svc-nlsbs [757.249675ms]
Apr 10 12:52:43.856: INFO: Created: latency-svc-9bsrq
Apr 10 12:52:43.857: INFO: Got endpoints: latency-svc-t528s [761.25337ms]
Apr 10 12:52:43.963: INFO: Got endpoints: latency-svc-cvncc [770.806754ms]
Apr 10 12:52:43.963: INFO: Got endpoints: latency-svc-f78f5 [810.223692ms]
Apr 10 12:52:43.982: INFO: Created: latency-svc-zdww9
Apr 10 12:52:44.008: INFO: Created: latency-svc-cl9xk
Apr 10 12:52:44.008: INFO: Got endpoints: latency-svc-s7crv [766.020163ms]
Apr 10 12:52:44.033: INFO: Created: latency-svc-dzxgz
Apr 10 12:52:44.053: INFO: Got endpoints: latency-svc-czl24 [741.771334ms]
Apr 10 12:52:44.053: INFO: Created: latency-svc-ghltl
Apr 10 12:52:44.100: INFO: Got endpoints: latency-svc-dpkcd [757.538052ms]
Apr 10 12:52:44.118: INFO: Created: latency-svc-5mfjg
Apr 10 12:52:44.143: INFO: Created: latency-svc-fgnk7
Apr 10 12:52:44.143: INFO: Got endpoints: latency-svc-t527v [751.576359ms]
Apr 10 12:52:44.186: INFO: Created: latency-svc-c2fbm
Apr 10 12:52:44.191: INFO: Got endpoints: latency-svc-cwdbl [733.757065ms]
Apr 10 12:52:44.248: INFO: Created: latency-svc-ch5xd
Apr 10 12:52:44.248: INFO: Got endpoints: latency-svc-48kdh [749.136492ms]
Apr 10 12:52:44.289: INFO: Created: latency-svc-nptqq
Apr 10 12:52:44.293: INFO: Got endpoints: latency-svc-tl7c7 [747.178706ms]
Apr 10 12:52:44.361: INFO: Created: latency-svc-7kpqh
Apr 10 12:52:44.362: INFO: Got endpoints: latency-svc-dgg4f [758.280146ms]
Apr 10 12:52:44.409: INFO: Created: latency-svc-dwt8f
Apr 10 12:52:44.409: INFO: Got endpoints: latency-svc-mpbc4 [764.782874ms]
Apr 10 12:52:44.453: INFO: Got endpoints: latency-svc-hpk4c [741.140765ms]
Apr 10 12:52:44.453: INFO: Created: latency-svc-zbs9h
Apr 10 12:52:44.492: INFO: Got endpoints: latency-svc-7xr8g [735.093121ms]
Apr 10 12:52:44.542: INFO: Got endpoints: latency-svc-9bsrq [742.922342ms]
Apr 10 12:52:44.605: INFO: Got endpoints: latency-svc-zdww9 [748.879251ms]
Apr 10 12:52:44.642: INFO: Got endpoints: latency-svc-cl9xk [678.746835ms]
Apr 10 12:52:44.693: INFO: Got endpoints: latency-svc-dzxgz [729.440237ms]
Apr 10 12:52:44.742: INFO: Got endpoints: latency-svc-ghltl [733.909161ms]
Apr 10 12:52:44.792: INFO: Got endpoints: latency-svc-5mfjg [738.705067ms]
Apr 10 12:52:44.842: INFO: Got endpoints: latency-svc-fgnk7 [741.918054ms]
Apr 10 12:52:44.892: INFO: Got endpoints: latency-svc-c2fbm [749.078119ms]
Apr 10 12:52:44.950: INFO: Got endpoints: latency-svc-ch5xd [758.959197ms]
Apr 10 12:52:44.992: INFO: Got endpoints: latency-svc-nptqq [743.307053ms]
Apr 10 12:52:45.042: INFO: Got endpoints: latency-svc-7kpqh [749.029652ms]
Apr 10 12:52:45.093: INFO: Got endpoints: latency-svc-dwt8f [731.307874ms]
Apr 10 12:52:45.142: INFO: Got endpoints: latency-svc-zbs9h [732.484802ms]
Apr 10 12:52:45.142: INFO: Latencies: [68.813895ms 87.931866ms 107.985201ms 160.502628ms 177.712346ms 206.29541ms 224.864521ms 246.801039ms 303.763248ms 308.134297ms 329.131469ms 331.036489ms 334.379042ms 338.429264ms 350.844798ms 358.500733ms 361.435931ms 362.870725ms 363.502837ms 363.61576ms 364.269028ms 366.774723ms 368.048132ms 368.548752ms 368.804297ms 370.039355ms 372.985372ms 373.155848ms 376.873804ms 382.30363ms 382.799334ms 385.649565ms 388.561214ms 392.515422ms 392.85882ms 393.061156ms 401.777321ms 402.488513ms 405.43277ms 405.616518ms 406.432181ms 408.443323ms 408.723251ms 410.598972ms 410.62956ms 412.933394ms 417.125547ms 430.256881ms 430.604544ms 434.061186ms 435.889807ms 449.471402ms 451.042365ms 456.156936ms 456.452996ms 457.144716ms 458.278715ms 463.507577ms 465.23071ms 467.783268ms 473.283987ms 477.828428ms 479.110634ms 485.511645ms 492.768728ms 495.800397ms 497.449218ms 499.728562ms 504.298891ms 534.664255ms 592.368957ms 603.654405ms 611.123419ms 614.772353ms 643.258211ms 668.06476ms 670.298844ms 678.746835ms 696.528814ms 697.147672ms 711.194452ms 711.669788ms 711.93054ms 716.43454ms 717.254912ms 725.471665ms 725.581139ms 726.249421ms 728.283965ms 728.454037ms 729.440237ms 730.614739ms 731.16404ms 731.307874ms 732.388417ms 732.484802ms 733.757065ms 733.909161ms 735.093121ms 736.147135ms 736.367916ms 736.832994ms 738.370652ms 738.471227ms 738.477426ms 738.705067ms 738.872176ms 739.268215ms 740.056859ms 741.140765ms 741.771334ms 741.918054ms 741.977769ms 742.509932ms 742.72253ms 742.922342ms 743.011731ms 743.307053ms 743.53383ms 743.595546ms 743.747112ms 744.135412ms 744.295231ms 744.596318ms 745.236059ms 746.076705ms 746.176953ms 746.597172ms 746.692701ms 746.782269ms 747.178706ms 747.400541ms 747.510871ms 747.513499ms 747.742757ms 747.863235ms 748.838997ms 748.879251ms 749.023934ms 749.029652ms 749.078119ms 749.136492ms 749.458442ms 749.531809ms 749.675204ms 749.744732ms 749.884512ms 749.90945ms 749.937767ms 750.075904ms 750.198ms 750.550391ms 750.633517ms 751.099654ms 751.546266ms 751.576359ms 752.177364ms 752.64795ms 753.360837ms 753.467065ms 753.811547ms 755.646275ms 755.659376ms 757.181274ms 757.249675ms 757.333917ms 757.339547ms 757.538052ms 758.280146ms 758.355166ms 758.959197ms 758.963041ms 759.883539ms 760.885977ms 760.910935ms 761.030148ms 761.204052ms 761.25337ms 761.623658ms 762.619732ms 764.782874ms 765.529893ms 765.605729ms 765.876124ms 766.020163ms 767.037251ms 767.593402ms 768.85975ms 770.668636ms 770.806754ms 771.514433ms 772.475552ms 773.634179ms 774.647889ms 778.279442ms 785.909932ms 786.892398ms 799.931351ms 810.223692ms 810.948873ms]
Apr 10 12:52:45.142: INFO: 50 %ile: 736.367916ms
Apr 10 12:52:45.142: INFO: 90 %ile: 764.782874ms
Apr 10 12:52:45.142: INFO: 99 %ile: 810.223692ms
Apr 10 12:52:45.142: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 12:52:45.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3008" for this suite.
Apr 10 12:53:15.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 12:53:16.191: INFO: namespace svc-latency-3008 deletion completed in 31.011712522s
•
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 12:53:16.192: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-288
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 12:53:16.585: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9be64f61-5b8f-11e9-8d1d-a6f828030f0b" in namespace "projected-288" to be "success or failure"
Apr 10 12:53:16.651: INFO: Pod "downwardapi-volume-9be64f61-5b8f-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 65.869426ms
Apr 10 12:53:18.676: INFO: Pod "downwardapi-volume-9be64f61-5b8f-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091527176s
Apr 10 12:53:20.702: INFO: Pod "downwardapi-volume-9be64f61-5b8f-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.117594674s
STEP: Saw pod success
Apr 10 12:53:20.702: INFO: Pod "downwardapi-volume-9be64f61-5b8f-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 12:53:20.727: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downwardapi-volume-9be64f61-5b8f-11e9-8d1d-a6f828030f0b container client-container: <nil>
STEP: delete the pod
Apr 10 12:53:20.999: INFO: Waiting for pod downwardapi-volume-9be64f61-5b8f-11e9-8d1d-a6f828030f0b to disappear
Apr 10 12:53:21.023: INFO: Pod downwardapi-volume-9be64f61-5b8f-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 12:53:21.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-288" for this suite.
Apr 10 12:53:27.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 12:53:28.060: INFO: namespace projected-288 deletion completed in 7.011391589s
•SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 12:53:28.060: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-167
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Apr 10 12:53:28.391: INFO: Waiting up to 5m0s for pod "pod-a2f0886a-5b8f-11e9-8d1d-a6f828030f0b" in namespace "emptydir-167" to be "success or failure"
Apr 10 12:53:28.428: INFO: Pod "pod-a2f0886a-5b8f-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 37.605816ms
Apr 10 12:53:30.454: INFO: Pod "pod-a2f0886a-5b8f-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063192857s
Apr 10 12:53:32.480: INFO: Pod "pod-a2f0886a-5b8f-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.088736428s
STEP: Saw pod success
Apr 10 12:53:32.480: INFO: Pod "pod-a2f0886a-5b8f-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 12:53:32.505: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-a2f0886a-5b8f-11e9-8d1d-a6f828030f0b container test-container: <nil>
STEP: delete the pod
Apr 10 12:53:32.577: INFO: Waiting for pod pod-a2f0886a-5b8f-11e9-8d1d-a6f828030f0b to disappear
Apr 10 12:53:32.601: INFO: Pod pod-a2f0886a-5b8f-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 12:53:32.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-167" for this suite.
Apr 10 12:53:38.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 12:53:39.746: INFO: namespace emptydir-167 deletion completed in 7.119919167s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 12:53:39.746: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8316
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 12:53:40.063: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-8316'
Apr 10 12:53:40.397: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 10 12:53:40.397: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Apr 10 12:53:42.454: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-8316'
Apr 10 12:53:42.747: INFO: stderr: ""
Apr 10 12:53:42.747: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 12:53:42.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8316" for this suite.
Apr 10 12:54:04.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 12:54:05.783: INFO: namespace kubectl-8316 deletion completed in 23.011393939s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 12:54:05.784: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3048
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-3048
Apr 10 12:54:10.316: INFO: Started pod liveness-exec in namespace container-probe-3048
STEP: checking the pod's current state and verifying that restartCount is present
Apr 10 12:54:10.340: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 12:58:11.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3048" for this suite.
Apr 10 12:58:17.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 12:58:18.569: INFO: namespace container-probe-3048 deletion completed in 7.005119763s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 12:58:18.569: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7030
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 12:58:18.891: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 12:58:20.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7030" for this suite.
Apr 10 12:58:26.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 12:58:27.175: INFO: namespace custom-resource-definition-7030 deletion completed in 7.00419658s
•SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 12:58:27.175: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 12:58:27.545: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr 10 12:58:27.605: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 10 12:58:32.630: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 10 12:58:32.630: INFO: Creating deployment "test-rolling-update-deployment"
Apr 10 12:58:32.656: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr 10 12:58:32.718: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr 10 12:58:32.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690497912, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690497912, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690497912, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690497912, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 12:58:34.771: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690497912, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690497912, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690497912, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690497912, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 12:58:36.770: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690497912, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690497912, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690497912, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690497912, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 12:58:38.769: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 10 12:58:38.844: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-4869,SelfLink:/apis/apps/v1/namespaces/deployment-4869/deployments/test-rolling-update-deployment,UID:584f262f-5b90-11e9-99e4-7640922b69f1,ResourceVersion:5743,Generation:1,CreationTimestamp:2019-04-10 12:58:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-04-10 12:58:32 +0000 UTC 2019-04-10 12:58:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-04-10 12:58:38 +0000 UTC 2019-04-10 12:58:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Apr 10 12:58:38.870: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-4869,SelfLink:/apis/apps/v1/namespaces/deployment-4869/replicasets/test-rolling-update-deployment-67599b4d9,UID:5851430b-5b90-11e9-99e4-7640922b69f1,ResourceVersion:5736,Generation:1,CreationTimestamp:2019-04-10 12:58:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 584f262f-5b90-11e9-99e4-7640922b69f1 0xc000b9ead0 0xc000b9ead1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr 10 12:58:38.870: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr 10 12:58:38.870: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-4869,SelfLink:/apis/apps/v1/namespaces/deployment-4869/replicasets/test-rolling-update-controller,UID:55472f68-5b90-11e9-99e4-7640922b69f1,ResourceVersion:5742,Generation:2,CreationTimestamp:2019-04-10 12:58:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 584f262f-5b90-11e9-99e4-7640922b69f1 0xc000b9e9ff 0xc000b9ea10}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 12:58:38.896: INFO: Pod "test-rolling-update-deployment-67599b4d9-mstbt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-mstbt,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-4869,SelfLink:/api/v1/namespaces/deployment-4869/pods/test-rolling-update-deployment-67599b4d9-mstbt,UID:5851e621-5b90-11e9-99e4-7640922b69f1,ResourceVersion:5735,Generation:0,CreationTimestamp:2019-04-10 12:58:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.14/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 5851430b-5b90-11e9-99e4-7640922b69f1 0xc000b9f340 0xc000b9f341}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xr77q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xr77q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-xr77q true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b9f3a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b9f3c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 12:58:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 12:58:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 12:58:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 12:58:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.14,StartTime:2019-04-10 12:58:32 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-04-10 12:58:37 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://9297b48c5397e979c808f4a155c2cb3141f3f6f453af00c9e6e97383b55404d7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 12:58:38.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4869" for this suite.
Apr 10 12:58:45.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 12:58:45.915: INFO: namespace deployment-4869 deletion completed in 6.993806625s
•SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 12:58:45.915: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1208
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 10 12:58:46.285: INFO: Waiting up to 5m0s for pod "pod-606b4b42-5b90-11e9-8d1d-a6f828030f0b" in namespace "emptydir-1208" to be "success or failure"
Apr 10 12:58:46.310: INFO: Pod "pod-606b4b42-5b90-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.26431ms
Apr 10 12:58:48.336: INFO: Pod "pod-606b4b42-5b90-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051035086s
Apr 10 12:58:50.361: INFO: Pod "pod-606b4b42-5b90-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076694876s
STEP: Saw pod success
Apr 10 12:58:50.362: INFO: Pod "pod-606b4b42-5b90-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 12:58:50.394: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-606b4b42-5b90-11e9-8d1d-a6f828030f0b container test-container: <nil>
STEP: delete the pod
Apr 10 12:58:50.471: INFO: Waiting for pod pod-606b4b42-5b90-11e9-8d1d-a6f828030f0b to disappear
Apr 10 12:58:50.495: INFO: Pod pod-606b4b42-5b90-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 12:58:50.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1208" for this suite.
Apr 10 12:58:56.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 12:58:57.528: INFO: namespace emptydir-1208 deletion completed in 6.994827493s
•S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 12:58:57.528: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7197
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 10 12:58:57.893: INFO: Waiting up to 5m0s for pod "pod-67567f4b-5b90-11e9-8d1d-a6f828030f0b" in namespace "emptydir-7197" to be "success or failure"
Apr 10 12:58:57.917: INFO: Pod "pod-67567f4b-5b90-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.628875ms
Apr 10 12:58:59.943: INFO: Pod "pod-67567f4b-5b90-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050403536s
Apr 10 12:59:01.971: INFO: Pod "pod-67567f4b-5b90-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078291742s
Apr 10 12:59:03.997: INFO: Pod "pod-67567f4b-5b90-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.104110411s
STEP: Saw pod success
Apr 10 12:59:03.997: INFO: Pod "pod-67567f4b-5b90-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 12:59:04.022: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-67567f4b-5b90-11e9-8d1d-a6f828030f0b container test-container: <nil>
STEP: delete the pod
Apr 10 12:59:04.094: INFO: Waiting for pod pod-67567f4b-5b90-11e9-8d1d-a6f828030f0b to disappear
Apr 10 12:59:04.119: INFO: Pod pod-67567f4b-5b90-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 12:59:04.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7197" for this suite.
Apr 10 12:59:10.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 12:59:11.147: INFO: namespace emptydir-7197 deletion completed in 7.003810067s
•SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 12:59:11.147: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7694
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Apr 10 12:59:11.479: INFO: namespace kubectl-7694
Apr 10 12:59:11.479: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7694'
Apr 10 12:59:12.039: INFO: stderr: ""
Apr 10 12:59:12.040: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 10 12:59:13.068: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 12:59:13.068: INFO: Found 0 / 1
Apr 10 12:59:14.066: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 12:59:14.066: INFO: Found 0 / 1
Apr 10 12:59:15.065: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 12:59:15.065: INFO: Found 1 / 1
Apr 10 12:59:15.065: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 10 12:59:15.098: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 12:59:15.098: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 10 12:59:15.098: INFO: wait on redis-master startup in kubectl-7694 
Apr 10 12:59:15.098: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-8vl54 redis-master --namespace=kubectl-7694'
Apr 10 12:59:15.419: INFO: stderr: ""
Apr 10 12:59:15.419: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 10 Apr 12:59:14.423 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 10 Apr 12:59:14.423 # Server started, Redis version 3.2.12\n1:M 10 Apr 12:59:14.423 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 10 Apr 12:59:14.423 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Apr 10 12:59:15.419: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-7694'
Apr 10 12:59:15.695: INFO: stderr: ""
Apr 10 12:59:15.695: INFO: stdout: "service/rm2 exposed\n"
Apr 10 12:59:15.719: INFO: Service rm2 in namespace kubectl-7694 found.
STEP: exposing service
Apr 10 12:59:17.771: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-7694'
Apr 10 12:59:18.080: INFO: stderr: ""
Apr 10 12:59:18.080: INFO: stdout: "service/rm3 exposed\n"
Apr 10 12:59:18.105: INFO: Service rm3 in namespace kubectl-7694 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 12:59:20.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7694" for this suite.
Apr 10 12:59:44.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 12:59:45.202: INFO: namespace kubectl-7694 deletion completed in 25.021429533s
•S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 12:59:45.202: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2062
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-83b6eea7-5b90-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume configMaps
Apr 10 12:59:45.526: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-83bac595-5b90-11e9-8d1d-a6f828030f0b" in namespace "projected-2062" to be "success or failure"
Apr 10 12:59:45.551: INFO: Pod "pod-projected-configmaps-83bac595-5b90-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.655474ms
Apr 10 12:59:47.576: INFO: Pod "pod-projected-configmaps-83bac595-5b90-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050297414s
Apr 10 12:59:49.602: INFO: Pod "pod-projected-configmaps-83bac595-5b90-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076148178s
STEP: Saw pod success
Apr 10 12:59:49.602: INFO: Pod "pod-projected-configmaps-83bac595-5b90-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 12:59:49.628: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-projected-configmaps-83bac595-5b90-11e9-8d1d-a6f828030f0b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 12:59:49.743: INFO: Waiting for pod pod-projected-configmaps-83bac595-5b90-11e9-8d1d-a6f828030f0b to disappear
Apr 10 12:59:49.768: INFO: Pod pod-projected-configmaps-83bac595-5b90-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 12:59:49.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2062" for this suite.
Apr 10 12:59:55.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 12:59:56.839: INFO: namespace projected-2062 deletion completed in 7.045906967s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 12:59:56.840: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8306
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Apr 10 12:59:57.274: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8306,SelfLink:/api/v1/namespaces/watch-8306/configmaps/e2e-watch-test-configmap-a,UID:8abedaea-5b90-11e9-99e4-7640922b69f1,ResourceVersion:6029,Generation:0,CreationTimestamp:2019-04-10 12:59:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 10 12:59:57.274: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8306,SelfLink:/api/v1/namespaces/watch-8306/configmaps/e2e-watch-test-configmap-a,UID:8abedaea-5b90-11e9-99e4-7640922b69f1,ResourceVersion:6029,Generation:0,CreationTimestamp:2019-04-10 12:59:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Apr 10 13:00:07.326: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8306,SelfLink:/api/v1/namespaces/watch-8306/configmaps/e2e-watch-test-configmap-a,UID:8abedaea-5b90-11e9-99e4-7640922b69f1,ResourceVersion:6052,Generation:0,CreationTimestamp:2019-04-10 12:59:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Apr 10 13:00:07.326: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8306,SelfLink:/api/v1/namespaces/watch-8306/configmaps/e2e-watch-test-configmap-a,UID:8abedaea-5b90-11e9-99e4-7640922b69f1,ResourceVersion:6052,Generation:0,CreationTimestamp:2019-04-10 12:59:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Apr 10 13:00:17.379: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8306,SelfLink:/api/v1/namespaces/watch-8306/configmaps/e2e-watch-test-configmap-a,UID:8abedaea-5b90-11e9-99e4-7640922b69f1,ResourceVersion:6074,Generation:0,CreationTimestamp:2019-04-10 12:59:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 10 13:00:17.379: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8306,SelfLink:/api/v1/namespaces/watch-8306/configmaps/e2e-watch-test-configmap-a,UID:8abedaea-5b90-11e9-99e4-7640922b69f1,ResourceVersion:6074,Generation:0,CreationTimestamp:2019-04-10 12:59:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Apr 10 13:00:27.408: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8306,SelfLink:/api/v1/namespaces/watch-8306/configmaps/e2e-watch-test-configmap-a,UID:8abedaea-5b90-11e9-99e4-7640922b69f1,ResourceVersion:6096,Generation:0,CreationTimestamp:2019-04-10 12:59:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 10 13:00:27.408: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8306,SelfLink:/api/v1/namespaces/watch-8306/configmaps/e2e-watch-test-configmap-a,UID:8abedaea-5b90-11e9-99e4-7640922b69f1,ResourceVersion:6096,Generation:0,CreationTimestamp:2019-04-10 12:59:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Apr 10 13:00:37.436: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8306,SelfLink:/api/v1/namespaces/watch-8306/configmaps/e2e-watch-test-configmap-b,UID:a2aedcba-5b90-11e9-99e4-7640922b69f1,ResourceVersion:6120,Generation:0,CreationTimestamp:2019-04-10 13:00:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 10 13:00:37.436: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8306,SelfLink:/api/v1/namespaces/watch-8306/configmaps/e2e-watch-test-configmap-b,UID:a2aedcba-5b90-11e9-99e4-7640922b69f1,ResourceVersion:6120,Generation:0,CreationTimestamp:2019-04-10 13:00:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Apr 10 13:00:47.464: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8306,SelfLink:/api/v1/namespaces/watch-8306/configmaps/e2e-watch-test-configmap-b,UID:a2aedcba-5b90-11e9-99e4-7640922b69f1,ResourceVersion:6142,Generation:0,CreationTimestamp:2019-04-10 13:00:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 10 13:00:47.464: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8306,SelfLink:/api/v1/namespaces/watch-8306/configmaps/e2e-watch-test-configmap-b,UID:a2aedcba-5b90-11e9-99e4-7640922b69f1,ResourceVersion:6142,Generation:0,CreationTimestamp:2019-04-10 13:00:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:00:57.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8306" for this suite.
Apr 10 13:01:03.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:01:04.473: INFO: namespace watch-8306 deletion completed in 6.982006222s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:01:04.473: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6647
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 10 13:01:04.794: INFO: Waiting up to 5m0s for pod "pod-b2fa3019-5b90-11e9-8d1d-a6f828030f0b" in namespace "emptydir-6647" to be "success or failure"
Apr 10 13:01:04.824: INFO: Pod "pod-b2fa3019-5b90-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 29.705952ms
Apr 10 13:01:06.849: INFO: Pod "pod-b2fa3019-5b90-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054983138s
Apr 10 13:01:08.875: INFO: Pod "pod-b2fa3019-5b90-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.080705015s
STEP: Saw pod success
Apr 10 13:01:08.875: INFO: Pod "pod-b2fa3019-5b90-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:01:08.901: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-b2fa3019-5b90-11e9-8d1d-a6f828030f0b container test-container: <nil>
STEP: delete the pod
Apr 10 13:01:08.991: INFO: Waiting for pod pod-b2fa3019-5b90-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:01:09.015: INFO: Pod pod-b2fa3019-5b90-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:01:09.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6647" for this suite.
Apr 10 13:01:15.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:01:16.033: INFO: namespace emptydir-6647 deletion completed in 6.992076239s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:01:16.033: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4440
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr 10 13:01:16.362: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 10 13:01:16.412: INFO: Waiting for terminating namespaces to be deleted...
Apr 10 13:01:16.437: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm before test
Apr 10 13:01:16.495: INFO: vpn-shoot-7b874bf844-m7l5b from kube-system started at 2019-04-10 12:34:12 +0000 UTC (1 container statuses recorded)
Apr 10 13:01:16.495: INFO: 	Container vpn-shoot ready: true, restart count 0
Apr 10 13:01:16.495: INFO: node-exporter-5m2qq from kube-system started at 2019-04-10 12:33:52 +0000 UTC (1 container statuses recorded)
Apr 10 13:01:16.495: INFO: 	Container node-exporter ready: true, restart count 0
Apr 10 13:01:16.495: INFO: addons-kubernetes-dashboard-665df4b66d-t5x2l from kube-system started at 2019-04-10 12:34:10 +0000 UTC (1 container statuses recorded)
Apr 10 13:01:16.495: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr 10 13:01:16.495: INFO: calico-node-zjgnp from kube-system started at 2019-04-10 12:33:52 +0000 UTC (1 container statuses recorded)
Apr 10 13:01:16.495: INFO: 	Container calico-node ready: true, restart count 0
Apr 10 13:01:16.495: INFO: coredns-7f7f7978c8-gspr8 from kube-system started at 2019-04-10 12:34:10 +0000 UTC (1 container statuses recorded)
Apr 10 13:01:16.495: INFO: 	Container coredns ready: true, restart count 0
Apr 10 13:01:16.495: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-rlq45 from kube-system started at 2019-04-10 12:34:12 +0000 UTC (1 container statuses recorded)
Apr 10 13:01:16.495: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Apr 10 13:01:16.495: INFO: addons-nginx-ingress-controller-d4f8c9cc5-bns7v from kube-system started at 2019-04-10 12:34:12 +0000 UTC (1 container statuses recorded)
Apr 10 13:01:16.495: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Apr 10 13:01:16.495: INFO: metrics-server-74cbf65f76-94nf4 from kube-system started at 2019-04-10 12:34:12 +0000 UTC (1 container statuses recorded)
Apr 10 13:01:16.495: INFO: 	Container metrics-server ready: true, restart count 0
Apr 10 13:01:16.495: INFO: blackbox-exporter-6dc58dcffc-qtzx5 from kube-system started at 2019-04-10 12:33:52 +0000 UTC (1 container statuses recorded)
Apr 10 13:01:16.495: INFO: 	Container blackbox-exporter ready: true, restart count 0
Apr 10 13:01:16.495: INFO: kube-proxy-bxtp6 from kube-system started at 2019-04-10 12:33:52 +0000 UTC (1 container statuses recorded)
Apr 10 13:01:16.495: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 10 13:01:16.495: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp before test
Apr 10 13:01:16.528: INFO: calico-node-l99mz from kube-system started at 2019-04-10 12:33:56 +0000 UTC (1 container statuses recorded)
Apr 10 13:01:16.528: INFO: 	Container calico-node ready: true, restart count 0
Apr 10 13:01:16.528: INFO: kube-proxy-h2r8s from kube-system started at 2019-04-10 12:33:56 +0000 UTC (1 container statuses recorded)
Apr 10 13:01:16.528: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 10 13:01:16.528: INFO: node-exporter-n962l from kube-system started at 2019-04-10 12:33:56 +0000 UTC (1 container statuses recorded)
Apr 10 13:01:16.528: INFO: 	Container node-exporter ready: true, restart count 0
Apr 10 13:01:16.528: INFO: coredns-7f7f7978c8-grrwz from kube-system started at 2019-04-10 12:34:19 +0000 UTC (1 container statuses recorded)
Apr 10 13:01:16.528: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15941e790e517f39], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:01:17.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4440" for this suite.
Apr 10 13:01:23.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:01:24.684: INFO: namespace sched-pred-4440 deletion completed in 7.002265516s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70
•SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:01:24.685: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3737
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 13:01:25.013: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf06598b-5b90-11e9-8d1d-a6f828030f0b" in namespace "downward-api-3737" to be "success or failure"
Apr 10 13:01:25.038: INFO: Pod "downwardapi-volume-bf06598b-5b90-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.368672ms
Apr 10 13:01:27.065: INFO: Pod "downwardapi-volume-bf06598b-5b90-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051887559s
Apr 10 13:01:29.091: INFO: Pod "downwardapi-volume-bf06598b-5b90-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077816996s
STEP: Saw pod success
Apr 10 13:01:29.091: INFO: Pod "downwardapi-volume-bf06598b-5b90-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:01:29.116: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downwardapi-volume-bf06598b-5b90-11e9-8d1d-a6f828030f0b container client-container: <nil>
STEP: delete the pod
Apr 10 13:01:29.354: INFO: Waiting for pod downwardapi-volume-bf06598b-5b90-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:01:29.379: INFO: Pod downwardapi-volume-bf06598b-5b90-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:01:29.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3737" for this suite.
Apr 10 13:01:35.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:01:36.395: INFO: namespace downward-api-3737 deletion completed in 6.991389097s
•SSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:01:36.396: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3203
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-c600e3ca-5b90-11e9-8d1d-a6f828030f0b
STEP: Creating configMap with name cm-test-opt-upd-c600e42b-5b90-11e9-8d1d-a6f828030f0b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-c600e3ca-5b90-11e9-8d1d-a6f828030f0b
STEP: Updating configmap cm-test-opt-upd-c600e42b-5b90-11e9-8d1d-a6f828030f0b
STEP: Creating configMap with name cm-test-opt-create-c600e449-5b90-11e9-8d1d-a6f828030f0b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:03:13.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3203" for this suite.
Apr 10 13:03:35.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:03:36.123: INFO: namespace configmap-3203 deletion completed in 23.007945151s
•SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:03:36.123: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7472
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 10 13:03:36.491: INFO: Waiting up to 5m0s for pod "pod-0d65484b-5b91-11e9-8d1d-a6f828030f0b" in namespace "emptydir-7472" to be "success or failure"
Apr 10 13:03:36.515: INFO: Pod "pod-0d65484b-5b91-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.240149ms
Apr 10 13:03:38.540: INFO: Pod "pod-0d65484b-5b91-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049308608s
Apr 10 13:03:40.565: INFO: Pod "pod-0d65484b-5b91-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074523185s
STEP: Saw pod success
Apr 10 13:03:40.565: INFO: Pod "pod-0d65484b-5b91-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:03:40.593: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-0d65484b-5b91-11e9-8d1d-a6f828030f0b container test-container: <nil>
STEP: delete the pod
Apr 10 13:03:40.704: INFO: Waiting for pod pod-0d65484b-5b91-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:03:40.729: INFO: Pod pod-0d65484b-5b91-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:03:40.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7472" for this suite.
Apr 10 13:03:46.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:03:47.802: INFO: namespace emptydir-7472 deletion completed in 7.034910513s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:03:47.803: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4851
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr 10 13:03:48.144: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:03:54.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4851" for this suite.
Apr 10 13:04:01.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:04:01.987: INFO: namespace init-container-4851 deletion completed in 7.009022562s
•SSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:04:01.987: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4983
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-1cca7bbd-5b91-11e9-8d1d-a6f828030f0b
STEP: Creating secret with name s-test-opt-upd-1cca7c31-5b91-11e9-8d1d-a6f828030f0b
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-1cca7bbd-5b91-11e9-8d1d-a6f828030f0b
STEP: Updating secret s-test-opt-upd-1cca7c31-5b91-11e9-8d1d-a6f828030f0b
STEP: Creating secret with name s-test-opt-create-1cca7d30-5b91-11e9-8d1d-a6f828030f0b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:05:15.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4983" for this suite.
Apr 10 13:05:34.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:05:34.947: INFO: namespace secrets-4983 deletion completed in 18.990141636s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:05:34.948: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-405
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr 10 13:05:40.141: INFO: Successfully updated pod "annotationupdate5434ea83-5b91-11e9-8d1d-a6f828030f0b"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:05:42.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-405" for this suite.
Apr 10 13:06:04.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:06:05.225: INFO: namespace downward-api-405 deletion completed in 22.989997201s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:06:05.226: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6629
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 13:06:05.592: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66442f8a-5b91-11e9-8d1d-a6f828030f0b" in namespace "projected-6629" to be "success or failure"
Apr 10 13:06:05.617: INFO: Pod "downwardapi-volume-66442f8a-5b91-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.578096ms
Apr 10 13:06:07.642: INFO: Pod "downwardapi-volume-66442f8a-5b91-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050018304s
Apr 10 13:06:09.672: INFO: Pod "downwardapi-volume-66442f8a-5b91-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.080041971s
STEP: Saw pod success
Apr 10 13:06:09.672: INFO: Pod "downwardapi-volume-66442f8a-5b91-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:06:09.697: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downwardapi-volume-66442f8a-5b91-11e9-8d1d-a6f828030f0b container client-container: <nil>
STEP: delete the pod
Apr 10 13:06:09.761: INFO: Waiting for pod downwardapi-volume-66442f8a-5b91-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:06:09.786: INFO: Pod downwardapi-volume-66442f8a-5b91-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:06:09.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6629" for this suite.
Apr 10 13:06:15.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:06:16.863: INFO: namespace projected-6629 deletion completed in 7.044081564s
•SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:06:16.863: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8531
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
Apr 10 13:06:17.173: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-8531'
Apr 10 13:06:17.818: INFO: stderr: ""
Apr 10 13:06:17.818: INFO: stdout: "pod/pause created\n"
Apr 10 13:06:17.818: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr 10 13:06:17.818: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8531" to be "running and ready"
Apr 10 13:06:17.843: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 24.671597ms
Apr 10 13:06:19.871: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05288008s
Apr 10 13:06:21.897: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.078935817s
Apr 10 13:06:21.897: INFO: Pod "pause" satisfied condition "running and ready"
Apr 10 13:06:21.897: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Apr 10 13:06:21.897: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label=testing-label-value --namespace=kubectl-8531'
Apr 10 13:06:22.136: INFO: stderr: ""
Apr 10 13:06:22.136: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Apr 10 13:06:22.136: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-8531'
Apr 10 13:06:22.326: INFO: stderr: ""
Apr 10 13:06:22.326: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Apr 10 13:06:22.327: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label- --namespace=kubectl-8531'
Apr 10 13:06:22.533: INFO: stderr: ""
Apr 10 13:06:22.533: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Apr 10 13:06:22.533: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-8531'
Apr 10 13:06:22.718: INFO: stderr: ""
Apr 10 13:06:22.718: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
Apr 10 13:06:22.718: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-8531'
Apr 10 13:06:22.934: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 13:06:22.934: INFO: stdout: "pod \"pause\" force deleted\n"
Apr 10 13:06:22.934: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=pause --no-headers --namespace=kubectl-8531'
Apr 10 13:06:23.225: INFO: stderr: "No resources found.\n"
Apr 10 13:06:23.225: INFO: stdout: ""
Apr 10 13:06:23.225: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=pause --namespace=kubectl-8531 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 10 13:06:23.405: INFO: stderr: ""
Apr 10 13:06:23.405: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:06:23.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8531" for this suite.
Apr 10 13:06:29.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:06:30.433: INFO: namespace kubectl-8531 deletion completed in 7.003341385s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:06:30.434: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8376
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Apr 10 13:06:30.771: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config api-versions'
Apr 10 13:06:31.021: INFO: stderr: ""
Apr 10 13:06:31.021: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:06:31.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8376" for this suite.
Apr 10 13:06:37.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:06:38.046: INFO: namespace kubectl-8376 deletion completed in 7.00012344s
•SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:06:38.046: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3345
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-79d256e5-5b91-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume secrets
Apr 10 13:06:38.424: INFO: Waiting up to 5m0s for pod "pod-secrets-79d623d5-5b91-11e9-8d1d-a6f828030f0b" in namespace "secrets-3345" to be "success or failure"
Apr 10 13:06:38.449: INFO: Pod "pod-secrets-79d623d5-5b91-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.432254ms
Apr 10 13:06:40.475: INFO: Pod "pod-secrets-79d623d5-5b91-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050046202s
Apr 10 13:06:42.500: INFO: Pod "pod-secrets-79d623d5-5b91-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075588767s
STEP: Saw pod success
Apr 10 13:06:42.500: INFO: Pod "pod-secrets-79d623d5-5b91-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:06:42.529: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-secrets-79d623d5-5b91-11e9-8d1d-a6f828030f0b container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 13:06:42.604: INFO: Waiting for pod pod-secrets-79d623d5-5b91-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:06:42.628: INFO: Pod pod-secrets-79d623d5-5b91-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:06:42.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3345" for this suite.
Apr 10 13:06:48.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:06:49.682: INFO: namespace secrets-3345 deletion completed in 7.009916109s
•SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:06:49.683: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7972
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 13:06:50.259: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config version'
Apr 10 13:06:50.510: INFO: stderr: ""
Apr 10 13:06:50.510: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:53:57Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:45:25Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:06:50.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7972" for this suite.
Apr 10 13:06:56.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:06:57.558: INFO: namespace kubectl-7972 deletion completed in 7.023333338s
•
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:06:57.559: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1134
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1134.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1134.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1134.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1134.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1134.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1134.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 10 13:07:20.552: INFO: DNS probes using dns-1134/dns-test-85736540-5b91-11e9-8d1d-a6f828030f0b succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:07:20.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1134" for this suite.
Apr 10 13:07:26.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:07:27.628: INFO: namespace dns-1134 deletion completed in 7.007253981s
•SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:07:27.629: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4680
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-4680
Apr 10 13:07:34.048: INFO: Started pod liveness-http in namespace container-probe-4680
STEP: checking the pod's current state and verifying that restartCount is present
Apr 10 13:07:34.073: INFO: Initial restart count of pod liveness-http is 0
Apr 10 13:07:58.422: INFO: Restart count of pod container-probe-4680/liveness-http is now 1 (24.348638176s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:07:58.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4680" for this suite.
Apr 10 13:08:04.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:08:05.522: INFO: namespace container-probe-4680 deletion completed in 7.033416428s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:08:05.523: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1308
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-adfcb52e-5b91-11e9-8d1d-a6f828030f0b
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-adfcb52e-5b91-11e9-8d1d-a6f828030f0b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:09:17.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1308" for this suite.
Apr 10 13:09:39.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:09:40.732: INFO: namespace projected-1308 deletion completed in 23.027896536s
•SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:09:40.733: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3241
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:09:45.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3241" for this suite.
Apr 10 13:10:07.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:10:08.338: INFO: namespace replication-controller-3241 deletion completed in 23.013658856s
•SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:10:08.339: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1527
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 10 13:10:08.693: INFO: Waiting up to 5m0s for pod "pod-f72a7594-5b91-11e9-8d1d-a6f828030f0b" in namespace "emptydir-1527" to be "success or failure"
Apr 10 13:10:08.728: INFO: Pod "pod-f72a7594-5b91-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 34.750137ms
Apr 10 13:10:10.753: INFO: Pod "pod-f72a7594-5b91-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059794661s
Apr 10 13:10:12.778: INFO: Pod "pod-f72a7594-5b91-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.085282121s
STEP: Saw pod success
Apr 10 13:10:12.778: INFO: Pod "pod-f72a7594-5b91-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:10:12.803: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-f72a7594-5b91-11e9-8d1d-a6f828030f0b container test-container: <nil>
STEP: delete the pod
Apr 10 13:10:12.941: INFO: Waiting for pod pod-f72a7594-5b91-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:10:12.965: INFO: Pod pod-f72a7594-5b91-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:10:12.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1527" for this suite.
Apr 10 13:10:19.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:10:20.016: INFO: namespace emptydir-1527 deletion completed in 7.025443609s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:10:20.016: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3690
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Apr 10 13:10:20.394: INFO: Waiting up to 5m0s for pod "client-containers-fe234ce0-5b91-11e9-8d1d-a6f828030f0b" in namespace "containers-3690" to be "success or failure"
Apr 10 13:10:20.419: INFO: Pod "client-containers-fe234ce0-5b91-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.852465ms
Apr 10 13:10:22.445: INFO: Pod "client-containers-fe234ce0-5b91-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050680353s
Apr 10 13:10:24.471: INFO: Pod "client-containers-fe234ce0-5b91-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076421952s
Apr 10 13:10:26.501: INFO: Pod "client-containers-fe234ce0-5b91-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.106928169s
STEP: Saw pod success
Apr 10 13:10:26.501: INFO: Pod "client-containers-fe234ce0-5b91-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:10:26.526: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod client-containers-fe234ce0-5b91-11e9-8d1d-a6f828030f0b container test-container: <nil>
STEP: delete the pod
Apr 10 13:10:26.591: INFO: Waiting for pod client-containers-fe234ce0-5b91-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:10:26.617: INFO: Pod client-containers-fe234ce0-5b91-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:10:26.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3690" for this suite.
Apr 10 13:10:32.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:10:33.658: INFO: namespace containers-3690 deletion completed in 7.015514196s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:10:33.659: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6145
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0410 13:11:14.176411    3207 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 10 13:11:14.176: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:11:14.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6145" for this suite.
Apr 10 13:11:20.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:11:21.224: INFO: namespace gc-6145 deletion completed in 7.023044042s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:11:21.225: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-8127
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-8127
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8127
STEP: Deleting pre-stop pod
Apr 10 13:11:36.907: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:11:36.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8127" for this suite.
Apr 10 13:12:17.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:12:17.984: INFO: namespace prestop-8127 deletion completed in 41.025001279s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:12:17.984: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8667
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 10 13:12:22.943: INFO: Successfully updated pod "pod-update-4468fba3-5b92-11e9-8d1d-a6f828030f0b"
STEP: verifying the updated pod is in kubernetes
Apr 10 13:12:23.006: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:12:23.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8667" for this suite.
Apr 10 13:12:45.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:12:46.063: INFO: namespace pods-8667 deletion completed in 23.030879928s
•SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:12:46.063: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1366
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 13:12:46.390: INFO: Waiting up to 5m0s for pod "downwardapi-volume-55292308-5b92-11e9-8d1d-a6f828030f0b" in namespace "projected-1366" to be "success or failure"
Apr 10 13:12:46.426: INFO: Pod "downwardapi-volume-55292308-5b92-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 35.513258ms
Apr 10 13:12:48.452: INFO: Pod "downwardapi-volume-55292308-5b92-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061772819s
Apr 10 13:12:50.478: INFO: Pod "downwardapi-volume-55292308-5b92-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.087696662s
STEP: Saw pod success
Apr 10 13:12:50.478: INFO: Pod "downwardapi-volume-55292308-5b92-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:12:50.503: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downwardapi-volume-55292308-5b92-11e9-8d1d-a6f828030f0b container client-container: <nil>
STEP: delete the pod
Apr 10 13:12:50.622: INFO: Waiting for pod downwardapi-volume-55292308-5b92-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:12:50.648: INFO: Pod downwardapi-volume-55292308-5b92-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:12:50.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1366" for this suite.
Apr 10 13:12:56.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:12:57.663: INFO: namespace projected-1366 deletion completed in 6.989972007s
•SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:12:57.663: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7807
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-5c13fe84-5b92-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume configMaps
Apr 10 13:12:58.024: INFO: Waiting up to 5m0s for pod "pod-configmaps-5c181817-5b92-11e9-8d1d-a6f828030f0b" in namespace "configmap-7807" to be "success or failure"
Apr 10 13:12:58.049: INFO: Pod "pod-configmaps-5c181817-5b92-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.843236ms
Apr 10 13:13:00.075: INFO: Pod "pod-configmaps-5c181817-5b92-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050333855s
Apr 10 13:13:02.100: INFO: Pod "pod-configmaps-5c181817-5b92-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075723319s
STEP: Saw pod success
Apr 10 13:13:02.100: INFO: Pod "pod-configmaps-5c181817-5b92-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:13:02.125: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-configmaps-5c181817-5b92-11e9-8d1d-a6f828030f0b container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 13:13:02.192: INFO: Waiting for pod pod-configmaps-5c181817-5b92-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:13:02.217: INFO: Pod pod-configmaps-5c181817-5b92-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:13:02.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7807" for this suite.
Apr 10 13:13:08.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:13:09.245: INFO: namespace configmap-7807 deletion completed in 7.003957098s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:13:09.246: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6145
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-630169de-5b92-11e9-8d1d-a6f828030f0b
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:13:13.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6145" for this suite.
Apr 10 13:13:35.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:13:36.872: INFO: namespace configmap-6145 deletion completed in 22.99781545s
•SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:13:36.872: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8979
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-73708686-5b92-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume configMaps
Apr 10 13:13:37.213: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-73744bd8-5b92-11e9-8d1d-a6f828030f0b" in namespace "projected-8979" to be "success or failure"
Apr 10 13:13:37.238: INFO: Pod "pod-projected-configmaps-73744bd8-5b92-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.520352ms
Apr 10 13:13:39.264: INFO: Pod "pod-projected-configmaps-73744bd8-5b92-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050055418s
Apr 10 13:13:41.289: INFO: Pod "pod-projected-configmaps-73744bd8-5b92-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075975132s
STEP: Saw pod success
Apr 10 13:13:41.290: INFO: Pod "pod-projected-configmaps-73744bd8-5b92-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:13:41.315: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-projected-configmaps-73744bd8-5b92-11e9-8d1d-a6f828030f0b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 13:13:41.402: INFO: Waiting for pod pod-projected-configmaps-73744bd8-5b92-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:13:41.426: INFO: Pod pod-projected-configmaps-73744bd8-5b92-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:13:41.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8979" for this suite.
Apr 10 13:13:47.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:13:48.517: INFO: namespace projected-8979 deletion completed in 7.064950128s
•SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:13:48.517: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8477
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-8477
Apr 10 13:13:52.951: INFO: Started pod liveness-http in namespace container-probe-8477
STEP: checking the pod's current state and verifying that restartCount is present
Apr 10 13:13:52.976: INFO: Initial restart count of pod liveness-http is 0
Apr 10 13:14:07.185: INFO: Restart count of pod container-probe-8477/liveness-http is now 1 (14.20856441s elapsed)
Apr 10 13:14:27.442: INFO: Restart count of pod container-probe-8477/liveness-http is now 2 (34.466258238s elapsed)
Apr 10 13:14:47.707: INFO: Restart count of pod container-probe-8477/liveness-http is now 3 (54.730856449s elapsed)
Apr 10 13:15:07.963: INFO: Restart count of pod container-probe-8477/liveness-http is now 4 (1m14.987153259s elapsed)
Apr 10 13:15:26.194: INFO: Restart count of pod container-probe-8477/liveness-http is now 5 (1m33.21799136s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:15:26.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8477" for this suite.
Apr 10 13:15:32.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:15:33.271: INFO: namespace container-probe-8477 deletion completed in 7.015866307s
•SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:15:33.271: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-379
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 13:15:53.698: INFO: Container started at 2019-04-10 13:15:36 +0000 UTC, pod became ready at 2019-04-10 13:15:53 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:15:53.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-379" for this suite.
Apr 10 13:16:15.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:16:16.746: INFO: namespace container-probe-379 deletion completed in 23.021404881s
•SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:16:16.746: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6269
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-qj79
STEP: Creating a pod to test atomic-volume-subpath
Apr 10 13:16:17.165: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-qj79" in namespace "subpath-6269" to be "success or failure"
Apr 10 13:16:17.190: INFO: Pod "pod-subpath-test-secret-qj79": Phase="Pending", Reason="", readiness=false. Elapsed: 24.626654ms
Apr 10 13:16:19.215: INFO: Pod "pod-subpath-test-secret-qj79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050049833s
Apr 10 13:16:21.241: INFO: Pod "pod-subpath-test-secret-qj79": Phase="Running", Reason="", readiness=true. Elapsed: 4.07573935s
Apr 10 13:16:23.267: INFO: Pod "pod-subpath-test-secret-qj79": Phase="Running", Reason="", readiness=true. Elapsed: 6.101755045s
Apr 10 13:16:25.293: INFO: Pod "pod-subpath-test-secret-qj79": Phase="Running", Reason="", readiness=true. Elapsed: 8.12785624s
Apr 10 13:16:27.319: INFO: Pod "pod-subpath-test-secret-qj79": Phase="Running", Reason="", readiness=true. Elapsed: 10.153776806s
Apr 10 13:16:29.345: INFO: Pod "pod-subpath-test-secret-qj79": Phase="Running", Reason="", readiness=true. Elapsed: 12.179720494s
Apr 10 13:16:31.371: INFO: Pod "pod-subpath-test-secret-qj79": Phase="Running", Reason="", readiness=true. Elapsed: 14.205720919s
Apr 10 13:16:33.397: INFO: Pod "pod-subpath-test-secret-qj79": Phase="Running", Reason="", readiness=true. Elapsed: 16.231772673s
Apr 10 13:16:35.422: INFO: Pod "pod-subpath-test-secret-qj79": Phase="Running", Reason="", readiness=true. Elapsed: 18.257317617s
Apr 10 13:16:37.448: INFO: Pod "pod-subpath-test-secret-qj79": Phase="Running", Reason="", readiness=true. Elapsed: 20.283155936s
Apr 10 13:16:39.474: INFO: Pod "pod-subpath-test-secret-qj79": Phase="Running", Reason="", readiness=true. Elapsed: 22.308596254s
Apr 10 13:16:41.499: INFO: Pod "pod-subpath-test-secret-qj79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.334222747s
STEP: Saw pod success
Apr 10 13:16:41.499: INFO: Pod "pod-subpath-test-secret-qj79" satisfied condition "success or failure"
Apr 10 13:16:41.525: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-subpath-test-secret-qj79 container test-container-subpath-secret-qj79: <nil>
STEP: delete the pod
Apr 10 13:16:41.591: INFO: Waiting for pod pod-subpath-test-secret-qj79 to disappear
Apr 10 13:16:41.616: INFO: Pod pod-subpath-test-secret-qj79 no longer exists
STEP: Deleting pod pod-subpath-test-secret-qj79
Apr 10 13:16:41.616: INFO: Deleting pod "pod-subpath-test-secret-qj79" in namespace "subpath-6269"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:16:41.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6269" for this suite.
Apr 10 13:16:47.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:16:48.696: INFO: namespace subpath-6269 deletion completed in 7.030552523s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:16:48.697: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-716
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 10 13:16:53.651: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e5c3b21a-5b92-11e9-8d1d-a6f828030f0b"
Apr 10 13:16:53.651: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e5c3b21a-5b92-11e9-8d1d-a6f828030f0b" in namespace "pods-716" to be "terminated due to deadline exceeded"
Apr 10 13:16:53.694: INFO: Pod "pod-update-activedeadlineseconds-e5c3b21a-5b92-11e9-8d1d-a6f828030f0b": Phase="Running", Reason="", readiness=true. Elapsed: 42.939347ms
Apr 10 13:16:55.721: INFO: Pod "pod-update-activedeadlineseconds-e5c3b21a-5b92-11e9-8d1d-a6f828030f0b": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.069468846s
Apr 10 13:16:55.721: INFO: Pod "pod-update-activedeadlineseconds-e5c3b21a-5b92-11e9-8d1d-a6f828030f0b" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:16:55.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-716" for this suite.
Apr 10 13:17:01.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:17:02.747: INFO: namespace pods-716 deletion completed in 7.001301886s
•
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:17:02.748: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8964
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-pq2v
STEP: Creating a pod to test atomic-volume-subpath
Apr 10 13:17:03.156: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pq2v" in namespace "subpath-8964" to be "success or failure"
Apr 10 13:17:03.187: INFO: Pod "pod-subpath-test-configmap-pq2v": Phase="Pending", Reason="", readiness=false. Elapsed: 31.151406ms
Apr 10 13:17:05.213: INFO: Pod "pod-subpath-test-configmap-pq2v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057117997s
Apr 10 13:17:07.239: INFO: Pod "pod-subpath-test-configmap-pq2v": Phase="Running", Reason="", readiness=true. Elapsed: 4.082966723s
Apr 10 13:17:09.265: INFO: Pod "pod-subpath-test-configmap-pq2v": Phase="Running", Reason="", readiness=true. Elapsed: 6.108986293s
Apr 10 13:17:11.290: INFO: Pod "pod-subpath-test-configmap-pq2v": Phase="Running", Reason="", readiness=true. Elapsed: 8.134916055s
Apr 10 13:17:13.316: INFO: Pod "pod-subpath-test-configmap-pq2v": Phase="Running", Reason="", readiness=true. Elapsed: 10.160704445s
Apr 10 13:17:15.342: INFO: Pod "pod-subpath-test-configmap-pq2v": Phase="Running", Reason="", readiness=true. Elapsed: 12.186240469s
Apr 10 13:17:17.367: INFO: Pod "pod-subpath-test-configmap-pq2v": Phase="Running", Reason="", readiness=true. Elapsed: 14.211936087s
Apr 10 13:17:19.393: INFO: Pod "pod-subpath-test-configmap-pq2v": Phase="Running", Reason="", readiness=true. Elapsed: 16.237869433s
Apr 10 13:17:21.419: INFO: Pod "pod-subpath-test-configmap-pq2v": Phase="Running", Reason="", readiness=true. Elapsed: 18.263462879s
Apr 10 13:17:23.445: INFO: Pod "pod-subpath-test-configmap-pq2v": Phase="Running", Reason="", readiness=true. Elapsed: 20.289751049s
Apr 10 13:17:25.472: INFO: Pod "pod-subpath-test-configmap-pq2v": Phase="Running", Reason="", readiness=true. Elapsed: 22.315976829s
Apr 10 13:17:27.497: INFO: Pod "pod-subpath-test-configmap-pq2v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.34186817s
STEP: Saw pod success
Apr 10 13:17:27.497: INFO: Pod "pod-subpath-test-configmap-pq2v" satisfied condition "success or failure"
Apr 10 13:17:27.522: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-subpath-test-configmap-pq2v container test-container-subpath-configmap-pq2v: <nil>
STEP: delete the pod
Apr 10 13:17:27.595: INFO: Waiting for pod pod-subpath-test-configmap-pq2v to disappear
Apr 10 13:17:27.619: INFO: Pod pod-subpath-test-configmap-pq2v no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pq2v
Apr 10 13:17:27.619: INFO: Deleting pod "pod-subpath-test-configmap-pq2v" in namespace "subpath-8964"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:17:27.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8964" for this suite.
Apr 10 13:17:33.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:17:34.719: INFO: namespace subpath-8964 deletion completed in 7.050644277s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:17:34.719: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9814
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 13:17:35.093: INFO: Waiting up to 5m0s for pod "downwardapi-volume-013dcbd3-5b93-11e9-8d1d-a6f828030f0b" in namespace "downward-api-9814" to be "success or failure"
Apr 10 13:17:35.118: INFO: Pod "downwardapi-volume-013dcbd3-5b93-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.509129ms
Apr 10 13:17:37.144: INFO: Pod "downwardapi-volume-013dcbd3-5b93-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050125858s
Apr 10 13:17:39.169: INFO: Pod "downwardapi-volume-013dcbd3-5b93-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075839025s
STEP: Saw pod success
Apr 10 13:17:39.169: INFO: Pod "downwardapi-volume-013dcbd3-5b93-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:17:39.194: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downwardapi-volume-013dcbd3-5b93-11e9-8d1d-a6f828030f0b container client-container: <nil>
STEP: delete the pod
Apr 10 13:17:39.449: INFO: Waiting for pod downwardapi-volume-013dcbd3-5b93-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:17:39.474: INFO: Pod downwardapi-volume-013dcbd3-5b93-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:17:39.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9814" for this suite.
Apr 10 13:17:45.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:17:46.486: INFO: namespace downward-api-9814 deletion completed in 6.987659565s
•SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:17:46.487: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-864
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Apr 10 13:17:51.419: INFO: Running '/bin/kubectl exec --namespace=svcaccounts-864 pod-service-account-088eca21-5b93-11e9-8d1d-a6f828030f0b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Apr 10 13:17:52.329: INFO: Running '/bin/kubectl exec --namespace=svcaccounts-864 pod-service-account-088eca21-5b93-11e9-8d1d-a6f828030f0b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Apr 10 13:17:52.982: INFO: Running '/bin/kubectl exec --namespace=svcaccounts-864 pod-service-account-088eca21-5b93-11e9-8d1d-a6f828030f0b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:17:53.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-864" for this suite.
Apr 10 13:17:59.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:18:00.729: INFO: namespace svcaccounts-864 deletion completed in 7.004691471s
•SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:18:00.729: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4218
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Apr 10 13:18:01.113: INFO: Waiting up to 5m0s for pod "client-containers-10c0102f-5b93-11e9-8d1d-a6f828030f0b" in namespace "containers-4218" to be "success or failure"
Apr 10 13:18:01.138: INFO: Pod "client-containers-10c0102f-5b93-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.788587ms
Apr 10 13:18:03.164: INFO: Pod "client-containers-10c0102f-5b93-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050561933s
Apr 10 13:18:05.190: INFO: Pod "client-containers-10c0102f-5b93-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076676574s
STEP: Saw pod success
Apr 10 13:18:05.190: INFO: Pod "client-containers-10c0102f-5b93-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:18:05.215: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod client-containers-10c0102f-5b93-11e9-8d1d-a6f828030f0b container test-container: <nil>
STEP: delete the pod
Apr 10 13:18:05.289: INFO: Waiting for pod client-containers-10c0102f-5b93-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:18:05.313: INFO: Pod client-containers-10c0102f-5b93-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:18:05.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4218" for this suite.
Apr 10 13:18:11.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:18:12.322: INFO: namespace containers-4218 deletion completed in 6.982978402s
•SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:18:12.322: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5288
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0410 13:18:18.862446    3207 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 10 13:18:18.862: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:18:18.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5288" for this suite.
Apr 10 13:18:24.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:18:25.892: INFO: namespace gc-5288 deletion completed in 7.005778723s
•SS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:18:25.893: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-290
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 10 13:18:26.215: INFO: Waiting up to 5m0s for pod "downward-api-1fb67830-5b93-11e9-8d1d-a6f828030f0b" in namespace "downward-api-290" to be "success or failure"
Apr 10 13:18:26.240: INFO: Pod "downward-api-1fb67830-5b93-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.410402ms
Apr 10 13:18:28.266: INFO: Pod "downward-api-1fb67830-5b93-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05017141s
Apr 10 13:18:30.291: INFO: Pod "downward-api-1fb67830-5b93-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075675891s
STEP: Saw pod success
Apr 10 13:18:30.291: INFO: Pod "downward-api-1fb67830-5b93-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:18:30.316: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downward-api-1fb67830-5b93-11e9-8d1d-a6f828030f0b container dapi-container: <nil>
STEP: delete the pod
Apr 10 13:18:30.440: INFO: Waiting for pod downward-api-1fb67830-5b93-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:18:30.464: INFO: Pod downward-api-1fb67830-5b93-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:18:30.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-290" for this suite.
Apr 10 13:18:36.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:18:37.511: INFO: namespace downward-api-290 deletion completed in 7.022050478s
•SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:18:37.512: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-25
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:18:37.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-25" for this suite.
Apr 10 13:18:44.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:18:44.963: INFO: namespace kubelet-test-25 deletion completed in 7.005421636s
•SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:18:44.963: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6682
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-2b1785c6-5b93-11e9-8d1d-a6f828030f0b
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-2b1785c6-5b93-11e9-8d1d-a6f828030f0b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:20:02.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6682" for this suite.
Apr 10 13:20:24.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:20:25.897: INFO: namespace configmap-6682 deletion completed in 22.996077155s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:20:25.897: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9483
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Apr 10 13:20:26.283: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-9483'
Apr 10 13:20:26.896: INFO: stderr: ""
Apr 10 13:20:26.896: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 10 13:20:26.896: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9483'
Apr 10 13:20:27.085: INFO: stderr: ""
Apr 10 13:20:27.085: INFO: stdout: "update-demo-nautilus-drffn update-demo-nautilus-pcxcx "
Apr 10 13:20:27.086: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-drffn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9483'
Apr 10 13:20:27.305: INFO: stderr: ""
Apr 10 13:20:27.305: INFO: stdout: ""
Apr 10 13:20:27.305: INFO: update-demo-nautilus-drffn is created but not running
Apr 10 13:20:32.305: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9483'
Apr 10 13:20:32.490: INFO: stderr: ""
Apr 10 13:20:32.490: INFO: stdout: "update-demo-nautilus-drffn update-demo-nautilus-pcxcx "
Apr 10 13:20:32.490: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-drffn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9483'
Apr 10 13:20:32.693: INFO: stderr: ""
Apr 10 13:20:32.693: INFO: stdout: "true"
Apr 10 13:20:32.693: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-drffn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9483'
Apr 10 13:20:32.883: INFO: stderr: ""
Apr 10 13:20:32.883: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 13:20:32.883: INFO: validating pod update-demo-nautilus-drffn
Apr 10 13:20:33.000: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 13:20:33.000: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 13:20:33.000: INFO: update-demo-nautilus-drffn is verified up and running
Apr 10 13:20:33.000: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-pcxcx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9483'
Apr 10 13:20:33.188: INFO: stderr: ""
Apr 10 13:20:33.188: INFO: stdout: "true"
Apr 10 13:20:33.188: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-pcxcx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9483'
Apr 10 13:20:33.389: INFO: stderr: ""
Apr 10 13:20:33.389: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 13:20:33.389: INFO: validating pod update-demo-nautilus-pcxcx
Apr 10 13:20:33.499: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 13:20:33.499: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 13:20:33.499: INFO: update-demo-nautilus-pcxcx is verified up and running
STEP: using delete to clean up resources
Apr 10 13:20:33.499: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-9483'
Apr 10 13:20:33.708: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 13:20:33.708: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 10 13:20:33.708: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9483'
Apr 10 13:20:33.909: INFO: stderr: "No resources found.\n"
Apr 10 13:20:33.909: INFO: stdout: ""
Apr 10 13:20:33.910: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-9483 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 10 13:20:34.087: INFO: stderr: ""
Apr 10 13:20:34.087: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:20:34.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9483" for this suite.
Apr 10 13:20:56.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:20:57.097: INFO: namespace kubectl-9483 deletion completed in 22.98535232s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:20:57.097: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-3992
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Apr 10 13:20:58.956: INFO: Pod name wrapped-volume-race-7ab866ee-5b93-11e9-8d1d-a6f828030f0b: Found 0 pods out of 5
Apr 10 13:21:04.006: INFO: Pod name wrapped-volume-race-7ab866ee-5b93-11e9-8d1d-a6f828030f0b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7ab866ee-5b93-11e9-8d1d-a6f828030f0b in namespace emptydir-wrapper-3992, will wait for the garbage collector to delete the pods
Apr 10 13:21:16.301: INFO: Deleting ReplicationController wrapped-volume-race-7ab866ee-5b93-11e9-8d1d-a6f828030f0b took: 27.861095ms
Apr 10 13:21:16.401: INFO: Terminating ReplicationController wrapped-volume-race-7ab866ee-5b93-11e9-8d1d-a6f828030f0b pods took: 100.36934ms
STEP: Creating RC which spawns configmap-volume pods
Apr 10 13:21:59.488: INFO: Pod name wrapped-volume-race-9ecc43d9-5b93-11e9-8d1d-a6f828030f0b: Found 3 pods out of 5
Apr 10 13:22:04.540: INFO: Pod name wrapped-volume-race-9ecc43d9-5b93-11e9-8d1d-a6f828030f0b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9ecc43d9-5b93-11e9-8d1d-a6f828030f0b in namespace emptydir-wrapper-3992, will wait for the garbage collector to delete the pods
Apr 10 13:22:14.798: INFO: Deleting ReplicationController wrapped-volume-race-9ecc43d9-5b93-11e9-8d1d-a6f828030f0b took: 27.489539ms
Apr 10 13:22:14.898: INFO: Terminating ReplicationController wrapped-volume-race-9ecc43d9-5b93-11e9-8d1d-a6f828030f0b pods took: 100.307902ms
STEP: Creating RC which spawns configmap-volume pods
Apr 10 13:22:59.402: INFO: Pod name wrapped-volume-race-c27fcc79-5b93-11e9-8d1d-a6f828030f0b: Found 3 pods out of 5
Apr 10 13:23:04.453: INFO: Pod name wrapped-volume-race-c27fcc79-5b93-11e9-8d1d-a6f828030f0b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c27fcc79-5b93-11e9-8d1d-a6f828030f0b in namespace emptydir-wrapper-3992, will wait for the garbage collector to delete the pods
Apr 10 13:23:12.734: INFO: Deleting ReplicationController wrapped-volume-race-c27fcc79-5b93-11e9-8d1d-a6f828030f0b took: 28.174668ms
Apr 10 13:23:13.435: INFO: Terminating ReplicationController wrapped-volume-race-c27fcc79-5b93-11e9-8d1d-a6f828030f0b pods took: 700.368578ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:24:00.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3992" for this suite.
Apr 10 13:24:08.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:24:09.917: INFO: namespace emptydir-wrapper-3992 deletion completed in 9.048287807s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:24:09.917: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4819
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-eccc823e-5b93-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume configMaps
Apr 10 13:24:10.318: INFO: Waiting up to 5m0s for pod "pod-configmaps-ecd0509e-5b93-11e9-8d1d-a6f828030f0b" in namespace "configmap-4819" to be "success or failure"
Apr 10 13:24:10.342: INFO: Pod "pod-configmaps-ecd0509e-5b93-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.754264ms
Apr 10 13:24:12.368: INFO: Pod "pod-configmaps-ecd0509e-5b93-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050398937s
Apr 10 13:24:14.394: INFO: Pod "pod-configmaps-ecd0509e-5b93-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076100457s
STEP: Saw pod success
Apr 10 13:24:14.394: INFO: Pod "pod-configmaps-ecd0509e-5b93-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:24:14.418: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-configmaps-ecd0509e-5b93-11e9-8d1d-a6f828030f0b container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 13:24:14.499: INFO: Waiting for pod pod-configmaps-ecd0509e-5b93-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:24:14.530: INFO: Pod pod-configmaps-ecd0509e-5b93-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:24:14.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4819" for this suite.
Apr 10 13:24:20.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:24:21.572: INFO: namespace configmap-4819 deletion completed in 7.016911735s
•SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:24:21.572: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9939
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:24:21.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9939" for this suite.
Apr 10 13:24:44.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:24:44.945: INFO: namespace pods-9939 deletion completed in 23.012913281s
•SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:24:44.945: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5215
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr 10 13:24:49.983: INFO: Successfully updated pod "annotationupdate01a79d32-5b94-11e9-8d1d-a6f828030f0b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:24:52.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5215" for this suite.
Apr 10 13:25:14.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:25:15.047: INFO: namespace projected-5215 deletion completed in 22.977165764s
•SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:25:15.048: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8338
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-13a62935-5b94-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume configMaps
Apr 10 13:25:15.502: INFO: Waiting up to 5m0s for pod "pod-configmaps-13a9f86b-5b94-11e9-8d1d-a6f828030f0b" in namespace "configmap-8338" to be "success or failure"
Apr 10 13:25:15.526: INFO: Pod "pod-configmaps-13a9f86b-5b94-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.626109ms
Apr 10 13:25:17.552: INFO: Pod "pod-configmaps-13a9f86b-5b94-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050433971s
Apr 10 13:25:19.578: INFO: Pod "pod-configmaps-13a9f86b-5b94-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076200808s
STEP: Saw pod success
Apr 10 13:25:19.578: INFO: Pod "pod-configmaps-13a9f86b-5b94-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:25:19.603: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-configmaps-13a9f86b-5b94-11e9-8d1d-a6f828030f0b container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 13:25:19.667: INFO: Waiting for pod pod-configmaps-13a9f86b-5b94-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:25:19.692: INFO: Pod pod-configmaps-13a9f86b-5b94-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:25:19.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8338" for this suite.
Apr 10 13:25:25.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:25:26.751: INFO: namespace configmap-8338 deletion completed in 7.034732254s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:25:26.751: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6438
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Apr 10 13:25:27.062: INFO: Asynchronously running '/bin/kubectl kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:25:27.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6438" for this suite.
Apr 10 13:25:33.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:25:34.304: INFO: namespace kubectl-6438 deletion completed in 6.996182209s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:25:34.304: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2671
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-2671
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 10 13:25:34.655: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 10 13:26:01.176: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.1.68 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2671 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 13:26:01.176: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 13:26:02.615: INFO: Found all expected endpoints: [netserver-0]
Apr 10 13:26:02.641: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.0.34 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2671 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 13:26:02.641: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 13:26:04.168: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:26:04.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2671" for this suite.
Apr 10 13:26:26.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:26:27.190: INFO: namespace pod-network-test-2671 deletion completed in 22.995125273s
•SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:26:27.190: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2438
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:27:27.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2438" for this suite.
Apr 10 13:27:49.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:27:50.701: INFO: namespace container-probe-2438 deletion completed in 23.044866597s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:27:50.702: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9863
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-70678f23-5b94-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume configMaps
Apr 10 13:27:51.116: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-706b52bb-5b94-11e9-8d1d-a6f828030f0b" in namespace "projected-9863" to be "success or failure"
Apr 10 13:27:51.141: INFO: Pod "pod-projected-configmaps-706b52bb-5b94-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.593765ms
Apr 10 13:27:53.166: INFO: Pod "pod-projected-configmaps-706b52bb-5b94-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050002733s
Apr 10 13:27:55.191: INFO: Pod "pod-projected-configmaps-706b52bb-5b94-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075228254s
STEP: Saw pod success
Apr 10 13:27:55.191: INFO: Pod "pod-projected-configmaps-706b52bb-5b94-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:27:55.216: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-projected-configmaps-706b52bb-5b94-11e9-8d1d-a6f828030f0b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 13:27:55.296: INFO: Waiting for pod pod-projected-configmaps-706b52bb-5b94-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:27:55.348: INFO: Pod pod-projected-configmaps-706b52bb-5b94-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:27:55.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9863" for this suite.
Apr 10 13:28:01.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:28:02.455: INFO: namespace projected-9863 deletion completed in 7.081918197s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:28:02.455: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-507
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Apr 10 13:28:02.874: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-507" to be "success or failure"
Apr 10 13:28:02.899: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 24.656049ms
Apr 10 13:28:04.925: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050758865s
Apr 10 13:28:06.950: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076167799s
Apr 10 13:28:08.976: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.101917249s
STEP: Saw pod success
Apr 10 13:28:08.976: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Apr 10 13:28:09.000: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Apr 10 13:28:09.081: INFO: Waiting for pod pod-host-path-test to disappear
Apr 10 13:28:09.106: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:28:09.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-507" for this suite.
Apr 10 13:28:15.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:28:16.153: INFO: namespace hostpath-507 deletion completed in 6.986718155s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:28:16.154: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-2801
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Apr 10 13:28:17.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 13:28:19.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 13:28:21.477: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 13:28:23.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 13:28:25.451: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 13:28:27.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 13:28:29.502: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690499697, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 13:28:32.586: INFO: Waited 1.109497875s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:28:34.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2801" for this suite.
Apr 10 13:28:40.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:28:41.145: INFO: namespace aggregator-2801 deletion completed in 6.986999631s
•SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:28:41.145: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4563
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 10 13:28:41.519: INFO: Waiting up to 5m0s for pod "downward-api-8e763428-5b94-11e9-8d1d-a6f828030f0b" in namespace "downward-api-4563" to be "success or failure"
Apr 10 13:28:41.545: INFO: Pod "downward-api-8e763428-5b94-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.286848ms
Apr 10 13:28:43.571: INFO: Pod "downward-api-8e763428-5b94-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051145301s
Apr 10 13:28:45.596: INFO: Pod "downward-api-8e763428-5b94-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076903262s
STEP: Saw pod success
Apr 10 13:28:45.596: INFO: Pod "downward-api-8e763428-5b94-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:28:45.622: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downward-api-8e763428-5b94-11e9-8d1d-a6f828030f0b container dapi-container: <nil>
STEP: delete the pod
Apr 10 13:28:45.688: INFO: Waiting for pod downward-api-8e763428-5b94-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:28:45.713: INFO: Pod downward-api-8e763428-5b94-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:28:45.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4563" for this suite.
Apr 10 13:28:51.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:28:52.796: INFO: namespace downward-api-4563 deletion completed in 7.057733592s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:28:52.796: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9232
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 13:28:53.097: INFO: Waiting up to 5m0s for pod "downwardapi-volume-955c7731-5b94-11e9-8d1d-a6f828030f0b" in namespace "downward-api-9232" to be "success or failure"
Apr 10 13:28:53.122: INFO: Pod "downwardapi-volume-955c7731-5b94-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.16966ms
Apr 10 13:28:55.148: INFO: Pod "downwardapi-volume-955c7731-5b94-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051494447s
Apr 10 13:28:57.174: INFO: Pod "downwardapi-volume-955c7731-5b94-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077030865s
STEP: Saw pod success
Apr 10 13:28:57.174: INFO: Pod "downwardapi-volume-955c7731-5b94-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:28:57.199: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downwardapi-volume-955c7731-5b94-11e9-8d1d-a6f828030f0b container client-container: <nil>
STEP: delete the pod
Apr 10 13:28:57.393: INFO: Waiting for pod downwardapi-volume-955c7731-5b94-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:28:57.417: INFO: Pod downwardapi-volume-955c7731-5b94-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:28:57.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9232" for this suite.
Apr 10 13:29:03.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:29:04.445: INFO: namespace downward-api-9232 deletion completed in 7.002241217s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:29:04.445: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6675
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-htpnq in namespace proxy-6675
I0410 13:29:04.832120    3207 runners.go:184] Created replication controller with name: proxy-service-htpnq, namespace: proxy-6675, replica count: 1
I0410 13:29:05.882699    3207 runners.go:184] proxy-service-htpnq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0410 13:29:06.882971    3207 runners.go:184] proxy-service-htpnq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0410 13:29:07.883246    3207 runners.go:184] proxy-service-htpnq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0410 13:29:08.883611    3207 runners.go:184] proxy-service-htpnq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0410 13:29:09.884025    3207 runners.go:184] proxy-service-htpnq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0410 13:29:10.884359    3207 runners.go:184] proxy-service-htpnq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0410 13:29:11.884618    3207 runners.go:184] proxy-service-htpnq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0410 13:29:12.884917    3207 runners.go:184] proxy-service-htpnq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0410 13:29:13.885154    3207 runners.go:184] proxy-service-htpnq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0410 13:29:14.885415    3207 runners.go:184] proxy-service-htpnq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0410 13:29:15.885654    3207 runners.go:184] proxy-service-htpnq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0410 13:29:16.885941    3207 runners.go:184] proxy-service-htpnq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0410 13:29:17.886186    3207 runners.go:184] proxy-service-htpnq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0410 13:29:18.886433    3207 runners.go:184] proxy-service-htpnq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 10 13:29:18.911: INFO: setup took 14.156247274s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Apr 10 13:29:18.986: INFO: (0) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 74.812647ms)
Apr 10 13:29:18.986: INFO: (0) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 74.87386ms)
Apr 10 13:29:18.986: INFO: (0) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 75.011701ms)
Apr 10 13:29:18.986: INFO: (0) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 75.186191ms)
Apr 10 13:29:18.986: INFO: (0) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 74.973423ms)
Apr 10 13:29:18.986: INFO: (0) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 74.950901ms)
Apr 10 13:29:18.986: INFO: (0) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 74.969555ms)
Apr 10 13:29:18.987: INFO: (0) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 75.067068ms)
Apr 10 13:29:18.991: INFO: (0) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 79.087317ms)
Apr 10 13:29:18.991: INFO: (0) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 79.256664ms)
Apr 10 13:29:18.991: INFO: (0) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 79.063942ms)
Apr 10 13:29:19.009: INFO: (0) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 97.110399ms)
Apr 10 13:29:19.009: INFO: (0) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 97.043909ms)
Apr 10 13:29:19.009: INFO: (0) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 97.052387ms)
Apr 10 13:29:19.009: INFO: (0) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 97.109571ms)
Apr 10 13:29:19.042: INFO: (0) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 130.921314ms)
Apr 10 13:29:19.079: INFO: (1) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 36.285065ms)
Apr 10 13:29:19.079: INFO: (1) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 36.243062ms)
Apr 10 13:29:19.079: INFO: (1) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 36.468268ms)
Apr 10 13:29:19.079: INFO: (1) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 36.311263ms)
Apr 10 13:29:19.079: INFO: (1) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 36.66316ms)
Apr 10 13:29:19.079: INFO: (1) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 36.516586ms)
Apr 10 13:29:19.079: INFO: (1) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 36.425358ms)
Apr 10 13:29:19.079: INFO: (1) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 36.425582ms)
Apr 10 13:29:19.079: INFO: (1) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 36.428253ms)
Apr 10 13:29:19.079: INFO: (1) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 36.470776ms)
Apr 10 13:29:19.079: INFO: (1) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 36.518206ms)
Apr 10 13:29:19.079: INFO: (1) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 36.419311ms)
Apr 10 13:29:19.079: INFO: (1) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 36.551805ms)
Apr 10 13:29:19.079: INFO: (1) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 36.495757ms)
Apr 10 13:29:19.079: INFO: (1) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 36.694491ms)
Apr 10 13:29:19.079: INFO: (1) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 36.570924ms)
Apr 10 13:29:19.108: INFO: (2) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 28.463872ms)
Apr 10 13:29:19.108: INFO: (2) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 28.502769ms)
Apr 10 13:29:19.108: INFO: (2) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 28.440224ms)
Apr 10 13:29:19.108: INFO: (2) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 28.521251ms)
Apr 10 13:29:19.108: INFO: (2) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 28.446445ms)
Apr 10 13:29:19.108: INFO: (2) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 28.462123ms)
Apr 10 13:29:19.108: INFO: (2) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 28.660781ms)
Apr 10 13:29:19.108: INFO: (2) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 28.623152ms)
Apr 10 13:29:19.108: INFO: (2) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 28.712961ms)
Apr 10 13:29:19.108: INFO: (2) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 28.476072ms)
Apr 10 13:29:19.108: INFO: (2) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 28.445088ms)
Apr 10 13:29:19.108: INFO: (2) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 28.630002ms)
Apr 10 13:29:19.109: INFO: (2) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 29.949062ms)
Apr 10 13:29:19.111: INFO: (2) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 31.831411ms)
Apr 10 13:29:19.111: INFO: (2) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 31.754427ms)
Apr 10 13:29:19.111: INFO: (2) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 31.929194ms)
Apr 10 13:29:19.139: INFO: (3) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 27.237164ms)
Apr 10 13:29:19.139: INFO: (3) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 27.228113ms)
Apr 10 13:29:19.139: INFO: (3) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 27.394492ms)
Apr 10 13:29:19.139: INFO: (3) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 27.188691ms)
Apr 10 13:29:19.139: INFO: (3) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 27.143329ms)
Apr 10 13:29:19.140: INFO: (3) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 28.410467ms)
Apr 10 13:29:19.140: INFO: (3) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 28.212694ms)
Apr 10 13:29:19.140: INFO: (3) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 28.231073ms)
Apr 10 13:29:19.140: INFO: (3) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 28.176369ms)
Apr 10 13:29:19.140: INFO: (3) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 28.265498ms)
Apr 10 13:29:19.141: INFO: (3) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 29.014015ms)
Apr 10 13:29:19.141: INFO: (3) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 29.920026ms)
Apr 10 13:29:19.142: INFO: (3) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 30.895415ms)
Apr 10 13:29:19.142: INFO: (3) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 30.748755ms)
Apr 10 13:29:19.143: INFO: (3) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 32.201758ms)
Apr 10 13:29:19.144: INFO: (3) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 32.01686ms)
Apr 10 13:29:19.185: INFO: (4) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 40.80094ms)
Apr 10 13:29:19.185: INFO: (4) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 40.716936ms)
Apr 10 13:29:19.185: INFO: (4) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 40.7719ms)
Apr 10 13:29:19.185: INFO: (4) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 41.00827ms)
Apr 10 13:29:19.185: INFO: (4) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 40.843283ms)
Apr 10 13:29:19.185: INFO: (4) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 40.853551ms)
Apr 10 13:29:19.185: INFO: (4) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 40.978558ms)
Apr 10 13:29:19.185: INFO: (4) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 40.787984ms)
Apr 10 13:29:19.185: INFO: (4) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 40.99904ms)
Apr 10 13:29:19.185: INFO: (4) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 40.857111ms)
Apr 10 13:29:19.206: INFO: (4) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 62.391347ms)
Apr 10 13:29:19.206: INFO: (4) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 62.451713ms)
Apr 10 13:29:19.206: INFO: (4) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 62.365292ms)
Apr 10 13:29:19.206: INFO: (4) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 62.341809ms)
Apr 10 13:29:19.206: INFO: (4) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 62.474344ms)
Apr 10 13:29:19.206: INFO: (4) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 62.429545ms)
Apr 10 13:29:19.235: INFO: (5) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 28.204862ms)
Apr 10 13:29:19.235: INFO: (5) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 28.286992ms)
Apr 10 13:29:19.235: INFO: (5) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 28.110553ms)
Apr 10 13:29:19.235: INFO: (5) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 28.150224ms)
Apr 10 13:29:19.235: INFO: (5) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 28.177188ms)
Apr 10 13:29:19.235: INFO: (5) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 28.158717ms)
Apr 10 13:29:19.236: INFO: (5) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 29.569098ms)
Apr 10 13:29:19.236: INFO: (5) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 29.355283ms)
Apr 10 13:29:19.236: INFO: (5) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 29.3743ms)
Apr 10 13:29:19.236: INFO: (5) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 29.328956ms)
Apr 10 13:29:19.237: INFO: (5) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 30.064757ms)
Apr 10 13:29:19.237: INFO: (5) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 30.153253ms)
Apr 10 13:29:19.237: INFO: (5) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 30.798199ms)
Apr 10 13:29:19.237: INFO: (5) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 30.878455ms)
Apr 10 13:29:19.239: INFO: (5) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 32.108258ms)
Apr 10 13:29:19.240: INFO: (5) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 32.964726ms)
Apr 10 13:29:19.267: INFO: (6) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 27.593674ms)
Apr 10 13:29:19.268: INFO: (6) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 27.606543ms)
Apr 10 13:29:19.267: INFO: (6) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 27.63971ms)
Apr 10 13:29:19.268: INFO: (6) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 27.765065ms)
Apr 10 13:29:19.268: INFO: (6) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 27.665051ms)
Apr 10 13:29:19.268: INFO: (6) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 27.756985ms)
Apr 10 13:29:19.268: INFO: (6) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 28.056604ms)
Apr 10 13:29:19.268: INFO: (6) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 27.964781ms)
Apr 10 13:29:19.268: INFO: (6) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 28.036894ms)
Apr 10 13:29:19.268: INFO: (6) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 28.17767ms)
Apr 10 13:29:19.268: INFO: (6) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 28.041371ms)
Apr 10 13:29:19.268: INFO: (6) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 27.95775ms)
Apr 10 13:29:19.312: INFO: (6) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 72.022474ms)
Apr 10 13:29:19.312: INFO: (6) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 72.088009ms)
Apr 10 13:29:19.312: INFO: (6) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 72.135902ms)
Apr 10 13:29:19.312: INFO: (6) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 71.992635ms)
Apr 10 13:29:19.340: INFO: (7) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 27.570179ms)
Apr 10 13:29:19.340: INFO: (7) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 27.711582ms)
Apr 10 13:29:19.340: INFO: (7) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 27.652145ms)
Apr 10 13:29:19.340: INFO: (7) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 27.770972ms)
Apr 10 13:29:19.340: INFO: (7) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 27.692491ms)
Apr 10 13:29:19.340: INFO: (7) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 27.813712ms)
Apr 10 13:29:19.340: INFO: (7) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 27.810531ms)
Apr 10 13:29:19.340: INFO: (7) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 27.900898ms)
Apr 10 13:29:19.340: INFO: (7) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 28.061062ms)
Apr 10 13:29:19.340: INFO: (7) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 28.068162ms)
Apr 10 13:29:19.340: INFO: (7) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 28.179883ms)
Apr 10 13:29:19.340: INFO: (7) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 28.23282ms)
Apr 10 13:29:19.341: INFO: (7) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 29.053766ms)
Apr 10 13:29:19.383: INFO: (7) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 70.459058ms)
Apr 10 13:29:19.383: INFO: (7) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 70.475442ms)
Apr 10 13:29:19.383: INFO: (7) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 70.507654ms)
Apr 10 13:29:19.414: INFO: (8) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 30.741616ms)
Apr 10 13:29:19.414: INFO: (8) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 30.817998ms)
Apr 10 13:29:19.414: INFO: (8) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 30.647237ms)
Apr 10 13:29:19.414: INFO: (8) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 30.656468ms)
Apr 10 13:29:19.414: INFO: (8) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 30.727441ms)
Apr 10 13:29:19.414: INFO: (8) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 30.764998ms)
Apr 10 13:29:19.415: INFO: (8) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 31.919629ms)
Apr 10 13:29:19.415: INFO: (8) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 32.223373ms)
Apr 10 13:29:19.415: INFO: (8) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 32.160604ms)
Apr 10 13:29:19.415: INFO: (8) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 32.314631ms)
Apr 10 13:29:19.415: INFO: (8) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 31.868895ms)
Apr 10 13:29:19.415: INFO: (8) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 31.982677ms)
Apr 10 13:29:19.415: INFO: (8) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 32.038266ms)
Apr 10 13:29:19.415: INFO: (8) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 32.43294ms)
Apr 10 13:29:19.416: INFO: (8) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 32.926903ms)
Apr 10 13:29:19.416: INFO: (8) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 32.927121ms)
Apr 10 13:29:19.445: INFO: (9) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 28.27393ms)
Apr 10 13:29:19.445: INFO: (9) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 28.424877ms)
Apr 10 13:29:19.445: INFO: (9) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 28.419107ms)
Apr 10 13:29:19.445: INFO: (9) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 28.455995ms)
Apr 10 13:29:19.445: INFO: (9) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 28.411403ms)
Apr 10 13:29:19.445: INFO: (9) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 28.534901ms)
Apr 10 13:29:19.445: INFO: (9) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 28.407025ms)
Apr 10 13:29:19.445: INFO: (9) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 28.43396ms)
Apr 10 13:29:19.445: INFO: (9) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 28.396057ms)
Apr 10 13:29:19.445: INFO: (9) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 28.456782ms)
Apr 10 13:29:19.445: INFO: (9) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 28.639072ms)
Apr 10 13:29:19.445: INFO: (9) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 28.882595ms)
Apr 10 13:29:19.446: INFO: (9) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 29.704485ms)
Apr 10 13:29:19.447: INFO: (9) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 30.712477ms)
Apr 10 13:29:19.447: INFO: (9) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 30.747728ms)
Apr 10 13:29:19.447: INFO: (9) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 30.619136ms)
Apr 10 13:29:19.475: INFO: (10) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 27.894248ms)
Apr 10 13:29:19.475: INFO: (10) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 27.83121ms)
Apr 10 13:29:19.475: INFO: (10) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 27.781398ms)
Apr 10 13:29:19.475: INFO: (10) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 27.881736ms)
Apr 10 13:29:19.475: INFO: (10) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 27.920648ms)
Apr 10 13:29:19.475: INFO: (10) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 28.056442ms)
Apr 10 13:29:19.475: INFO: (10) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 27.941461ms)
Apr 10 13:29:19.475: INFO: (10) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 27.887491ms)
Apr 10 13:29:19.475: INFO: (10) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 27.894727ms)
Apr 10 13:29:19.475: INFO: (10) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 28.055043ms)
Apr 10 13:29:19.475: INFO: (10) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 27.988954ms)
Apr 10 13:29:19.475: INFO: (10) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 28.02052ms)
Apr 10 13:29:19.477: INFO: (10) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 29.275925ms)
Apr 10 13:29:19.519: INFO: (10) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 71.560733ms)
Apr 10 13:29:19.519: INFO: (10) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 71.632213ms)
Apr 10 13:29:19.519: INFO: (10) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 71.554076ms)
Apr 10 13:29:19.547: INFO: (11) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 28.299876ms)
Apr 10 13:29:19.547: INFO: (11) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 28.155594ms)
Apr 10 13:29:19.547: INFO: (11) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 28.247075ms)
Apr 10 13:29:19.547: INFO: (11) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 28.184394ms)
Apr 10 13:29:19.548: INFO: (11) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 28.477109ms)
Apr 10 13:29:19.548: INFO: (11) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 28.583238ms)
Apr 10 13:29:19.548: INFO: (11) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 28.636837ms)
Apr 10 13:29:19.548: INFO: (11) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 28.777453ms)
Apr 10 13:29:19.548: INFO: (11) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 28.668784ms)
Apr 10 13:29:19.549: INFO: (11) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 29.414722ms)
Apr 10 13:29:19.550: INFO: (11) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 30.697375ms)
Apr 10 13:29:19.550: INFO: (11) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 30.737757ms)
Apr 10 13:29:19.550: INFO: (11) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 30.832228ms)
Apr 10 13:29:19.550: INFO: (11) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 30.800546ms)
Apr 10 13:29:19.550: INFO: (11) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 30.748923ms)
Apr 10 13:29:19.550: INFO: (11) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 30.795727ms)
Apr 10 13:29:19.577: INFO: (12) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 26.424389ms)
Apr 10 13:29:19.577: INFO: (12) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 26.994731ms)
Apr 10 13:29:19.577: INFO: (12) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 26.830635ms)
Apr 10 13:29:19.578: INFO: (12) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 26.572839ms)
Apr 10 13:29:19.578: INFO: (12) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 27.114495ms)
Apr 10 13:29:19.578: INFO: (12) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 27.203523ms)
Apr 10 13:29:19.578: INFO: (12) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 27.358021ms)
Apr 10 13:29:19.578: INFO: (12) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 26.471432ms)
Apr 10 13:29:19.578: INFO: (12) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 27.080028ms)
Apr 10 13:29:19.578: INFO: (12) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 26.547651ms)
Apr 10 13:29:19.578: INFO: (12) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 27.711649ms)
Apr 10 13:29:19.578: INFO: (12) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 26.888045ms)
Apr 10 13:29:19.579: INFO: (12) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 28.887599ms)
Apr 10 13:29:19.621: INFO: (12) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 70.18021ms)
Apr 10 13:29:19.621: INFO: (12) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 70.142833ms)
Apr 10 13:29:19.621: INFO: (12) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 70.472417ms)
Apr 10 13:29:19.649: INFO: (13) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 27.198834ms)
Apr 10 13:29:19.649: INFO: (13) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 27.159476ms)
Apr 10 13:29:19.650: INFO: (13) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 28.043111ms)
Apr 10 13:29:19.650: INFO: (13) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 28.176309ms)
Apr 10 13:29:19.650: INFO: (13) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 28.213838ms)
Apr 10 13:29:19.650: INFO: (13) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 28.311118ms)
Apr 10 13:29:19.650: INFO: (13) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 28.115641ms)
Apr 10 13:29:19.650: INFO: (13) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 28.171992ms)
Apr 10 13:29:19.650: INFO: (13) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 28.115921ms)
Apr 10 13:29:19.650: INFO: (13) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 28.110034ms)
Apr 10 13:29:19.650: INFO: (13) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 28.14632ms)
Apr 10 13:29:19.651: INFO: (13) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 29.152891ms)
Apr 10 13:29:19.652: INFO: (13) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 30.156338ms)
Apr 10 13:29:19.652: INFO: (13) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 30.21641ms)
Apr 10 13:29:19.652: INFO: (13) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 30.852469ms)
Apr 10 13:29:19.652: INFO: (13) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 30.954064ms)
Apr 10 13:29:19.680: INFO: (14) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 27.374024ms)
Apr 10 13:29:19.680: INFO: (14) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 27.471222ms)
Apr 10 13:29:19.680: INFO: (14) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 27.412546ms)
Apr 10 13:29:19.680: INFO: (14) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 27.512854ms)
Apr 10 13:29:19.680: INFO: (14) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 27.527755ms)
Apr 10 13:29:19.680: INFO: (14) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 27.553145ms)
Apr 10 13:29:19.681: INFO: (14) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 27.942583ms)
Apr 10 13:29:19.681: INFO: (14) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 28.133736ms)
Apr 10 13:29:19.681: INFO: (14) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 28.165527ms)
Apr 10 13:29:19.682: INFO: (14) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 28.984166ms)
Apr 10 13:29:19.682: INFO: (14) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 28.900032ms)
Apr 10 13:29:19.682: INFO: (14) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 28.959283ms)
Apr 10 13:29:19.682: INFO: (14) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 29.713795ms)
Apr 10 13:29:19.683: INFO: (14) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 30.795154ms)
Apr 10 13:29:19.683: INFO: (14) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 30.757056ms)
Apr 10 13:29:19.684: INFO: (14) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 31.767215ms)
Apr 10 13:29:19.712: INFO: (15) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 27.48832ms)
Apr 10 13:29:19.712: INFO: (15) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 27.503026ms)
Apr 10 13:29:19.712: INFO: (15) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 27.430044ms)
Apr 10 13:29:19.712: INFO: (15) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 27.58331ms)
Apr 10 13:29:19.712: INFO: (15) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 27.447634ms)
Apr 10 13:29:19.712: INFO: (15) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 28.027098ms)
Apr 10 13:29:19.712: INFO: (15) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 27.881688ms)
Apr 10 13:29:19.713: INFO: (15) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 27.831019ms)
Apr 10 13:29:19.713: INFO: (15) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 27.980475ms)
Apr 10 13:29:19.713: INFO: (15) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 27.914496ms)
Apr 10 13:29:19.713: INFO: (15) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 27.95839ms)
Apr 10 13:29:19.716: INFO: (15) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 31.007063ms)
Apr 10 13:29:19.716: INFO: (15) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 31.072311ms)
Apr 10 13:29:19.716: INFO: (15) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 31.176938ms)
Apr 10 13:29:19.716: INFO: (15) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 31.109447ms)
Apr 10 13:29:19.716: INFO: (15) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 31.049368ms)
Apr 10 13:29:19.743: INFO: (16) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 26.889496ms)
Apr 10 13:29:19.743: INFO: (16) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 26.947344ms)
Apr 10 13:29:19.743: INFO: (16) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 26.914894ms)
Apr 10 13:29:19.743: INFO: (16) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 27.035152ms)
Apr 10 13:29:19.743: INFO: (16) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 27.120495ms)
Apr 10 13:29:19.744: INFO: (16) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 27.825769ms)
Apr 10 13:29:19.744: INFO: (16) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 27.961244ms)
Apr 10 13:29:19.744: INFO: (16) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 27.925575ms)
Apr 10 13:29:19.744: INFO: (16) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 28.023916ms)
Apr 10 13:29:19.744: INFO: (16) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 27.920344ms)
Apr 10 13:29:19.744: INFO: (16) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 27.931549ms)
Apr 10 13:29:19.745: INFO: (16) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 28.902166ms)
Apr 10 13:29:19.745: INFO: (16) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 28.666864ms)
Apr 10 13:29:19.745: INFO: (16) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 28.738492ms)
Apr 10 13:29:19.746: INFO: (16) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 29.785222ms)
Apr 10 13:29:19.746: INFO: (16) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 29.904024ms)
Apr 10 13:29:19.773: INFO: (17) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 27.430892ms)
Apr 10 13:29:19.773: INFO: (17) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 27.47973ms)
Apr 10 13:29:19.773: INFO: (17) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 27.556041ms)
Apr 10 13:29:19.773: INFO: (17) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 27.427429ms)
Apr 10 13:29:19.773: INFO: (17) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 27.463553ms)
Apr 10 13:29:19.774: INFO: (17) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 27.718628ms)
Apr 10 13:29:19.774: INFO: (17) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 27.745629ms)
Apr 10 13:29:19.774: INFO: (17) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 27.79406ms)
Apr 10 13:29:19.774: INFO: (17) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 27.822737ms)
Apr 10 13:29:19.775: INFO: (17) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 29.247982ms)
Apr 10 13:29:19.775: INFO: (17) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 29.101039ms)
Apr 10 13:29:19.775: INFO: (17) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 29.142396ms)
Apr 10 13:29:19.776: INFO: (17) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 29.578047ms)
Apr 10 13:29:19.777: INFO: (17) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 30.759049ms)
Apr 10 13:29:19.778: INFO: (17) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 31.816043ms)
Apr 10 13:29:19.778: INFO: (17) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 31.866504ms)
Apr 10 13:29:19.806: INFO: (18) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 27.453393ms)
Apr 10 13:29:19.806: INFO: (18) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 27.413889ms)
Apr 10 13:29:19.806: INFO: (18) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 27.387898ms)
Apr 10 13:29:19.806: INFO: (18) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 27.556821ms)
Apr 10 13:29:19.806: INFO: (18) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 27.429197ms)
Apr 10 13:29:19.806: INFO: (18) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 27.627492ms)
Apr 10 13:29:19.807: INFO: (18) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 28.641326ms)
Apr 10 13:29:19.807: INFO: (18) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 28.339455ms)
Apr 10 13:29:19.807: INFO: (18) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 28.503231ms)
Apr 10 13:29:19.807: INFO: (18) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 28.397479ms)
Apr 10 13:29:19.807: INFO: (18) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 28.671095ms)
Apr 10 13:29:19.808: INFO: (18) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 29.327308ms)
Apr 10 13:29:19.808: INFO: (18) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 29.244659ms)
Apr 10 13:29:19.808: INFO: (18) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 29.449163ms)
Apr 10 13:29:19.809: INFO: (18) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 30.6498ms)
Apr 10 13:29:19.810: INFO: (18) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 31.496011ms)
Apr 10 13:29:19.838: INFO: (19) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:443/proxy/tlsrewritem... (200; 27.978009ms)
Apr 10 13:29:19.838: INFO: (19) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:162/proxy/: bar (200; 27.912954ms)
Apr 10 13:29:19.838: INFO: (19) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:460/proxy/: tls baz (200; 28.175919ms)
Apr 10 13:29:19.838: INFO: (19) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:1080/proxy/rewriteme">... (200; 27.930747ms)
Apr 10 13:29:19.838: INFO: (19) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:162/proxy/: bar (200; 28.000953ms)
Apr 10 13:29:19.838: INFO: (19) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:160/proxy/: foo (200; 28.211097ms)
Apr 10 13:29:19.839: INFO: (19) /api/v1/namespaces/proxy-6675/pods/http:proxy-service-htpnq-js6jt:160/proxy/: foo (200; 28.651377ms)
Apr 10 13:29:19.839: INFO: (19) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname1/proxy/: tls baz (200; 28.66759ms)
Apr 10 13:29:19.839: INFO: (19) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt/proxy/rewriteme">test</a> (200; 28.679743ms)
Apr 10 13:29:19.839: INFO: (19) /api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/: <a href="/api/v1/namespaces/proxy-6675/pods/proxy-service-htpnq-js6jt:1080/proxy/rewriteme">test<... (200; 28.799296ms)
Apr 10 13:29:19.839: INFO: (19) /api/v1/namespaces/proxy-6675/pods/https:proxy-service-htpnq-js6jt:462/proxy/: tls qux (200; 28.544524ms)
Apr 10 13:29:19.839: INFO: (19) /api/v1/namespaces/proxy-6675/services/https:proxy-service-htpnq:tlsportname2/proxy/: tls qux (200; 28.792298ms)
Apr 10 13:29:19.845: INFO: (19) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname2/proxy/: bar (200; 34.690675ms)
Apr 10 13:29:19.845: INFO: (19) /api/v1/namespaces/proxy-6675/services/proxy-service-htpnq:portname1/proxy/: foo (200; 34.705502ms)
Apr 10 13:29:19.846: INFO: (19) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname1/proxy/: foo (200; 36.081005ms)
Apr 10 13:29:19.846: INFO: (19) /api/v1/namespaces/proxy-6675/services/http:proxy-service-htpnq:portname2/proxy/: bar (200; 36.061406ms)
STEP: deleting ReplicationController proxy-service-htpnq in namespace proxy-6675, will wait for the garbage collector to delete the pods
Apr 10 13:29:19.948: INFO: Deleting ReplicationController proxy-service-htpnq took: 26.712219ms
Apr 10 13:29:20.049: INFO: Terminating ReplicationController proxy-service-htpnq pods took: 100.441806ms
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:29:32.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6675" for this suite.
Apr 10 13:29:38.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:29:39.688: INFO: namespace proxy-6675 deletion completed in 7.013207043s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:29:39.688: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3062
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr 10 13:29:39.965: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 10 13:29:40.016: INFO: Waiting for terminating namespaces to be deleted...
Apr 10 13:29:40.041: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm before test
Apr 10 13:29:40.097: INFO: addons-kubernetes-dashboard-665df4b66d-t5x2l from kube-system started at 2019-04-10 12:34:10 +0000 UTC (1 container statuses recorded)
Apr 10 13:29:40.097: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr 10 13:29:40.097: INFO: addons-nginx-ingress-controller-d4f8c9cc5-bns7v from kube-system started at 2019-04-10 12:34:12 +0000 UTC (1 container statuses recorded)
Apr 10 13:29:40.097: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Apr 10 13:29:40.097: INFO: kube-proxy-bxtp6 from kube-system started at 2019-04-10 12:33:52 +0000 UTC (1 container statuses recorded)
Apr 10 13:29:40.097: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 10 13:29:40.097: INFO: calico-node-zjgnp from kube-system started at 2019-04-10 12:33:52 +0000 UTC (1 container statuses recorded)
Apr 10 13:29:40.097: INFO: 	Container calico-node ready: true, restart count 0
Apr 10 13:29:40.097: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-rlq45 from kube-system started at 2019-04-10 12:34:12 +0000 UTC (1 container statuses recorded)
Apr 10 13:29:40.097: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Apr 10 13:29:40.097: INFO: node-exporter-5m2qq from kube-system started at 2019-04-10 12:33:52 +0000 UTC (1 container statuses recorded)
Apr 10 13:29:40.097: INFO: 	Container node-exporter ready: true, restart count 0
Apr 10 13:29:40.097: INFO: vpn-shoot-7b874bf844-m7l5b from kube-system started at 2019-04-10 12:34:12 +0000 UTC (1 container statuses recorded)
Apr 10 13:29:40.097: INFO: 	Container vpn-shoot ready: true, restart count 0
Apr 10 13:29:40.097: INFO: blackbox-exporter-6dc58dcffc-qtzx5 from kube-system started at 2019-04-10 12:33:52 +0000 UTC (1 container statuses recorded)
Apr 10 13:29:40.097: INFO: 	Container blackbox-exporter ready: true, restart count 0
Apr 10 13:29:40.097: INFO: coredns-7f7f7978c8-gspr8 from kube-system started at 2019-04-10 12:34:10 +0000 UTC (1 container statuses recorded)
Apr 10 13:29:40.097: INFO: 	Container coredns ready: true, restart count 0
Apr 10 13:29:40.097: INFO: metrics-server-74cbf65f76-94nf4 from kube-system started at 2019-04-10 12:34:12 +0000 UTC (1 container statuses recorded)
Apr 10 13:29:40.097: INFO: 	Container metrics-server ready: true, restart count 0
Apr 10 13:29:40.097: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp before test
Apr 10 13:29:40.151: INFO: calico-node-l99mz from kube-system started at 2019-04-10 12:33:56 +0000 UTC (1 container statuses recorded)
Apr 10 13:29:40.151: INFO: 	Container calico-node ready: true, restart count 0
Apr 10 13:29:40.151: INFO: node-exporter-n962l from kube-system started at 2019-04-10 12:33:56 +0000 UTC (1 container statuses recorded)
Apr 10 13:29:40.151: INFO: 	Container node-exporter ready: true, restart count 0
Apr 10 13:29:40.151: INFO: coredns-7f7f7978c8-grrwz from kube-system started at 2019-04-10 12:34:19 +0000 UTC (1 container statuses recorded)
Apr 10 13:29:40.151: INFO: 	Container coredns ready: true, restart count 0
Apr 10 13:29:40.151: INFO: kube-proxy-h2r8s from kube-system started at 2019-04-10 12:33:56 +0000 UTC (1 container statuses recorded)
Apr 10 13:29:40.151: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm
STEP: verifying the node has the label node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp
Apr 10 13:29:40.316: INFO: Pod addons-kubernetes-dashboard-665df4b66d-t5x2l requesting resource cpu=50m on Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm
Apr 10 13:29:40.316: INFO: Pod addons-nginx-ingress-controller-d4f8c9cc5-bns7v requesting resource cpu=100m on Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm
Apr 10 13:29:40.316: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-rlq45 requesting resource cpu=0m on Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm
Apr 10 13:29:40.316: INFO: Pod blackbox-exporter-6dc58dcffc-qtzx5 requesting resource cpu=5m on Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm
Apr 10 13:29:40.316: INFO: Pod calico-node-l99mz requesting resource cpu=100m on Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp
Apr 10 13:29:40.316: INFO: Pod calico-node-zjgnp requesting resource cpu=100m on Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm
Apr 10 13:29:40.316: INFO: Pod coredns-7f7f7978c8-grrwz requesting resource cpu=50m on Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp
Apr 10 13:29:40.316: INFO: Pod coredns-7f7f7978c8-gspr8 requesting resource cpu=50m on Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm
Apr 10 13:29:40.316: INFO: Pod kube-proxy-bxtp6 requesting resource cpu=20m on Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm
Apr 10 13:29:40.316: INFO: Pod kube-proxy-h2r8s requesting resource cpu=20m on Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp
Apr 10 13:29:40.316: INFO: Pod metrics-server-74cbf65f76-94nf4 requesting resource cpu=20m on Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm
Apr 10 13:29:40.316: INFO: Pod node-exporter-5m2qq requesting resource cpu=5m on Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm
Apr 10 13:29:40.316: INFO: Pod node-exporter-n962l requesting resource cpu=5m on Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp
Apr 10 13:29:40.316: INFO: Pod vpn-shoot-7b874bf844-m7l5b requesting resource cpu=50m on Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b186594d-5b94-11e9-8d1d-a6f828030f0b.15942005bbc4cb94], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3062/filler-pod-b186594d-5b94-11e9-8d1d-a6f828030f0b to shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b186594d-5b94-11e9-8d1d-a6f828030f0b.15942006166c0497], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b186594d-5b94-11e9-8d1d-a6f828030f0b.15942006435efa00], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b186594d-5b94-11e9-8d1d-a6f828030f0b.15942006791c61e9], Reason = [Created], Message = [Created container filler-pod-b186594d-5b94-11e9-8d1d-a6f828030f0b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b186594d-5b94-11e9-8d1d-a6f828030f0b.15942006839ec2cf], Reason = [Started], Message = [Started container filler-pod-b186594d-5b94-11e9-8d1d-a6f828030f0b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b18a894e-5b94-11e9-8d1d-a6f828030f0b.15942005bdbe1ced], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3062/filler-pod-b18a894e-5b94-11e9-8d1d-a6f828030f0b to shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b18a894e-5b94-11e9-8d1d-a6f828030f0b.159420061c039401], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b18a894e-5b94-11e9-8d1d-a6f828030f0b.159420064ff1c45d], Reason = [Created], Message = [Created container filler-pod-b18a894e-5b94-11e9-8d1d-a6f828030f0b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b18a894e-5b94-11e9-8d1d-a6f828030f0b.159420065a0d2507], Reason = [Started], Message = [Started container filler-pod-b18a894e-5b94-11e9-8d1d-a6f828030f0b]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15942006b5a5de94], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:29:45.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3062" for this suite.
Apr 10 13:29:51.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:29:52.743: INFO: namespace sched-pred-3062 deletion completed in 6.994053146s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70
•S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:29:52.743: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-7089
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:29:57.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7089" for this suite.
Apr 10 13:30:03.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:30:04.324: INFO: namespace emptydir-wrapper-7089 deletion completed in 6.992154889s
•SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:30:04.324: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2777
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-c007bfe7-5b94-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume secrets
Apr 10 13:30:04.706: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c00b8849-5b94-11e9-8d1d-a6f828030f0b" in namespace "projected-2777" to be "success or failure"
Apr 10 13:30:04.731: INFO: Pod "pod-projected-secrets-c00b8849-5b94-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.846856ms
Apr 10 13:30:06.757: INFO: Pod "pod-projected-secrets-c00b8849-5b94-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050695739s
Apr 10 13:30:08.783: INFO: Pod "pod-projected-secrets-c00b8849-5b94-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077036369s
STEP: Saw pod success
Apr 10 13:30:08.787: INFO: Pod "pod-projected-secrets-c00b8849-5b94-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:30:08.811: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-projected-secrets-c00b8849-5b94-11e9-8d1d-a6f828030f0b container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 10 13:30:08.891: INFO: Waiting for pod pod-projected-secrets-c00b8849-5b94-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:30:08.916: INFO: Pod pod-projected-secrets-c00b8849-5b94-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:30:08.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2777" for this suite.
Apr 10 13:30:15.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:30:15.957: INFO: namespace projected-2777 deletion completed in 7.015919429s
•SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:30:15.957: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2933
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr 10 13:30:16.275: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 10 13:30:16.325: INFO: Waiting for terminating namespaces to be deleted...
Apr 10 13:30:16.350: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm before test
Apr 10 13:30:16.406: INFO: kube-proxy-bxtp6 from kube-system started at 2019-04-10 12:33:52 +0000 UTC (1 container statuses recorded)
Apr 10 13:30:16.406: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 10 13:30:16.406: INFO: calico-node-zjgnp from kube-system started at 2019-04-10 12:33:52 +0000 UTC (1 container statuses recorded)
Apr 10 13:30:16.406: INFO: 	Container calico-node ready: true, restart count 0
Apr 10 13:30:16.406: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-rlq45 from kube-system started at 2019-04-10 12:34:12 +0000 UTC (1 container statuses recorded)
Apr 10 13:30:16.406: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Apr 10 13:30:16.406: INFO: addons-nginx-ingress-controller-d4f8c9cc5-bns7v from kube-system started at 2019-04-10 12:34:12 +0000 UTC (1 container statuses recorded)
Apr 10 13:30:16.406: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Apr 10 13:30:16.406: INFO: node-exporter-5m2qq from kube-system started at 2019-04-10 12:33:52 +0000 UTC (1 container statuses recorded)
Apr 10 13:30:16.406: INFO: 	Container node-exporter ready: true, restart count 0
Apr 10 13:30:16.406: INFO: vpn-shoot-7b874bf844-m7l5b from kube-system started at 2019-04-10 12:34:12 +0000 UTC (1 container statuses recorded)
Apr 10 13:30:16.406: INFO: 	Container vpn-shoot ready: true, restart count 0
Apr 10 13:30:16.406: INFO: blackbox-exporter-6dc58dcffc-qtzx5 from kube-system started at 2019-04-10 12:33:52 +0000 UTC (1 container statuses recorded)
Apr 10 13:30:16.406: INFO: 	Container blackbox-exporter ready: true, restart count 0
Apr 10 13:30:16.406: INFO: coredns-7f7f7978c8-gspr8 from kube-system started at 2019-04-10 12:34:10 +0000 UTC (1 container statuses recorded)
Apr 10 13:30:16.406: INFO: 	Container coredns ready: true, restart count 0
Apr 10 13:30:16.406: INFO: metrics-server-74cbf65f76-94nf4 from kube-system started at 2019-04-10 12:34:12 +0000 UTC (1 container statuses recorded)
Apr 10 13:30:16.406: INFO: 	Container metrics-server ready: true, restart count 0
Apr 10 13:30:16.406: INFO: addons-kubernetes-dashboard-665df4b66d-t5x2l from kube-system started at 2019-04-10 12:34:10 +0000 UTC (1 container statuses recorded)
Apr 10 13:30:16.406: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr 10 13:30:16.406: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp before test
Apr 10 13:30:16.437: INFO: kube-proxy-h2r8s from kube-system started at 2019-04-10 12:33:56 +0000 UTC (1 container statuses recorded)
Apr 10 13:30:16.437: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 10 13:30:16.437: INFO: calico-node-l99mz from kube-system started at 2019-04-10 12:33:56 +0000 UTC (1 container statuses recorded)
Apr 10 13:30:16.437: INFO: 	Container calico-node ready: true, restart count 0
Apr 10 13:30:16.437: INFO: node-exporter-n962l from kube-system started at 2019-04-10 12:33:56 +0000 UTC (1 container statuses recorded)
Apr 10 13:30:16.437: INFO: 	Container node-exporter ready: true, restart count 0
Apr 10 13:30:16.437: INFO: coredns-7f7f7978c8-grrwz from kube-system started at 2019-04-10 12:34:19 +0000 UTC (1 container statuses recorded)
Apr 10 13:30:16.437: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c984b942-5b94-11e9-8d1d-a6f828030f0b 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-c984b942-5b94-11e9-8d1d-a6f828030f0b off the node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c984b942-5b94-11e9-8d1d-a6f828030f0b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:30:26.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2933" for this suite.
Apr 10 13:30:36.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:30:37.834: INFO: namespace sched-pred-2933 deletion completed in 11.008536657s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70
•SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:30:37.834: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5818
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 10 13:30:46.393: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 10 13:30:46.417: INFO: Pod pod-with-prestop-http-hook still exists
Apr 10 13:30:48.418: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 10 13:30:48.443: INFO: Pod pod-with-prestop-http-hook still exists
Apr 10 13:30:50.418: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 10 13:30:50.444: INFO: Pod pod-with-prestop-http-hook still exists
Apr 10 13:30:52.418: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 10 13:30:52.444: INFO: Pod pod-with-prestop-http-hook still exists
Apr 10 13:30:54.418: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 10 13:30:54.444: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:30:54.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5818" for this suite.
Apr 10 13:31:16.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:31:17.499: INFO: namespace container-lifecycle-hook-5818 deletion completed in 22.998849843s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:31:17.500: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9018
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0410 13:31:27.967246    3207 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 10 13:31:27.967: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:31:27.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9018" for this suite.
Apr 10 13:31:34.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:31:35.013: INFO: namespace gc-9018 deletion completed in 7.020474537s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:31:35.013: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5840
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 13:31:35.469: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"f6247751-5b94-11e9-99e4-7640922b69f1", Controller:(*bool)(0xc0027ef92a), BlockOwnerDeletion:(*bool)(0xc0027ef92b)}}
Apr 10 13:31:35.511: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f61cac74-5b94-11e9-99e4-7640922b69f1", Controller:(*bool)(0xc0027efb06), BlockOwnerDeletion:(*bool)(0xc0027efb07)}}
Apr 10 13:31:35.537: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"f620978f-5b94-11e9-99e4-7640922b69f1", Controller:(*bool)(0xc0033959c6), BlockOwnerDeletion:(*bool)(0xc0033959c7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:31:40.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5840" for this suite.
Apr 10 13:31:46.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:31:47.630: INFO: namespace gc-5840 deletion completed in 7.015992967s
•SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:31:47.630: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7932
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-7932
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 10 13:31:47.969: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 10 13:32:10.457: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.86:8080/dial?request=hostName&protocol=udp&host=100.96.1.85&port=8081&tries=1'] Namespace:pod-network-test-7932 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 13:32:10.457: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 13:32:10.996: INFO: Waiting for endpoints: map[]
Apr 10 13:32:11.023: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.86:8080/dial?request=hostName&protocol=udp&host=100.96.0.37&port=8081&tries=1'] Namespace:pod-network-test-7932 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 13:32:11.023: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 13:32:11.653: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:32:11.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7932" for this suite.
Apr 10 13:32:35.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:32:36.674: INFO: namespace pod-network-test-7932 deletion completed in 24.995119157s
•SS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:32:36.674: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9476
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 13:32:43.186: INFO: Waiting up to 5m0s for pod "client-envvars-1e8203e2-5b95-11e9-8d1d-a6f828030f0b" in namespace "pods-9476" to be "success or failure"
Apr 10 13:32:43.210: INFO: Pod "client-envvars-1e8203e2-5b95-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.327343ms
Apr 10 13:32:45.243: INFO: Pod "client-envvars-1e8203e2-5b95-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05718456s
Apr 10 13:32:47.269: INFO: Pod "client-envvars-1e8203e2-5b95-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.082800992s
STEP: Saw pod success
Apr 10 13:32:47.269: INFO: Pod "client-envvars-1e8203e2-5b95-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:32:47.294: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod client-envvars-1e8203e2-5b95-11e9-8d1d-a6f828030f0b container env3cont: <nil>
STEP: delete the pod
Apr 10 13:32:47.494: INFO: Waiting for pod client-envvars-1e8203e2-5b95-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:32:47.528: INFO: Pod client-envvars-1e8203e2-5b95-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:32:47.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9476" for this suite.
Apr 10 13:33:27.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:33:28.596: INFO: namespace pods-9476 deletion completed in 41.018923536s
•SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:33:28.597: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2034
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Apr 10 13:33:28.900: INFO: Waiting up to 5m0s for pod "var-expansion-39c111b1-5b95-11e9-8d1d-a6f828030f0b" in namespace "var-expansion-2034" to be "success or failure"
Apr 10 13:33:28.925: INFO: Pod "var-expansion-39c111b1-5b95-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.671999ms
Apr 10 13:33:30.950: INFO: Pod "var-expansion-39c111b1-5b95-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050473057s
Apr 10 13:33:32.976: INFO: Pod "var-expansion-39c111b1-5b95-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076032686s
STEP: Saw pod success
Apr 10 13:33:32.976: INFO: Pod "var-expansion-39c111b1-5b95-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:33:33.001: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod var-expansion-39c111b1-5b95-11e9-8d1d-a6f828030f0b container dapi-container: <nil>
STEP: delete the pod
Apr 10 13:33:33.335: INFO: Waiting for pod var-expansion-39c111b1-5b95-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:33:33.360: INFO: Pod var-expansion-39c111b1-5b95-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:33:33.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2034" for this suite.
Apr 10 13:33:39.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:33:40.408: INFO: namespace var-expansion-2034 deletion completed in 7.000468162s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:33:40.409: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6899
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6899.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6899.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 10 13:33:47.443: INFO: DNS probes using dns-6899/dns-test-40d5e6dc-5b95-11e9-8d1d-a6f828030f0b succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:33:47.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6899" for this suite.
Apr 10 13:33:53.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:33:54.513: INFO: namespace dns-6899 deletion completed in 6.97912428s
•SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:33:54.514: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9543
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-9543/configmap-test-493eab48-5b95-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume configMaps
Apr 10 13:33:54.916: INFO: Waiting up to 5m0s for pod "pod-configmaps-494291c4-5b95-11e9-8d1d-a6f828030f0b" in namespace "configmap-9543" to be "success or failure"
Apr 10 13:33:54.953: INFO: Pod "pod-configmaps-494291c4-5b95-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 37.338686ms
Apr 10 13:33:56.979: INFO: Pod "pod-configmaps-494291c4-5b95-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063156004s
Apr 10 13:33:59.005: INFO: Pod "pod-configmaps-494291c4-5b95-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.089242454s
STEP: Saw pod success
Apr 10 13:33:59.005: INFO: Pod "pod-configmaps-494291c4-5b95-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:33:59.030: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-configmaps-494291c4-5b95-11e9-8d1d-a6f828030f0b container env-test: <nil>
STEP: delete the pod
Apr 10 13:33:59.116: INFO: Waiting for pod pod-configmaps-494291c4-5b95-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:33:59.140: INFO: Pod pod-configmaps-494291c4-5b95-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:33:59.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9543" for this suite.
Apr 10 13:34:05.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:34:06.219: INFO: namespace configmap-9543 deletion completed in 7.029529605s
•SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:34:06.220: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-693
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 13:34:06.717: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Apr 10 13:34:06.786: INFO: Number of nodes with available pods: 0
Apr 10 13:34:06.786: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Apr 10 13:34:06.908: INFO: Number of nodes with available pods: 0
Apr 10 13:34:06.908: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:34:07.941: INFO: Number of nodes with available pods: 0
Apr 10 13:34:07.941: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:34:08.934: INFO: Number of nodes with available pods: 0
Apr 10 13:34:08.934: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:34:09.934: INFO: Number of nodes with available pods: 0
Apr 10 13:34:09.934: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:34:10.934: INFO: Number of nodes with available pods: 1
Apr 10 13:34:10.934: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Apr 10 13:34:11.036: INFO: Number of nodes with available pods: 0
Apr 10 13:34:11.037: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Apr 10 13:34:11.087: INFO: Number of nodes with available pods: 0
Apr 10 13:34:11.087: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:34:12.112: INFO: Number of nodes with available pods: 0
Apr 10 13:34:12.112: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:34:13.112: INFO: Number of nodes with available pods: 0
Apr 10 13:34:13.112: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:34:14.112: INFO: Number of nodes with available pods: 0
Apr 10 13:34:14.112: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:34:15.113: INFO: Number of nodes with available pods: 0
Apr 10 13:34:15.113: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:34:16.113: INFO: Number of nodes with available pods: 0
Apr 10 13:34:16.113: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:34:17.113: INFO: Number of nodes with available pods: 0
Apr 10 13:34:17.113: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:34:18.113: INFO: Number of nodes with available pods: 0
Apr 10 13:34:18.113: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:34:19.112: INFO: Number of nodes with available pods: 1
Apr 10 13:34:19.113: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-693, will wait for the garbage collector to delete the pods
Apr 10 13:34:19.264: INFO: Deleting DaemonSet.extensions daemon-set took: 27.625039ms
Apr 10 13:34:19.665: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.327865ms
Apr 10 13:34:29.289: INFO: Number of nodes with available pods: 0
Apr 10 13:34:29.289: INFO: Number of running nodes: 0, number of available pods: 0
Apr 10 13:34:29.316: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-693/daemonsets","resourceVersion":"12410"},"items":null}

Apr 10 13:34:29.340: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-693/pods","resourceVersion":"12410"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:34:29.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-693" for this suite.
Apr 10 13:34:35.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:34:36.503: INFO: namespace daemonsets-693 deletion completed in 7.013523128s
•SSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:34:36.503: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3251
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-3251
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3251 to expose endpoints map[]
Apr 10 13:34:36.919: INFO: successfully validated that service multi-endpoint-test in namespace services-3251 exposes endpoints map[] (24.383155ms elapsed)
STEP: Creating pod pod1 in namespace services-3251
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3251 to expose endpoints map[pod1:[100]]
Apr 10 13:34:40.154: INFO: successfully validated that service multi-endpoint-test in namespace services-3251 exposes endpoints map[pod1:[100]] (3.206714554s elapsed)
STEP: Creating pod pod2 in namespace services-3251
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3251 to expose endpoints map[pod1:[100] pod2:[101]]
Apr 10 13:34:44.582: INFO: successfully validated that service multi-endpoint-test in namespace services-3251 exposes endpoints map[pod1:[100] pod2:[101]] (4.401997279s elapsed)
STEP: Deleting pod pod1 in namespace services-3251
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3251 to expose endpoints map[pod2:[101]]
Apr 10 13:34:44.657: INFO: successfully validated that service multi-endpoint-test in namespace services-3251 exposes endpoints map[pod2:[101]] (48.475415ms elapsed)
STEP: Deleting pod pod2 in namespace services-3251
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3251 to expose endpoints map[]
Apr 10 13:34:44.710: INFO: successfully validated that service multi-endpoint-test in namespace services-3251 exposes endpoints map[] (24.255154ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:34:44.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3251" for this suite.
Apr 10 13:35:06.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:35:07.810: INFO: namespace services-3251 deletion completed in 23.00696419s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
•SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:35:07.810: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-727
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-727/configmap-test-74efa64e-5b95-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume configMaps
Apr 10 13:35:08.216: INFO: Waiting up to 5m0s for pod "pod-configmaps-74f37caf-5b95-11e9-8d1d-a6f828030f0b" in namespace "configmap-727" to be "success or failure"
Apr 10 13:35:08.241: INFO: Pod "pod-configmaps-74f37caf-5b95-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.623872ms
Apr 10 13:35:10.267: INFO: Pod "pod-configmaps-74f37caf-5b95-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051527729s
Apr 10 13:35:12.295: INFO: Pod "pod-configmaps-74f37caf-5b95-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079088982s
STEP: Saw pod success
Apr 10 13:35:12.295: INFO: Pod "pod-configmaps-74f37caf-5b95-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:35:12.320: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-configmaps-74f37caf-5b95-11e9-8d1d-a6f828030f0b container env-test: <nil>
STEP: delete the pod
Apr 10 13:35:12.545: INFO: Waiting for pod pod-configmaps-74f37caf-5b95-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:35:12.570: INFO: Pod pod-configmaps-74f37caf-5b95-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:35:12.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-727" for this suite.
Apr 10 13:35:18.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:35:19.615: INFO: namespace configmap-727 deletion completed in 6.997926781s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:35:19.616: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5840
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 13:35:19.966: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:35:24.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5840" for this suite.
Apr 10 13:36:18.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:36:19.232: INFO: namespace pods-5840 deletion completed in 54.989046676s
•SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:36:19.232: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3729
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 13:36:19.564: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config version --client'
Apr 10 13:36:19.673: INFO: stderr: ""
Apr 10 13:36:19.673: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:53:57Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Apr 10 13:36:19.714: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-3729'
Apr 10 13:36:20.385: INFO: stderr: ""
Apr 10 13:36:20.385: INFO: stdout: "replicationcontroller/redis-master created\n"
Apr 10 13:36:20.385: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-3729'
Apr 10 13:36:20.744: INFO: stderr: ""
Apr 10 13:36:20.745: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 10 13:36:21.771: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 13:36:21.771: INFO: Found 0 / 1
Apr 10 13:36:22.770: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 13:36:22.770: INFO: Found 0 / 1
Apr 10 13:36:23.771: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 13:36:23.771: INFO: Found 1 / 1
Apr 10 13:36:23.771: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 10 13:36:23.795: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 13:36:23.795: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 10 13:36:23.795: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe pod redis-master-dd7qp --namespace=kubectl-3729'
Apr 10 13:36:24.048: INFO: stderr: ""
Apr 10 13:36:24.048: INFO: stdout: "Name:               redis-master-dd7qp\nNamespace:          kubectl-3729\nPriority:           0\nPriorityClassName:  <none>\nNode:               shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp/10.250.0.5\nStart Time:         Wed, 10 Apr 2019 13:36:20 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 100.96.1.95/32\n                    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 100.96.1.95\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://bc8d80c5602f42e9986fd786a16b9acf1cbf6fe22ffb71436aba9c6548c597b1\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 10 Apr 2019 13:36:22 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-nncs7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-nncs7:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-nncs7\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                      Message\n  ----    ------     ----  ----                                                      -------\n  Normal  Scheduled  4s    default-scheduler                                         Successfully assigned kubectl-3729/redis-master-dd7qp to shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp\n  Normal  Pulled     3s    kubelet, shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp  Created container redis-master\n  Normal  Started    2s    kubelet, shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp  Started container redis-master\n"
Apr 10 13:36:24.048: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe rc redis-master --namespace=kubectl-3729'
Apr 10 13:36:24.361: INFO: stderr: ""
Apr 10 13:36:24.361: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-3729\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-dd7qp\n"
Apr 10 13:36:24.361: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe service redis-master --namespace=kubectl-3729'
Apr 10 13:36:24.671: INFO: stderr: ""
Apr 10 13:36:24.671: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-3729\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.68.63.183\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.96.1.95:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr 10 13:36:24.720: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm'
Apr 10 13:36:25.000: INFO: stderr: ""
Apr 10 13:36:25.000: INFO: stdout: "Name:               shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_DS2_v2\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=westeurope\n                    failure-domain.beta.kubernetes.io/zone=1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=node\n                    node-role.kubernetes.io/node=\n                    worker.garden.sapcloud.io/group=cpu-worker\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 10 Apr 2019 12:33:50 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 10 Apr 2019 12:34:04 +0000   Wed, 10 Apr 2019 12:34:04 +0000   RouteCreated                 RouteController created a route\n  MemoryPressure       False   Wed, 10 Apr 2019 13:36:20 +0000   Wed, 10 Apr 2019 12:33:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 10 Apr 2019 13:36:20 +0000   Wed, 10 Apr 2019 12:33:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 10 Apr 2019 13:36:20 +0000   Wed, 10 Apr 2019 12:33:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 10 Apr 2019 13:36:20 +0000   Wed, 10 Apr 2019 12:34:10 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.250.0.4\n  Hostname:    shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm\nCapacity:\n attachable-volumes-azure-disk:  8\n cpu:                            2\n ephemeral-storage:              33136428Ki\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         7114664Ki\n pods:                           110\nAllocatable:\n attachable-volumes-azure-disk:  8\n cpu:                            1920m\n ephemeral-storage:              32235117134\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         5847403310\n pods:                           110\nSystem Info:\n Machine ID:                 0841476929674b6a81fe23d725923773\n System UUID:                b7173bc8-3b55-604d-bb62-7f247192b566\n Boot ID:                    324d480a-b73b-4d9b-a7fb-0ef76ec33785\n Kernel Version:             4.19.25-coreos\n OS Image:                   Container Linux by CoreOS 2023.5.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.14.0\n Kube-Proxy Version:         v1.14.0\nPodCIDR:                     100.96.0.0/24\nProviderID:                  azure:///subscriptions/00d2caa5-cd29-46f7-845a-2f8ee0360ef5/resourceGroups/shoot--it--tm-fzm8j/providers/Microsoft.Compute/virtualMachines/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm\nNon-terminated Pods:         (10 in total)\n  Namespace                  Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                               ------------  ----------  ---------------  -------------  ---\n  kube-system                addons-kubernetes-dashboard-665df4b66d-t5x2l                       50m (2%)      100m (5%)   50Mi (0%)        256Mi (4%)     65m\n  kube-system                addons-nginx-ingress-controller-d4f8c9cc5-bns7v                    100m (5%)     2 (104%)    100Mi (1%)       800Mi (14%)    65m\n  kube-system                addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-rlq45    0 (0%)        0 (0%)      0 (0%)           0 (0%)         65m\n  kube-system                blackbox-exporter-6dc58dcffc-qtzx5                                 5m (0%)       10m (0%)    5Mi (0%)         35Mi (0%)      65m\n  kube-system                calico-node-zjgnp                                                  100m (5%)     500m (26%)  100Mi (1%)       700Mi (12%)    62m\n  kube-system                coredns-7f7f7978c8-gspr8                                           50m (2%)      100m (5%)   15Mi (0%)        100Mi (1%)     65m\n  kube-system                kube-proxy-bxtp6                                                   20m (1%)      0 (0%)      64Mi (1%)        0 (0%)         62m\n  kube-system                metrics-server-74cbf65f76-94nf4                                    20m (1%)      80m (4%)    100Mi (1%)       400Mi (7%)     65m\n  kube-system                node-exporter-5m2qq                                                5m (0%)       15m (0%)    10Mi (0%)        100Mi (1%)     62m\n  kube-system                vpn-shoot-7b874bf844-m7l5b                                         50m (2%)      100m (5%)   50Mi (0%)        100Mi (1%)     65m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                       Requests    Limits\n  --------                       --------    ------\n  cpu                            400m (20%)  2905m (151%)\n  memory                         494Mi (8%)  2491Mi (44%)\n  ephemeral-storage              0 (0%)      0 (0%)\n  attachable-volumes-azure-disk  0           0\nEvents:                          <none>\n"
Apr 10 13:36:25.001: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe namespace kubectl-3729'
Apr 10 13:36:25.357: INFO: stderr: ""
Apr 10 13:36:25.357: INFO: stdout: "Name:         kubectl-3729\nLabels:       e2e-framework=kubectl\n              e2e-run=480187d1-5b8f-11e9-8d1d-a6f828030f0b\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:36:25.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3729" for this suite.
Apr 10 13:36:47.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:36:48.487: INFO: namespace kubectl-3729 deletion completed in 23.079680509s
•SSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:36:48.487: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1956
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 13:36:48.827: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 10 13:36:52.878: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 10 13:36:57.084: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-1956,SelfLink:/apis/apps/v1/namespaces/deployment-1956/deployments/test-cleanup-deployment,UID:b3647538-5b95-11e9-99e4-7640922b69f1,ResourceVersion:12886,Generation:1,CreationTimestamp:2019-04-10 13:36:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-04-10 13:36:52 +0000 UTC 2019-04-10 13:36:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-04-10 13:36:56 +0000 UTC 2019-04-10 13:36:52 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55cbfbc8f5" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Apr 10 13:36:57.109: INFO: New ReplicaSet "test-cleanup-deployment-55cbfbc8f5" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5,GenerateName:,Namespace:deployment-1956,SelfLink:/apis/apps/v1/namespaces/deployment-1956/replicasets/test-cleanup-deployment-55cbfbc8f5,UID:b366444e-5b95-11e9-99e4-7640922b69f1,ResourceVersion:12879,Generation:1,CreationTimestamp:2019-04-10 13:36:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment b3647538-5b95-11e9-99e4-7640922b69f1 0xc002239507 0xc002239508}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr 10 13:36:57.135: INFO: Pod "test-cleanup-deployment-55cbfbc8f5-qtms5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5-qtms5,GenerateName:test-cleanup-deployment-55cbfbc8f5-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/test-cleanup-deployment-55cbfbc8f5-qtms5,UID:b366f8d0-5b95-11e9-99e4-7640922b69f1,ResourceVersion:12878,Generation:0,CreationTimestamp:2019-04-10 13:36:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.97/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55cbfbc8f5 b366444e-5b95-11e9-99e4-7640922b69f1 0xc002239f07 0xc002239f08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hxpdt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hxpdt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-hxpdt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002239f70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002239f90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 13:36:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 13:36:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 13:36:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 13:36:52 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.97,StartTime:2019-04-10 13:36:53 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-04-10 13:36:55 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://1cecf4d02bbd13a44d27fbd4cdaac396ed0b443e93c6e3fa17c4fc5a7a6e31ca}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:36:57.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1956" for this suite.
Apr 10 13:37:03.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:37:04.168: INFO: namespace deployment-1956 deletion completed in 6.985207398s
•SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:37:04.168: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7905
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Apr 10 13:37:04.490: INFO: Waiting up to 5m0s for pod "pod-ba41b3fe-5b95-11e9-8d1d-a6f828030f0b" in namespace "emptydir-7905" to be "success or failure"
Apr 10 13:37:04.518: INFO: Pod "pod-ba41b3fe-5b95-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 27.361715ms
Apr 10 13:37:06.551: INFO: Pod "pod-ba41b3fe-5b95-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060011526s
Apr 10 13:37:08.576: INFO: Pod "pod-ba41b3fe-5b95-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.085135452s
STEP: Saw pod success
Apr 10 13:37:08.576: INFO: Pod "pod-ba41b3fe-5b95-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:37:08.601: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-ba41b3fe-5b95-11e9-8d1d-a6f828030f0b container test-container: <nil>
STEP: delete the pod
Apr 10 13:37:08.702: INFO: Waiting for pod pod-ba41b3fe-5b95-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:37:08.726: INFO: Pod pod-ba41b3fe-5b95-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:37:08.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7905" for this suite.
Apr 10 13:37:14.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:37:15.775: INFO: namespace emptydir-7905 deletion completed in 6.995027446s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:37:15.776: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5081
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Apr 10 13:37:20.199: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-c12c9edc-5b95-11e9-8d1d-a6f828030f0b,GenerateName:,Namespace:events-5081,SelfLink:/api/v1/namespaces/events-5081/pods/send-events-c12c9edc-5b95-11e9-8d1d-a6f828030f0b,UID:c1300a44-5b95-11e9-99e4-7640922b69f1,ResourceVersion:12973,Generation:0,CreationTimestamp:2019-04-10 13:37:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 69250380,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.99/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f9vzb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f9vzb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-f9vzb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0009bb350} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0009bb3e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 13:37:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 13:37:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 13:37:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 13:37:16 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.99,StartTime:2019-04-10 13:37:16 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-04-10 13:37:18 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://1f40d60135f9c83c24b4fe12a277d7527d410d658fb4c5739911c59e56e0d327}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Apr 10 13:37:22.225: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Apr 10 13:37:24.251: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:37:24.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5081" for this suite.
Apr 10 13:38:04.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:38:05.352: INFO: namespace events-5081 deletion completed in 41.014129923s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:38:05.353: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4910
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Apr 10 13:38:05.694: INFO: Waiting up to 5m0s for pod "client-containers-debcaf3c-5b95-11e9-8d1d-a6f828030f0b" in namespace "containers-4910" to be "success or failure"
Apr 10 13:38:05.719: INFO: Pod "client-containers-debcaf3c-5b95-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.739929ms
Apr 10 13:38:07.744: INFO: Pod "client-containers-debcaf3c-5b95-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049857635s
Apr 10 13:38:09.770: INFO: Pod "client-containers-debcaf3c-5b95-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076057697s
STEP: Saw pod success
Apr 10 13:38:09.770: INFO: Pod "client-containers-debcaf3c-5b95-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:38:09.795: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod client-containers-debcaf3c-5b95-11e9-8d1d-a6f828030f0b container test-container: <nil>
STEP: delete the pod
Apr 10 13:38:09.927: INFO: Waiting for pod client-containers-debcaf3c-5b95-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:38:09.952: INFO: Pod client-containers-debcaf3c-5b95-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:38:09.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4910" for this suite.
Apr 10 13:38:16.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:38:17.064: INFO: namespace containers-4910 deletion completed in 7.065622299s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:38:17.065: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5795
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Apr 10 13:38:17.416: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:38:18.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5795" for this suite.
Apr 10 13:38:24.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:38:25.586: INFO: namespace replication-controller-5795 deletion completed in 6.990860245s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:38:25.586: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7716
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-eac5c267-5b95-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume secrets
Apr 10 13:38:25.913: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eac988c2-5b95-11e9-8d1d-a6f828030f0b" in namespace "projected-7716" to be "success or failure"
Apr 10 13:38:25.945: INFO: Pod "pod-projected-secrets-eac988c2-5b95-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 32.457127ms
Apr 10 13:38:27.971: INFO: Pod "pod-projected-secrets-eac988c2-5b95-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058139258s
Apr 10 13:38:30.010: INFO: Pod "pod-projected-secrets-eac988c2-5b95-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.096997619s
STEP: Saw pod success
Apr 10 13:38:30.010: INFO: Pod "pod-projected-secrets-eac988c2-5b95-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:38:30.034: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-projected-secrets-eac988c2-5b95-11e9-8d1d-a6f828030f0b container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 10 13:38:30.099: INFO: Waiting for pod pod-projected-secrets-eac988c2-5b95-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:38:30.126: INFO: Pod pod-projected-secrets-eac988c2-5b95-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:38:30.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7716" for this suite.
Apr 10 13:38:36.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:38:37.417: INFO: namespace projected-7716 deletion completed in 7.240115825s
•SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:38:37.418: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-954
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-f1dcdf19-5b95-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume secrets
Apr 10 13:38:37.808: INFO: Waiting up to 5m0s for pod "pod-secrets-f1e0bd34-5b95-11e9-8d1d-a6f828030f0b" in namespace "secrets-954" to be "success or failure"
Apr 10 13:38:37.832: INFO: Pod "pod-secrets-f1e0bd34-5b95-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.416472ms
Apr 10 13:38:39.858: INFO: Pod "pod-secrets-f1e0bd34-5b95-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049949858s
Apr 10 13:38:41.888: INFO: Pod "pod-secrets-f1e0bd34-5b95-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079874047s
STEP: Saw pod success
Apr 10 13:38:41.888: INFO: Pod "pod-secrets-f1e0bd34-5b95-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:38:41.912: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-secrets-f1e0bd34-5b95-11e9-8d1d-a6f828030f0b container secret-env-test: <nil>
STEP: delete the pod
Apr 10 13:38:41.985: INFO: Waiting for pod pod-secrets-f1e0bd34-5b95-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:38:42.030: INFO: Pod pod-secrets-f1e0bd34-5b95-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:38:42.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-954" for this suite.
Apr 10 13:38:48.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:38:49.098: INFO: namespace secrets-954 deletion completed in 7.019984443s
•SSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:38:49.098: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-1386
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Apr 10 13:38:59.567: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1386 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 13:38:59.567: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 13:39:00.080: INFO: Exec stderr: ""
Apr 10 13:39:00.080: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1386 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 13:39:00.080: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 13:39:00.594: INFO: Exec stderr: ""
Apr 10 13:39:00.594: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1386 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 13:39:00.594: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 13:39:01.088: INFO: Exec stderr: ""
Apr 10 13:39:01.088: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1386 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 13:39:01.088: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 13:39:01.592: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Apr 10 13:39:01.592: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1386 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 13:39:01.592: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 13:39:02.130: INFO: Exec stderr: ""
Apr 10 13:39:02.130: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1386 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 13:39:02.130: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 13:39:02.634: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Apr 10 13:39:02.634: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1386 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 13:39:02.634: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 13:39:03.141: INFO: Exec stderr: ""
Apr 10 13:39:03.141: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1386 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 13:39:03.141: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 13:39:03.628: INFO: Exec stderr: ""
Apr 10 13:39:03.628: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1386 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 13:39:03.628: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 13:39:04.121: INFO: Exec stderr: ""
Apr 10 13:39:04.121: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1386 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 13:39:04.121: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 13:39:04.652: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:39:04.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1386" for this suite.
Apr 10 13:39:52.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:39:53.699: INFO: namespace e2e-kubelet-etc-hosts-1386 deletion completed in 48.99650717s
•SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:39:53.699: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1578
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 13:39:53.994: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f49aaa3-5b96-11e9-8d1d-a6f828030f0b" in namespace "downward-api-1578" to be "success or failure"
Apr 10 13:39:54.018: INFO: Pod "downwardapi-volume-1f49aaa3-5b96-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.447492ms
Apr 10 13:39:56.044: INFO: Pod "downwardapi-volume-1f49aaa3-5b96-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050585016s
Apr 10 13:39:58.070: INFO: Pod "downwardapi-volume-1f49aaa3-5b96-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076091037s
STEP: Saw pod success
Apr 10 13:39:58.070: INFO: Pod "downwardapi-volume-1f49aaa3-5b96-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:39:58.095: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downwardapi-volume-1f49aaa3-5b96-11e9-8d1d-a6f828030f0b container client-container: <nil>
STEP: delete the pod
Apr 10 13:39:58.317: INFO: Waiting for pod downwardapi-volume-1f49aaa3-5b96-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:39:58.342: INFO: Pod downwardapi-volume-1f49aaa3-5b96-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:39:58.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1578" for this suite.
Apr 10 13:40:04.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:40:05.386: INFO: namespace downward-api-1578 deletion completed in 6.995296013s
•SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:40:05.386: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1176
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr 10 13:40:10.344: INFO: Successfully updated pod "labelsupdate26417ee6-5b96-11e9-8d1d-a6f828030f0b"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:40:12.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1176" for this suite.
Apr 10 13:40:34.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:40:35.460: INFO: namespace downward-api-1176 deletion completed in 23.002607257s
•SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:40:35.461: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9871
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-38326e1a-5b96-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume secrets
Apr 10 13:40:35.809: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3836481a-5b96-11e9-8d1d-a6f828030f0b" in namespace "projected-9871" to be "success or failure"
Apr 10 13:40:35.839: INFO: Pod "pod-projected-secrets-3836481a-5b96-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 30.066703ms
Apr 10 13:40:37.865: INFO: Pod "pod-projected-secrets-3836481a-5b96-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055877839s
Apr 10 13:40:39.891: INFO: Pod "pod-projected-secrets-3836481a-5b96-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.08170886s
STEP: Saw pod success
Apr 10 13:40:39.891: INFO: Pod "pod-projected-secrets-3836481a-5b96-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:40:39.916: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-projected-secrets-3836481a-5b96-11e9-8d1d-a6f828030f0b container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 10 13:40:39.980: INFO: Waiting for pod pod-projected-secrets-3836481a-5b96-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:40:40.006: INFO: Pod pod-projected-secrets-3836481a-5b96-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:40:40.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9871" for this suite.
Apr 10 13:40:46.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:40:47.057: INFO: namespace projected-9871 deletion completed in 7.004135759s
•SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:40:47.058: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7627
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-7627
Apr 10 13:40:51.456: INFO: Started pod liveness-exec in namespace container-probe-7627
STEP: checking the pod's current state and verifying that restartCount is present
Apr 10 13:40:51.482: INFO: Initial restart count of pod liveness-exec is 0
Apr 10 13:41:44.197: INFO: Restart count of pod container-probe-7627/liveness-exec is now 1 (52.71476466s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:41:44.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7627" for this suite.
Apr 10 13:41:50.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:41:51.320: INFO: namespace container-probe-7627 deletion completed in 7.03617266s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:41:51.321: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-754
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-65705a34-5b96-11e9-8d1d-a6f828030f0b
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:41:51.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-754" for this suite.
Apr 10 13:41:57.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:41:58.734: INFO: namespace configmap-754 deletion completed in 7.02606128s
•
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:41:58.734: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-432
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-69dea673-5b96-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume configMaps
Apr 10 13:41:59.147: INFO: Waiting up to 5m0s for pod "pod-configmaps-69e27ea9-5b96-11e9-8d1d-a6f828030f0b" in namespace "configmap-432" to be "success or failure"
Apr 10 13:41:59.173: INFO: Pod "pod-configmaps-69e27ea9-5b96-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.770557ms
Apr 10 13:42:01.199: INFO: Pod "pod-configmaps-69e27ea9-5b96-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05186976s
Apr 10 13:42:03.225: INFO: Pod "pod-configmaps-69e27ea9-5b96-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.078051787s
STEP: Saw pod success
Apr 10 13:42:03.225: INFO: Pod "pod-configmaps-69e27ea9-5b96-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:42:03.250: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-configmaps-69e27ea9-5b96-11e9-8d1d-a6f828030f0b container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 13:42:03.524: INFO: Waiting for pod pod-configmaps-69e27ea9-5b96-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:42:03.549: INFO: Pod pod-configmaps-69e27ea9-5b96-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:42:03.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-432" for this suite.
Apr 10 13:42:09.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:42:10.641: INFO: namespace configmap-432 deletion completed in 7.044382029s
•SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:42:10.642: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3152
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 10 13:42:10.982: INFO: Waiting up to 5m0s for pod "pod-70f0876a-5b96-11e9-8d1d-a6f828030f0b" in namespace "emptydir-3152" to be "success or failure"
Apr 10 13:42:11.007: INFO: Pod "pod-70f0876a-5b96-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.412683ms
Apr 10 13:42:13.032: INFO: Pod "pod-70f0876a-5b96-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049900813s
Apr 10 13:42:15.058: INFO: Pod "pod-70f0876a-5b96-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075747246s
STEP: Saw pod success
Apr 10 13:42:15.058: INFO: Pod "pod-70f0876a-5b96-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:42:15.083: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-70f0876a-5b96-11e9-8d1d-a6f828030f0b container test-container: <nil>
STEP: delete the pod
Apr 10 13:42:15.296: INFO: Waiting for pod pod-70f0876a-5b96-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:42:15.321: INFO: Pod pod-70f0876a-5b96-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:42:15.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3152" for this suite.
Apr 10 13:42:21.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:42:22.419: INFO: namespace emptydir-3152 deletion completed in 7.046594901s
•SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:42:22.420: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3944
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Apr 10 13:42:22.758: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-3944'
Apr 10 13:42:23.131: INFO: stderr: ""
Apr 10 13:42:23.131: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 10 13:42:23.131: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3944'
Apr 10 13:42:23.396: INFO: stderr: ""
Apr 10 13:42:23.396: INFO: stdout: "update-demo-nautilus-6n2ns update-demo-nautilus-rvsbj "
Apr 10 13:42:23.396: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-6n2ns -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3944'
Apr 10 13:42:23.599: INFO: stderr: ""
Apr 10 13:42:23.599: INFO: stdout: ""
Apr 10 13:42:23.599: INFO: update-demo-nautilus-6n2ns is created but not running
Apr 10 13:42:28.600: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3944'
Apr 10 13:42:28.795: INFO: stderr: ""
Apr 10 13:42:28.795: INFO: stdout: "update-demo-nautilus-6n2ns update-demo-nautilus-rvsbj "
Apr 10 13:42:28.795: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-6n2ns -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3944'
Apr 10 13:42:29.017: INFO: stderr: ""
Apr 10 13:42:29.017: INFO: stdout: "true"
Apr 10 13:42:29.017: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-6n2ns -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3944'
Apr 10 13:42:29.190: INFO: stderr: ""
Apr 10 13:42:29.190: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 13:42:29.190: INFO: validating pod update-demo-nautilus-6n2ns
Apr 10 13:42:29.302: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 13:42:29.302: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 13:42:29.302: INFO: update-demo-nautilus-6n2ns is verified up and running
Apr 10 13:42:29.302: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-rvsbj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3944'
Apr 10 13:42:29.488: INFO: stderr: ""
Apr 10 13:42:29.488: INFO: stdout: "true"
Apr 10 13:42:29.488: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-rvsbj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3944'
Apr 10 13:42:29.689: INFO: stderr: ""
Apr 10 13:42:29.689: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 13:42:29.689: INFO: validating pod update-demo-nautilus-rvsbj
Apr 10 13:42:29.799: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 13:42:29.799: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 13:42:29.799: INFO: update-demo-nautilus-rvsbj is verified up and running
STEP: rolling-update to new replication controller
Apr 10 13:42:29.801: INFO: scanned /root for discovery docs: <nil>
Apr 10 13:42:29.801: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-3944'
Apr 10 13:42:54.464: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Apr 10 13:42:54.464: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 10 13:42:54.464: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3944'
Apr 10 13:42:54.691: INFO: stderr: ""
Apr 10 13:42:54.691: INFO: stdout: "update-demo-kitten-gxsnv update-demo-kitten-hzk5j "
Apr 10 13:42:54.691: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-gxsnv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3944'
Apr 10 13:42:54.916: INFO: stderr: ""
Apr 10 13:42:54.916: INFO: stdout: "true"
Apr 10 13:42:54.916: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-gxsnv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3944'
Apr 10 13:42:55.095: INFO: stderr: ""
Apr 10 13:42:55.095: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Apr 10 13:42:55.095: INFO: validating pod update-demo-kitten-gxsnv
Apr 10 13:42:55.209: INFO: got data: {
  "image": "kitten.jpg"
}

Apr 10 13:42:55.209: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Apr 10 13:42:55.209: INFO: update-demo-kitten-gxsnv is verified up and running
Apr 10 13:42:55.209: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-hzk5j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3944'
Apr 10 13:42:55.384: INFO: stderr: ""
Apr 10 13:42:55.384: INFO: stdout: "true"
Apr 10 13:42:55.384: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-hzk5j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3944'
Apr 10 13:42:55.588: INFO: stderr: ""
Apr 10 13:42:55.588: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Apr 10 13:42:55.588: INFO: validating pod update-demo-kitten-hzk5j
Apr 10 13:42:55.698: INFO: got data: {
  "image": "kitten.jpg"
}

Apr 10 13:42:55.698: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Apr 10 13:42:55.698: INFO: update-demo-kitten-hzk5j is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:42:55.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3944" for this suite.
Apr 10 13:43:19.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:43:20.760: INFO: namespace kubectl-3944 deletion completed in 25.013631789s
•
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:43:20.760: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8785
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-9ab95c62-5b96-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume configMaps
Apr 10 13:43:21.113: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9abd1f04-5b96-11e9-8d1d-a6f828030f0b" in namespace "projected-8785" to be "success or failure"
Apr 10 13:43:21.137: INFO: Pod "pod-projected-configmaps-9abd1f04-5b96-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.431763ms
Apr 10 13:43:23.162: INFO: Pod "pod-projected-configmaps-9abd1f04-5b96-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04971046s
Apr 10 13:43:25.188: INFO: Pod "pod-projected-configmaps-9abd1f04-5b96-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075655297s
STEP: Saw pod success
Apr 10 13:43:25.188: INFO: Pod "pod-projected-configmaps-9abd1f04-5b96-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:43:25.213: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-projected-configmaps-9abd1f04-5b96-11e9-8d1d-a6f828030f0b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 13:43:25.548: INFO: Waiting for pod pod-projected-configmaps-9abd1f04-5b96-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:43:25.572: INFO: Pod pod-projected-configmaps-9abd1f04-5b96-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:43:25.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8785" for this suite.
Apr 10 13:43:31.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:43:32.647: INFO: namespace projected-8785 deletion completed in 7.027574944s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:43:32.648: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2908
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 10 13:43:32.989: INFO: Waiting up to 5m0s for pod "pod-a1d1d47c-5b96-11e9-8d1d-a6f828030f0b" in namespace "emptydir-2908" to be "success or failure"
Apr 10 13:43:33.014: INFO: Pod "pod-a1d1d47c-5b96-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.784343ms
Apr 10 13:43:35.040: INFO: Pod "pod-a1d1d47c-5b96-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050684092s
Apr 10 13:43:37.066: INFO: Pod "pod-a1d1d47c-5b96-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076476059s
STEP: Saw pod success
Apr 10 13:43:37.066: INFO: Pod "pod-a1d1d47c-5b96-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:43:37.092: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-a1d1d47c-5b96-11e9-8d1d-a6f828030f0b container test-container: <nil>
STEP: delete the pod
Apr 10 13:43:37.278: INFO: Waiting for pod pod-a1d1d47c-5b96-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:43:37.302: INFO: Pod pod-a1d1d47c-5b96-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:43:37.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2908" for this suite.
Apr 10 13:43:43.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:43:44.330: INFO: namespace emptydir-2908 deletion completed in 6.981136109s
•
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:43:44.331: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1959
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-1959
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 10 13:43:44.664: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 10 13:44:07.102: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.117:8080/dial?request=hostName&protocol=http&host=100.96.0.43&port=8080&tries=1'] Namespace:pod-network-test-1959 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 13:44:07.102: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 13:44:07.669: INFO: Waiting for endpoints: map[]
Apr 10 13:44:07.694: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.117:8080/dial?request=hostName&protocol=http&host=100.96.1.116&port=8080&tries=1'] Namespace:pod-network-test-1959 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 13:44:07.695: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 13:44:08.160: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:44:08.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1959" for this suite.
Apr 10 13:44:30.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:44:31.206: INFO: namespace pod-network-test-1959 deletion completed in 22.998144193s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:44:31.208: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5045
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 13:44:31.598: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c4c00ca0-5b96-11e9-8d1d-a6f828030f0b" in namespace "projected-5045" to be "success or failure"
Apr 10 13:44:31.623: INFO: Pod "downwardapi-volume-c4c00ca0-5b96-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.910778ms
Apr 10 13:44:33.649: INFO: Pod "downwardapi-volume-c4c00ca0-5b96-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05128938s
Apr 10 13:44:35.675: INFO: Pod "downwardapi-volume-c4c00ca0-5b96-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077096357s
STEP: Saw pod success
Apr 10 13:44:35.675: INFO: Pod "downwardapi-volume-c4c00ca0-5b96-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:44:35.700: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downwardapi-volume-c4c00ca0-5b96-11e9-8d1d-a6f828030f0b container client-container: <nil>
STEP: delete the pod
Apr 10 13:44:35.771: INFO: Waiting for pod downwardapi-volume-c4c00ca0-5b96-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:44:35.796: INFO: Pod downwardapi-volume-c4c00ca0-5b96-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:44:35.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5045" for this suite.
Apr 10 13:44:41.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:44:42.855: INFO: namespace projected-5045 deletion completed in 7.011855675s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:44:42.855: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8919
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 13:44:43.189: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cba95783-5b96-11e9-8d1d-a6f828030f0b" in namespace "projected-8919" to be "success or failure"
Apr 10 13:44:43.214: INFO: Pod "downwardapi-volume-cba95783-5b96-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.444701ms
Apr 10 13:44:45.240: INFO: Pod "downwardapi-volume-cba95783-5b96-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051515632s
Apr 10 13:44:47.266: INFO: Pod "downwardapi-volume-cba95783-5b96-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077461273s
STEP: Saw pod success
Apr 10 13:44:47.266: INFO: Pod "downwardapi-volume-cba95783-5b96-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:44:47.291: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downwardapi-volume-cba95783-5b96-11e9-8d1d-a6f828030f0b container client-container: <nil>
STEP: delete the pod
Apr 10 13:44:47.543: INFO: Waiting for pod downwardapi-volume-cba95783-5b96-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:44:47.567: INFO: Pod downwardapi-volume-cba95783-5b96-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:44:47.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8919" for this suite.
Apr 10 13:44:53.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:44:54.611: INFO: namespace projected-8919 deletion completed in 6.996212292s
•SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:44:54.611: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4829
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Apr 10 13:44:55.007: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Apr 10 13:44:55.008: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-4829'
Apr 10 13:44:55.348: INFO: stderr: ""
Apr 10 13:44:55.348: INFO: stdout: "service/redis-slave created\n"
Apr 10 13:44:55.378: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Apr 10 13:44:55.378: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-4829'
Apr 10 13:44:55.737: INFO: stderr: ""
Apr 10 13:44:55.737: INFO: stdout: "service/redis-master created\n"
Apr 10 13:44:55.738: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr 10 13:44:55.738: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-4829'
Apr 10 13:44:56.139: INFO: stderr: ""
Apr 10 13:44:56.139: INFO: stdout: "service/frontend created\n"
Apr 10 13:44:56.140: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Apr 10 13:44:56.141: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-4829'
Apr 10 13:44:56.424: INFO: stderr: ""
Apr 10 13:44:56.424: INFO: stdout: "deployment.apps/frontend created\n"
Apr 10 13:44:56.425: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 10 13:44:56.425: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-4829'
Apr 10 13:44:56.824: INFO: stderr: ""
Apr 10 13:44:56.824: INFO: stdout: "deployment.apps/redis-master created\n"
Apr 10 13:44:56.825: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Apr 10 13:44:56.825: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-4829'
Apr 10 13:44:57.223: INFO: stderr: ""
Apr 10 13:44:57.223: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Apr 10 13:44:57.223: INFO: Waiting for all frontend pods to be Running.
Apr 10 13:45:32.275: INFO: Waiting for frontend to serve content.
Apr 10 13:45:32.392: INFO: Trying to add a new entry to the guestbook.
Apr 10 13:45:32.521: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Apr 10 13:45:32.648: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-4829'
Apr 10 13:45:32.876: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 13:45:32.876: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Apr 10 13:45:32.876: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-4829'
Apr 10 13:45:33.222: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 13:45:33.222: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Apr 10 13:45:33.222: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-4829'
Apr 10 13:45:33.428: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 13:45:33.428: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 10 13:45:33.428: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-4829'
Apr 10 13:45:33.641: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 13:45:33.641: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 10 13:45:33.641: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-4829'
Apr 10 13:45:33.899: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 13:45:33.899: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Apr 10 13:45:33.899: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-4829'
Apr 10 13:45:34.121: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 13:45:34.121: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:45:34.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4829" for this suite.
Apr 10 13:46:14.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:46:15.194: INFO: namespace kubectl-4829 deletion completed in 41.024496767s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:46:15.194: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6074
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6074
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Apr 10 13:46:15.540: INFO: Found 1 stateful pods, waiting for 3
Apr 10 13:46:25.567: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 13:46:25.567: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 13:46:25.567: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Apr 10 13:46:35.567: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 13:46:35.567: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 13:46:35.567: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Apr 10 13:46:35.703: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Apr 10 13:46:35.815: INFO: Updating stateful set ss2
Apr 10 13:46:35.869: INFO: Waiting for Pod statefulset-6074/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Apr 10 13:46:46.041: INFO: Found 2 stateful pods, waiting for 3
Apr 10 13:46:56.068: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 13:46:56.068: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 13:46:56.068: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Apr 10 13:46:56.179: INFO: Updating stateful set ss2
Apr 10 13:46:56.230: INFO: Waiting for Pod statefulset-6074/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 13:47:06.283: INFO: Waiting for Pod statefulset-6074/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 13:47:16.343: INFO: Updating stateful set ss2
Apr 10 13:47:16.394: INFO: Waiting for StatefulSet statefulset-6074/ss2 to complete update
Apr 10 13:47:16.394: INFO: Waiting for Pod statefulset-6074/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 13:47:26.445: INFO: Waiting for StatefulSet statefulset-6074/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 10 13:47:36.445: INFO: Deleting all statefulset in ns statefulset-6074
Apr 10 13:47:36.470: INFO: Scaling statefulset ss2 to 0
Apr 10 13:48:06.572: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 13:48:06.597: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:48:06.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6074" for this suite.
Apr 10 13:48:12.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:48:13.734: INFO: namespace statefulset-6074 deletion completed in 7.012154037s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:48:13.735: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-7139
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6275
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-615
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:48:40.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7139" for this suite.
Apr 10 13:48:46.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:48:47.842: INFO: namespace namespaces-7139 deletion completed in 7.031245222s
STEP: Destroying namespace "nsdeletetest-6275" for this suite.
Apr 10 13:48:47.937: INFO: Namespace nsdeletetest-6275 was already deleted
STEP: Destroying namespace "nsdeletetest-615" for this suite.
Apr 10 13:48:54.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:48:55.078: INFO: namespace nsdeletetest-615 deletion completed in 7.141030041s
•SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:48:55.078: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3854
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 13:48:55.365: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3854'
Apr 10 13:48:55.865: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 10 13:48:55.865: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Apr 10 13:48:55.900: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-3854'
Apr 10 13:48:56.112: INFO: stderr: ""
Apr 10 13:48:56.112: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:48:56.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3854" for this suite.
Apr 10 13:49:02.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:49:03.175: INFO: namespace kubectl-3854 deletion completed in 7.015123161s
•SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:49:03.175: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4624
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 13:49:03.492: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66d07b84-5b97-11e9-8d1d-a6f828030f0b" in namespace "projected-4624" to be "success or failure"
Apr 10 13:49:03.517: INFO: Pod "downwardapi-volume-66d07b84-5b97-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.660931ms
Apr 10 13:49:05.542: INFO: Pod "downwardapi-volume-66d07b84-5b97-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050365239s
Apr 10 13:49:07.568: INFO: Pod "downwardapi-volume-66d07b84-5b97-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076212737s
STEP: Saw pod success
Apr 10 13:49:07.568: INFO: Pod "downwardapi-volume-66d07b84-5b97-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:49:07.593: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downwardapi-volume-66d07b84-5b97-11e9-8d1d-a6f828030f0b container client-container: <nil>
STEP: delete the pod
Apr 10 13:49:07.805: INFO: Waiting for pod downwardapi-volume-66d07b84-5b97-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:49:07.832: INFO: Pod downwardapi-volume-66d07b84-5b97-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:49:07.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4624" for this suite.
Apr 10 13:49:13.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:49:14.870: INFO: namespace projected-4624 deletion completed in 6.990505525s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:49:14.870: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-6ddbbe8d-5b97-11e9-8d1d-a6f828030f0b
STEP: Creating secret with name s-test-opt-upd-6ddbbef2-5b97-11e9-8d1d-a6f828030f0b
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-6ddbbe8d-5b97-11e9-8d1d-a6f828030f0b
STEP: Updating secret s-test-opt-upd-6ddbbef2-5b97-11e9-8d1d-a6f828030f0b
STEP: Creating secret with name s-test-opt-create-6ddbbf0e-5b97-11e9-8d1d-a6f828030f0b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:50:37.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9" for this suite.
Apr 10 13:50:59.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:51:00.100: INFO: namespace projected-9 deletion completed in 23.015145545s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:51:00.100: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7842
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Apr 10 13:51:00.467: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-7842,SelfLink:/api/v1/namespaces/watch-7842/configmaps/e2e-watch-test-watch-closed,UID:ac85800c-5b97-11e9-99e4-7640922b69f1,ResourceVersion:15621,Generation:0,CreationTimestamp:2019-04-10 13:51:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 10 13:51:00.468: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-7842,SelfLink:/api/v1/namespaces/watch-7842/configmaps/e2e-watch-test-watch-closed,UID:ac85800c-5b97-11e9-99e4-7640922b69f1,ResourceVersion:15622,Generation:0,CreationTimestamp:2019-04-10 13:51:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Apr 10 13:51:00.574: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-7842,SelfLink:/api/v1/namespaces/watch-7842/configmaps/e2e-watch-test-watch-closed,UID:ac85800c-5b97-11e9-99e4-7640922b69f1,ResourceVersion:15623,Generation:0,CreationTimestamp:2019-04-10 13:51:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 10 13:51:00.575: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-7842,SelfLink:/api/v1/namespaces/watch-7842/configmaps/e2e-watch-test-watch-closed,UID:ac85800c-5b97-11e9-99e4-7640922b69f1,ResourceVersion:15624,Generation:0,CreationTimestamp:2019-04-10 13:51:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:51:00.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7842" for this suite.
Apr 10 13:51:06.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:51:07.608: INFO: namespace watch-7842 deletion completed in 7.008037017s
•SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:51:07.608: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4527
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-b10563db-5b97-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume secrets
Apr 10 13:51:08.015: INFO: Waiting up to 5m0s for pod "pod-secrets-b10922c0-5b97-11e9-8d1d-a6f828030f0b" in namespace "secrets-4527" to be "success or failure"
Apr 10 13:51:08.039: INFO: Pod "pod-secrets-b10922c0-5b97-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.647568ms
Apr 10 13:51:10.066: INFO: Pod "pod-secrets-b10922c0-5b97-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051018697s
Apr 10 13:51:12.092: INFO: Pod "pod-secrets-b10922c0-5b97-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076850211s
STEP: Saw pod success
Apr 10 13:51:12.092: INFO: Pod "pod-secrets-b10922c0-5b97-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:51:12.116: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-secrets-b10922c0-5b97-11e9-8d1d-a6f828030f0b container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 13:51:12.181: INFO: Waiting for pod pod-secrets-b10922c0-5b97-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:51:12.206: INFO: Pod pod-secrets-b10922c0-5b97-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:51:12.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4527" for this suite.
Apr 10 13:51:18.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:51:19.259: INFO: namespace secrets-4527 deletion completed in 7.004552556s
•SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:51:19.260: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1960
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
Apr 10 13:51:19.576: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1960'
Apr 10 13:51:19.919: INFO: stderr: ""
Apr 10 13:51:19.919: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Apr 10 13:51:20.948: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 13:51:20.948: INFO: Found 0 / 1
Apr 10 13:51:21.945: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 13:51:21.945: INFO: Found 0 / 1
Apr 10 13:51:22.945: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 13:51:22.945: INFO: Found 0 / 1
Apr 10 13:51:23.971: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 13:51:23.971: INFO: Found 1 / 1
Apr 10 13:51:23.971: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 10 13:51:23.995: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 13:51:23.996: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Apr 10 13:51:23.996: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-qkc2z redis-master --namespace=kubectl-1960'
Apr 10 13:51:24.234: INFO: stderr: ""
Apr 10 13:51:24.234: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 10 Apr 13:51:22.487 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 10 Apr 13:51:22.487 # Server started, Redis version 3.2.12\n1:M 10 Apr 13:51:22.487 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 10 Apr 13:51:22.487 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Apr 10 13:51:24.234: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-qkc2z redis-master --namespace=kubectl-1960 --tail=1'
Apr 10 13:51:24.490: INFO: stderr: ""
Apr 10 13:51:24.490: INFO: stdout: "1:M 10 Apr 13:51:22.487 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Apr 10 13:51:24.490: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-qkc2z redis-master --namespace=kubectl-1960 --limit-bytes=1'
Apr 10 13:51:24.722: INFO: stderr: ""
Apr 10 13:51:24.722: INFO: stdout: " "
STEP: exposing timestamps
Apr 10 13:51:24.723: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-qkc2z redis-master --namespace=kubectl-1960 --tail=1 --timestamps'
Apr 10 13:51:24.938: INFO: stderr: ""
Apr 10 13:51:24.938: INFO: stdout: "2019-04-10T13:51:22.487648997Z 1:M 10 Apr 13:51:22.487 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Apr 10 13:51:27.439: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-qkc2z redis-master --namespace=kubectl-1960 --since=1s'
Apr 10 13:51:27.632: INFO: stderr: ""
Apr 10 13:51:27.632: INFO: stdout: ""
Apr 10 13:51:27.632: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-qkc2z redis-master --namespace=kubectl-1960 --since=24h'
Apr 10 13:51:27.833: INFO: stderr: ""
Apr 10 13:51:27.833: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 10 Apr 13:51:22.487 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 10 Apr 13:51:22.487 # Server started, Redis version 3.2.12\n1:M 10 Apr 13:51:22.487 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 10 Apr 13:51:22.487 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
Apr 10 13:51:27.833: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-1960'
Apr 10 13:51:28.022: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 13:51:28.022: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Apr 10 13:51:28.023: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=nginx --no-headers --namespace=kubectl-1960'
Apr 10 13:51:28.218: INFO: stderr: "No resources found.\n"
Apr 10 13:51:28.219: INFO: stdout: ""
Apr 10 13:51:28.219: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=nginx --namespace=kubectl-1960 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 10 13:51:28.399: INFO: stderr: ""
Apr 10 13:51:28.399: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:51:28.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1960" for this suite.
Apr 10 13:51:50.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:51:51.459: INFO: namespace kubectl-1960 deletion completed in 23.010930579s
•SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:51:51.459: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5374
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 13:51:51.754: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5374'
Apr 10 13:51:51.988: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 10 13:51:51.988: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Apr 10 13:51:52.013: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete jobs e2e-test-nginx-job --namespace=kubectl-5374'
Apr 10 13:51:52.218: INFO: stderr: ""
Apr 10 13:51:52.218: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:51:52.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5374" for this suite.
Apr 10 13:51:58.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:51:59.257: INFO: namespace kubectl-5374 deletion completed in 6.99113128s
•SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:51:59.258: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9125
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr 10 13:51:59.577: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:52:06.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9125" for this suite.
Apr 10 13:52:30.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:52:31.712: INFO: namespace init-container-9125 deletion completed in 25.044233799s
•
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:52:31.712: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1542
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-e3243a12-5b97-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume secrets
Apr 10 13:52:32.102: INFO: Waiting up to 5m0s for pod "pod-secrets-e32802a9-5b97-11e9-8d1d-a6f828030f0b" in namespace "secrets-1542" to be "success or failure"
Apr 10 13:52:32.127: INFO: Pod "pod-secrets-e32802a9-5b97-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.298788ms
Apr 10 13:52:34.152: INFO: Pod "pod-secrets-e32802a9-5b97-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049890098s
Apr 10 13:52:36.178: INFO: Pod "pod-secrets-e32802a9-5b97-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075872542s
STEP: Saw pod success
Apr 10 13:52:36.178: INFO: Pod "pod-secrets-e32802a9-5b97-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:52:36.204: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-secrets-e32802a9-5b97-11e9-8d1d-a6f828030f0b container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 13:52:36.318: INFO: Waiting for pod pod-secrets-e32802a9-5b97-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:52:36.348: INFO: Pod pod-secrets-e32802a9-5b97-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:52:36.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1542" for this suite.
Apr 10 13:52:42.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:52:43.413: INFO: namespace secrets-1542 deletion completed in 7.015788693s
•SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:52:43.414: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8074
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 13:52:43.773: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-8074'
Apr 10 13:52:44.008: INFO: stderr: ""
Apr 10 13:52:44.008: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Apr 10 13:52:49.059: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod e2e-test-nginx-pod --namespace=kubectl-8074 -o json'
Apr 10 13:52:49.293: INFO: stderr: ""
Apr 10 13:52:49.293: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.1.139/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-04-10T13:52:43Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-8074\",\n        \"resourceVersion\": \"15980\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-8074/pods/e2e-test-nginx-pod\",\n        \"uid\": \"ea42f376-5b97-11e9-99e4-7640922b69f1\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-qg89h\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-qg89h\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-qg89h\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-10T13:52:44Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-10T13:52:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-10T13:52:47Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-10T13:52:43Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://a95e03fad24bb6a8edb940286f99d2f56eba7ba2599390e8b1d00b801c0e7016\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:0ad702c1a9d38787ca35b4b3c22fb5a69dbb6ffb8a2ce002701b8aff115bc053\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-04-10T13:52:46Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.0.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.1.139\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-04-10T13:52:44Z\"\n    }\n}\n"
STEP: replace the image in the pod
Apr 10 13:52:49.293: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config replace -f - --namespace=kubectl-8074'
Apr 10 13:52:49.627: INFO: stderr: ""
Apr 10 13:52:49.627: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Apr 10 13:52:49.652: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-8074'
Apr 10 13:53:02.590: INFO: stderr: ""
Apr 10 13:53:02.590: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:53:02.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8074" for this suite.
Apr 10 13:53:08.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:53:09.629: INFO: namespace kubectl-8074 deletion completed in 6.99021603s
•
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:53:09.629: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4641
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-f9bdaf81-5b97-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume configMaps
Apr 10 13:53:10.019: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f9c1904d-5b97-11e9-8d1d-a6f828030f0b" in namespace "projected-4641" to be "success or failure"
Apr 10 13:53:10.044: INFO: Pod "pod-projected-configmaps-f9c1904d-5b97-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.947825ms
Apr 10 13:53:12.071: INFO: Pod "pod-projected-configmaps-f9c1904d-5b97-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052051364s
Apr 10 13:53:14.097: INFO: Pod "pod-projected-configmaps-f9c1904d-5b97-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077927716s
STEP: Saw pod success
Apr 10 13:53:14.097: INFO: Pod "pod-projected-configmaps-f9c1904d-5b97-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:53:14.122: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-projected-configmaps-f9c1904d-5b97-11e9-8d1d-a6f828030f0b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 13:53:14.239: INFO: Waiting for pod pod-projected-configmaps-f9c1904d-5b97-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:53:14.264: INFO: Pod pod-projected-configmaps-f9c1904d-5b97-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:53:14.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4641" for this suite.
Apr 10 13:53:20.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:53:21.327: INFO: namespace projected-4641 deletion completed in 7.015296915s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:53:21.328: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7704
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-00b7790e-5b98-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume configMaps
Apr 10 13:53:21.724: INFO: Waiting up to 5m0s for pod "pod-configmaps-00bb58a7-5b98-11e9-8d1d-a6f828030f0b" in namespace "configmap-7704" to be "success or failure"
Apr 10 13:53:21.749: INFO: Pod "pod-configmaps-00bb58a7-5b98-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.183544ms
Apr 10 13:53:23.775: INFO: Pod "pod-configmaps-00bb58a7-5b98-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0510795s
Apr 10 13:53:25.801: INFO: Pod "pod-configmaps-00bb58a7-5b98-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077260563s
STEP: Saw pod success
Apr 10 13:53:25.801: INFO: Pod "pod-configmaps-00bb58a7-5b98-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:53:25.827: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-configmaps-00bb58a7-5b98-11e9-8d1d-a6f828030f0b container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 13:53:25.912: INFO: Waiting for pod pod-configmaps-00bb58a7-5b98-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:53:25.937: INFO: Pod pod-configmaps-00bb58a7-5b98-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:53:25.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7704" for this suite.
Apr 10 13:53:32.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:53:32.996: INFO: namespace configmap-7704 deletion completed in 7.009971659s
•SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:53:32.996: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5307
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 13:53:33.309: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 10 13:53:37.370: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr 10 13:53:39.396: INFO: Creating deployment "test-rollover-deployment"
Apr 10 13:53:39.448: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr 10 13:53:41.509: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr 10 13:53:41.560: INFO: Ensure that both replica sets have 1 created replica
Apr 10 13:53:41.610: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr 10 13:53:41.661: INFO: Updating deployment test-rollover-deployment
Apr 10 13:53:41.661: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr 10 13:53:43.731: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr 10 13:53:43.781: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr 10 13:53:43.841: INFO: all replica sets need to contain the pod-template-hash label
Apr 10 13:53:43.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501219, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501219, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501221, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501219, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 13:53:45.900: INFO: all replica sets need to contain the pod-template-hash label
Apr 10 13:53:45.900: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501219, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501219, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501225, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501219, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 13:53:47.893: INFO: all replica sets need to contain the pod-template-hash label
Apr 10 13:53:47.893: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501219, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501219, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501225, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501219, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 13:53:49.892: INFO: all replica sets need to contain the pod-template-hash label
Apr 10 13:53:49.892: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501219, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501219, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501225, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501219, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 13:53:51.892: INFO: all replica sets need to contain the pod-template-hash label
Apr 10 13:53:51.892: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501219, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501219, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501225, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501219, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 13:53:53.892: INFO: all replica sets need to contain the pod-template-hash label
Apr 10 13:53:53.892: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501219, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501219, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501225, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501219, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 13:53:55.894: INFO: 
Apr 10 13:53:55.894: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 10 13:53:55.969: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-5307,SelfLink:/apis/apps/v1/namespaces/deployment-5307/deployments/test-rollover-deployment,UID:0b4b8be7-5b98-11e9-99e4-7640922b69f1,ResourceVersion:16238,Generation:2,CreationTimestamp:2019-04-10 13:53:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-04-10 13:53:39 +0000 UTC 2019-04-10 13:53:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-04-10 13:53:55 +0000 UTC 2019-04-10 13:53:39 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Apr 10 13:53:55.995: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-5307,SelfLink:/apis/apps/v1/namespaces/deployment-5307/replicasets/test-rollover-deployment-766b4d6c9d,UID:0ca1d020-5b98-11e9-99e4-7640922b69f1,ResourceVersion:16231,Generation:2,CreationTimestamp:2019-04-10 13:53:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0b4b8be7-5b98-11e9-99e4-7640922b69f1 0xc000059d57 0xc000059d58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr 10 13:53:55.995: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr 10 13:53:55.995: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-5307,SelfLink:/apis/apps/v1/namespaces/deployment-5307/replicasets/test-rollover-controller,UID:07a2780d-5b98-11e9-99e4-7640922b69f1,ResourceVersion:16237,Generation:2,CreationTimestamp:2019-04-10 13:53:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0b4b8be7-5b98-11e9-99e4-7640922b69f1 0xc000059a77 0xc000059a78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 13:53:55.995: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-5307,SelfLink:/apis/apps/v1/namespaces/deployment-5307/replicasets/test-rollover-deployment-6455657675,UID:0b4f7e5a-5b98-11e9-99e4-7640922b69f1,ResourceVersion:16194,Generation:2,CreationTimestamp:2019-04-10 13:53:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0b4b8be7-5b98-11e9-99e4-7640922b69f1 0xc000059bf7 0xc000059bf8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 13:53:56.021: INFO: Pod "test-rollover-deployment-766b4d6c9d-bgg2q" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-bgg2q,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-5307,SelfLink:/api/v1/namespaces/deployment-5307/pods/test-rollover-deployment-766b4d6c9d-bgg2q,UID:0cac94f6-5b98-11e9-99e4-7640922b69f1,ResourceVersion:16206,Generation:0,CreationTimestamp:2019-04-10 13:53:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.144/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d 0ca1d020-5b98-11e9-99e4-7640922b69f1 0xc002dd89b7 0xc002dd89b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-6gnzs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6gnzs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-6gnzs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dd8a20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dd8a40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 13:53:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 13:53:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 13:53:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 13:53:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.144,StartTime:2019-04-10 13:53:41 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-04-10 13:53:44 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://04abe58a2af5847fb269a1ee9bf7bc4bdfe581829a2826d355aaedf0d75b61ae}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:53:56.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5307" for this suite.
Apr 10 13:54:02.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:54:03.085: INFO: namespace deployment-5307 deletion completed in 7.015249361s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:54:03.085: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 10 13:54:03.400: INFO: Waiting up to 5m0s for pod "pod-1992e69b-5b98-11e9-8d1d-a6f828030f0b" in namespace "emptydir-4119" to be "success or failure"
Apr 10 13:54:03.425: INFO: Pod "pod-1992e69b-5b98-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.80924ms
Apr 10 13:54:05.451: INFO: Pod "pod-1992e69b-5b98-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050763132s
Apr 10 13:54:07.477: INFO: Pod "pod-1992e69b-5b98-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076572691s
Apr 10 13:54:09.503: INFO: Pod "pod-1992e69b-5b98-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.103118498s
STEP: Saw pod success
Apr 10 13:54:09.503: INFO: Pod "pod-1992e69b-5b98-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:54:09.529: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-1992e69b-5b98-11e9-8d1d-a6f828030f0b container test-container: <nil>
STEP: delete the pod
Apr 10 13:54:09.611: INFO: Waiting for pod pod-1992e69b-5b98-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:54:09.636: INFO: Pod pod-1992e69b-5b98-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:54:09.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4119" for this suite.
Apr 10 13:54:15.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:54:16.716: INFO: namespace emptydir-4119 deletion completed in 7.032214823s
•SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:54:16.716: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4334
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 10 13:54:17.079: INFO: Waiting up to 5m0s for pod "pod-21ba2baf-5b98-11e9-8d1d-a6f828030f0b" in namespace "emptydir-4334" to be "success or failure"
Apr 10 13:54:17.115: INFO: Pod "pod-21ba2baf-5b98-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 35.427488ms
Apr 10 13:54:19.141: INFO: Pod "pod-21ba2baf-5b98-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061408902s
Apr 10 13:54:21.167: INFO: Pod "pod-21ba2baf-5b98-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.087259843s
STEP: Saw pod success
Apr 10 13:54:21.167: INFO: Pod "pod-21ba2baf-5b98-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:54:21.192: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-21ba2baf-5b98-11e9-8d1d-a6f828030f0b container test-container: <nil>
STEP: delete the pod
Apr 10 13:54:21.338: INFO: Waiting for pod pod-21ba2baf-5b98-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:54:21.362: INFO: Pod pod-21ba2baf-5b98-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:54:21.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4334" for this suite.
Apr 10 13:54:27.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:54:28.432: INFO: namespace emptydir-4334 deletion completed in 7.022079243s
•SSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:54:28.432: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4058
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 13:54:28.848: INFO: Creating deployment "test-recreate-deployment"
Apr 10 13:54:28.873: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr 10 13:54:28.940: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr 10 13:54:28.965: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501268, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501268, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501268, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501268, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 13:54:30.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501268, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501268, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501268, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690501268, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 13:54:32.991: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr 10 13:54:33.042: INFO: Updating deployment test-recreate-deployment
Apr 10 13:54:33.042: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 10 13:54:33.204: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-4058,SelfLink:/apis/apps/v1/namespaces/deployment-4058/deployments/test-recreate-deployment,UID:28c53ea1-5b98-11e9-99e4-7640922b69f1,ResourceVersion:16406,Generation:2,CreationTimestamp:2019-04-10 13:54:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-04-10 13:54:33 +0000 UTC 2019-04-10 13:54:33 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-04-10 13:54:33 +0000 UTC 2019-04-10 13:54:28 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Apr 10 13:54:33.233: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-4058,SelfLink:/apis/apps/v1/namespaces/deployment-4058/replicasets/test-recreate-deployment-c9cbd8684,UID:2b4856e2-5b98-11e9-99e4-7640922b69f1,ResourceVersion:16405,Generation:1,CreationTimestamp:2019-04-10 13:54:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 28c53ea1-5b98-11e9-99e4-7640922b69f1 0xc002deb9c0 0xc002deb9c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 13:54:33.233: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr 10 13:54:33.233: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-4058,SelfLink:/apis/apps/v1/namespaces/deployment-4058/replicasets/test-recreate-deployment-7d57d5ff7c,UID:28c5d4bb-5b98-11e9-99e4-7640922b69f1,ResourceVersion:16397,Generation:2,CreationTimestamp:2019-04-10 13:54:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 28c53ea1-5b98-11e9-99e4-7640922b69f1 0xc002deb8f7 0xc002deb8f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 13:54:33.271: INFO: Pod "test-recreate-deployment-c9cbd8684-lgtnq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-lgtnq,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-4058,SelfLink:/api/v1/namespaces/deployment-4058/pods/test-recreate-deployment-c9cbd8684-lgtnq,UID:2b4a4da5-5b98-11e9-99e4-7640922b69f1,ResourceVersion:16404,Generation:0,CreationTimestamp:2019-04-10 13:54:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 2b4856e2-5b98-11e9-99e4-7640922b69f1 0xc000b9e1e0 0xc000b9e1e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xtcbg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xtcbg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xtcbg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b9e240} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b9e260}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 13:54:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 13:54:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 13:54:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 13:54:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-04-10 13:54:33 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:54:33.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4058" for this suite.
Apr 10 13:54:39.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:54:40.342: INFO: namespace deployment-4058 deletion completed in 7.022764449s
•SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:54:40.343: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3033
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 10 13:54:48.993: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 13:54:49.019: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 13:54:51.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 13:54:51.045: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 13:54:53.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 13:54:53.045: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 13:54:55.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 13:54:55.044: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 13:54:57.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 13:54:57.044: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 13:54:59.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 13:54:59.044: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 13:55:01.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 13:55:01.045: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 13:55:03.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 13:55:03.046: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 13:55:05.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 13:55:05.045: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 13:55:07.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 13:55:07.045: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 13:55:09.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 13:55:09.045: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 13:55:11.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 13:55:11.045: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 13:55:13.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 13:55:13.045: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:55:13.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3033" for this suite.
Apr 10 13:55:35.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:55:36.083: INFO: namespace container-lifecycle-hook-3033 deletion completed in 22.990777627s
•S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:55:36.083: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9582
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-50ff04bf-5b98-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume secrets
Apr 10 13:55:36.412: INFO: Waiting up to 5m0s for pod "pod-secrets-5102e1ba-5b98-11e9-8d1d-a6f828030f0b" in namespace "secrets-9582" to be "success or failure"
Apr 10 13:55:36.437: INFO: Pod "pod-secrets-5102e1ba-5b98-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.747647ms
Apr 10 13:55:38.462: INFO: Pod "pod-secrets-5102e1ba-5b98-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050109664s
Apr 10 13:55:40.488: INFO: Pod "pod-secrets-5102e1ba-5b98-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075780318s
STEP: Saw pod success
Apr 10 13:55:40.488: INFO: Pod "pod-secrets-5102e1ba-5b98-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:55:40.513: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-secrets-5102e1ba-5b98-11e9-8d1d-a6f828030f0b container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 13:55:40.579: INFO: Waiting for pod pod-secrets-5102e1ba-5b98-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:55:40.604: INFO: Pod pod-secrets-5102e1ba-5b98-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:55:40.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9582" for this suite.
Apr 10 13:55:46.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:55:47.671: INFO: namespace secrets-9582 deletion completed in 7.008843044s
•SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:55:47.671: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9618
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 13:55:47.988: INFO: Creating ReplicaSet my-hostname-basic-57ee00d6-5b98-11e9-8d1d-a6f828030f0b
Apr 10 13:55:48.045: INFO: Pod name my-hostname-basic-57ee00d6-5b98-11e9-8d1d-a6f828030f0b: Found 0 pods out of 1
Apr 10 13:55:53.070: INFO: Pod name my-hostname-basic-57ee00d6-5b98-11e9-8d1d-a6f828030f0b: Found 1 pods out of 1
Apr 10 13:55:53.070: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-57ee00d6-5b98-11e9-8d1d-a6f828030f0b" is running
Apr 10 13:55:53.095: INFO: Pod "my-hostname-basic-57ee00d6-5b98-11e9-8d1d-a6f828030f0b-tdwbv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 13:55:48 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 13:55:51 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 13:55:51 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 13:55:48 +0000 UTC Reason: Message:}])
Apr 10 13:55:53.095: INFO: Trying to dial the pod
Apr 10 13:55:58.264: INFO: Controller my-hostname-basic-57ee00d6-5b98-11e9-8d1d-a6f828030f0b: Got expected result from replica 1 [my-hostname-basic-57ee00d6-5b98-11e9-8d1d-a6f828030f0b-tdwbv]: "my-hostname-basic-57ee00d6-5b98-11e9-8d1d-a6f828030f0b-tdwbv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:55:58.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9618" for this suite.
Apr 10 13:56:04.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:56:05.339: INFO: namespace replicaset-9618 deletion completed in 7.021964535s
•SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:56:05.339: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1794
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 10 13:56:05.886: INFO: Number of nodes with available pods: 0
Apr 10 13:56:05.886: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:56:06.960: INFO: Number of nodes with available pods: 0
Apr 10 13:56:06.960: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:56:07.962: INFO: Number of nodes with available pods: 0
Apr 10 13:56:07.962: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:56:08.964: INFO: Number of nodes with available pods: 1
Apr 10 13:56:08.964: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 13:56:09.960: INFO: Number of nodes with available pods: 1
Apr 10 13:56:09.960: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 13:56:10.963: INFO: Number of nodes with available pods: 2
Apr 10 13:56:10.963: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Apr 10 13:56:11.107: INFO: Number of nodes with available pods: 1
Apr 10 13:56:11.107: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 13:56:12.181: INFO: Number of nodes with available pods: 1
Apr 10 13:56:12.181: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 13:56:13.183: INFO: Number of nodes with available pods: 1
Apr 10 13:56:13.183: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 13:56:14.181: INFO: Number of nodes with available pods: 1
Apr 10 13:56:14.181: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 13:56:15.185: INFO: Number of nodes with available pods: 1
Apr 10 13:56:15.185: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 13:56:16.180: INFO: Number of nodes with available pods: 1
Apr 10 13:56:16.180: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 13:56:17.182: INFO: Number of nodes with available pods: 1
Apr 10 13:56:17.182: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 13:56:18.180: INFO: Number of nodes with available pods: 1
Apr 10 13:56:18.180: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 13:56:19.190: INFO: Number of nodes with available pods: 1
Apr 10 13:56:19.190: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 13:56:20.196: INFO: Number of nodes with available pods: 1
Apr 10 13:56:20.196: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 13:56:21.183: INFO: Number of nodes with available pods: 1
Apr 10 13:56:21.184: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 13:56:22.181: INFO: Number of nodes with available pods: 1
Apr 10 13:56:22.181: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 13:56:23.180: INFO: Number of nodes with available pods: 1
Apr 10 13:56:23.180: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 13:56:24.198: INFO: Number of nodes with available pods: 1
Apr 10 13:56:24.198: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 13:56:25.181: INFO: Number of nodes with available pods: 1
Apr 10 13:56:25.181: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 13:56:26.180: INFO: Number of nodes with available pods: 2
Apr 10 13:56:26.180: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1794, will wait for the garbage collector to delete the pods
Apr 10 13:56:26.308: INFO: Deleting DaemonSet.extensions daemon-set took: 26.913566ms
Apr 10 13:56:26.709: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.326397ms
Apr 10 13:56:42.634: INFO: Number of nodes with available pods: 0
Apr 10 13:56:42.634: INFO: Number of running nodes: 0, number of available pods: 0
Apr 10 13:56:42.659: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1794/daemonsets","resourceVersion":"16816"},"items":null}

Apr 10 13:56:42.683: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1794/pods","resourceVersion":"16816"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:56:42.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1794" for this suite.
Apr 10 13:56:48.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:56:49.869: INFO: namespace daemonsets-1794 deletion completed in 7.062063026s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:56:49.869: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1054
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-7cff4ae1-5b98-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume secrets
Apr 10 13:56:50.230: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7d031c3f-5b98-11e9-8d1d-a6f828030f0b" in namespace "projected-1054" to be "success or failure"
Apr 10 13:56:50.257: INFO: Pod "pod-projected-secrets-7d031c3f-5b98-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 26.656302ms
Apr 10 13:56:52.283: INFO: Pod "pod-projected-secrets-7d031c3f-5b98-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052639698s
Apr 10 13:56:54.308: INFO: Pod "pod-projected-secrets-7d031c3f-5b98-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.078154617s
STEP: Saw pod success
Apr 10 13:56:54.309: INFO: Pod "pod-projected-secrets-7d031c3f-5b98-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:56:54.333: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-projected-secrets-7d031c3f-5b98-11e9-8d1d-a6f828030f0b container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 10 13:56:54.407: INFO: Waiting for pod pod-projected-secrets-7d031c3f-5b98-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:56:54.443: INFO: Pod pod-projected-secrets-7d031c3f-5b98-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:56:54.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1054" for this suite.
Apr 10 13:57:00.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:57:01.499: INFO: namespace projected-1054 deletion completed in 7.007942127s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:57:01.499: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7652
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Apr 10 13:57:01.852: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7652'
Apr 10 13:57:02.220: INFO: stderr: ""
Apr 10 13:57:02.220: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 10 13:57:03.247: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 13:57:03.247: INFO: Found 0 / 1
Apr 10 13:57:04.246: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 13:57:04.246: INFO: Found 0 / 1
Apr 10 13:57:05.246: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 13:57:05.246: INFO: Found 1 / 1
Apr 10 13:57:05.246: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Apr 10 13:57:05.271: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 13:57:05.271: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 10 13:57:05.272: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config patch pod redis-master-czbxt --namespace=kubectl-7652 -p {"metadata":{"annotations":{"x":"y"}}}'
Apr 10 13:57:05.521: INFO: stderr: ""
Apr 10 13:57:05.521: INFO: stdout: "pod/redis-master-czbxt patched\n"
STEP: checking annotations
Apr 10 13:57:05.547: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 13:57:05.547: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:57:05.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7652" for this suite.
Apr 10 13:57:27.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:57:28.744: INFO: namespace kubectl-7652 deletion completed in 23.149556692s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:57:28.745: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6661
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Apr 10 13:57:29.253: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-6661,SelfLink:/api/v1/namespaces/watch-6661/configmaps/e2e-watch-test-resource-version,UID:942ff505-5b98-11e9-99e4-7640922b69f1,ResourceVersion:16977,Generation:0,CreationTimestamp:2019-04-10 13:57:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 10 13:57:29.253: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-6661,SelfLink:/api/v1/namespaces/watch-6661/configmaps/e2e-watch-test-resource-version,UID:942ff505-5b98-11e9-99e4-7640922b69f1,ResourceVersion:16978,Generation:0,CreationTimestamp:2019-04-10 13:57:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:57:29.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6661" for this suite.
Apr 10 13:57:35.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:57:36.306: INFO: namespace watch-6661 deletion completed in 7.026217312s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:57:36.306: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4701
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-cv54
STEP: Creating a pod to test atomic-volume-subpath
Apr 10 13:57:36.744: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-cv54" in namespace "subpath-4701" to be "success or failure"
Apr 10 13:57:36.772: INFO: Pod "pod-subpath-test-downwardapi-cv54": Phase="Pending", Reason="", readiness=false. Elapsed: 27.767534ms
Apr 10 13:57:38.803: INFO: Pod "pod-subpath-test-downwardapi-cv54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05906787s
Apr 10 13:57:40.829: INFO: Pod "pod-subpath-test-downwardapi-cv54": Phase="Running", Reason="", readiness=true. Elapsed: 4.085090885s
Apr 10 13:57:42.855: INFO: Pod "pod-subpath-test-downwardapi-cv54": Phase="Running", Reason="", readiness=true. Elapsed: 6.110981041s
Apr 10 13:57:44.881: INFO: Pod "pod-subpath-test-downwardapi-cv54": Phase="Running", Reason="", readiness=true. Elapsed: 8.136986598s
Apr 10 13:57:46.994: INFO: Pod "pod-subpath-test-downwardapi-cv54": Phase="Running", Reason="", readiness=true. Elapsed: 10.24997804s
Apr 10 13:57:49.020: INFO: Pod "pod-subpath-test-downwardapi-cv54": Phase="Running", Reason="", readiness=true. Elapsed: 12.27613151s
Apr 10 13:57:51.046: INFO: Pod "pod-subpath-test-downwardapi-cv54": Phase="Running", Reason="", readiness=true. Elapsed: 14.301831303s
Apr 10 13:57:53.071: INFO: Pod "pod-subpath-test-downwardapi-cv54": Phase="Running", Reason="", readiness=true. Elapsed: 16.327473568s
Apr 10 13:57:55.097: INFO: Pod "pod-subpath-test-downwardapi-cv54": Phase="Running", Reason="", readiness=true. Elapsed: 18.353028497s
Apr 10 13:57:57.123: INFO: Pod "pod-subpath-test-downwardapi-cv54": Phase="Running", Reason="", readiness=true. Elapsed: 20.378983413s
Apr 10 13:57:59.149: INFO: Pod "pod-subpath-test-downwardapi-cv54": Phase="Running", Reason="", readiness=true. Elapsed: 22.404978251s
Apr 10 13:58:01.175: INFO: Pod "pod-subpath-test-downwardapi-cv54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.430995161s
STEP: Saw pod success
Apr 10 13:58:01.175: INFO: Pod "pod-subpath-test-downwardapi-cv54" satisfied condition "success or failure"
Apr 10 13:58:01.200: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-subpath-test-downwardapi-cv54 container test-container-subpath-downwardapi-cv54: <nil>
STEP: delete the pod
Apr 10 13:58:01.404: INFO: Waiting for pod pod-subpath-test-downwardapi-cv54 to disappear
Apr 10 13:58:01.428: INFO: Pod pod-subpath-test-downwardapi-cv54 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-cv54
Apr 10 13:58:01.428: INFO: Deleting pod "pod-subpath-test-downwardapi-cv54" in namespace "subpath-4701"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:58:01.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4701" for this suite.
Apr 10 13:58:07.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:58:08.503: INFO: namespace subpath-4701 deletion completed in 7.002423118s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:58:08.504: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8930
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 10 13:58:08.994: INFO: Number of nodes with available pods: 0
Apr 10 13:58:08.994: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:58:10.068: INFO: Number of nodes with available pods: 0
Apr 10 13:58:10.068: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:58:11.068: INFO: Number of nodes with available pods: 0
Apr 10 13:58:11.068: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:58:12.087: INFO: Number of nodes with available pods: 2
Apr 10 13:58:12.087: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Apr 10 13:58:12.230: INFO: Number of nodes with available pods: 1
Apr 10 13:58:12.230: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:58:13.305: INFO: Number of nodes with available pods: 1
Apr 10 13:58:13.305: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:58:14.304: INFO: Number of nodes with available pods: 1
Apr 10 13:58:14.304: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:58:15.304: INFO: Number of nodes with available pods: 1
Apr 10 13:58:15.304: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 13:58:16.304: INFO: Number of nodes with available pods: 2
Apr 10 13:58:16.305: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8930, will wait for the garbage collector to delete the pods
Apr 10 13:58:16.456: INFO: Deleting DaemonSet.extensions daemon-set took: 26.782607ms
Apr 10 13:58:16.857: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.555326ms
Apr 10 13:58:29.282: INFO: Number of nodes with available pods: 0
Apr 10 13:58:29.282: INFO: Number of running nodes: 0, number of available pods: 0
Apr 10 13:58:29.307: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8930/daemonsets","resourceVersion":"17180"},"items":null}

Apr 10 13:58:29.331: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8930/pods","resourceVersion":"17180"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:58:29.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8930" for this suite.
Apr 10 13:58:35.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:58:36.467: INFO: namespace daemonsets-8930 deletion completed in 6.998644882s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:58:36.467: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6012
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 13:58:36.763: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6012'
Apr 10 13:58:37.002: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 10 13:58:37.002: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Apr 10 13:58:37.051: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-th989]
Apr 10 13:58:37.051: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-th989" in namespace "kubectl-6012" to be "running and ready"
Apr 10 13:58:37.076: INFO: Pod "e2e-test-nginx-rc-th989": Phase="Pending", Reason="", readiness=false. Elapsed: 25.254364ms
Apr 10 13:58:39.102: INFO: Pod "e2e-test-nginx-rc-th989": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051070226s
Apr 10 13:58:41.131: INFO: Pod "e2e-test-nginx-rc-th989": Phase="Running", Reason="", readiness=true. Elapsed: 4.079879353s
Apr 10 13:58:41.131: INFO: Pod "e2e-test-nginx-rc-th989" satisfied condition "running and ready"
Apr 10 13:58:41.131: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-th989]
Apr 10 13:58:41.131: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs rc/e2e-test-nginx-rc --namespace=kubectl-6012'
Apr 10 13:58:41.415: INFO: stderr: ""
Apr 10 13:58:41.415: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Apr 10 13:58:41.415: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-6012'
Apr 10 13:58:41.620: INFO: stderr: ""
Apr 10 13:58:41.620: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:58:41.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6012" for this suite.
Apr 10 13:59:03.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:59:04.657: INFO: namespace kubectl-6012 deletion completed in 22.98810973s
•SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:59:04.657: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9204
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 10 13:59:05.073: INFO: Waiting up to 5m0s for pod "downward-api-cd627ba0-5b98-11e9-8d1d-a6f828030f0b" in namespace "downward-api-9204" to be "success or failure"
Apr 10 13:59:05.098: INFO: Pod "downward-api-cd627ba0-5b98-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.127274ms
Apr 10 13:59:07.124: INFO: Pod "downward-api-cd627ba0-5b98-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05076166s
Apr 10 13:59:09.149: INFO: Pod "downward-api-cd627ba0-5b98-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076415378s
STEP: Saw pod success
Apr 10 13:59:09.149: INFO: Pod "downward-api-cd627ba0-5b98-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:59:09.177: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downward-api-cd627ba0-5b98-11e9-8d1d-a6f828030f0b container dapi-container: <nil>
STEP: delete the pod
Apr 10 13:59:09.282: INFO: Waiting for pod downward-api-cd627ba0-5b98-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:59:09.307: INFO: Pod downward-api-cd627ba0-5b98-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:59:09.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9204" for this suite.
Apr 10 13:59:15.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:59:16.355: INFO: namespace downward-api-9204 deletion completed in 6.991499118s
•SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:59:16.355: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2769
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-d44fbe0d-5b98-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume secrets
Apr 10 13:59:16.719: INFO: Waiting up to 5m0s for pod "pod-secrets-d453959a-5b98-11e9-8d1d-a6f828030f0b" in namespace "secrets-2769" to be "success or failure"
Apr 10 13:59:16.744: INFO: Pod "pod-secrets-d453959a-5b98-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.119791ms
Apr 10 13:59:18.771: INFO: Pod "pod-secrets-d453959a-5b98-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051408398s
Apr 10 13:59:20.797: INFO: Pod "pod-secrets-d453959a-5b98-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077205135s
STEP: Saw pod success
Apr 10 13:59:20.797: INFO: Pod "pod-secrets-d453959a-5b98-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 13:59:20.822: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-secrets-d453959a-5b98-11e9-8d1d-a6f828030f0b container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 13:59:20.900: INFO: Waiting for pod pod-secrets-d453959a-5b98-11e9-8d1d-a6f828030f0b to disappear
Apr 10 13:59:20.925: INFO: Pod pod-secrets-d453959a-5b98-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 13:59:20.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2769" for this suite.
Apr 10 13:59:27.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 13:59:27.986: INFO: namespace secrets-2769 deletion completed in 7.013022906s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 13:59:27.986: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-1771
Apr 10 13:59:32.351: INFO: Started pod liveness-http in namespace container-probe-1771
STEP: checking the pod's current state and verifying that restartCount is present
Apr 10 13:59:32.376: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:03:33.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1771" for this suite.
Apr 10 14:03:39.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:03:40.794: INFO: namespace container-probe-1771 deletion completed in 7.181050382s
•S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:03:40.794: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 14:03:41.192: INFO: Waiting up to 5m0s for pod "downwardapi-volume-71f6d923-5b99-11e9-8d1d-a6f828030f0b" in namespace "projected-5373" to be "success or failure"
Apr 10 14:03:41.217: INFO: Pod "downwardapi-volume-71f6d923-5b99-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.340101ms
Apr 10 14:03:43.243: INFO: Pod "downwardapi-volume-71f6d923-5b99-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050885234s
Apr 10 14:03:45.269: INFO: Pod "downwardapi-volume-71f6d923-5b99-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077092799s
STEP: Saw pod success
Apr 10 14:03:45.269: INFO: Pod "downwardapi-volume-71f6d923-5b99-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:03:45.294: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downwardapi-volume-71f6d923-5b99-11e9-8d1d-a6f828030f0b container client-container: <nil>
STEP: delete the pod
Apr 10 14:03:45.379: INFO: Waiting for pod downwardapi-volume-71f6d923-5b99-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:03:45.404: INFO: Pod downwardapi-volume-71f6d923-5b99-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:03:45.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5373" for this suite.
Apr 10 14:03:51.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:03:52.466: INFO: namespace projected-5373 deletion completed in 7.01428436s
•SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:03:52.466: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1929
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 14:03:52.760: INFO: Creating deployment "nginx-deployment"
Apr 10 14:03:52.786: INFO: Waiting for observed generation 1
Apr 10 14:03:54.843: INFO: Waiting for all required pods to come up
Apr 10 14:03:54.891: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Apr 10 14:04:02.972: INFO: Waiting for deployment "nginx-deployment" to complete
Apr 10 14:04:03.022: INFO: Updating deployment "nginx-deployment" with a non-existent image
Apr 10 14:04:03.073: INFO: Updating deployment nginx-deployment
Apr 10 14:04:03.073: INFO: Waiting for observed generation 2
Apr 10 14:04:05.151: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr 10 14:04:05.176: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr 10 14:04:05.202: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Apr 10 14:04:05.278: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr 10 14:04:05.278: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr 10 14:04:05.303: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Apr 10 14:04:05.357: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Apr 10 14:04:05.357: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Apr 10 14:04:05.409: INFO: Updating deployment nginx-deployment
Apr 10 14:04:05.409: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Apr 10 14:04:05.483: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr 10 14:04:05.538: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 10 14:04:05.591: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-1929,SelfLink:/apis/apps/v1/namespaces/deployment-1929/deployments/nginx-deployment,UID:78e387e7-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18178,Generation:3,CreationTimestamp:2019-04-10 14:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-04-10 14:04:05 +0000 UTC 2019-04-10 14:04:05 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-04-10 14:04:05 +0000 UTC 2019-04-10 14:03:52 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Apr 10 14:04:05.616: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-1929,SelfLink:/apis/apps/v1/namespaces/deployment-1929/replicasets/nginx-deployment-5f9595f595,UID:7f05c8e6-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18176,Generation:3,CreationTimestamp:2019-04-10 14:04:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 78e387e7-5b99-11e9-99e4-7640922b69f1 0xc00291cb57 0xc00291cb58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 14:04:05.616: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Apr 10 14:04:05.616: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-1929,SelfLink:/apis/apps/v1/namespaces/deployment-1929/replicasets/nginx-deployment-6f478d8d8,UID:78e42f63-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18174,Generation:3,CreationTimestamp:2019-04-10 14:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 78e387e7-5b99-11e9-99e4-7640922b69f1 0xc00291cc27 0xc00291cc28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Apr 10 14:04:05.666: INFO: Pod "nginx-deployment-5f9595f595-24b7v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-24b7v,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-5f9595f595-24b7v,UID:7f0706fb-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18105,Generation:0,CreationTimestamp:2019-04-10 14:04:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 7f05c8e6-5b99-11e9-99e4-7640922b69f1 0xc000f3fb17 0xc000f3fb18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f3fb90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f3fbe0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-04-10 14:04:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.666: INFO: Pod "nginx-deployment-5f9595f595-5xtcp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-5xtcp,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-5f9595f595-5xtcp,UID:8073537b-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18173,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 7f05c8e6-5b99-11e9-99e4-7640922b69f1 0xc000f3fcc0 0xc000f3fcc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f3fd30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f3fd50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.666: INFO: Pod "nginx-deployment-5f9595f595-7t49t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-7t49t,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-5f9595f595-7t49t,UID:7f16bd50-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18118,Generation:0,CreationTimestamp:2019-04-10 14:04:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 7f05c8e6-5b99-11e9-99e4-7640922b69f1 0xc000f3fdd0 0xc000f3fdd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f3fe40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f3fe60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-04-10 14:04:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.666: INFO: Pod "nginx-deployment-5f9595f595-9p9b6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-9p9b6,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-5f9595f595-9p9b6,UID:7f070331-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18104,Generation:0,CreationTimestamp:2019-04-10 14:04:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 7f05c8e6-5b99-11e9-99e4-7640922b69f1 0xc000f3ff30 0xc000f3ff31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f3ffa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f3ffc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-04-10 14:04:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.667: INFO: Pod "nginx-deployment-5f9595f595-9rhp5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-9rhp5,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-5f9595f595-9rhp5,UID:806cbfae-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18137,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 7f05c8e6-5b99-11e9-99e4-7640922b69f1 0xc000f00090 0xc000f00091}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f00100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f00120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.667: INFO: Pod "nginx-deployment-5f9595f595-cp22m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-cp22m,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-5f9595f595-cp22m,UID:806d70b5-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18151,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 7f05c8e6-5b99-11e9-99e4-7640922b69f1 0xc000f001a0 0xc000f001a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f00210} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f00230}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.667: INFO: Pod "nginx-deployment-5f9595f595-d4bln" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-d4bln,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-5f9595f595-d4bln,UID:807080c9-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18165,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 7f05c8e6-5b99-11e9-99e4-7640922b69f1 0xc000f002b0 0xc000f002b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f00320} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f00340}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.667: INFO: Pod "nginx-deployment-5f9595f595-df6jk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-df6jk,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-5f9595f595-df6jk,UID:7f0679a8-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18101,Generation:0,CreationTimestamp:2019-04-10 14:04:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 7f05c8e6-5b99-11e9-99e4-7640922b69f1 0xc000f003d0 0xc000f003d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f00440} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f00460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-04-10 14:04:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.667: INFO: Pod "nginx-deployment-5f9595f595-gm777" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-gm777,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-5f9595f595-gm777,UID:8070a7b0-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18169,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 7f05c8e6-5b99-11e9-99e4-7640922b69f1 0xc000f00530 0xc000f00531}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f005a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f005c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.667: INFO: Pod "nginx-deployment-5f9595f595-lksjf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-lksjf,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-5f9595f595-lksjf,UID:7f176157-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18117,Generation:0,CreationTimestamp:2019-04-10 14:04:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 7f05c8e6-5b99-11e9-99e4-7640922b69f1 0xc000f00640 0xc000f00641}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f006b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f006d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:03 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-04-10 14:04:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.667: INFO: Pod "nginx-deployment-5f9595f595-qcknw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-qcknw,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-5f9595f595-qcknw,UID:8070a306-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18170,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 7f05c8e6-5b99-11e9-99e4-7640922b69f1 0xc000f007a0 0xc000f007a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f00810} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f00830}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.667: INFO: Pod "nginx-deployment-5f9595f595-rk42f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-rk42f,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-5f9595f595-rk42f,UID:806d7147-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18148,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 7f05c8e6-5b99-11e9-99e4-7640922b69f1 0xc000f008b0 0xc000f008b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f00930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f00950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.667: INFO: Pod "nginx-deployment-5f9595f595-sf5ff" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-sf5ff,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-5f9595f595-sf5ff,UID:80707b1e-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18163,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 7f05c8e6-5b99-11e9-99e4-7640922b69f1 0xc000f009d0 0xc000f009d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f00a40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f00a60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.667: INFO: Pod "nginx-deployment-6f478d8d8-656vb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-656vb,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-656vb,UID:78f45fe2-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18083,Generation:0,CreationTimestamp:2019-04-10 14:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.168/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc000f00af0 0xc000f00af1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f00b50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f00b70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:03:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:03:52 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.168,StartTime:2019-04-10 14:03:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 14:04:01 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:0ad702c1a9d38787ca35b4b3c22fb5a69dbb6ffb8a2ce002701b8aff115bc053 docker://5fb8412b3c996d43d2d08a987347f35d0e34aacd00ca6b669391fb7cf2ff23e7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.668: INFO: Pod "nginx-deployment-6f478d8d8-7g9gr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7g9gr,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-7g9gr,UID:80708b3b-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18167,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc000f00c40 0xc000f00c41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f00cb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f00cd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.668: INFO: Pod "nginx-deployment-6f478d8d8-8mpnm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-8mpnm,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-8mpnm,UID:806d6a33-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18147,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc000f00d50 0xc000f00d51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f00db0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f00dd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.668: INFO: Pod "nginx-deployment-6f478d8d8-bq8r9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-bq8r9,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-bq8r9,UID:806d6aa0-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18149,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc000f00e50 0xc000f00e51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f00eb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f00ed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.668: INFO: Pod "nginx-deployment-6f478d8d8-cnfb6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-cnfb6,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-cnfb6,UID:806cd4bc-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18138,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc000f00f50 0xc000f00f51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f00fb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f00fd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.668: INFO: Pod "nginx-deployment-6f478d8d8-f5x6c" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-f5x6c,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-f5x6c,UID:78e8d7a0-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18077,Generation:0,CreationTimestamp:2019-04-10 14:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.167/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc000f01060 0xc000f01061}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f010c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f010e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:03:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:03:52 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.167,StartTime:2019-04-10 14:03:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 14:04:01 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:0ad702c1a9d38787ca35b4b3c22fb5a69dbb6ffb8a2ce002701b8aff115bc053 docker://6d3b8a5cfb013bdf90e2ac59a7394dbbd0ef58174bb17a386e20b70ba31703b7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.668: INFO: Pod "nginx-deployment-6f478d8d8-ftbrf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ftbrf,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-ftbrf,UID:78e834fe-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18059,Generation:0,CreationTimestamp:2019-04-10 14:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.165/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc000f011d0 0xc000f011d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f01230} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f01250}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:03:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:03:52 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.165,StartTime:2019-04-10 14:03:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 14:04:00 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:0ad702c1a9d38787ca35b4b3c22fb5a69dbb6ffb8a2ce002701b8aff115bc053 docker://a0b497764b480c3fd3e585d4b4b785940b2a5e1620c2986b99f9708f46a7ac5b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.668: INFO: Pod "nginx-deployment-6f478d8d8-jbtsr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-jbtsr,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-jbtsr,UID:807091d3-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18168,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc000f01320 0xc000f01321}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f01390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f013b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.668: INFO: Pod "nginx-deployment-6f478d8d8-ks77k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ks77k,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-ks77k,UID:80708354-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18166,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc000f01430 0xc000f01431}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f01490} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f014b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.668: INFO: Pod "nginx-deployment-6f478d8d8-lhmxg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-lhmxg,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-lhmxg,UID:806d73bb-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18150,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc000f01530 0xc000f01531}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f01590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f015b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.668: INFO: Pod "nginx-deployment-6f478d8d8-mk9fn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-mk9fn,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-mk9fn,UID:78e8cd1a-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18055,Generation:0,CreationTimestamp:2019-04-10 14:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.52/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc000f01650 0xc000f01651}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f016d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f016f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:03:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:03:52 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.52,StartTime:2019-04-10 14:03:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 14:04:00 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:0ad702c1a9d38787ca35b4b3c22fb5a69dbb6ffb8a2ce002701b8aff115bc053 docker://b45734d901f26b4571f9657937b137d5ed10dac6c6e293ee616a8dfc75df02d5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.669: INFO: Pod "nginx-deployment-6f478d8d8-msq5m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-msq5m,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-msq5m,UID:80707f57-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18162,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc000f017d0 0xc000f017d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f01830} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f01850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.669: INFO: Pod "nginx-deployment-6f478d8d8-pbzkp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-pbzkp,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-pbzkp,UID:78f4640e-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18066,Generation:0,CreationTimestamp:2019-04-10 14:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.53/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc000f018f0 0xc000f018f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f01950} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f01970}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:03:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:03:52 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.53,StartTime:2019-04-10 14:03:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 14:04:00 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:0ad702c1a9d38787ca35b4b3c22fb5a69dbb6ffb8a2ce002701b8aff115bc053 docker://a118124e6ead22dade627f1a51bc065b9e09801a4b8d38ccbe9f4d40d6fff708}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.669: INFO: Pod "nginx-deployment-6f478d8d8-qbsrh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-qbsrh,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-qbsrh,UID:806cd35a-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18139,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc000f01a40 0xc000f01a41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f01aa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f01ac0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.669: INFO: Pod "nginx-deployment-6f478d8d8-qfq8d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-qfq8d,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-qfq8d,UID:806d780d-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18157,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc000f01b40 0xc000f01b41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f01ba0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f01bc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.669: INFO: Pod "nginx-deployment-6f478d8d8-s2lj4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-s2lj4,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-s2lj4,UID:78e735eb-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18062,Generation:0,CreationTimestamp:2019-04-10 14:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.164/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc000f01c50 0xc000f01c51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f01cb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f01cd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:03:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:03:52 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.164,StartTime:2019-04-10 14:03:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 14:03:59 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:0ad702c1a9d38787ca35b4b3c22fb5a69dbb6ffb8a2ce002701b8aff115bc053 docker://45fca7c970e0800f98e4f2c4cdbfc33a1b6e53de44eed87ba6f87a196e656f03}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.669: INFO: Pod "nginx-deployment-6f478d8d8-tbtqk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-tbtqk,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-tbtqk,UID:78e818e8-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18052,Generation:0,CreationTimestamp:2019-04-10 14:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.51/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc000f01db0 0xc000f01db1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f01e10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f01e30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:03:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:03:52 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.51,StartTime:2019-04-10 14:03:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 14:03:58 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:0ad702c1a9d38787ca35b4b3c22fb5a69dbb6ffb8a2ce002701b8aff115bc053 docker://7e3ee189f6adfcc2ebb59f25bc42e954867dd3fddd8bc806075ba5555a3bef00}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.669: INFO: Pod "nginx-deployment-6f478d8d8-vhmqz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-vhmqz,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-vhmqz,UID:78f46007-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18072,Generation:0,CreationTimestamp:2019-04-10 14:03:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.55/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc000f01f10 0xc000f01f11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f01f70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f01f90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:03:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:03:52 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.55,StartTime:2019-04-10 14:03:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 14:04:00 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:0ad702c1a9d38787ca35b4b3c22fb5a69dbb6ffb8a2ce002701b8aff115bc053 docker://46d504e3e4bf136a9fa564fb2457b7584dcdbea7e15a94d21ef770841ad808aa}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.670: INFO: Pod "nginx-deployment-6f478d8d8-wdjz4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-wdjz4,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-wdjz4,UID:8070822f-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18164,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc002612060 0xc002612061}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002612140} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002612180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 14:04:05.670: INFO: Pod "nginx-deployment-6f478d8d8-xb5t6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-xb5t6,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1929,SelfLink:/api/v1/namespaces/deployment-1929/pods/nginx-deployment-6f478d8d8-xb5t6,UID:806c2dc2-5b99-11e9-99e4-7640922b69f1,ResourceVersion:18134,Generation:0,CreationTimestamp:2019-04-10 14:04:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 78e42f63-5b99-11e9-99e4-7640922b69f1 0xc002612330 0xc002612331}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h7qc2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h7qc2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h7qc2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002612390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026123b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:04:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:04:05.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1929" for this suite.
Apr 10 14:04:13.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:04:14.685: INFO: namespace deployment-1929 deletion completed in 8.989806081s
•SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:04:14.685: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2982
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-2982
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 10 14:04:14.964: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 10 14:04:51.585: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.0.60:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2982 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 14:04:51.585: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 14:04:52.113: INFO: Found all expected endpoints: [netserver-0]
Apr 10 14:04:52.138: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.172:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2982 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 14:04:52.138: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 14:04:52.648: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:04:52.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2982" for this suite.
Apr 10 14:05:14.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:05:15.719: INFO: namespace pod-network-test-2982 deletion completed in 23.022006471s
•SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:05:15.720: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3834
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-aa8c4f43-5b99-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume configMaps
Apr 10 14:05:16.150: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-aa90351b-5b99-11e9-8d1d-a6f828030f0b" in namespace "projected-3834" to be "success or failure"
Apr 10 14:05:16.175: INFO: Pod "pod-projected-configmaps-aa90351b-5b99-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.904489ms
Apr 10 14:05:18.201: INFO: Pod "pod-projected-configmaps-aa90351b-5b99-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051059644s
Apr 10 14:05:20.233: INFO: Pod "pod-projected-configmaps-aa90351b-5b99-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083111125s
Apr 10 14:05:22.259: INFO: Pod "pod-projected-configmaps-aa90351b-5b99-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.108940906s
STEP: Saw pod success
Apr 10 14:05:22.259: INFO: Pod "pod-projected-configmaps-aa90351b-5b99-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:05:22.284: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-projected-configmaps-aa90351b-5b99-11e9-8d1d-a6f828030f0b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 14:05:22.380: INFO: Waiting for pod pod-projected-configmaps-aa90351b-5b99-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:05:22.406: INFO: Pod pod-projected-configmaps-aa90351b-5b99-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:05:22.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3834" for this suite.
Apr 10 14:05:28.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:05:29.485: INFO: namespace projected-3834 deletion completed in 7.018470089s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:05:29.485: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9506
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr 10 14:05:34.536: INFO: Successfully updated pod "labelsupdateb2be4855-5b99-11e9-8d1d-a6f828030f0b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:05:36.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9506" for this suite.
Apr 10 14:05:58.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:05:59.659: INFO: namespace projected-9506 deletion completed in 23.01129395s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:05:59.659: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9765
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-9765/secret-test-c4b1bfcf-5b99-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume secrets
Apr 10 14:06:00.015: INFO: Waiting up to 5m0s for pod "pod-configmaps-c4b58f9b-5b99-11e9-8d1d-a6f828030f0b" in namespace "secrets-9765" to be "success or failure"
Apr 10 14:06:00.040: INFO: Pod "pod-configmaps-c4b58f9b-5b99-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.947138ms
Apr 10 14:06:02.067: INFO: Pod "pod-configmaps-c4b58f9b-5b99-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051880073s
Apr 10 14:06:04.092: INFO: Pod "pod-configmaps-c4b58f9b-5b99-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077515682s
STEP: Saw pod success
Apr 10 14:06:04.092: INFO: Pod "pod-configmaps-c4b58f9b-5b99-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:06:04.118: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-configmaps-c4b58f9b-5b99-11e9-8d1d-a6f828030f0b container env-test: <nil>
STEP: delete the pod
Apr 10 14:06:04.193: INFO: Waiting for pod pod-configmaps-c4b58f9b-5b99-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:06:04.218: INFO: Pod pod-configmaps-c4b58f9b-5b99-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:06:04.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9765" for this suite.
Apr 10 14:06:10.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:06:11.276: INFO: namespace secrets-9765 deletion completed in 7.01056067s
•SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:06:11.276: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5799
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5799
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5799
STEP: Creating statefulset with conflicting port in namespace statefulset-5799
STEP: Waiting until pod test-pod will start running in namespace statefulset-5799
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5799
Apr 10 14:06:17.963: INFO: Observed stateful pod in namespace: statefulset-5799, name: ss-0, uid: cdb83fa4-5b99-11e9-99e4-7640922b69f1, status phase: Pending. Waiting for statefulset controller to delete.
Apr 10 14:06:19.178: INFO: Observed stateful pod in namespace: statefulset-5799, name: ss-0, uid: cdb83fa4-5b99-11e9-99e4-7640922b69f1, status phase: Failed. Waiting for statefulset controller to delete.
Apr 10 14:06:19.184: INFO: Observed stateful pod in namespace: statefulset-5799, name: ss-0, uid: cdb83fa4-5b99-11e9-99e4-7640922b69f1, status phase: Failed. Waiting for statefulset controller to delete.
Apr 10 14:06:19.202: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5799
STEP: Removing pod with conflicting port in namespace statefulset-5799
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5799 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 10 14:06:25.336: INFO: Deleting all statefulset in ns statefulset-5799
Apr 10 14:06:25.363: INFO: Scaling statefulset ss to 0
Apr 10 14:06:45.474: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 14:06:45.499: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:06:45.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5799" for this suite.
Apr 10 14:06:51.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:06:52.622: INFO: namespace statefulset-5799 deletion completed in 6.990599106s
•
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:06:52.622: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4046
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-e4495166-5b99-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume secrets
Apr 10 14:06:53.017: INFO: Waiting up to 5m0s for pod "pod-secrets-e44d1c37-5b99-11e9-8d1d-a6f828030f0b" in namespace "secrets-4046" to be "success or failure"
Apr 10 14:06:53.041: INFO: Pod "pod-secrets-e44d1c37-5b99-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.191394ms
Apr 10 14:06:55.067: INFO: Pod "pod-secrets-e44d1c37-5b99-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050084177s
Apr 10 14:06:57.093: INFO: Pod "pod-secrets-e44d1c37-5b99-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075805891s
STEP: Saw pod success
Apr 10 14:06:57.093: INFO: Pod "pod-secrets-e44d1c37-5b99-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:06:57.120: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-secrets-e44d1c37-5b99-11e9-8d1d-a6f828030f0b container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 14:06:57.195: INFO: Waiting for pod pod-secrets-e44d1c37-5b99-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:06:57.219: INFO: Pod pod-secrets-e44d1c37-5b99-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:06:57.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4046" for this suite.
Apr 10 14:07:03.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:07:04.285: INFO: namespace secrets-4046 deletion completed in 7.018055959s
•SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:07:04.286: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6802
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0410 14:07:05.372405    3207 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 10 14:07:05.372: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:07:05.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6802" for this suite.
Apr 10 14:07:11.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:07:12.405: INFO: namespace gc-6802 deletion completed in 7.007228891s
•SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:07:12.405: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7834
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-f015c501-5b99-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume secrets
Apr 10 14:07:12.813: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f019a762-5b99-11e9-8d1d-a6f828030f0b" in namespace "projected-7834" to be "success or failure"
Apr 10 14:07:12.844: INFO: Pod "pod-projected-secrets-f019a762-5b99-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 30.197473ms
Apr 10 14:07:14.873: INFO: Pod "pod-projected-secrets-f019a762-5b99-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059287308s
Apr 10 14:07:16.899: INFO: Pod "pod-projected-secrets-f019a762-5b99-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.085384622s
Apr 10 14:07:18.925: INFO: Pod "pod-projected-secrets-f019a762-5b99-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.111386977s
STEP: Saw pod success
Apr 10 14:07:18.925: INFO: Pod "pod-projected-secrets-f019a762-5b99-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:07:18.950: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-projected-secrets-f019a762-5b99-11e9-8d1d-a6f828030f0b container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 10 14:07:19.051: INFO: Waiting for pod pod-projected-secrets-f019a762-5b99-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:07:19.076: INFO: Pod pod-projected-secrets-f019a762-5b99-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:07:19.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7834" for this suite.
Apr 10 14:07:25.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:07:26.156: INFO: namespace projected-7834 deletion completed in 7.032729499s
•SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:07:26.156: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9598
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 10 14:07:34.723: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 10 14:07:34.749: INFO: Pod pod-with-poststart-http-hook still exists
Apr 10 14:07:36.749: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 10 14:07:36.775: INFO: Pod pod-with-poststart-http-hook still exists
Apr 10 14:07:38.749: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 10 14:07:38.774: INFO: Pod pod-with-poststart-http-hook still exists
Apr 10 14:07:40.749: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 10 14:07:40.775: INFO: Pod pod-with-poststart-http-hook still exists
Apr 10 14:07:42.749: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 10 14:07:42.775: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:07:42.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9598" for this suite.
Apr 10 14:08:04.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:08:05.826: INFO: namespace container-lifecycle-hook-9598 deletion completed in 23.003953386s
•S
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:08:05.827: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4497
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Apr 10 14:08:10.293: INFO: Pod pod-hostip-0feae1cf-5b9a-11e9-8d1d-a6f828030f0b has hostIP: 10.250.0.5
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:08:10.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4497" for this suite.
Apr 10 14:08:32.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:08:33.351: INFO: namespace pods-4497 deletion completed in 23.009870467s
•SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:08:33.351: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5983
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-r8fc
STEP: Creating a pod to test atomic-volume-subpath
Apr 10 14:08:33.754: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-r8fc" in namespace "subpath-5983" to be "success or failure"
Apr 10 14:08:33.779: INFO: Pod "pod-subpath-test-configmap-r8fc": Phase="Pending", Reason="", readiness=false. Elapsed: 25.051654ms
Apr 10 14:08:35.805: INFO: Pod "pod-subpath-test-configmap-r8fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050929087s
Apr 10 14:08:37.831: INFO: Pod "pod-subpath-test-configmap-r8fc": Phase="Running", Reason="", readiness=true. Elapsed: 4.077099338s
Apr 10 14:08:39.857: INFO: Pod "pod-subpath-test-configmap-r8fc": Phase="Running", Reason="", readiness=true. Elapsed: 6.102672209s
Apr 10 14:08:41.882: INFO: Pod "pod-subpath-test-configmap-r8fc": Phase="Running", Reason="", readiness=true. Elapsed: 8.128350492s
Apr 10 14:08:43.910: INFO: Pod "pod-subpath-test-configmap-r8fc": Phase="Running", Reason="", readiness=true. Elapsed: 10.15591738s
Apr 10 14:08:45.936: INFO: Pod "pod-subpath-test-configmap-r8fc": Phase="Running", Reason="", readiness=true. Elapsed: 12.181895827s
Apr 10 14:08:47.962: INFO: Pod "pod-subpath-test-configmap-r8fc": Phase="Running", Reason="", readiness=true. Elapsed: 14.207600461s
Apr 10 14:08:49.987: INFO: Pod "pod-subpath-test-configmap-r8fc": Phase="Running", Reason="", readiness=true. Elapsed: 16.233479921s
Apr 10 14:08:52.014: INFO: Pod "pod-subpath-test-configmap-r8fc": Phase="Running", Reason="", readiness=true. Elapsed: 18.25959445s
Apr 10 14:08:54.040: INFO: Pod "pod-subpath-test-configmap-r8fc": Phase="Running", Reason="", readiness=true. Elapsed: 20.285556724s
Apr 10 14:08:56.066: INFO: Pod "pod-subpath-test-configmap-r8fc": Phase="Running", Reason="", readiness=true. Elapsed: 22.311560322s
Apr 10 14:08:58.092: INFO: Pod "pod-subpath-test-configmap-r8fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.337634434s
STEP: Saw pod success
Apr 10 14:08:58.092: INFO: Pod "pod-subpath-test-configmap-r8fc" satisfied condition "success or failure"
Apr 10 14:08:58.119: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-subpath-test-configmap-r8fc container test-container-subpath-configmap-r8fc: <nil>
STEP: delete the pod
Apr 10 14:08:58.189: INFO: Waiting for pod pod-subpath-test-configmap-r8fc to disappear
Apr 10 14:08:58.216: INFO: Pod pod-subpath-test-configmap-r8fc no longer exists
STEP: Deleting pod pod-subpath-test-configmap-r8fc
Apr 10 14:08:58.216: INFO: Deleting pod "pod-subpath-test-configmap-r8fc" in namespace "subpath-5983"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:08:58.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5983" for this suite.
Apr 10 14:09:04.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:09:05.320: INFO: namespace subpath-5983 deletion completed in 7.015807176s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:09:05.321: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8760
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 14:09:05.694: INFO: Waiting up to 5m0s for pod "downwardapi-volume-33620678-5b9a-11e9-8d1d-a6f828030f0b" in namespace "projected-8760" to be "success or failure"
Apr 10 14:09:05.719: INFO: Pod "downwardapi-volume-33620678-5b9a-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.756432ms
Apr 10 14:09:07.745: INFO: Pod "downwardapi-volume-33620678-5b9a-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050571339s
Apr 10 14:09:09.770: INFO: Pod "downwardapi-volume-33620678-5b9a-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076414712s
STEP: Saw pod success
Apr 10 14:09:09.771: INFO: Pod "downwardapi-volume-33620678-5b9a-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:09:09.796: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downwardapi-volume-33620678-5b9a-11e9-8d1d-a6f828030f0b container client-container: <nil>
STEP: delete the pod
Apr 10 14:09:09.877: INFO: Waiting for pod downwardapi-volume-33620678-5b9a-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:09:09.902: INFO: Pod downwardapi-volume-33620678-5b9a-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:09:09.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8760" for this suite.
Apr 10 14:09:16.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:09:17.006: INFO: namespace projected-8760 deletion completed in 7.056232092s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:09:17.006: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3920
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr 10 14:09:17.364: INFO: PodSpec: initContainers in spec.initContainers
Apr 10 14:10:05.933: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-3a5b04c0-5b9a-11e9-8d1d-a6f828030f0b", GenerateName:"", Namespace:"init-container-3920", SelfLink:"/api/v1/namespaces/init-container-3920/pods/pod-init-3a5b04c0-5b9a-11e9-8d1d-a6f828030f0b", UID:"3a5e676a-5b9a-11e9-99e4-7640922b69f1", ResourceVersion:"19468", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63690502157, loc:(*time.Location)(0x89f10e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"364556157"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.184/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-zc84w", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001da2100), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-zc84w", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-zc84w", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-zc84w", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0032eadf8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002c73740), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0032eae70)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0032eae90)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0032eae98), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0032eae9c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690502157, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690502157, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690502157, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690502157, loc:(*time.Location)(0x89f10e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.5", PodIP:"100.96.1.184", StartTime:(*v1.Time)(0xc000fa1380), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c56bd0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c56c40)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://ddafe93ebe2c7390b64673c4ac82075fc1811ec8bbb5b21da8e244fc0e5f3396"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000fa13c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000fa13a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:10:05.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3920" for this suite.
Apr 10 14:10:28.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:10:29.021: INFO: namespace init-container-3920 deletion completed in 23.031072459s
•SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:10:29.021: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2169
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 10 14:10:37.598: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 14:10:37.623: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 14:10:39.624: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 14:10:39.650: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 14:10:41.624: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 14:10:41.649: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 14:10:43.624: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 14:10:43.649: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 14:10:45.624: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 14:10:45.649: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 14:10:47.624: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 14:10:47.649: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 14:10:49.623: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 14:10:49.650: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 14:10:51.624: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 14:10:51.649: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 14:10:53.624: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 14:10:53.649: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 14:10:55.623: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 14:10:55.649: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 14:10:57.624: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 14:10:57.649: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 14:10:59.623: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 14:10:59.649: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 14:11:01.624: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 14:11:01.663: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 14:11:03.624: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 14:11:03.649: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:11:03.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2169" for this suite.
Apr 10 14:11:25.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:11:26.762: INFO: namespace container-lifecycle-hook-2169 deletion completed in 23.030872988s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:11:26.763: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7041
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 10 14:11:27.223: INFO: Waiting up to 5m0s for pod "pod-87bd835a-5b9a-11e9-8d1d-a6f828030f0b" in namespace "emptydir-7041" to be "success or failure"
Apr 10 14:11:27.249: INFO: Pod "pod-87bd835a-5b9a-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.922801ms
Apr 10 14:11:29.274: INFO: Pod "pod-87bd835a-5b9a-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051669762s
Apr 10 14:11:31.300: INFO: Pod "pod-87bd835a-5b9a-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077367201s
STEP: Saw pod success
Apr 10 14:11:31.300: INFO: Pod "pod-87bd835a-5b9a-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:11:31.325: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-87bd835a-5b9a-11e9-8d1d-a6f828030f0b container test-container: <nil>
STEP: delete the pod
Apr 10 14:11:31.549: INFO: Waiting for pod pod-87bd835a-5b9a-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:11:31.573: INFO: Pod pod-87bd835a-5b9a-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:11:31.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7041" for this suite.
Apr 10 14:11:37.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:11:38.650: INFO: namespace emptydir-7041 deletion completed in 7.0286864s
•SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:11:38.650: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8572
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 14:11:38.985: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8ec056b3-5b9a-11e9-8d1d-a6f828030f0b" in namespace "downward-api-8572" to be "success or failure"
Apr 10 14:11:39.010: INFO: Pod "downwardapi-volume-8ec056b3-5b9a-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.918517ms
Apr 10 14:11:41.036: INFO: Pod "downwardapi-volume-8ec056b3-5b9a-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050551723s
Apr 10 14:11:43.062: INFO: Pod "downwardapi-volume-8ec056b3-5b9a-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076403402s
STEP: Saw pod success
Apr 10 14:11:43.062: INFO: Pod "downwardapi-volume-8ec056b3-5b9a-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:11:43.087: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downwardapi-volume-8ec056b3-5b9a-11e9-8d1d-a6f828030f0b container client-container: <nil>
STEP: delete the pod
Apr 10 14:11:43.162: INFO: Waiting for pod downwardapi-volume-8ec056b3-5b9a-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:11:43.197: INFO: Pod downwardapi-volume-8ec056b3-5b9a-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:11:43.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8572" for this suite.
Apr 10 14:11:49.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:11:50.280: INFO: namespace downward-api-8572 deletion completed in 7.034677849s
•SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:11:50.280: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2737
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-95b14cc0-5b9a-11e9-8d1d-a6f828030f0b
STEP: Creating configMap with name cm-test-opt-upd-95b14d0a-5b9a-11e9-8d1d-a6f828030f0b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-95b14cc0-5b9a-11e9-8d1d-a6f828030f0b
STEP: Updating configmap cm-test-opt-upd-95b14d0a-5b9a-11e9-8d1d-a6f828030f0b
STEP: Creating configMap with name cm-test-opt-create-95b14d1b-5b9a-11e9-8d1d-a6f828030f0b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:13:28.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2737" for this suite.
Apr 10 14:13:50.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:13:51.845: INFO: namespace projected-2737 deletion completed in 23.071886591s
•SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:13:51.846: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6948
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-6948
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6948 to expose endpoints map[]
Apr 10 14:13:52.250: INFO: successfully validated that service endpoint-test2 in namespace services-6948 exposes endpoints map[] (41.133974ms elapsed)
STEP: Creating pod pod1 in namespace services-6948
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6948 to expose endpoints map[pod1:[80]]
Apr 10 14:13:56.552: INFO: successfully validated that service endpoint-test2 in namespace services-6948 exposes endpoints map[pod1:[80]] (4.274356246s elapsed)
STEP: Creating pod pod2 in namespace services-6948
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6948 to expose endpoints map[pod1:[80] pod2:[80]]
Apr 10 14:14:00.960: INFO: successfully validated that service endpoint-test2 in namespace services-6948 exposes endpoints map[pod1:[80] pod2:[80]] (4.380549604s elapsed)
STEP: Deleting pod pod1 in namespace services-6948
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6948 to expose endpoints map[pod2:[80]]
Apr 10 14:14:01.036: INFO: successfully validated that service endpoint-test2 in namespace services-6948 exposes endpoints map[pod2:[80]] (49.223652ms elapsed)
STEP: Deleting pod pod2 in namespace services-6948
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6948 to expose endpoints map[]
Apr 10 14:14:01.095: INFO: successfully validated that service endpoint-test2 in namespace services-6948 exposes endpoints map[] (32.678527ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:14:01.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6948" for this suite.
Apr 10 14:14:25.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:14:26.191: INFO: namespace services-6948 deletion completed in 25.003949809s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
•SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:14:26.192: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-127
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Apr 10 14:14:26.479: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config cluster-info'
Apr 10 14:14:26.998: INFO: stderr: ""
Apr 10 14:14:26.998: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:14:26.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-127" for this suite.
Apr 10 14:14:33.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:14:34.070: INFO: namespace kubectl-127 deletion completed in 7.023328067s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:14:34.071: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8575
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:14:38.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8575" for this suite.
Apr 10 14:15:26.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:15:27.724: INFO: namespace kubelet-test-8575 deletion completed in 49.163277579s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:15:27.724: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6273
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6273
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Apr 10 14:15:28.155: INFO: Found 1 stateful pods, waiting for 3
Apr 10 14:15:38.182: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 14:15:38.182: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 14:15:38.182: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Apr 10 14:15:48.182: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 14:15:48.182: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 14:15:48.182: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 14:15:48.259: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6273 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 14:15:49.074: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 14:15:49.074: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 14:15:49.074: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Apr 10 14:15:49.187: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Apr 10 14:15:49.268: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6273 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 14:15:50.065: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 14:15:50.066: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 14:15:50.066: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 14:15:50.181: INFO: Waiting for StatefulSet statefulset-6273/ss2 to complete update
Apr 10 14:15:50.181: INFO: Waiting for Pod statefulset-6273/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 14:15:50.181: INFO: Waiting for Pod statefulset-6273/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 14:15:50.181: INFO: Waiting for Pod statefulset-6273/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 14:16:00.238: INFO: Waiting for StatefulSet statefulset-6273/ss2 to complete update
Apr 10 14:16:00.238: INFO: Waiting for Pod statefulset-6273/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 14:16:00.238: INFO: Waiting for Pod statefulset-6273/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 14:16:00.238: INFO: Waiting for Pod statefulset-6273/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 14:16:10.233: INFO: Waiting for StatefulSet statefulset-6273/ss2 to complete update
Apr 10 14:16:10.233: INFO: Waiting for Pod statefulset-6273/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 14:16:10.233: INFO: Waiting for Pod statefulset-6273/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 14:16:20.232: INFO: Waiting for StatefulSet statefulset-6273/ss2 to complete update
Apr 10 14:16:20.232: INFO: Waiting for Pod statefulset-6273/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 14:16:30.233: INFO: Waiting for StatefulSet statefulset-6273/ss2 to complete update
Apr 10 14:16:30.233: INFO: Waiting for Pod statefulset-6273/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Apr 10 14:16:40.233: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6273 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 14:16:41.053: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 14:16:41.053: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 14:16:41.053: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 14:16:51.220: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Apr 10 14:16:51.298: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6273 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 14:16:52.012: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 14:16:52.012: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 14:16:52.012: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 14:17:02.165: INFO: Waiting for StatefulSet statefulset-6273/ss2 to complete update
Apr 10 14:17:02.165: INFO: Waiting for Pod statefulset-6273/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 10 14:17:02.165: INFO: Waiting for Pod statefulset-6273/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 10 14:17:02.165: INFO: Waiting for Pod statefulset-6273/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 10 14:17:12.227: INFO: Waiting for StatefulSet statefulset-6273/ss2 to complete update
Apr 10 14:17:12.227: INFO: Waiting for Pod statefulset-6273/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 10 14:17:22.222: INFO: Waiting for StatefulSet statefulset-6273/ss2 to complete update
Apr 10 14:17:22.222: INFO: Waiting for Pod statefulset-6273/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 10 14:17:32.216: INFO: Deleting all statefulset in ns statefulset-6273
Apr 10 14:17:32.241: INFO: Scaling statefulset ss2 to 0
Apr 10 14:18:02.345: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 14:18:02.371: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:18:02.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6273" for this suite.
Apr 10 14:18:08.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:18:09.498: INFO: namespace statefulset-6273 deletion completed in 7.000856759s
•S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:18:09.499: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7675
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-77bfe31e-5b9b-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume configMaps
Apr 10 14:18:09.917: INFO: Waiting up to 5m0s for pod "pod-configmaps-77c3cdbc-5b9b-11e9-8d1d-a6f828030f0b" in namespace "configmap-7675" to be "success or failure"
Apr 10 14:18:09.942: INFO: Pod "pod-configmaps-77c3cdbc-5b9b-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.978004ms
Apr 10 14:18:11.969: INFO: Pod "pod-configmaps-77c3cdbc-5b9b-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051207343s
Apr 10 14:18:13.995: INFO: Pod "pod-configmaps-77c3cdbc-5b9b-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077160875s
Apr 10 14:18:16.020: INFO: Pod "pod-configmaps-77c3cdbc-5b9b-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.103073869s
STEP: Saw pod success
Apr 10 14:18:16.021: INFO: Pod "pod-configmaps-77c3cdbc-5b9b-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:18:16.046: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-configmaps-77c3cdbc-5b9b-11e9-8d1d-a6f828030f0b container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 14:18:16.128: INFO: Waiting for pod pod-configmaps-77c3cdbc-5b9b-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:18:16.153: INFO: Pod pod-configmaps-77c3cdbc-5b9b-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:18:16.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7675" for this suite.
Apr 10 14:18:22.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:18:23.235: INFO: namespace configmap-7675 deletion completed in 7.033881298s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:18:23.235: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3175
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 14:18:23.688: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Apr 10 14:18:23.806: INFO: Number of nodes with available pods: 0
Apr 10 14:18:23.807: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 14:18:24.880: INFO: Number of nodes with available pods: 0
Apr 10 14:18:24.881: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 14:18:25.880: INFO: Number of nodes with available pods: 0
Apr 10 14:18:25.880: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 14:18:26.881: INFO: Number of nodes with available pods: 1
Apr 10 14:18:26.881: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 14:18:27.880: INFO: Number of nodes with available pods: 2
Apr 10 14:18:27.880: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Apr 10 14:18:28.070: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:28.070: INFO: Wrong image for pod: daemon-set-nhdj4. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:29.121: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:29.121: INFO: Wrong image for pod: daemon-set-nhdj4. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:30.122: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:30.122: INFO: Wrong image for pod: daemon-set-nhdj4. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:31.122: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:31.122: INFO: Wrong image for pod: daemon-set-nhdj4. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:32.122: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:32.122: INFO: Wrong image for pod: daemon-set-nhdj4. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:32.122: INFO: Pod daemon-set-nhdj4 is not available
Apr 10 14:18:33.121: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:33.121: INFO: Wrong image for pod: daemon-set-nhdj4. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:33.121: INFO: Pod daemon-set-nhdj4 is not available
Apr 10 14:18:34.122: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:34.122: INFO: Wrong image for pod: daemon-set-nhdj4. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:34.122: INFO: Pod daemon-set-nhdj4 is not available
Apr 10 14:18:35.121: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:35.121: INFO: Wrong image for pod: daemon-set-nhdj4. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:35.121: INFO: Pod daemon-set-nhdj4 is not available
Apr 10 14:18:36.125: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:36.125: INFO: Wrong image for pod: daemon-set-nhdj4. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:36.125: INFO: Pod daemon-set-nhdj4 is not available
Apr 10 14:18:37.121: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:37.121: INFO: Wrong image for pod: daemon-set-nhdj4. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:37.121: INFO: Pod daemon-set-nhdj4 is not available
Apr 10 14:18:38.122: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:38.122: INFO: Wrong image for pod: daemon-set-nhdj4. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:38.122: INFO: Pod daemon-set-nhdj4 is not available
Apr 10 14:18:39.121: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:39.121: INFO: Wrong image for pod: daemon-set-nhdj4. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:39.121: INFO: Pod daemon-set-nhdj4 is not available
Apr 10 14:18:40.122: INFO: Pod daemon-set-bkngv is not available
Apr 10 14:18:40.122: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:41.121: INFO: Pod daemon-set-bkngv is not available
Apr 10 14:18:41.122: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:42.121: INFO: Pod daemon-set-bkngv is not available
Apr 10 14:18:42.121: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:43.122: INFO: Pod daemon-set-bkngv is not available
Apr 10 14:18:43.122: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:44.123: INFO: Pod daemon-set-bkngv is not available
Apr 10 14:18:44.123: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:45.121: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:46.122: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:47.121: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:47.121: INFO: Pod daemon-set-d2jgm is not available
Apr 10 14:18:48.122: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:48.122: INFO: Pod daemon-set-d2jgm is not available
Apr 10 14:18:49.122: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:49.122: INFO: Pod daemon-set-d2jgm is not available
Apr 10 14:18:50.121: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:50.121: INFO: Pod daemon-set-d2jgm is not available
Apr 10 14:18:51.122: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:51.122: INFO: Pod daemon-set-d2jgm is not available
Apr 10 14:18:52.159: INFO: Wrong image for pod: daemon-set-d2jgm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 14:18:52.159: INFO: Pod daemon-set-d2jgm is not available
Apr 10 14:18:53.121: INFO: Pod daemon-set-w7cbx is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Apr 10 14:18:53.220: INFO: Number of nodes with available pods: 1
Apr 10 14:18:53.220: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 14:18:54.305: INFO: Number of nodes with available pods: 1
Apr 10 14:18:54.305: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 14:18:55.295: INFO: Number of nodes with available pods: 1
Apr 10 14:18:55.295: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp is running more than one daemon pod
Apr 10 14:18:56.293: INFO: Number of nodes with available pods: 2
Apr 10 14:18:56.293: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3175, will wait for the garbage collector to delete the pods
Apr 10 14:18:56.523: INFO: Deleting DaemonSet.extensions daemon-set took: 27.435191ms
Apr 10 14:18:56.623: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.373899ms
Apr 10 14:19:12.660: INFO: Number of nodes with available pods: 0
Apr 10 14:19:12.660: INFO: Number of running nodes: 0, number of available pods: 0
Apr 10 14:19:12.684: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3175/daemonsets","resourceVersion":"21098"},"items":null}

Apr 10 14:19:12.709: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3175/pods","resourceVersion":"21098"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:19:12.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3175" for this suite.
Apr 10 14:19:18.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:19:19.830: INFO: namespace daemonsets-3175 deletion completed in 6.997750801s
•SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:19:19.831: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3447
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Apr 10 14:19:24.338: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:19:24.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3447" for this suite.
Apr 10 14:19:46.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:19:47.478: INFO: namespace replicaset-3447 deletion completed in 23.005925565s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:19:47.478: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8959
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Apr 10 14:19:47.937: INFO: Waiting up to 5m0s for pod "client-containers-b22b50b2-5b9b-11e9-8d1d-a6f828030f0b" in namespace "containers-8959" to be "success or failure"
Apr 10 14:19:47.978: INFO: Pod "client-containers-b22b50b2-5b9b-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 40.52874ms
Apr 10 14:19:50.004: INFO: Pod "client-containers-b22b50b2-5b9b-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066575665s
Apr 10 14:19:52.029: INFO: Pod "client-containers-b22b50b2-5b9b-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.092261805s
Apr 10 14:19:54.055: INFO: Pod "client-containers-b22b50b2-5b9b-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.118162123s
STEP: Saw pod success
Apr 10 14:19:54.055: INFO: Pod "client-containers-b22b50b2-5b9b-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:19:54.080: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod client-containers-b22b50b2-5b9b-11e9-8d1d-a6f828030f0b container test-container: <nil>
STEP: delete the pod
Apr 10 14:19:54.154: INFO: Waiting for pod client-containers-b22b50b2-5b9b-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:19:54.179: INFO: Pod client-containers-b22b50b2-5b9b-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:19:54.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8959" for this suite.
Apr 10 14:20:00.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:20:01.217: INFO: namespace containers-8959 deletion completed in 6.98963066s
•SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:20:01.218: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6965
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 10 14:20:01.621: INFO: Waiting up to 5m0s for pod "downward-api-ba5884b1-5b9b-11e9-8d1d-a6f828030f0b" in namespace "downward-api-6965" to be "success or failure"
Apr 10 14:20:01.646: INFO: Pod "downward-api-ba5884b1-5b9b-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.273113ms
Apr 10 14:20:03.673: INFO: Pod "downward-api-ba5884b1-5b9b-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051898403s
Apr 10 14:20:05.698: INFO: Pod "downward-api-ba5884b1-5b9b-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077636089s
STEP: Saw pod success
Apr 10 14:20:05.699: INFO: Pod "downward-api-ba5884b1-5b9b-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:20:05.731: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downward-api-ba5884b1-5b9b-11e9-8d1d-a6f828030f0b container dapi-container: <nil>
STEP: delete the pod
Apr 10 14:20:05.870: INFO: Waiting for pod downward-api-ba5884b1-5b9b-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:20:05.895: INFO: Pod downward-api-ba5884b1-5b9b-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:20:05.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6965" for this suite.
Apr 10 14:20:12.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:20:12.984: INFO: namespace downward-api-6965 deletion completed in 7.040549419s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:20:12.984: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8340
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-c14db780-5b9b-11e9-8d1d-a6f828030f0b
Apr 10 14:20:13.317: INFO: Pod name my-hostname-basic-c14db780-5b9b-11e9-8d1d-a6f828030f0b: Found 0 pods out of 1
Apr 10 14:20:18.343: INFO: Pod name my-hostname-basic-c14db780-5b9b-11e9-8d1d-a6f828030f0b: Found 1 pods out of 1
Apr 10 14:20:18.343: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c14db780-5b9b-11e9-8d1d-a6f828030f0b" are running
Apr 10 14:20:18.368: INFO: Pod "my-hostname-basic-c14db780-5b9b-11e9-8d1d-a6f828030f0b-zpd2q" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 14:20:13 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 14:20:16 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 14:20:16 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 14:20:13 +0000 UTC Reason: Message:}])
Apr 10 14:20:18.368: INFO: Trying to dial the pod
Apr 10 14:20:23.534: INFO: Controller my-hostname-basic-c14db780-5b9b-11e9-8d1d-a6f828030f0b: Got expected result from replica 1 [my-hostname-basic-c14db780-5b9b-11e9-8d1d-a6f828030f0b-zpd2q]: "my-hostname-basic-c14db780-5b9b-11e9-8d1d-a6f828030f0b-zpd2q", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:20:23.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8340" for this suite.
Apr 10 14:20:29.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:20:30.600: INFO: namespace replication-controller-8340 deletion completed in 7.018525544s
•S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:20:30.600: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7849
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 10 14:20:30.986: INFO: Waiting up to 5m0s for pod "pod-cbd9397d-5b9b-11e9-8d1d-a6f828030f0b" in namespace "emptydir-7849" to be "success or failure"
Apr 10 14:20:31.011: INFO: Pod "pod-cbd9397d-5b9b-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.928339ms
Apr 10 14:20:33.036: INFO: Pod "pod-cbd9397d-5b9b-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050587258s
Apr 10 14:20:35.062: INFO: Pod "pod-cbd9397d-5b9b-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07634482s
Apr 10 14:20:37.089: INFO: Pod "pod-cbd9397d-5b9b-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.10324302s
STEP: Saw pod success
Apr 10 14:20:37.089: INFO: Pod "pod-cbd9397d-5b9b-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:20:37.114: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-cbd9397d-5b9b-11e9-8d1d-a6f828030f0b container test-container: <nil>
STEP: delete the pod
Apr 10 14:20:37.185: INFO: Waiting for pod pod-cbd9397d-5b9b-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:20:37.211: INFO: Pod pod-cbd9397d-5b9b-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:20:37.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7849" for this suite.
Apr 10 14:20:43.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:20:44.259: INFO: namespace emptydir-7849 deletion completed in 6.999107487s
•SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:20:44.259: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7796
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Apr 10 14:20:44.565: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7796'
Apr 10 14:20:45.045: INFO: stderr: ""
Apr 10 14:20:45.045: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 10 14:20:45.046: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7796'
Apr 10 14:20:45.315: INFO: stderr: ""
Apr 10 14:20:45.315: INFO: stdout: "update-demo-nautilus-42r4t update-demo-nautilus-mbkqr "
Apr 10 14:20:45.315: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-42r4t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7796'
Apr 10 14:20:45.510: INFO: stderr: ""
Apr 10 14:20:45.510: INFO: stdout: ""
Apr 10 14:20:45.510: INFO: update-demo-nautilus-42r4t is created but not running
Apr 10 14:20:50.510: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7796'
Apr 10 14:20:50.717: INFO: stderr: ""
Apr 10 14:20:50.718: INFO: stdout: "update-demo-nautilus-42r4t update-demo-nautilus-mbkqr "
Apr 10 14:20:50.718: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-42r4t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7796'
Apr 10 14:20:50.917: INFO: stderr: ""
Apr 10 14:20:50.917: INFO: stdout: "true"
Apr 10 14:20:50.917: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-42r4t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7796'
Apr 10 14:20:51.112: INFO: stderr: ""
Apr 10 14:20:51.112: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 14:20:51.112: INFO: validating pod update-demo-nautilus-42r4t
Apr 10 14:20:51.231: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 14:20:51.231: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 14:20:51.231: INFO: update-demo-nautilus-42r4t is verified up and running
Apr 10 14:20:51.231: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-mbkqr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7796'
Apr 10 14:20:51.433: INFO: stderr: ""
Apr 10 14:20:51.433: INFO: stdout: "true"
Apr 10 14:20:51.433: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-mbkqr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7796'
Apr 10 14:20:51.632: INFO: stderr: ""
Apr 10 14:20:51.632: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 14:20:51.632: INFO: validating pod update-demo-nautilus-mbkqr
Apr 10 14:20:51.743: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 14:20:51.743: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 14:20:51.743: INFO: update-demo-nautilus-mbkqr is verified up and running
STEP: scaling down the replication controller
Apr 10 14:20:51.745: INFO: scanned /root for discovery docs: <nil>
Apr 10 14:20:51.746: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7796'
Apr 10 14:20:53.167: INFO: stderr: ""
Apr 10 14:20:53.167: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 10 14:20:53.167: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7796'
Apr 10 14:20:53.402: INFO: stderr: ""
Apr 10 14:20:53.402: INFO: stdout: "update-demo-nautilus-42r4t update-demo-nautilus-mbkqr "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 10 14:20:58.403: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7796'
Apr 10 14:20:58.618: INFO: stderr: ""
Apr 10 14:20:58.618: INFO: stdout: "update-demo-nautilus-42r4t update-demo-nautilus-mbkqr "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 10 14:21:03.618: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7796'
Apr 10 14:21:03.805: INFO: stderr: ""
Apr 10 14:21:03.805: INFO: stdout: "update-demo-nautilus-mbkqr "
Apr 10 14:21:03.805: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-mbkqr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7796'
Apr 10 14:21:04.065: INFO: stderr: ""
Apr 10 14:21:04.065: INFO: stdout: "true"
Apr 10 14:21:04.066: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-mbkqr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7796'
Apr 10 14:21:04.309: INFO: stderr: ""
Apr 10 14:21:04.309: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 14:21:04.309: INFO: validating pod update-demo-nautilus-mbkqr
Apr 10 14:21:04.337: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 14:21:04.337: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 14:21:04.337: INFO: update-demo-nautilus-mbkqr is verified up and running
STEP: scaling up the replication controller
Apr 10 14:21:04.339: INFO: scanned /root for discovery docs: <nil>
Apr 10 14:21:04.339: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7796'
Apr 10 14:21:05.710: INFO: stderr: ""
Apr 10 14:21:05.710: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 10 14:21:05.710: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7796'
Apr 10 14:21:05.909: INFO: stderr: ""
Apr 10 14:21:05.910: INFO: stdout: "update-demo-nautilus-mbkqr update-demo-nautilus-tltqz "
Apr 10 14:21:05.910: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-mbkqr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7796'
Apr 10 14:21:06.113: INFO: stderr: ""
Apr 10 14:21:06.113: INFO: stdout: "true"
Apr 10 14:21:06.113: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-mbkqr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7796'
Apr 10 14:21:06.306: INFO: stderr: ""
Apr 10 14:21:06.306: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 14:21:06.306: INFO: validating pod update-demo-nautilus-mbkqr
Apr 10 14:21:06.334: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 14:21:06.334: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 14:21:06.334: INFO: update-demo-nautilus-mbkqr is verified up and running
Apr 10 14:21:06.335: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-tltqz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7796'
Apr 10 14:21:06.539: INFO: stderr: ""
Apr 10 14:21:06.539: INFO: stdout: ""
Apr 10 14:21:06.539: INFO: update-demo-nautilus-tltqz is created but not running
Apr 10 14:21:11.540: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7796'
Apr 10 14:21:11.733: INFO: stderr: ""
Apr 10 14:21:11.733: INFO: stdout: "update-demo-nautilus-mbkqr update-demo-nautilus-tltqz "
Apr 10 14:21:11.733: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-mbkqr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7796'
Apr 10 14:21:11.920: INFO: stderr: ""
Apr 10 14:21:11.920: INFO: stdout: "true"
Apr 10 14:21:11.920: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-mbkqr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7796'
Apr 10 14:21:12.109: INFO: stderr: ""
Apr 10 14:21:12.109: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 14:21:12.110: INFO: validating pod update-demo-nautilus-mbkqr
Apr 10 14:21:12.142: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 14:21:12.142: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 14:21:12.142: INFO: update-demo-nautilus-mbkqr is verified up and running
Apr 10 14:21:12.142: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-tltqz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7796'
Apr 10 14:21:12.397: INFO: stderr: ""
Apr 10 14:21:12.397: INFO: stdout: "true"
Apr 10 14:21:12.397: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-tltqz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7796'
Apr 10 14:21:12.607: INFO: stderr: ""
Apr 10 14:21:12.607: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 14:21:12.607: INFO: validating pod update-demo-nautilus-tltqz
Apr 10 14:21:12.719: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 14:21:12.719: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 14:21:12.719: INFO: update-demo-nautilus-tltqz is verified up and running
STEP: using delete to clean up resources
Apr 10 14:21:12.719: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-7796'
Apr 10 14:21:12.930: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 14:21:12.930: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 10 14:21:12.930: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7796'
Apr 10 14:21:13.163: INFO: stderr: "No resources found.\n"
Apr 10 14:21:13.163: INFO: stdout: ""
Apr 10 14:21:13.163: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-7796 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 10 14:21:13.408: INFO: stderr: ""
Apr 10 14:21:13.408: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:21:13.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7796" for this suite.
Apr 10 14:21:37.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:21:38.469: INFO: namespace kubectl-7796 deletion completed in 25.012359479s
•SSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:21:38.469: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-692
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-692.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-692.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-692.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-692.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-692.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-692.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-692.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-692.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-692.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-692.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-692.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-692.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-692.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 246.198.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.198.246_udp@PTR;check="$$(dig +tcp +noall +answer +search 246.198.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.198.246_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-692.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-692.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-692.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-692.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-692.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-692.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-692.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-692.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-692.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-692.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-692.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-692.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-692.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 246.198.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.198.246_udp@PTR;check="$$(dig +tcp +noall +answer +search 246.198.69.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.69.198.246_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 10 14:21:45.074: INFO: Unable to read wheezy_udp@dns-test-service.dns-692.svc.cluster.local from pod dns-692/dns-test-f44ef9b2-5b9b-11e9-8d1d-a6f828030f0b: the server could not find the requested resource (get pods dns-test-f44ef9b2-5b9b-11e9-8d1d-a6f828030f0b)
Apr 10 14:21:45.117: INFO: Unable to read wheezy_tcp@dns-test-service.dns-692.svc.cluster.local from pod dns-692/dns-test-f44ef9b2-5b9b-11e9-8d1d-a6f828030f0b: the server could not find the requested resource (get pods dns-test-f44ef9b2-5b9b-11e9-8d1d-a6f828030f0b)
Apr 10 14:21:45.144: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-692.svc.cluster.local from pod dns-692/dns-test-f44ef9b2-5b9b-11e9-8d1d-a6f828030f0b: the server could not find the requested resource (get pods dns-test-f44ef9b2-5b9b-11e9-8d1d-a6f828030f0b)
Apr 10 14:21:45.176: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-692.svc.cluster.local from pod dns-692/dns-test-f44ef9b2-5b9b-11e9-8d1d-a6f828030f0b: the server could not find the requested resource (get pods dns-test-f44ef9b2-5b9b-11e9-8d1d-a6f828030f0b)
Apr 10 14:21:45.594: INFO: Unable to read jessie_udp@dns-test-service.dns-692.svc.cluster.local from pod dns-692/dns-test-f44ef9b2-5b9b-11e9-8d1d-a6f828030f0b: the server could not find the requested resource (get pods dns-test-f44ef9b2-5b9b-11e9-8d1d-a6f828030f0b)
Apr 10 14:21:45.621: INFO: Unable to read jessie_tcp@dns-test-service.dns-692.svc.cluster.local from pod dns-692/dns-test-f44ef9b2-5b9b-11e9-8d1d-a6f828030f0b: the server could not find the requested resource (get pods dns-test-f44ef9b2-5b9b-11e9-8d1d-a6f828030f0b)
Apr 10 14:21:45.649: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-692.svc.cluster.local from pod dns-692/dns-test-f44ef9b2-5b9b-11e9-8d1d-a6f828030f0b: the server could not find the requested resource (get pods dns-test-f44ef9b2-5b9b-11e9-8d1d-a6f828030f0b)
Apr 10 14:21:45.676: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-692.svc.cluster.local from pod dns-692/dns-test-f44ef9b2-5b9b-11e9-8d1d-a6f828030f0b: the server could not find the requested resource (get pods dns-test-f44ef9b2-5b9b-11e9-8d1d-a6f828030f0b)
Apr 10 14:21:46.055: INFO: Lookups using dns-692/dns-test-f44ef9b2-5b9b-11e9-8d1d-a6f828030f0b failed for: [wheezy_udp@dns-test-service.dns-692.svc.cluster.local wheezy_tcp@dns-test-service.dns-692.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-692.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-692.svc.cluster.local jessie_udp@dns-test-service.dns-692.svc.cluster.local jessie_tcp@dns-test-service.dns-692.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-692.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-692.svc.cluster.local]

Apr 10 14:21:52.404: INFO: DNS probes using dns-692/dns-test-f44ef9b2-5b9b-11e9-8d1d-a6f828030f0b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:21:52.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-692" for this suite.
Apr 10 14:21:58.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:21:59.577: INFO: namespace dns-692 deletion completed in 7.003163863s
•SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:21:59.578: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1495
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 14:21:59.923: INFO: (0) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 29.317162ms)
Apr 10 14:21:59.967: INFO: (1) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 43.885624ms)
Apr 10 14:21:59.995: INFO: (2) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 27.240198ms)
Apr 10 14:22:00.022: INFO: (3) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 27.228877ms)
Apr 10 14:22:00.049: INFO: (4) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 27.073855ms)
Apr 10 14:22:00.076: INFO: (5) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.97492ms)
Apr 10 14:22:00.108: INFO: (6) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 31.428859ms)
Apr 10 14:22:00.134: INFO: (7) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.836287ms)
Apr 10 14:22:00.161: INFO: (8) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.681773ms)
Apr 10 14:22:00.188: INFO: (9) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.684546ms)
Apr 10 14:22:00.215: INFO: (10) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 27.294688ms)
Apr 10 14:22:00.242: INFO: (11) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.858197ms)
Apr 10 14:22:00.437: INFO: (12) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 194.377707ms)
Apr 10 14:22:00.541: INFO: (13) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 103.880357ms)
Apr 10 14:22:00.586: INFO: (14) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 45.324295ms)
Apr 10 14:22:00.613: INFO: (15) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 27.024728ms)
Apr 10 14:22:00.640: INFO: (16) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.872025ms)
Apr 10 14:22:00.667: INFO: (17) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.955111ms)
Apr 10 14:22:00.694: INFO: (18) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.845931ms)
Apr 10 14:22:00.721: INFO: (19) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 27.15257ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:22:00.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1495" for this suite.
Apr 10 14:22:06.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:22:07.748: INFO: namespace proxy-1495 deletion completed in 7.002233234s
•SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:22:07.749: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1420
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:22:08.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1420" for this suite.
Apr 10 14:22:14.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:22:15.178: INFO: namespace services-1420 deletion completed in 7.065901274s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
•SSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:22:15.178: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5817
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Apr 10 14:22:15.506: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:22:24.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5817" for this suite.
Apr 10 14:22:30.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:22:31.742: INFO: namespace pods-5817 deletion completed in 6.997007127s
•SSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:22:31.742: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8978
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 10 14:22:32.094: INFO: Waiting up to 5m0s for pod "downward-api-1408dc7a-5b9c-11e9-8d1d-a6f828030f0b" in namespace "downward-api-8978" to be "success or failure"
Apr 10 14:22:32.119: INFO: Pod "downward-api-1408dc7a-5b9c-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.513905ms
Apr 10 14:22:34.145: INFO: Pod "downward-api-1408dc7a-5b9c-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051467928s
Apr 10 14:22:36.171: INFO: Pod "downward-api-1408dc7a-5b9c-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077756426s
STEP: Saw pod success
Apr 10 14:22:36.172: INFO: Pod "downward-api-1408dc7a-5b9c-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:22:36.197: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downward-api-1408dc7a-5b9c-11e9-8d1d-a6f828030f0b container dapi-container: <nil>
STEP: delete the pod
Apr 10 14:22:36.561: INFO: Waiting for pod downward-api-1408dc7a-5b9c-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:22:36.586: INFO: Pod downward-api-1408dc7a-5b9c-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:22:36.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8978" for this suite.
Apr 10 14:22:42.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:22:43.647: INFO: namespace downward-api-8978 deletion completed in 7.012857881s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:22:43.647: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5745
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0410 14:22:54.382602    3207 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 10 14:22:54.382: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:22:54.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5745" for this suite.
Apr 10 14:23:02.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:23:03.428: INFO: namespace gc-5745 deletion completed in 9.020683127s
•SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:23:03.429: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5102
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-5122
STEP: Creating secret with name secret-test-26f29c51-5b9c-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume secrets
Apr 10 14:23:04.118: INFO: Waiting up to 5m0s for pod "pod-secrets-271f56df-5b9c-11e9-8d1d-a6f828030f0b" in namespace "secrets-5102" to be "success or failure"
Apr 10 14:23:04.152: INFO: Pod "pod-secrets-271f56df-5b9c-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 34.215288ms
Apr 10 14:23:06.178: INFO: Pod "pod-secrets-271f56df-5b9c-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060507483s
Apr 10 14:23:08.204: INFO: Pod "pod-secrets-271f56df-5b9c-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.086603767s
Apr 10 14:23:10.230: INFO: Pod "pod-secrets-271f56df-5b9c-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.112351335s
STEP: Saw pod success
Apr 10 14:23:10.230: INFO: Pod "pod-secrets-271f56df-5b9c-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:23:10.255: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-secrets-271f56df-5b9c-11e9-8d1d-a6f828030f0b container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 14:23:10.334: INFO: Waiting for pod pod-secrets-271f56df-5b9c-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:23:10.359: INFO: Pod pod-secrets-271f56df-5b9c-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:23:10.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5102" for this suite.
Apr 10 14:23:16.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:23:17.407: INFO: namespace secrets-5102 deletion completed in 7.000798484s
STEP: Destroying namespace "secret-namespace-5122" for this suite.
Apr 10 14:23:23.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:23:24.410: INFO: namespace secret-namespace-5122 deletion completed in 7.002496233s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:23:24.410: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7742
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0410 14:23:55.003540    3207 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 10 14:23:55.003: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:23:55.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7742" for this suite.
Apr 10 14:24:01.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:24:02.024: INFO: namespace gc-7742 deletion completed in 6.995631411s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:24:02.024: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8924
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-49db778c-5b9c-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume secrets
Apr 10 14:24:02.419: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-49df4e13-5b9c-11e9-8d1d-a6f828030f0b" in namespace "projected-8924" to be "success or failure"
Apr 10 14:24:02.444: INFO: Pod "pod-projected-secrets-49df4e13-5b9c-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.074704ms
Apr 10 14:24:04.470: INFO: Pod "pod-projected-secrets-49df4e13-5b9c-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050748488s
Apr 10 14:24:06.495: INFO: Pod "pod-projected-secrets-49df4e13-5b9c-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076461992s
STEP: Saw pod success
Apr 10 14:24:06.495: INFO: Pod "pod-projected-secrets-49df4e13-5b9c-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:24:06.520: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-projected-secrets-49df4e13-5b9c-11e9-8d1d-a6f828030f0b container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 14:24:06.672: INFO: Waiting for pod pod-projected-secrets-49df4e13-5b9c-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:24:06.702: INFO: Pod pod-projected-secrets-49df4e13-5b9c-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:24:06.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8924" for this suite.
Apr 10 14:24:12.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:24:13.767: INFO: namespace projected-8924 deletion completed in 7.016838823s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:24:13.768: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5129
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:24:50.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5129" for this suite.
Apr 10 14:24:56.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:24:57.778: INFO: namespace container-runtime-5129 deletion completed in 7.035220958s
•SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:24:57.778: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5903
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:25:02.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5903" for this suite.
Apr 10 14:25:48.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:25:49.312: INFO: namespace kubelet-test-5903 deletion completed in 47.046915714s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:25:49.312: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1605
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Apr 10 14:25:49.689: INFO: Waiting up to 5m0s for pod "var-expansion-89cf91b9-5b9c-11e9-8d1d-a6f828030f0b" in namespace "var-expansion-1605" to be "success or failure"
Apr 10 14:25:49.715: INFO: Pod "var-expansion-89cf91b9-5b9c-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.192846ms
Apr 10 14:25:51.740: INFO: Pod "var-expansion-89cf91b9-5b9c-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050837019s
Apr 10 14:25:53.766: INFO: Pod "var-expansion-89cf91b9-5b9c-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076643102s
STEP: Saw pod success
Apr 10 14:25:53.766: INFO: Pod "var-expansion-89cf91b9-5b9c-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:25:53.792: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod var-expansion-89cf91b9-5b9c-11e9-8d1d-a6f828030f0b container dapi-container: <nil>
STEP: delete the pod
Apr 10 14:25:54.103: INFO: Waiting for pod var-expansion-89cf91b9-5b9c-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:25:54.127: INFO: Pod var-expansion-89cf91b9-5b9c-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:25:54.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1605" for this suite.
Apr 10 14:26:00.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:26:01.245: INFO: namespace var-expansion-1605 deletion completed in 7.069605909s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:26:01.245: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8686
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 14:26:01.591: INFO: Waiting up to 5m0s for pod "downwardapi-volume-90e77af6-5b9c-11e9-8d1d-a6f828030f0b" in namespace "downward-api-8686" to be "success or failure"
Apr 10 14:26:01.622: INFO: Pod "downwardapi-volume-90e77af6-5b9c-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 30.910261ms
Apr 10 14:26:03.649: INFO: Pod "downwardapi-volume-90e77af6-5b9c-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058087097s
Apr 10 14:26:05.675: INFO: Pod "downwardapi-volume-90e77af6-5b9c-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.084282353s
STEP: Saw pod success
Apr 10 14:26:05.675: INFO: Pod "downwardapi-volume-90e77af6-5b9c-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:26:05.701: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downwardapi-volume-90e77af6-5b9c-11e9-8d1d-a6f828030f0b container client-container: <nil>
STEP: delete the pod
Apr 10 14:26:05.932: INFO: Waiting for pod downwardapi-volume-90e77af6-5b9c-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:26:05.957: INFO: Pod downwardapi-volume-90e77af6-5b9c-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:26:05.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8686" for this suite.
Apr 10 14:26:12.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:26:13.002: INFO: namespace downward-api-8686 deletion completed in 6.992712357s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:26:13.003: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9948
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 14:26:13.274: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:26:17.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9948" for this suite.
Apr 10 14:27:05.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:27:06.846: INFO: namespace pods-9948 deletion completed in 48.988132174s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:27:06.846: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3073
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-b4bq
STEP: Creating a pod to test atomic-volume-subpath
Apr 10 14:27:07.298: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-b4bq" in namespace "subpath-3073" to be "success or failure"
Apr 10 14:27:07.323: INFO: Pod "pod-subpath-test-projected-b4bq": Phase="Pending", Reason="", readiness=false. Elapsed: 24.924003ms
Apr 10 14:27:09.349: INFO: Pod "pod-subpath-test-projected-b4bq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050811537s
Apr 10 14:27:11.374: INFO: Pod "pod-subpath-test-projected-b4bq": Phase="Running", Reason="", readiness=true. Elapsed: 4.075945168s
Apr 10 14:27:13.400: INFO: Pod "pod-subpath-test-projected-b4bq": Phase="Running", Reason="", readiness=true. Elapsed: 6.1019469s
Apr 10 14:27:15.429: INFO: Pod "pod-subpath-test-projected-b4bq": Phase="Running", Reason="", readiness=true. Elapsed: 8.131265448s
Apr 10 14:27:17.455: INFO: Pod "pod-subpath-test-projected-b4bq": Phase="Running", Reason="", readiness=true. Elapsed: 10.157190798s
Apr 10 14:27:19.481: INFO: Pod "pod-subpath-test-projected-b4bq": Phase="Running", Reason="", readiness=true. Elapsed: 12.182879602s
Apr 10 14:27:21.507: INFO: Pod "pod-subpath-test-projected-b4bq": Phase="Running", Reason="", readiness=true. Elapsed: 14.208878922s
Apr 10 14:27:23.533: INFO: Pod "pod-subpath-test-projected-b4bq": Phase="Running", Reason="", readiness=true. Elapsed: 16.234886307s
Apr 10 14:27:25.559: INFO: Pod "pod-subpath-test-projected-b4bq": Phase="Running", Reason="", readiness=true. Elapsed: 18.260616805s
Apr 10 14:27:27.584: INFO: Pod "pod-subpath-test-projected-b4bq": Phase="Running", Reason="", readiness=true. Elapsed: 20.286054515s
Apr 10 14:27:29.610: INFO: Pod "pod-subpath-test-projected-b4bq": Phase="Running", Reason="", readiness=true. Elapsed: 22.311936111s
Apr 10 14:27:31.635: INFO: Pod "pod-subpath-test-projected-b4bq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.33719482s
STEP: Saw pod success
Apr 10 14:27:31.635: INFO: Pod "pod-subpath-test-projected-b4bq" satisfied condition "success or failure"
Apr 10 14:27:31.660: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-subpath-test-projected-b4bq container test-container-subpath-projected-b4bq: <nil>
STEP: delete the pod
Apr 10 14:27:31.724: INFO: Waiting for pod pod-subpath-test-projected-b4bq to disappear
Apr 10 14:27:31.749: INFO: Pod pod-subpath-test-projected-b4bq no longer exists
STEP: Deleting pod pod-subpath-test-projected-b4bq
Apr 10 14:27:31.749: INFO: Deleting pod "pod-subpath-test-projected-b4bq" in namespace "subpath-3073"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:27:31.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3073" for this suite.
Apr 10 14:27:37.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:27:38.828: INFO: namespace subpath-3073 deletion completed in 6.992417541s
•SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:27:38.828: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2593
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:27:47.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2593" for this suite.
Apr 10 14:27:53.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:27:54.335: INFO: namespace kubelet-test-2593 deletion completed in 7.015416405s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:27:54.335: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7934
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 10 14:27:54.844: INFO: Waiting up to 5m0s for pod "pod-d468b073-5b9c-11e9-8d1d-a6f828030f0b" in namespace "emptydir-7934" to be "success or failure"
Apr 10 14:27:54.869: INFO: Pod "pod-d468b073-5b9c-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.72437ms
Apr 10 14:27:56.895: INFO: Pod "pod-d468b073-5b9c-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05075779s
Apr 10 14:27:58.921: INFO: Pod "pod-d468b073-5b9c-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076746541s
STEP: Saw pod success
Apr 10 14:27:58.921: INFO: Pod "pod-d468b073-5b9c-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:27:58.946: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-d468b073-5b9c-11e9-8d1d-a6f828030f0b container test-container: <nil>
STEP: delete the pod
Apr 10 14:27:59.305: INFO: Waiting for pod pod-d468b073-5b9c-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:27:59.329: INFO: Pod pod-d468b073-5b9c-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:27:59.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7934" for this suite.
Apr 10 14:28:05.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:28:06.397: INFO: namespace emptydir-7934 deletion completed in 7.019308303s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:28:06.397: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5016
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 14:28:06.897: INFO: Create a RollingUpdate DaemonSet
Apr 10 14:28:06.923: INFO: Check that daemon pods launch on every node of the cluster
Apr 10 14:28:06.980: INFO: Number of nodes with available pods: 0
Apr 10 14:28:06.980: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 14:28:08.053: INFO: Number of nodes with available pods: 0
Apr 10 14:28:08.053: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 14:28:09.053: INFO: Number of nodes with available pods: 0
Apr 10 14:28:09.053: INFO: Node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm is running more than one daemon pod
Apr 10 14:28:10.053: INFO: Number of nodes with available pods: 2
Apr 10 14:28:10.053: INFO: Number of running nodes: 2, number of available pods: 2
Apr 10 14:28:10.053: INFO: Update the DaemonSet to trigger a rollout
Apr 10 14:28:10.105: INFO: Updating DaemonSet daemon-set
Apr 10 14:28:23.181: INFO: Roll back the DaemonSet before rollout is complete
Apr 10 14:28:23.233: INFO: Updating DaemonSet daemon-set
Apr 10 14:28:23.234: INFO: Make sure DaemonSet rollback is complete
Apr 10 14:28:23.266: INFO: Wrong image for pod: daemon-set-nhlwf. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr 10 14:28:23.266: INFO: Pod daemon-set-nhlwf is not available
Apr 10 14:28:24.340: INFO: Wrong image for pod: daemon-set-nhlwf. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr 10 14:28:24.340: INFO: Pod daemon-set-nhlwf is not available
Apr 10 14:28:25.340: INFO: Pod daemon-set-z5zv8 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5016, will wait for the garbage collector to delete the pods
Apr 10 14:28:25.541: INFO: Deleting DaemonSet.extensions daemon-set took: 27.165055ms
Apr 10 14:28:25.942: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.498742ms
Apr 10 14:28:39.267: INFO: Number of nodes with available pods: 0
Apr 10 14:28:39.267: INFO: Number of running nodes: 0, number of available pods: 0
Apr 10 14:28:39.293: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5016/daemonsets","resourceVersion":"23016"},"items":null}

Apr 10 14:28:39.319: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5016/pods","resourceVersion":"23016"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:28:39.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5016" for this suite.
Apr 10 14:28:45.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:28:46.477: INFO: namespace daemonsets-5016 deletion completed in 7.031268744s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:28:46.478: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6196
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Apr 10 14:28:46.769: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-6196 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Apr 10 14:28:51.872: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Apr 10 14:28:51.872: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:28:53.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6196" for this suite.
Apr 10 14:29:04.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:29:04.988: INFO: namespace kubectl-6196 deletion completed in 11.017826229s
•SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:29:04.988: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4257
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 14:29:05.392: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fe75413e-5b9c-11e9-8d1d-a6f828030f0b" in namespace "downward-api-4257" to be "success or failure"
Apr 10 14:29:05.417: INFO: Pod "downwardapi-volume-fe75413e-5b9c-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.915147ms
Apr 10 14:29:07.443: INFO: Pod "downwardapi-volume-fe75413e-5b9c-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050836789s
Apr 10 14:29:09.469: INFO: Pod "downwardapi-volume-fe75413e-5b9c-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076613415s
STEP: Saw pod success
Apr 10 14:29:09.469: INFO: Pod "downwardapi-volume-fe75413e-5b9c-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:29:09.494: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downwardapi-volume-fe75413e-5b9c-11e9-8d1d-a6f828030f0b container client-container: <nil>
STEP: delete the pod
Apr 10 14:29:09.733: INFO: Waiting for pod downwardapi-volume-fe75413e-5b9c-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:29:09.757: INFO: Pod downwardapi-volume-fe75413e-5b9c-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:29:09.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4257" for this suite.
Apr 10 14:29:15.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:29:16.801: INFO: namespace downward-api-4257 deletion completed in 6.995606551s
•
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:29:16.801: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5259
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 14:29:17.183: INFO: Waiting up to 5m0s for pod "downwardapi-volume-057c9d55-5b9d-11e9-8d1d-a6f828030f0b" in namespace "downward-api-5259" to be "success or failure"
Apr 10 14:29:17.208: INFO: Pod "downwardapi-volume-057c9d55-5b9d-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.769983ms
Apr 10 14:29:19.235: INFO: Pod "downwardapi-volume-057c9d55-5b9d-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05221233s
Apr 10 14:29:21.261: INFO: Pod "downwardapi-volume-057c9d55-5b9d-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077938374s
Apr 10 14:29:23.287: INFO: Pod "downwardapi-volume-057c9d55-5b9d-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.103879156s
STEP: Saw pod success
Apr 10 14:29:23.287: INFO: Pod "downwardapi-volume-057c9d55-5b9d-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:29:23.312: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downwardapi-volume-057c9d55-5b9d-11e9-8d1d-a6f828030f0b container client-container: <nil>
STEP: delete the pod
Apr 10 14:29:23.389: INFO: Waiting for pod downwardapi-volume-057c9d55-5b9d-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:29:23.414: INFO: Pod downwardapi-volume-057c9d55-5b9d-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:29:23.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5259" for this suite.
Apr 10 14:29:29.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:29:30.459: INFO: namespace downward-api-5259 deletion completed in 6.996623498s
•SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:29:30.459: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3681
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-3681
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3681
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3681
Apr 10 14:29:30.958: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Apr 10 14:29:40.985: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Apr 10 14:29:41.012: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3681 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 14:29:41.751: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 14:29:41.751: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 14:29:41.751: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 14:29:41.776: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 10 14:29:51.803: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 14:29:51.803: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 14:29:51.905: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998995s
Apr 10 14:29:52.931: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.974177422s
Apr 10 14:29:53.957: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.94844215s
Apr 10 14:29:54.983: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.9221541s
Apr 10 14:29:56.009: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.896206299s
Apr 10 14:29:57.035: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.870170906s
Apr 10 14:29:58.061: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.844146275s
Apr 10 14:29:59.087: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.818415669s
Apr 10 14:30:00.113: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.792380282s
Apr 10 14:30:01.139: INFO: Verifying statefulset ss doesn't scale past 1 for another 766.449272ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3681
Apr 10 14:30:02.164: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3681 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 14:30:02.884: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 14:30:02.884: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 14:30:02.884: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 14:30:02.911: INFO: Found 2 stateful pods, waiting for 3
Apr 10 14:30:12.938: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 14:30:12.938: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 14:30:12.938: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Apr 10 14:30:12.987: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3681 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 14:30:13.703: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 14:30:13.703: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 14:30:13.703: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 14:30:13.704: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3681 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 14:30:14.439: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 14:30:14.440: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 14:30:14.440: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 14:30:14.440: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3681 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 14:30:15.154: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 14:30:15.154: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 14:30:15.154: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 14:30:15.154: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 14:30:15.179: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Apr 10 14:30:25.231: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 14:30:25.231: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 14:30:25.231: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 14:30:25.309: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999291s
Apr 10 14:30:26.336: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.973598604s
Apr 10 14:30:27.361: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.946622016s
Apr 10 14:30:28.387: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.920832112s
Apr 10 14:30:29.414: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.895003573s
Apr 10 14:30:30.440: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.868873186s
Apr 10 14:30:31.466: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.842460386s
Apr 10 14:30:32.492: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.816491064s
Apr 10 14:30:33.517: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.79087671s
Apr 10 14:30:34.543: INFO: Verifying statefulset ss doesn't scale past 3 for another 765.166207ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3681
Apr 10 14:30:35.569: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3681 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 14:30:36.290: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 14:30:36.290: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 14:30:36.290: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 14:30:36.290: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3681 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 14:30:37.003: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 14:30:37.003: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 14:30:37.003: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 14:30:37.003: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3681 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 14:30:37.727: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 14:30:37.727: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 14:30:37.727: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 14:30:37.727: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 10 14:31:07.845: INFO: Deleting all statefulset in ns statefulset-3681
Apr 10 14:31:07.869: INFO: Scaling statefulset ss to 0
Apr 10 14:31:07.944: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 14:31:07.968: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:31:08.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3681" for this suite.
Apr 10 14:31:14.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:31:15.099: INFO: namespace statefulset-3681 deletion completed in 7.004827797s
•SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:31:15.099: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9149
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Apr 10 14:31:16.084: INFO: created pod pod-service-account-defaultsa
Apr 10 14:31:16.084: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr 10 14:31:16.109: INFO: created pod pod-service-account-mountsa
Apr 10 14:31:16.109: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr 10 14:31:16.170: INFO: created pod pod-service-account-nomountsa
Apr 10 14:31:16.170: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr 10 14:31:16.196: INFO: created pod pod-service-account-defaultsa-mountspec
Apr 10 14:31:16.196: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr 10 14:31:16.224: INFO: created pod pod-service-account-mountsa-mountspec
Apr 10 14:31:16.224: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr 10 14:31:16.250: INFO: created pod pod-service-account-nomountsa-mountspec
Apr 10 14:31:16.250: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr 10 14:31:16.276: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr 10 14:31:16.276: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr 10 14:31:16.308: INFO: created pod pod-service-account-mountsa-nomountspec
Apr 10 14:31:16.308: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr 10 14:31:16.342: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr 10 14:31:16.342: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:31:16.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9149" for this suite.
Apr 10 14:31:40.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:31:41.395: INFO: namespace svcaccounts-9149 deletion completed in 25.004011481s
•SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:31:41.395: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-177
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 14:31:41.706: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5b9f156f-5b9d-11e9-8d1d-a6f828030f0b" in namespace "projected-177" to be "success or failure"
Apr 10 14:31:41.731: INFO: Pod "downwardapi-volume-5b9f156f-5b9d-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.809833ms
Apr 10 14:31:43.757: INFO: Pod "downwardapi-volume-5b9f156f-5b9d-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050750917s
Apr 10 14:31:45.783: INFO: Pod "downwardapi-volume-5b9f156f-5b9d-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076633665s
STEP: Saw pod success
Apr 10 14:31:45.783: INFO: Pod "downwardapi-volume-5b9f156f-5b9d-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:31:45.808: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downwardapi-volume-5b9f156f-5b9d-11e9-8d1d-a6f828030f0b container client-container: <nil>
STEP: delete the pod
Apr 10 14:31:45.965: INFO: Waiting for pod downwardapi-volume-5b9f156f-5b9d-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:31:45.990: INFO: Pod downwardapi-volume-5b9f156f-5b9d-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:31:45.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-177" for this suite.
Apr 10 14:31:52.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:31:53.069: INFO: namespace projected-177 deletion completed in 7.03157954s
•SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:31:53.070: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2343
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 14:31:53.407: INFO: (0) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 28.664998ms)
Apr 10 14:31:53.451: INFO: (1) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 43.762612ms)
Apr 10 14:31:53.477: INFO: (2) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.460508ms)
Apr 10 14:31:53.504: INFO: (3) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.589518ms)
Apr 10 14:31:53.530: INFO: (4) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.253769ms)
Apr 10 14:31:53.557: INFO: (5) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.93722ms)
Apr 10 14:31:53.584: INFO: (6) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.567972ms)
Apr 10 14:31:53.610: INFO: (7) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.207248ms)
Apr 10 14:31:53.637: INFO: (8) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 27.026344ms)
Apr 10 14:31:53.664: INFO: (9) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.805044ms)
Apr 10 14:31:53.691: INFO: (10) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.658867ms)
Apr 10 14:31:53.717: INFO: (11) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.654296ms)
Apr 10 14:31:53.744: INFO: (12) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.42746ms)
Apr 10 14:31:53.771: INFO: (13) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.481005ms)
Apr 10 14:31:53.797: INFO: (14) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.687723ms)
Apr 10 14:31:53.824: INFO: (15) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.674664ms)
Apr 10 14:31:53.850: INFO: (16) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.011683ms)
Apr 10 14:31:53.877: INFO: (17) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.945741ms)
Apr 10 14:31:53.904: INFO: (18) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.716571ms)
Apr 10 14:31:53.931: INFO: (19) /api/v1/nodes/shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.574217ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:31:53.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2343" for this suite.
Apr 10 14:32:00.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:32:00.967: INFO: namespace proxy-2343 deletion completed in 7.010935761s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:32:00.967: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:32:05.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5869" for this suite.
Apr 10 14:32:45.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:32:46.428: INFO: namespace kubelet-test-5869 deletion completed in 40.977612213s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:32:46.428: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5065
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Apr 10 14:32:46.821: INFO: Waiting up to 5m0s for pod "var-expansion-8270c6be-5b9d-11e9-8d1d-a6f828030f0b" in namespace "var-expansion-5065" to be "success or failure"
Apr 10 14:32:46.846: INFO: Pod "var-expansion-8270c6be-5b9d-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.326324ms
Apr 10 14:32:48.871: INFO: Pod "var-expansion-8270c6be-5b9d-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05010653s
Apr 10 14:32:50.897: INFO: Pod "var-expansion-8270c6be-5b9d-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075334085s
STEP: Saw pod success
Apr 10 14:32:50.897: INFO: Pod "var-expansion-8270c6be-5b9d-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:32:50.921: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod var-expansion-8270c6be-5b9d-11e9-8d1d-a6f828030f0b container dapi-container: <nil>
STEP: delete the pod
Apr 10 14:32:51.059: INFO: Waiting for pod var-expansion-8270c6be-5b9d-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:32:51.083: INFO: Pod var-expansion-8270c6be-5b9d-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:32:51.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5065" for this suite.
Apr 10 14:32:57.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:32:58.137: INFO: namespace var-expansion-5065 deletion completed in 7.004393917s
•SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:32:58.137: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3312
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 14:32:58.493: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8965c661-5b9d-11e9-8d1d-a6f828030f0b" in namespace "downward-api-3312" to be "success or failure"
Apr 10 14:32:58.518: INFO: Pod "downwardapi-volume-8965c661-5b9d-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.646451ms
Apr 10 14:33:00.544: INFO: Pod "downwardapi-volume-8965c661-5b9d-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050510299s
Apr 10 14:33:02.570: INFO: Pod "downwardapi-volume-8965c661-5b9d-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076702975s
STEP: Saw pod success
Apr 10 14:33:02.570: INFO: Pod "downwardapi-volume-8965c661-5b9d-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:33:02.595: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod downwardapi-volume-8965c661-5b9d-11e9-8d1d-a6f828030f0b container client-container: <nil>
STEP: delete the pod
Apr 10 14:33:02.667: INFO: Waiting for pod downwardapi-volume-8965c661-5b9d-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:33:02.697: INFO: Pod downwardapi-volume-8965c661-5b9d-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:33:02.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3312" for this suite.
Apr 10 14:33:08.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:33:09.765: INFO: namespace downward-api-3312 deletion completed in 7.020873639s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:33:09.765: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4538
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-90500987-5b9d-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test consume configMaps
Apr 10 14:33:10.120: INFO: Waiting up to 5m0s for pod "pod-configmaps-9053e4b0-5b9d-11e9-8d1d-a6f828030f0b" in namespace "configmap-4538" to be "success or failure"
Apr 10 14:33:10.145: INFO: Pod "pod-configmaps-9053e4b0-5b9d-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.137169ms
Apr 10 14:33:12.170: INFO: Pod "pod-configmaps-9053e4b0-5b9d-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050546485s
Apr 10 14:33:14.197: INFO: Pod "pod-configmaps-9053e4b0-5b9d-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077408107s
STEP: Saw pod success
Apr 10 14:33:14.197: INFO: Pod "pod-configmaps-9053e4b0-5b9d-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:33:14.223: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod pod-configmaps-9053e4b0-5b9d-11e9-8d1d-a6f828030f0b container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 14:33:14.558: INFO: Waiting for pod pod-configmaps-9053e4b0-5b9d-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:33:14.583: INFO: Pod pod-configmaps-9053e4b0-5b9d-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:33:14.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4538" for this suite.
Apr 10 14:33:20.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:33:21.638: INFO: namespace configmap-4538 deletion completed in 7.007150851s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:33:21.638: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2477
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-97681cee-5b9d-11e9-8d1d-a6f828030f0b
STEP: Creating secret with name secret-projected-all-test-volume-97681cdc-5b9d-11e9-8d1d-a6f828030f0b
STEP: Creating a pod to test Check all projections for projected volume plugin
Apr 10 14:33:22.046: INFO: Waiting up to 5m0s for pod "projected-volume-97681c9d-5b9d-11e9-8d1d-a6f828030f0b" in namespace "projected-2477" to be "success or failure"
Apr 10 14:33:22.071: INFO: Pod "projected-volume-97681c9d-5b9d-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.694655ms
Apr 10 14:33:24.097: INFO: Pod "projected-volume-97681c9d-5b9d-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050732549s
Apr 10 14:33:26.123: INFO: Pod "projected-volume-97681c9d-5b9d-11e9-8d1d-a6f828030f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076906113s
Apr 10 14:33:28.149: INFO: Pod "projected-volume-97681c9d-5b9d-11e9-8d1d-a6f828030f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.102442617s
STEP: Saw pod success
Apr 10 14:33:28.149: INFO: Pod "projected-volume-97681c9d-5b9d-11e9-8d1d-a6f828030f0b" satisfied condition "success or failure"
Apr 10 14:33:28.173: INFO: Trying to get logs from node shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp pod projected-volume-97681c9d-5b9d-11e9-8d1d-a6f828030f0b container projected-all-volume-test: <nil>
STEP: delete the pod
Apr 10 14:33:28.237: INFO: Waiting for pod projected-volume-97681c9d-5b9d-11e9-8d1d-a6f828030f0b to disappear
Apr 10 14:33:28.278: INFO: Pod projected-volume-97681c9d-5b9d-11e9-8d1d-a6f828030f0b no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:33:28.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2477" for this suite.
Apr 10 14:33:34.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:33:35.328: INFO: namespace projected-2477 deletion completed in 7.002163052s
•SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:33:35.328: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-183
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Apr 10 14:33:35.664: INFO: Asynchronously running '/bin/kubectl kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy --unix-socket=/tmp/kubectl-proxy-unix452137484/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:33:35.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-183" for this suite.
Apr 10 14:33:41.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:33:42.824: INFO: namespace kubectl-183 deletion completed in 7.02406403s
•SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:33:42.824: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2904
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-2904
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-2904
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2904
Apr 10 14:33:43.246: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Apr 10 14:33:53.273: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Apr 10 14:33:53.299: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2904 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 14:33:54.049: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 14:33:54.049: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 14:33:54.049: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 14:33:54.075: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 10 14:34:04.101: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 14:34:04.101: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 14:34:04.211: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999076s
Apr 10 14:34:05.237: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.96505875s
Apr 10 14:34:06.264: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.939051421s
Apr 10 14:34:07.290: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.912799367s
Apr 10 14:34:08.323: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.886740495s
Apr 10 14:34:09.350: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.853269507s
Apr 10 14:34:10.376: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.826879174s
Apr 10 14:34:11.402: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.800858915s
Apr 10 14:34:12.428: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.774582928s
Apr 10 14:34:13.454: INFO: Verifying statefulset ss doesn't scale past 3 for another 748.806988ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2904
Apr 10 14:34:14.480: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2904 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 14:34:15.194: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 14:34:15.194: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 14:34:15.194: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 14:34:15.194: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2904 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 14:34:15.876: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 10 14:34:15.877: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 14:34:15.877: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 14:34:15.877: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2904 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 14:34:16.598: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 10 14:34:16.598: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 14:34:16.598: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 14:34:16.625: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 14:34:16.625: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 14:34:16.625: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Apr 10 14:34:16.650: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2904 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 14:34:17.306: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 14:34:17.306: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 14:34:17.306: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 14:34:17.307: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2904 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 14:34:18.005: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 14:34:18.005: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 14:34:18.005: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 14:34:18.005: INFO: Running '/bin/kubectl --server=https://api.tm-fzm8j.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2904 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 14:34:18.698: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 14:34:18.698: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 14:34:18.698: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 14:34:18.698: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 14:34:18.723: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr 10 14:34:28.775: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 14:34:28.775: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 14:34:28.775: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 14:34:28.851: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Apr 10 14:34:28.851: INFO: ss-0  shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:33:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:33:43 +0000 UTC  }]
Apr 10 14:34:28.851: INFO: ss-1  shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:04 +0000 UTC  }]
Apr 10 14:34:28.851: INFO: ss-2  shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:04 +0000 UTC  }]
Apr 10 14:34:28.851: INFO: 
Apr 10 14:34:28.851: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 10 14:34:29.877: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Apr 10 14:34:29.877: INFO: ss-0  shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:33:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:33:43 +0000 UTC  }]
Apr 10 14:34:29.877: INFO: ss-1  shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:04 +0000 UTC  }]
Apr 10 14:34:29.877: INFO: ss-2  shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:04 +0000 UTC  }]
Apr 10 14:34:29.877: INFO: 
Apr 10 14:34:29.877: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 10 14:34:30.903: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Apr 10 14:34:30.903: INFO: ss-0  shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:33:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:33:43 +0000 UTC  }]
Apr 10 14:34:30.903: INFO: ss-1  shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:04 +0000 UTC  }]
Apr 10 14:34:30.903: INFO: ss-2  shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:04 +0000 UTC  }]
Apr 10 14:34:30.903: INFO: 
Apr 10 14:34:30.903: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 10 14:34:31.930: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Apr 10 14:34:31.930: INFO: ss-0  shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:33:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:33:43 +0000 UTC  }]
Apr 10 14:34:31.930: INFO: ss-1  shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-nvvbm  Pending  0s     [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:04 +0000 UTC  }]
Apr 10 14:34:31.930: INFO: ss-2  shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:04 +0000 UTC  }]
Apr 10 14:34:31.930: INFO: 
Apr 10 14:34:31.930: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 10 14:34:32.956: INFO: POD   NODE                                             PHASE    GRACE  CONDITIONS
Apr 10 14:34:32.956: INFO: ss-0  shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:33:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:33:43 +0000 UTC  }]
Apr 10 14:34:32.956: INFO: ss-2  shoot--it--tm-fzm8j-cpu-worker-5c8f889c59-qndrp  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 14:34:04 +0000 UTC  }]
Apr 10 14:34:32.956: INFO: 
Apr 10 14:34:32.956: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 10 14:34:33.982: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.870150361s
Apr 10 14:34:35.007: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.844734504s
Apr 10 14:34:36.035: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.819655488s
Apr 10 14:34:37.060: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.791633779s
Apr 10 14:34:38.086: INFO: Verifying statefulset ss doesn't scale past 0 for another 765.656456ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2904
Apr 10 14:34:39.112: INFO: Scaling statefulset ss to 0
Apr 10 14:34:39.198: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 10 14:34:39.223: INFO: Deleting all statefulset in ns statefulset-2904
Apr 10 14:34:39.247: INFO: Scaling statefulset ss to 0
Apr 10 14:34:39.323: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 14:34:39.348: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:34:39.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2904" for this suite.
Apr 10 14:34:45.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:34:46.475: INFO: namespace statefulset-2904 deletion completed in 7.002627604s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:34:46.475: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3582
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Apr 10 14:34:46.918: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3582,SelfLink:/api/v1/namespaces/watch-3582/configmaps/e2e-watch-test-label-changed,UID:c9fa4cad-5b9d-11e9-99e4-7640922b69f1,ResourceVersion:24295,Generation:0,CreationTimestamp:2019-04-10 14:34:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 10 14:34:46.919: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3582,SelfLink:/api/v1/namespaces/watch-3582/configmaps/e2e-watch-test-label-changed,UID:c9fa4cad-5b9d-11e9-99e4-7640922b69f1,ResourceVersion:24296,Generation:0,CreationTimestamp:2019-04-10 14:34:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Apr 10 14:34:46.919: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3582,SelfLink:/api/v1/namespaces/watch-3582/configmaps/e2e-watch-test-label-changed,UID:c9fa4cad-5b9d-11e9-99e4-7640922b69f1,ResourceVersion:24297,Generation:0,CreationTimestamp:2019-04-10 14:34:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Apr 10 14:34:57.100: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3582,SelfLink:/api/v1/namespaces/watch-3582/configmaps/e2e-watch-test-label-changed,UID:c9fa4cad-5b9d-11e9-99e4-7640922b69f1,ResourceVersion:24320,Generation:0,CreationTimestamp:2019-04-10 14:34:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 10 14:34:57.100: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3582,SelfLink:/api/v1/namespaces/watch-3582/configmaps/e2e-watch-test-label-changed,UID:c9fa4cad-5b9d-11e9-99e4-7640922b69f1,ResourceVersion:24322,Generation:0,CreationTimestamp:2019-04-10 14:34:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Apr 10 14:34:57.100: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3582,SelfLink:/api/v1/namespaces/watch-3582/configmaps/e2e-watch-test-label-changed,UID:c9fa4cad-5b9d-11e9-99e4-7640922b69f1,ResourceVersion:24324,Generation:0,CreationTimestamp:2019-04-10 14:34:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:34:57.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3582" for this suite.
Apr 10 14:35:03.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:35:04.145: INFO: namespace watch-3582 deletion completed in 6.997091663s
•SSSSSApr 10 14:35:04.146: INFO: Running AfterSuite actions on all nodes
Apr 10 14:35:04.146: INFO: Running AfterSuite actions on node 1
Apr 10 14:35:04.146: INFO: Skipping dumping logs from cluster

Ran 204 of 3584 Specs in 6245.853 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Flaked | 0 Pending | 3380 Skipped PASS

Ginkgo ran 1 suite in 1h44m8.476151081s
Test Suite Passed
