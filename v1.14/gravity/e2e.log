I0513 19:15:15.354695      16 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-742566056
I0513 19:15:15.354819      16 e2e.go:240] Starting e2e run "6fb6ff37-75b3-11e9-9153-920e960bc5b9" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1557774914 - Will randomize all specs
Will run 204 of 3584 specs

May 13 19:15:15.487: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
May 13 19:15:15.489: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 13 19:15:15.511: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 13 19:15:15.538: INFO: 6 / 6 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 13 19:15:15.538: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
May 13 19:15:15.538: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 13 19:15:15.550: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'coredns' (0 seconds elapsed)
May 13 19:15:15.550: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'coredns-worker' (0 seconds elapsed)
May 13 19:15:15.550: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'gravity-site' (0 seconds elapsed)
May 13 19:15:15.550: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'log-forwarder' (0 seconds elapsed)
May 13 19:15:15.550: INFO: e2e test version: v1.14.0
May 13 19:15:15.552: INFO: kube-apiserver version: v1.14.0
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:15:15.552: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename configmap
May 13 19:15:15.578: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
May 13 19:15:15.591: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1323
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-7068f2b5-75b3-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume configMaps
May 13 19:15:15.710: INFO: Waiting up to 5m0s for pod "pod-configmaps-70695263-75b3-11e9-9153-920e960bc5b9" in namespace "configmap-1323" to be "success or failure"
May 13 19:15:15.713: INFO: Pod "pod-configmaps-70695263-75b3-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.438331ms
May 13 19:15:17.716: INFO: Pod "pod-configmaps-70695263-75b3-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005191001s
May 13 19:15:19.718: INFO: Pod "pod-configmaps-70695263-75b3-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00781276s
STEP: Saw pod success
May 13 19:15:19.718: INFO: Pod "pod-configmaps-70695263-75b3-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:15:19.720: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-configmaps-70695263-75b3-11e9-9153-920e960bc5b9 container configmap-volume-test: <nil>
STEP: delete the pod
May 13 19:15:19.731: INFO: Waiting for pod pod-configmaps-70695263-75b3-11e9-9153-920e960bc5b9 to disappear
May 13 19:15:19.739: INFO: Pod pod-configmaps-70695263-75b3-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:15:19.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1323" for this suite.
May 13 19:15:27.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:15:27.816: INFO: namespace configmap-1323 deletion completed in 8.074635397s

• [SLOW TEST:12.264 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:15:27.816: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5199
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-5199
May 13 19:15:37.970: INFO: Started pod liveness-http in namespace container-probe-5199
STEP: checking the pod's current state and verifying that restartCount is present
May 13 19:15:37.973: INFO: Initial restart count of pod liveness-http is 0
May 13 19:15:49.991: INFO: Restart count of pod container-probe-5199/liveness-http is now 1 (12.018135816s elapsed)
May 13 19:16:10.023: INFO: Restart count of pod container-probe-5199/liveness-http is now 2 (32.049930642s elapsed)
May 13 19:16:30.051: INFO: Restart count of pod container-probe-5199/liveness-http is now 3 (52.078312827s elapsed)
May 13 19:16:50.077: INFO: Restart count of pod container-probe-5199/liveness-http is now 4 (1m12.104794635s elapsed)
May 13 19:18:04.178: INFO: Restart count of pod container-probe-5199/liveness-http is now 5 (2m26.205581039s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:18:04.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5199" for this suite.
May 13 19:18:10.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:18:10.253: INFO: namespace container-probe-5199 deletion completed in 6.061403939s

• [SLOW TEST:162.437 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:18:10.253: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5137
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0513 19:18:40.904521      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 13 19:18:40.904: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:18:40.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5137" for this suite.
May 13 19:18:46.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:18:46.966: INFO: namespace gc-5137 deletion completed in 6.060023281s

• [SLOW TEST:36.713 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:18:46.966: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5025
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 13 19:18:47.327: INFO: Pod name wrapped-volume-race-ee789ccd-75b3-11e9-9153-920e960bc5b9: Found 1 pods out of 5
May 13 19:18:52.335: INFO: Pod name wrapped-volume-race-ee789ccd-75b3-11e9-9153-920e960bc5b9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ee789ccd-75b3-11e9-9153-920e960bc5b9 in namespace emptydir-wrapper-5025, will wait for the garbage collector to delete the pods
May 13 19:19:02.415: INFO: Deleting ReplicationController wrapped-volume-race-ee789ccd-75b3-11e9-9153-920e960bc5b9 took: 5.935085ms
May 13 19:19:02.515: INFO: Terminating ReplicationController wrapped-volume-race-ee789ccd-75b3-11e9-9153-920e960bc5b9 pods took: 100.257559ms
STEP: Creating RC which spawns configmap-volume pods
May 13 19:19:43.938: INFO: Pod name wrapped-volume-race-1046f8a5-75b4-11e9-9153-920e960bc5b9: Found 0 pods out of 5
May 13 19:19:48.947: INFO: Pod name wrapped-volume-race-1046f8a5-75b4-11e9-9153-920e960bc5b9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1046f8a5-75b4-11e9-9153-920e960bc5b9 in namespace emptydir-wrapper-5025, will wait for the garbage collector to delete the pods
May 13 19:20:01.052: INFO: Deleting ReplicationController wrapped-volume-race-1046f8a5-75b4-11e9-9153-920e960bc5b9 took: 5.954696ms
May 13 19:20:01.452: INFO: Terminating ReplicationController wrapped-volume-race-1046f8a5-75b4-11e9-9153-920e960bc5b9 pods took: 400.247443ms
STEP: Creating RC which spawns configmap-volume pods
May 13 19:20:43.988: INFO: Pod name wrapped-volume-race-340fd092-75b4-11e9-9153-920e960bc5b9: Found 0 pods out of 5
May 13 19:20:49.001: INFO: Pod name wrapped-volume-race-340fd092-75b4-11e9-9153-920e960bc5b9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-340fd092-75b4-11e9-9153-920e960bc5b9 in namespace emptydir-wrapper-5025, will wait for the garbage collector to delete the pods
May 13 19:20:59.081: INFO: Deleting ReplicationController wrapped-volume-race-340fd092-75b4-11e9-9153-920e960bc5b9 took: 4.195174ms
May 13 19:20:59.481: INFO: Terminating ReplicationController wrapped-volume-race-340fd092-75b4-11e9-9153-920e960bc5b9 pods took: 400.233062ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:21:44.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5025" for this suite.
May 13 19:21:50.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:21:50.229: INFO: namespace emptydir-wrapper-5025 deletion completed in 6.098789852s

• [SLOW TEST:183.264 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:21:50.230: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6633
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-nkc4
STEP: Creating a pod to test atomic-volume-subpath
May 13 19:21:50.365: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-nkc4" in namespace "subpath-6633" to be "success or failure"
May 13 19:21:50.368: INFO: Pod "pod-subpath-test-downwardapi-nkc4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.146085ms
May 13 19:21:52.371: INFO: Pod "pod-subpath-test-downwardapi-nkc4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005598158s
May 13 19:21:54.373: INFO: Pod "pod-subpath-test-downwardapi-nkc4": Phase="Running", Reason="", readiness=true. Elapsed: 4.008026576s
May 13 19:21:56.376: INFO: Pod "pod-subpath-test-downwardapi-nkc4": Phase="Running", Reason="", readiness=true. Elapsed: 6.010796154s
May 13 19:21:58.379: INFO: Pod "pod-subpath-test-downwardapi-nkc4": Phase="Running", Reason="", readiness=true. Elapsed: 8.013437597s
May 13 19:22:00.381: INFO: Pod "pod-subpath-test-downwardapi-nkc4": Phase="Running", Reason="", readiness=true. Elapsed: 10.016272463s
May 13 19:22:02.384: INFO: Pod "pod-subpath-test-downwardapi-nkc4": Phase="Running", Reason="", readiness=true. Elapsed: 12.018826256s
May 13 19:22:04.386: INFO: Pod "pod-subpath-test-downwardapi-nkc4": Phase="Running", Reason="", readiness=true. Elapsed: 14.021236792s
May 13 19:22:06.389: INFO: Pod "pod-subpath-test-downwardapi-nkc4": Phase="Running", Reason="", readiness=true. Elapsed: 16.023623983s
May 13 19:22:08.392: INFO: Pod "pod-subpath-test-downwardapi-nkc4": Phase="Running", Reason="", readiness=true. Elapsed: 18.026300675s
May 13 19:22:10.394: INFO: Pod "pod-subpath-test-downwardapi-nkc4": Phase="Running", Reason="", readiness=true. Elapsed: 20.028984936s
May 13 19:22:12.397: INFO: Pod "pod-subpath-test-downwardapi-nkc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.031646057s
STEP: Saw pod success
May 13 19:22:12.397: INFO: Pod "pod-subpath-test-downwardapi-nkc4" satisfied condition "success or failure"
May 13 19:22:12.399: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-subpath-test-downwardapi-nkc4 container test-container-subpath-downwardapi-nkc4: <nil>
STEP: delete the pod
May 13 19:22:12.413: INFO: Waiting for pod pod-subpath-test-downwardapi-nkc4 to disappear
May 13 19:22:12.417: INFO: Pod pod-subpath-test-downwardapi-nkc4 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-nkc4
May 13 19:22:12.417: INFO: Deleting pod "pod-subpath-test-downwardapi-nkc4" in namespace "subpath-6633"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:22:12.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6633" for this suite.
May 13 19:22:18.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:22:18.480: INFO: namespace subpath-6633 deletion completed in 6.059112016s

• [SLOW TEST:28.250 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:22:18.480: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8874
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
May 13 19:22:18.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 create -f - --namespace=kubectl-8874'
May 13 19:22:18.995: INFO: stderr: ""
May 13 19:22:18.995: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 13 19:22:19.998: INFO: Selector matched 1 pods for map[app:redis]
May 13 19:22:19.998: INFO: Found 0 / 1
May 13 19:22:20.998: INFO: Selector matched 1 pods for map[app:redis]
May 13 19:22:20.998: INFO: Found 0 / 1
May 13 19:22:21.998: INFO: Selector matched 1 pods for map[app:redis]
May 13 19:22:21.998: INFO: Found 1 / 1
May 13 19:22:21.998: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 13 19:22:22.000: INFO: Selector matched 1 pods for map[app:redis]
May 13 19:22:22.000: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 13 19:22:22.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 patch pod redis-master-pnf65 --namespace=kubectl-8874 -p {"metadata":{"annotations":{"x":"y"}}}'
May 13 19:22:22.072: INFO: stderr: ""
May 13 19:22:22.072: INFO: stdout: "pod/redis-master-pnf65 patched\n"
STEP: checking annotations
May 13 19:22:22.074: INFO: Selector matched 1 pods for map[app:redis]
May 13 19:22:22.074: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:22:22.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8874" for this suite.
May 13 19:22:44.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:22:44.196: INFO: namespace kubectl-8874 deletion completed in 22.120757383s

• [SLOW TEST:25.717 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:22:44.197: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4028
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-7bceef8b-75b4-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume configMaps
May 13 19:22:44.335: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7bcf4c51-75b4-11e9-9153-920e960bc5b9" in namespace "projected-4028" to be "success or failure"
May 13 19:22:44.338: INFO: Pod "pod-projected-configmaps-7bcf4c51-75b4-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.237963ms
May 13 19:22:46.341: INFO: Pod "pod-projected-configmaps-7bcf4c51-75b4-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005892905s
STEP: Saw pod success
May 13 19:22:46.341: INFO: Pod "pod-projected-configmaps-7bcf4c51-75b4-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:22:46.342: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-projected-configmaps-7bcf4c51-75b4-11e9-9153-920e960bc5b9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 13 19:22:46.355: INFO: Waiting for pod pod-projected-configmaps-7bcf4c51-75b4-11e9-9153-920e960bc5b9 to disappear
May 13 19:22:46.357: INFO: Pod pod-projected-configmaps-7bcf4c51-75b4-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:22:46.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4028" for this suite.
May 13 19:22:52.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:22:52.423: INFO: namespace projected-4028 deletion completed in 6.063944803s

• [SLOW TEST:8.227 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:22:52.424: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7600
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
May 13 19:22:52.547: INFO: namespace kubectl-7600
May 13 19:22:52.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 create -f - --namespace=kubectl-7600'
May 13 19:22:52.685: INFO: stderr: ""
May 13 19:22:52.685: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 13 19:22:53.687: INFO: Selector matched 1 pods for map[app:redis]
May 13 19:22:53.687: INFO: Found 0 / 1
May 13 19:22:54.688: INFO: Selector matched 1 pods for map[app:redis]
May 13 19:22:54.688: INFO: Found 0 / 1
May 13 19:22:55.688: INFO: Selector matched 1 pods for map[app:redis]
May 13 19:22:55.688: INFO: Found 1 / 1
May 13 19:22:55.688: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 13 19:22:55.689: INFO: Selector matched 1 pods for map[app:redis]
May 13 19:22:55.689: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 13 19:22:55.689: INFO: wait on redis-master startup in kubectl-7600 
May 13 19:22:55.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 logs redis-master-cv5nh redis-master --namespace=kubectl-7600'
May 13 19:22:55.762: INFO: stderr: ""
May 13 19:22:55.762: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 May 19:22:53.806 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 May 19:22:53.806 # Server started, Redis version 3.2.12\n1:M 13 May 19:22:53.806 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 May 19:22:53.806 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
May 13 19:22:55.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-7600'
May 13 19:22:55.840: INFO: stderr: ""
May 13 19:22:55.840: INFO: stdout: "service/rm2 exposed\n"
May 13 19:22:55.844: INFO: Service rm2 in namespace kubectl-7600 found.
STEP: exposing service
May 13 19:22:57.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-7600'
May 13 19:22:57.942: INFO: stderr: ""
May 13 19:22:57.942: INFO: stdout: "service/rm3 exposed\n"
May 13 19:22:57.946: INFO: Service rm3 in namespace kubectl-7600 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:22:59.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7600" for this suite.
May 13 19:23:21.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:23:22.016: INFO: namespace kubectl-7600 deletion completed in 22.064241971s

• [SLOW TEST:29.592 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:23:22.016: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5371
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-92594953-75b4-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume secrets
May 13 19:23:22.148: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9259a973-75b4-11e9-9153-920e960bc5b9" in namespace "projected-5371" to be "success or failure"
May 13 19:23:22.152: INFO: Pod "pod-projected-secrets-9259a973-75b4-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.439421ms
May 13 19:23:24.155: INFO: Pod "pod-projected-secrets-9259a973-75b4-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006930614s
May 13 19:23:26.158: INFO: Pod "pod-projected-secrets-9259a973-75b4-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009829968s
STEP: Saw pod success
May 13 19:23:26.158: INFO: Pod "pod-projected-secrets-9259a973-75b4-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:23:26.159: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-projected-secrets-9259a973-75b4-11e9-9153-920e960bc5b9 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 13 19:23:26.171: INFO: Waiting for pod pod-projected-secrets-9259a973-75b4-11e9-9153-920e960bc5b9 to disappear
May 13 19:23:26.175: INFO: Pod pod-projected-secrets-9259a973-75b4-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:23:26.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5371" for this suite.
May 13 19:23:32.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:23:32.237: INFO: namespace projected-5371 deletion completed in 6.059604034s

• [SLOW TEST:10.221 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:23:32.237: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7659
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7659
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7659
STEP: Creating statefulset with conflicting port in namespace statefulset-7659
STEP: Waiting until pod test-pod will start running in namespace statefulset-7659
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7659
May 13 19:23:34.396: INFO: Observed stateful pod in namespace: statefulset-7659, name: ss-0, uid: 994cd76d-75b4-11e9-ac1e-0215dc200466, status phase: Pending. Waiting for statefulset controller to delete.
May 13 19:23:34.782: INFO: Observed stateful pod in namespace: statefulset-7659, name: ss-0, uid: 994cd76d-75b4-11e9-ac1e-0215dc200466, status phase: Failed. Waiting for statefulset controller to delete.
May 13 19:23:34.787: INFO: Observed stateful pod in namespace: statefulset-7659, name: ss-0, uid: 994cd76d-75b4-11e9-ac1e-0215dc200466, status phase: Failed. Waiting for statefulset controller to delete.
May 13 19:23:34.791: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7659
STEP: Removing pod with conflicting port in namespace statefulset-7659
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7659 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 13 19:23:38.812: INFO: Deleting all statefulset in ns statefulset-7659
May 13 19:23:38.814: INFO: Scaling statefulset ss to 0
May 13 19:23:48.824: INFO: Waiting for statefulset status.replicas updated to 0
May 13 19:23:48.826: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:23:48.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7659" for this suite.
May 13 19:23:54.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:23:54.897: INFO: namespace statefulset-7659 deletion completed in 6.060773248s

• [SLOW TEST:22.660 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:23:54.897: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-9999
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 13 19:23:55.022: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 13 19:24:15.068: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.84.49:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9999 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 19:24:15.068: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
May 13 19:24:15.165: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:24:15.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9999" for this suite.
May 13 19:24:37.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:24:37.234: INFO: namespace pod-network-test-9999 deletion completed in 22.066194759s

• [SLOW TEST:42.336 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:24:37.234: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4368
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 19:24:37.364: INFO: (0) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 4.482664ms)
May 13 19:24:37.366: INFO: (1) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 1.848838ms)
May 13 19:24:37.368: INFO: (2) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 1.938278ms)
May 13 19:24:37.370: INFO: (3) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 1.951162ms)
May 13 19:24:37.372: INFO: (4) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 1.786311ms)
May 13 19:24:37.374: INFO: (5) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 1.832639ms)
May 13 19:24:37.376: INFO: (6) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 1.948348ms)
May 13 19:24:37.378: INFO: (7) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 2.610504ms)
May 13 19:24:37.387: INFO: (8) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 9.136204ms)
May 13 19:24:37.390: INFO: (9) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 2.656429ms)
May 13 19:24:37.394: INFO: (10) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 3.482583ms)
May 13 19:24:37.398: INFO: (11) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 4.270256ms)
May 13 19:24:37.406: INFO: (12) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 8.139888ms)
May 13 19:24:37.419: INFO: (13) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 13.266552ms)
May 13 19:24:37.429: INFO: (14) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 9.194061ms)
May 13 19:24:37.441: INFO: (15) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 12.711678ms)
May 13 19:24:37.448: INFO: (16) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 6.433495ms)
May 13 19:24:37.451: INFO: (17) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 3.037821ms)
May 13 19:24:37.454: INFO: (18) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 2.956646ms)
May 13 19:24:37.456: INFO: (19) /api/v1/nodes/ip-10-0-0-248.ec2.internal:10250/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 2.633677ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:24:37.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4368" for this suite.
May 13 19:24:43.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:24:43.524: INFO: namespace proxy-4368 deletion completed in 6.064981083s

• [SLOW TEST:6.290 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:24:43.524: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3797
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-9ft6
STEP: Creating a pod to test atomic-volume-subpath
May 13 19:24:43.663: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9ft6" in namespace "subpath-3797" to be "success or failure"
May 13 19:24:43.668: INFO: Pod "pod-subpath-test-configmap-9ft6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.740673ms
May 13 19:24:45.670: INFO: Pod "pod-subpath-test-configmap-9ft6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007269474s
May 13 19:24:47.673: INFO: Pod "pod-subpath-test-configmap-9ft6": Phase="Running", Reason="", readiness=true. Elapsed: 4.009781387s
May 13 19:24:49.675: INFO: Pod "pod-subpath-test-configmap-9ft6": Phase="Running", Reason="", readiness=true. Elapsed: 6.012222752s
May 13 19:24:51.677: INFO: Pod "pod-subpath-test-configmap-9ft6": Phase="Running", Reason="", readiness=true. Elapsed: 8.014683069s
May 13 19:24:53.680: INFO: Pod "pod-subpath-test-configmap-9ft6": Phase="Running", Reason="", readiness=true. Elapsed: 10.01734741s
May 13 19:24:55.683: INFO: Pod "pod-subpath-test-configmap-9ft6": Phase="Running", Reason="", readiness=true. Elapsed: 12.019957151s
May 13 19:24:57.686: INFO: Pod "pod-subpath-test-configmap-9ft6": Phase="Running", Reason="", readiness=true. Elapsed: 14.023065822s
May 13 19:24:59.690: INFO: Pod "pod-subpath-test-configmap-9ft6": Phase="Running", Reason="", readiness=true. Elapsed: 16.026694619s
May 13 19:25:01.692: INFO: Pod "pod-subpath-test-configmap-9ft6": Phase="Running", Reason="", readiness=true. Elapsed: 18.029499174s
May 13 19:25:03.695: INFO: Pod "pod-subpath-test-configmap-9ft6": Phase="Running", Reason="", readiness=true. Elapsed: 20.032185816s
May 13 19:25:05.698: INFO: Pod "pod-subpath-test-configmap-9ft6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.034941705s
STEP: Saw pod success
May 13 19:25:05.698: INFO: Pod "pod-subpath-test-configmap-9ft6" satisfied condition "success or failure"
May 13 19:25:05.699: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-subpath-test-configmap-9ft6 container test-container-subpath-configmap-9ft6: <nil>
STEP: delete the pod
May 13 19:25:05.713: INFO: Waiting for pod pod-subpath-test-configmap-9ft6 to disappear
May 13 19:25:05.715: INFO: Pod pod-subpath-test-configmap-9ft6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9ft6
May 13 19:25:05.715: INFO: Deleting pod "pod-subpath-test-configmap-9ft6" in namespace "subpath-3797"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:25:05.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3797" for this suite.
May 13 19:25:11.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:25:11.777: INFO: namespace subpath-3797 deletion completed in 6.058805682s

• [SLOW TEST:28.252 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:25:11.777: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5178
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-d3c582f1-75b4-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume secrets
May 13 19:25:11.908: INFO: Waiting up to 5m0s for pod "pod-secrets-d3c5d8d2-75b4-11e9-9153-920e960bc5b9" in namespace "secrets-5178" to be "success or failure"
May 13 19:25:11.910: INFO: Pod "pod-secrets-d3c5d8d2-75b4-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026561ms
May 13 19:25:13.913: INFO: Pod "pod-secrets-d3c5d8d2-75b4-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004637014s
STEP: Saw pod success
May 13 19:25:13.913: INFO: Pod "pod-secrets-d3c5d8d2-75b4-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:25:13.915: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-secrets-d3c5d8d2-75b4-11e9-9153-920e960bc5b9 container secret-volume-test: <nil>
STEP: delete the pod
May 13 19:25:13.927: INFO: Waiting for pod pod-secrets-d3c5d8d2-75b4-11e9-9153-920e960bc5b9 to disappear
May 13 19:25:13.928: INFO: Pod pod-secrets-d3c5d8d2-75b4-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:25:13.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5178" for this suite.
May 13 19:25:19.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:25:19.988: INFO: namespace secrets-5178 deletion completed in 6.058068558s

• [SLOW TEST:8.211 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:25:19.989: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7864
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-d8ac62c8-75b4-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume secrets
May 13 19:25:20.134: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d8acd2de-75b4-11e9-9153-920e960bc5b9" in namespace "projected-7864" to be "success or failure"
May 13 19:25:20.138: INFO: Pod "pod-projected-secrets-d8acd2de-75b4-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.743934ms
May 13 19:25:22.140: INFO: Pod "pod-projected-secrets-d8acd2de-75b4-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006463819s
STEP: Saw pod success
May 13 19:25:22.140: INFO: Pod "pod-projected-secrets-d8acd2de-75b4-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:25:22.142: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-projected-secrets-d8acd2de-75b4-11e9-9153-920e960bc5b9 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 13 19:25:22.156: INFO: Waiting for pod pod-projected-secrets-d8acd2de-75b4-11e9-9153-920e960bc5b9 to disappear
May 13 19:25:22.159: INFO: Pod pod-projected-secrets-d8acd2de-75b4-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:25:22.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7864" for this suite.
May 13 19:25:28.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:25:28.220: INFO: namespace projected-7864 deletion completed in 6.059121592s

• [SLOW TEST:8.232 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:25:28.221: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3196
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-dd927852-75b4-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume configMaps
May 13 19:25:28.351: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dd92cabb-75b4-11e9-9153-920e960bc5b9" in namespace "projected-3196" to be "success or failure"
May 13 19:25:28.353: INFO: Pod "pod-projected-configmaps-dd92cabb-75b4-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.698352ms
May 13 19:25:30.355: INFO: Pod "pod-projected-configmaps-dd92cabb-75b4-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003799943s
STEP: Saw pod success
May 13 19:25:30.355: INFO: Pod "pod-projected-configmaps-dd92cabb-75b4-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:25:30.357: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-projected-configmaps-dd92cabb-75b4-11e9-9153-920e960bc5b9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 13 19:25:30.370: INFO: Waiting for pod pod-projected-configmaps-dd92cabb-75b4-11e9-9153-920e960bc5b9 to disappear
May 13 19:25:30.373: INFO: Pod pod-projected-configmaps-dd92cabb-75b4-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:25:30.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3196" for this suite.
May 13 19:25:36.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:25:36.433: INFO: namespace projected-3196 deletion completed in 6.057630483s

• [SLOW TEST:8.212 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:25:36.433: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9343
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 19:25:36.561: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e27783d6-75b4-11e9-9153-920e960bc5b9" in namespace "downward-api-9343" to be "success or failure"
May 13 19:25:36.565: INFO: Pod "downwardapi-volume-e27783d6-75b4-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.317657ms
May 13 19:25:38.568: INFO: Pod "downwardapi-volume-e27783d6-75b4-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007204641s
STEP: Saw pod success
May 13 19:25:38.568: INFO: Pod "downwardapi-volume-e27783d6-75b4-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:25:38.570: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downwardapi-volume-e27783d6-75b4-11e9-9153-920e960bc5b9 container client-container: <nil>
STEP: delete the pod
May 13 19:25:38.585: INFO: Waiting for pod downwardapi-volume-e27783d6-75b4-11e9-9153-920e960bc5b9 to disappear
May 13 19:25:38.593: INFO: Pod downwardapi-volume-e27783d6-75b4-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:25:38.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9343" for this suite.
May 13 19:25:44.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:25:44.662: INFO: namespace downward-api-9343 deletion completed in 6.065181937s

• [SLOW TEST:8.229 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:25:44.662: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3498
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-e75f39ac-75b4-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume secrets
May 13 19:25:44.792: INFO: Waiting up to 5m0s for pod "pod-secrets-e75f9558-75b4-11e9-9153-920e960bc5b9" in namespace "secrets-3498" to be "success or failure"
May 13 19:25:44.795: INFO: Pod "pod-secrets-e75f9558-75b4-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.662945ms
May 13 19:25:46.797: INFO: Pod "pod-secrets-e75f9558-75b4-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005310854s
STEP: Saw pod success
May 13 19:25:46.797: INFO: Pod "pod-secrets-e75f9558-75b4-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:25:46.799: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-secrets-e75f9558-75b4-11e9-9153-920e960bc5b9 container secret-env-test: <nil>
STEP: delete the pod
May 13 19:25:46.811: INFO: Waiting for pod pod-secrets-e75f9558-75b4-11e9-9153-920e960bc5b9 to disappear
May 13 19:25:46.815: INFO: Pod pod-secrets-e75f9558-75b4-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:25:46.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3498" for this suite.
May 13 19:25:52.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:25:52.881: INFO: namespace secrets-3498 deletion completed in 6.063710299s

• [SLOW TEST:8.219 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:25:52.881: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7652
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 19:25:53.003: INFO: Creating deployment "test-recreate-deployment"
May 13 19:25:53.007: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 13 19:25:53.012: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 13 19:25:55.016: INFO: Waiting deployment "test-recreate-deployment" to complete
May 13 19:25:55.018: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 13 19:25:55.024: INFO: Updating deployment test-recreate-deployment
May 13 19:25:55.024: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 13 19:25:55.110: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-7652,SelfLink:/apis/apps/v1/namespaces/deployment-7652/deployments/test-recreate-deployment,UID:ec458250-75b4-11e9-ac1e-0215dc200466,ResourceVersion:4369,Generation:2,CreationTimestamp:2019-05-13 19:25:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update apps/v1 2019-05-13 19:25:53 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:name:{map[]}]}]},f:spec: {map[f:progressDeadlineSeconds:{map[]} f:replicas:{map[]} f:revisionHistoryLimit:{map[]} f:selector:{map[f:matchLabels:{map[.:{map[]} f:name:{map[]}]}]} f:strategy:{map[f:type:{map[]}]} f:template:{map[f:metadata:{map[f:labels:{map[.:{map[]} f:name:{map[]}]}]} f:spec:{map[f:dnsPolicy:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:terminationGracePeriodSeconds:{map[]}]}]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 19:25:53 +0000 UTC &Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[]}]},f:status: {map[f:conditions:{map[.:{map[]} k:{"type":"Available"}:{map[.:{map[]} f:type:{map[]}]} k:{"type":"Progressing"}:{map[.:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]}]}]},},}} {e2e.test Update apps/v1 2019-05-13 19:25:55 +0000 UTC &Fields{Map:map[string]Fields{f:spec: {map[f:template:{map[f:spec:{map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]}]}]}]}]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 19:25:55 +0000 UTC &Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[f:deployment.kubernetes.io/revision:{map[]}]}]},f:status: {map[f:conditions:{map[k:{"type":"Available"}:{map[f:lastTransitionTime:{map[]} f:lastUpdateTime:{map[]} f:message:{map[]} f:reason:{map[]} f:status:{map[]}]} k:{"type":"Progressing"}:{map[f:lastUpdateTime:{map[]} f:message:{map[]} f:reason:{map[]}]}]} f:observedGeneration:{map[]} f:replicas:{map[]} f:unavailableReplicas:{map[]} f:updatedReplicas:{map[]}]},},}}],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-05-13 19:25:55 +0000 UTC 2019-05-13 19:25:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-05-13 19:25:55 +0000 UTC 2019-05-13 19:25:53 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

May 13 19:25:55.114: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-7652,SelfLink:/apis/apps/v1/namespaces/deployment-7652/replicasets/test-recreate-deployment-c9cbd8684,UID:ed7e1145-75b4-11e9-ac1e-0215dc200466,ResourceVersion:4366,Generation:1,CreationTimestamp:2019-05-13 19:25:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment ec458250-75b4-11e9-ac1e-0215dc200466 0xc002df94e0 0xc002df94e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update apps/v1 2019-05-13 19:25:55 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:deployment.kubernetes.io/desired-replicas:{map[]} f:deployment.kubernetes.io/max-replicas:{map[]} f:deployment.kubernetes.io/revision:{map[]}]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"ec458250-75b4-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:replicas:{map[]} f:selector:{map[f:matchLabels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]}]} f:template:{map[f:metadata:{map[f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]}]} f:spec:{map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]}]}]} f:dnsPolicy:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:terminationGracePeriodSeconds:{map[]}]}]}]},f:status: {map[f:fullyLabeledReplicas:{map[]} f:observedGeneration:{map[]} f:replicas:{map[]}]},},}}],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 13 19:25:55.114: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 13 19:25:55.114: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-7652,SelfLink:/apis/apps/v1/namespaces/deployment-7652/replicasets/test-recreate-deployment-7d57d5ff7c,UID:ec46229d-75b4-11e9-ac1e-0215dc200466,ResourceVersion:4357,Generation:2,CreationTimestamp:2019-05-13 19:25:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment ec458250-75b4-11e9-ac1e-0215dc200466 0xc002df92b7 0xc002df92b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update apps/v1 2019-05-13 19:25:53 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:deployment.kubernetes.io/desired-replicas:{map[]} f:deployment.kubernetes.io/max-replicas:{map[]} f:deployment.kubernetes.io/revision:{map[]}]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"ec458250-75b4-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:selector:{map[f:matchLabels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]}]} f:template:{map[f:metadata:{map[f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]}]} f:spec:{map[f:containers:{map[k:{"name":"redis"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]}]}]} f:dnsPolicy:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:terminationGracePeriodSeconds:{map[]}]}]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 19:25:55 +0000 UTC &Fields{Map:map[string]Fields{f:spec: {map[f:replicas:{map[]}]},f:status: {map[f:observedGeneration:{map[]} f:replicas:{map[]}]},},}}],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 13 19:25:55.116: INFO: Pod "test-recreate-deployment-c9cbd8684-j69b4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-j69b4,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-7652,SelfLink:/api/v1/namespaces/deployment-7652/pods/test-recreate-deployment-c9cbd8684-j69b4,UID:ed7e9fdd-75b4-11e9-ac1e-0215dc200466,ResourceVersion:4368,Generation:0,CreationTimestamp:2019-05-13 19:25:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 ed7e1145-75b4-11e9-ac1e-0215dc200466 0xc002df9e50 0xc002df9e51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:25:55 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"ed7e1145-75b4-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-lrqbn"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}} {kubelet Update v1 2019-05-13 19:25:55 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:message:{map[]} f:reason:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Initialized"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Ready"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:message:{map[]} f:reason:{map[]} f:status:{map[]} f:type:{map[]}]}]} f:containerStatuses:{map[]} f:hostIP:{map[]} f:startTime:{map[]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-lrqbn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lrqbn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lrqbn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:25:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:25:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:25:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:25:55 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.248,PodIP:,StartTime:2019-05-13 19:25:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:25:55.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7652" for this suite.
May 13 19:26:01.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:26:01.178: INFO: namespace deployment-7652 deletion completed in 6.059512102s

• [SLOW TEST:8.297 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:26:01.178: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5480
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
May 13 19:26:01.814: INFO: created pod pod-service-account-defaultsa
May 13 19:26:01.814: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 13 19:26:01.820: INFO: created pod pod-service-account-mountsa
May 13 19:26:01.820: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 13 19:26:01.825: INFO: created pod pod-service-account-nomountsa
May 13 19:26:01.825: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 13 19:26:01.830: INFO: created pod pod-service-account-defaultsa-mountspec
May 13 19:26:01.830: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 13 19:26:01.836: INFO: created pod pod-service-account-mountsa-mountspec
May 13 19:26:01.836: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 13 19:26:01.847: INFO: created pod pod-service-account-nomountsa-mountspec
May 13 19:26:01.847: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 13 19:26:01.858: INFO: created pod pod-service-account-defaultsa-nomountspec
May 13 19:26:01.858: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 13 19:26:01.872: INFO: created pod pod-service-account-mountsa-nomountspec
May 13 19:26:01.872: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 13 19:26:01.881: INFO: created pod pod-service-account-nomountsa-nomountspec
May 13 19:26:01.881: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:26:01.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5480" for this suite.
May 13 19:27:05.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:27:05.960: INFO: namespace svcaccounts-5480 deletion completed in 1m4.07420157s

• [SLOW TEST:64.782 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:27:05.960: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
May 13 19:27:06.084: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-742566056 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:27:06.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2621" for this suite.
May 13 19:27:12.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:27:12.202: INFO: namespace kubectl-2621 deletion completed in 6.060550442s

• [SLOW TEST:6.242 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:27:12.202: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2618
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
May 13 19:27:12.332: INFO: Waiting up to 5m0s for pod "pod-1b8ce878-75b5-11e9-9153-920e960bc5b9" in namespace "emptydir-2618" to be "success or failure"
May 13 19:27:12.339: INFO: Pod "pod-1b8ce878-75b5-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.314132ms
May 13 19:27:14.342: INFO: Pod "pod-1b8ce878-75b5-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009096679s
May 13 19:27:16.344: INFO: Pod "pod-1b8ce878-75b5-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011722382s
STEP: Saw pod success
May 13 19:27:16.344: INFO: Pod "pod-1b8ce878-75b5-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:27:16.346: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-1b8ce878-75b5-11e9-9153-920e960bc5b9 container test-container: <nil>
STEP: delete the pod
May 13 19:27:16.358: INFO: Waiting for pod pod-1b8ce878-75b5-11e9-9153-920e960bc5b9 to disappear
May 13 19:27:16.368: INFO: Pod pod-1b8ce878-75b5-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:27:16.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2618" for this suite.
May 13 19:27:22.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:27:22.431: INFO: namespace emptydir-2618 deletion completed in 6.060650735s

• [SLOW TEST:10.229 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:27:22.431: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4785
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
May 13 19:27:22.560: INFO: Waiting up to 5m0s for pod "pod-21a5b777-75b5-11e9-9153-920e960bc5b9" in namespace "emptydir-4785" to be "success or failure"
May 13 19:27:22.562: INFO: Pod "pod-21a5b777-75b5-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.941744ms
May 13 19:27:24.565: INFO: Pod "pod-21a5b777-75b5-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00498723s
STEP: Saw pod success
May 13 19:27:24.565: INFO: Pod "pod-21a5b777-75b5-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:27:24.567: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-21a5b777-75b5-11e9-9153-920e960bc5b9 container test-container: <nil>
STEP: delete the pod
May 13 19:27:24.581: INFO: Waiting for pod pod-21a5b777-75b5-11e9-9153-920e960bc5b9 to disappear
May 13 19:27:24.583: INFO: Pod pod-21a5b777-75b5-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:27:24.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4785" for this suite.
May 13 19:27:30.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:27:30.644: INFO: namespace emptydir-4785 deletion completed in 6.059500704s

• [SLOW TEST:8.213 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:27:30.644: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8936
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 19:27:30.779: INFO: Waiting up to 5m0s for pod "downwardapi-volume-268ba5eb-75b5-11e9-9153-920e960bc5b9" in namespace "projected-8936" to be "success or failure"
May 13 19:27:30.783: INFO: Pod "downwardapi-volume-268ba5eb-75b5-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.09749ms
May 13 19:27:32.786: INFO: Pod "downwardapi-volume-268ba5eb-75b5-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006813226s
STEP: Saw pod success
May 13 19:27:32.786: INFO: Pod "downwardapi-volume-268ba5eb-75b5-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:27:32.788: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downwardapi-volume-268ba5eb-75b5-11e9-9153-920e960bc5b9 container client-container: <nil>
STEP: delete the pod
May 13 19:27:32.801: INFO: Waiting for pod downwardapi-volume-268ba5eb-75b5-11e9-9153-920e960bc5b9 to disappear
May 13 19:27:32.803: INFO: Pod downwardapi-volume-268ba5eb-75b5-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:27:32.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8936" for this suite.
May 13 19:27:38.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:27:38.871: INFO: namespace projected-8936 deletion completed in 6.065752175s

• [SLOW TEST:8.227 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:27:38.871: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9988
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-2b72348d-75b5-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume secrets
May 13 19:27:39.002: INFO: Waiting up to 5m0s for pod "pod-secrets-2b728920-75b5-11e9-9153-920e960bc5b9" in namespace "secrets-9988" to be "success or failure"
May 13 19:27:39.004: INFO: Pod "pod-secrets-2b728920-75b5-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.929167ms
May 13 19:27:41.006: INFO: Pod "pod-secrets-2b728920-75b5-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00465809s
STEP: Saw pod success
May 13 19:27:41.007: INFO: Pod "pod-secrets-2b728920-75b5-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:27:41.008: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-secrets-2b728920-75b5-11e9-9153-920e960bc5b9 container secret-volume-test: <nil>
STEP: delete the pod
May 13 19:27:41.022: INFO: Waiting for pod pod-secrets-2b728920-75b5-11e9-9153-920e960bc5b9 to disappear
May 13 19:27:41.024: INFO: Pod pod-secrets-2b728920-75b5-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:27:41.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9988" for this suite.
May 13 19:27:47.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:27:47.087: INFO: namespace secrets-9988 deletion completed in 6.060976464s

• [SLOW TEST:8.215 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:27:47.087: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2086
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 13 19:27:51.241: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 13 19:27:51.243: INFO: Pod pod-with-poststart-http-hook still exists
May 13 19:27:53.244: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 13 19:27:53.246: INFO: Pod pod-with-poststart-http-hook still exists
May 13 19:27:55.244: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 13 19:27:55.246: INFO: Pod pod-with-poststart-http-hook still exists
May 13 19:27:57.244: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 13 19:27:57.246: INFO: Pod pod-with-poststart-http-hook still exists
May 13 19:27:59.244: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 13 19:27:59.246: INFO: Pod pod-with-poststart-http-hook still exists
May 13 19:28:01.244: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 13 19:28:01.246: INFO: Pod pod-with-poststart-http-hook still exists
May 13 19:28:03.244: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 13 19:28:03.246: INFO: Pod pod-with-poststart-http-hook still exists
May 13 19:28:05.244: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 13 19:28:05.246: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:28:05.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2086" for this suite.
May 13 19:28:27.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:28:27.308: INFO: namespace container-lifecycle-hook-2086 deletion completed in 22.060125463s

• [SLOW TEST:40.221 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:28:27.308: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9801
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
May 13 19:28:27.438: INFO: Waiting up to 5m0s for pod "var-expansion-48512e16-75b5-11e9-9153-920e960bc5b9" in namespace "var-expansion-9801" to be "success or failure"
May 13 19:28:27.439: INFO: Pod "var-expansion-48512e16-75b5-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.594046ms
May 13 19:28:29.442: INFO: Pod "var-expansion-48512e16-75b5-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004133207s
May 13 19:28:31.444: INFO: Pod "var-expansion-48512e16-75b5-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006552848s
STEP: Saw pod success
May 13 19:28:31.444: INFO: Pod "var-expansion-48512e16-75b5-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:28:31.446: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod var-expansion-48512e16-75b5-11e9-9153-920e960bc5b9 container dapi-container: <nil>
STEP: delete the pod
May 13 19:28:31.461: INFO: Waiting for pod var-expansion-48512e16-75b5-11e9-9153-920e960bc5b9 to disappear
May 13 19:28:31.463: INFO: Pod var-expansion-48512e16-75b5-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:28:31.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9801" for this suite.
May 13 19:28:37.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:28:37.523: INFO: namespace var-expansion-9801 deletion completed in 6.0580164s

• [SLOW TEST:10.216 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:28:37.524: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3094
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 19:28:37.660: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e6825d7-75b5-11e9-9153-920e960bc5b9" in namespace "downward-api-3094" to be "success or failure"
May 13 19:28:37.668: INFO: Pod "downwardapi-volume-4e6825d7-75b5-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.428247ms
May 13 19:28:39.670: INFO: Pod "downwardapi-volume-4e6825d7-75b5-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010122572s
May 13 19:28:41.673: INFO: Pod "downwardapi-volume-4e6825d7-75b5-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012616794s
STEP: Saw pod success
May 13 19:28:41.673: INFO: Pod "downwardapi-volume-4e6825d7-75b5-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:28:41.675: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downwardapi-volume-4e6825d7-75b5-11e9-9153-920e960bc5b9 container client-container: <nil>
STEP: delete the pod
May 13 19:28:41.692: INFO: Waiting for pod downwardapi-volume-4e6825d7-75b5-11e9-9153-920e960bc5b9 to disappear
May 13 19:28:41.693: INFO: Pod downwardapi-volume-4e6825d7-75b5-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:28:41.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3094" for this suite.
May 13 19:28:47.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:28:47.755: INFO: namespace downward-api-3094 deletion completed in 6.059626461s

• [SLOW TEST:10.232 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:28:47.756: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7164
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 13 19:28:47.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-7164'
May 13 19:28:47.958: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 13 19:28:47.958: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
May 13 19:28:49.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 delete deployment e2e-test-nginx-deployment --namespace=kubectl-7164'
May 13 19:28:50.039: INFO: stderr: ""
May 13 19:28:50.039: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:28:50.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7164" for this suite.
May 13 19:29:12.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:29:12.111: INFO: namespace kubectl-7164 deletion completed in 22.063097153s

• [SLOW TEST:24.356 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:29:12.111: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5915
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-5915/configmap-test-6305839c-75b5-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume configMaps
May 13 19:29:12.242: INFO: Waiting up to 5m0s for pod "pod-configmaps-6305e5ad-75b5-11e9-9153-920e960bc5b9" in namespace "configmap-5915" to be "success or failure"
May 13 19:29:12.245: INFO: Pod "pod-configmaps-6305e5ad-75b5-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.657905ms
May 13 19:29:14.248: INFO: Pod "pod-configmaps-6305e5ad-75b5-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006227775s
STEP: Saw pod success
May 13 19:29:14.248: INFO: Pod "pod-configmaps-6305e5ad-75b5-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:29:14.250: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-configmaps-6305e5ad-75b5-11e9-9153-920e960bc5b9 container env-test: <nil>
STEP: delete the pod
May 13 19:29:14.263: INFO: Waiting for pod pod-configmaps-6305e5ad-75b5-11e9-9153-920e960bc5b9 to disappear
May 13 19:29:14.264: INFO: Pod pod-configmaps-6305e5ad-75b5-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:29:14.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5915" for this suite.
May 13 19:29:20.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:29:20.347: INFO: namespace configmap-5915 deletion completed in 6.080515717s

• [SLOW TEST:8.236 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:29:20.347: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4722
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 13 19:29:24.501: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 19:29:24.503: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 19:29:26.503: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 19:29:26.505: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 19:29:28.503: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 19:29:28.505: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 19:29:30.503: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 19:29:30.506: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 19:29:32.503: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 19:29:32.505: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 19:29:34.503: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 19:29:34.506: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 19:29:36.503: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 19:29:36.505: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 19:29:38.503: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 19:29:38.506: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 19:29:40.503: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 19:29:40.505: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 19:29:42.503: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 19:29:42.505: INFO: Pod pod-with-prestop-exec-hook still exists
May 13 19:29:44.503: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 13 19:29:44.505: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:29:44.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4722" for this suite.
May 13 19:30:06.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:30:06.571: INFO: namespace container-lifecycle-hook-4722 deletion completed in 22.058811853s

• [SLOW TEST:46.224 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:30:06.571: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2601
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 13 19:30:06.711: INFO: Number of nodes with available pods: 0
May 13 19:30:06.711: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 19:30:07.716: INFO: Number of nodes with available pods: 0
May 13 19:30:07.716: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 19:30:08.716: INFO: Number of nodes with available pods: 1
May 13 19:30:08.716: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 13 19:30:08.727: INFO: Number of nodes with available pods: 0
May 13 19:30:08.727: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 19:30:09.731: INFO: Number of nodes with available pods: 0
May 13 19:30:09.731: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 19:30:10.731: INFO: Number of nodes with available pods: 0
May 13 19:30:10.731: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 19:30:11.731: INFO: Number of nodes with available pods: 0
May 13 19:30:11.731: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 19:30:12.731: INFO: Number of nodes with available pods: 0
May 13 19:30:12.731: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 19:30:13.732: INFO: Number of nodes with available pods: 1
May 13 19:30:13.732: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2601, will wait for the garbage collector to delete the pods
May 13 19:30:13.788: INFO: Deleting DaemonSet.extensions daemon-set took: 3.14398ms
May 13 19:30:14.189: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.318216ms
May 13 19:30:23.591: INFO: Number of nodes with available pods: 0
May 13 19:30:23.591: INFO: Number of running nodes: 0, number of available pods: 0
May 13 19:30:23.594: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2601/daemonsets","resourceVersion":"5256"},"items":null}

May 13 19:30:23.595: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2601/pods","resourceVersion":"5256"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:30:23.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2601" for this suite.
May 13 19:30:29.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:30:29.662: INFO: namespace daemonsets-2601 deletion completed in 6.061418488s

• [SLOW TEST:23.091 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:30:29.663: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2886
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-913ee2e5-75b5-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume secrets
May 13 19:30:29.794: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-913f3f6b-75b5-11e9-9153-920e960bc5b9" in namespace "projected-2886" to be "success or failure"
May 13 19:30:29.795: INFO: Pod "pod-projected-secrets-913f3f6b-75b5-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.722025ms
May 13 19:30:31.798: INFO: Pod "pod-projected-secrets-913f3f6b-75b5-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004274085s
STEP: Saw pod success
May 13 19:30:31.798: INFO: Pod "pod-projected-secrets-913f3f6b-75b5-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:30:31.800: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-projected-secrets-913f3f6b-75b5-11e9-9153-920e960bc5b9 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 13 19:30:31.813: INFO: Waiting for pod pod-projected-secrets-913f3f6b-75b5-11e9-9153-920e960bc5b9 to disappear
May 13 19:30:31.815: INFO: Pod pod-projected-secrets-913f3f6b-75b5-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:30:31.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2886" for this suite.
May 13 19:30:37.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:30:37.890: INFO: namespace projected-2886 deletion completed in 6.07312516s

• [SLOW TEST:8.227 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:30:37.890: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8801
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-8801
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
May 13 19:30:38.031: INFO: Found 0 stateful pods, waiting for 3
May 13 19:30:48.034: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 13 19:30:48.034: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 13 19:30:48.034: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 13 19:30:48.057: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 13 19:30:58.091: INFO: Updating stateful set ss2
May 13 19:30:58.097: INFO: Waiting for Pod statefulset-8801/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
May 13 19:31:08.183: INFO: Found 2 stateful pods, waiting for 3
May 13 19:31:18.187: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 13 19:31:18.187: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 13 19:31:18.187: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 13 19:31:18.211: INFO: Updating stateful set ss2
May 13 19:31:18.216: INFO: Waiting for Pod statefulset-8801/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 13 19:31:28.239: INFO: Updating stateful set ss2
May 13 19:31:28.244: INFO: Waiting for StatefulSet statefulset-8801/ss2 to complete update
May 13 19:31:28.244: INFO: Waiting for Pod statefulset-8801/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 13 19:31:38.250: INFO: Deleting all statefulset in ns statefulset-8801
May 13 19:31:38.252: INFO: Scaling statefulset ss2 to 0
May 13 19:31:48.265: INFO: Waiting for statefulset status.replicas updated to 0
May 13 19:31:48.267: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:31:48.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8801" for this suite.
May 13 19:31:54.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:31:54.337: INFO: namespace statefulset-8801 deletion completed in 6.061719462s

• [SLOW TEST:76.447 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:31:54.337: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9339
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 19:31:54.467: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c3b76b7d-75b5-11e9-9153-920e960bc5b9" in namespace "downward-api-9339" to be "success or failure"
May 13 19:31:54.470: INFO: Pod "downwardapi-volume-c3b76b7d-75b5-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.330737ms
May 13 19:31:56.473: INFO: Pod "downwardapi-volume-c3b76b7d-75b5-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00597208s
STEP: Saw pod success
May 13 19:31:56.473: INFO: Pod "downwardapi-volume-c3b76b7d-75b5-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:31:56.475: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downwardapi-volume-c3b76b7d-75b5-11e9-9153-920e960bc5b9 container client-container: <nil>
STEP: delete the pod
May 13 19:31:56.487: INFO: Waiting for pod downwardapi-volume-c3b76b7d-75b5-11e9-9153-920e960bc5b9 to disappear
May 13 19:31:56.489: INFO: Pod downwardapi-volume-c3b76b7d-75b5-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:31:56.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9339" for this suite.
May 13 19:32:02.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:32:02.552: INFO: namespace downward-api-9339 deletion completed in 6.061103494s

• [SLOW TEST:8.215 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:32:02.553: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2655
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
May 13 19:32:02.681: INFO: Waiting up to 5m0s for pod "pod-c89cc75e-75b5-11e9-9153-920e960bc5b9" in namespace "emptydir-2655" to be "success or failure"
May 13 19:32:02.683: INFO: Pod "pod-c89cc75e-75b5-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.971047ms
May 13 19:32:04.686: INFO: Pod "pod-c89cc75e-75b5-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004775056s
STEP: Saw pod success
May 13 19:32:04.686: INFO: Pod "pod-c89cc75e-75b5-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:32:04.688: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-c89cc75e-75b5-11e9-9153-920e960bc5b9 container test-container: <nil>
STEP: delete the pod
May 13 19:32:04.702: INFO: Waiting for pod pod-c89cc75e-75b5-11e9-9153-920e960bc5b9 to disappear
May 13 19:32:04.703: INFO: Pod pod-c89cc75e-75b5-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:32:04.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2655" for this suite.
May 13 19:32:10.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:32:10.771: INFO: namespace emptydir-2655 deletion completed in 6.065083971s

• [SLOW TEST:8.218 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:32:10.771: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7572
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
May 13 19:32:10.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 api-versions'
May 13 19:32:10.962: INFO: stderr: ""
May 13 19:32:10.962: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\nchangeset.gravitational.io/v1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:32:10.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7572" for this suite.
May 13 19:32:16.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:32:17.024: INFO: namespace kubectl-7572 deletion completed in 6.059288065s

• [SLOW TEST:6.253 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:32:17.024: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7946
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 13 19:32:17.151: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
May 13 19:32:24.201: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:32:24.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7946" for this suite.
May 13 19:32:30.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:32:30.266: INFO: namespace pods-7946 deletion completed in 6.060013385s

• [SLOW TEST:13.242 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:32:30.266: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7773
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7773.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7773.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7773.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7773.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7773.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7773.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7773.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7773.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7773.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7773.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7773.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 69.45.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.45.69_udp@PTR;check="$$(dig +tcp +noall +answer +search 69.45.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.45.69_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7773.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7773.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7773.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7773.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7773.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7773.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7773.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7773.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7773.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7773.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7773.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 69.45.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.45.69_udp@PTR;check="$$(dig +tcp +noall +answer +search 69.45.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.45.69_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 13 19:32:40.425: INFO: Unable to read wheezy_udp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:40.427: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:40.429: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:40.432: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:40.438: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:40.440: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:40.446: INFO: Unable to read jessie_udp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:40.451: INFO: Unable to read jessie_tcp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:40.453: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:40.456: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:40.463: INFO: Unable to read jessie_udp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:40.465: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:40.468: INFO: Lookups using dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9 failed for: [wheezy_udp@dns-test-service.dns-7773.svc.cluster.local wheezy_tcp@dns-test-service.dns-7773.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-7773.svc.cluster.local jessie_tcp@dns-test-service.dns-7773.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

May 13 19:32:45.471: INFO: Unable to read wheezy_udp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:45.473: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:45.475: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:45.477: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:45.482: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:45.484: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:45.489: INFO: Unable to read jessie_udp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:45.491: INFO: Unable to read jessie_tcp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:45.493: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:45.494: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:45.500: INFO: Unable to read jessie_udp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:45.502: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:45.505: INFO: Lookups using dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9 failed for: [wheezy_udp@dns-test-service.dns-7773.svc.cluster.local wheezy_tcp@dns-test-service.dns-7773.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-7773.svc.cluster.local jessie_tcp@dns-test-service.dns-7773.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

May 13 19:32:50.472: INFO: Unable to read wheezy_udp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:50.479: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:50.481: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:50.483: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:50.488: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:50.490: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:50.495: INFO: Unable to read jessie_udp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:50.497: INFO: Unable to read jessie_tcp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:50.499: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:50.501: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:50.506: INFO: Unable to read jessie_udp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:50.507: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:50.511: INFO: Lookups using dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9 failed for: [wheezy_udp@dns-test-service.dns-7773.svc.cluster.local wheezy_tcp@dns-test-service.dns-7773.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-7773.svc.cluster.local jessie_tcp@dns-test-service.dns-7773.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

May 13 19:32:55.472: INFO: Unable to read wheezy_udp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:55.475: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:55.480: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:55.486: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:55.496: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:55.499: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:55.507: INFO: Unable to read jessie_udp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:55.509: INFO: Unable to read jessie_tcp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:55.512: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:55.516: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:55.523: INFO: Unable to read jessie_udp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:55.525: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:32:55.529: INFO: Lookups using dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9 failed for: [wheezy_udp@dns-test-service.dns-7773.svc.cluster.local wheezy_tcp@dns-test-service.dns-7773.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-7773.svc.cluster.local jessie_tcp@dns-test-service.dns-7773.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

May 13 19:33:00.472: INFO: Unable to read wheezy_udp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:33:00.474: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:33:00.476: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:33:00.478: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:33:00.487: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:33:00.489: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:33:00.499: INFO: Unable to read jessie_udp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:33:00.501: INFO: Unable to read jessie_tcp@dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:33:00.503: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:33:00.505: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:33:00.510: INFO: Unable to read jessie_udp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:33:00.512: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9)
May 13 19:33:00.515: INFO: Lookups using dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9 failed for: [wheezy_udp@dns-test-service.dns-7773.svc.cluster.local wheezy_tcp@dns-test-service.dns-7773.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-7773.svc.cluster.local jessie_tcp@dns-test-service.dns-7773.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7773.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

May 13 19:33:05.506: INFO: DNS probes using dns-7773/dns-test-d922e7f6-75b5-11e9-9153-920e960bc5b9 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:33:05.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7773" for this suite.
May 13 19:33:11.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:33:11.627: INFO: namespace dns-7773 deletion completed in 6.072718792s

• [SLOW TEST:41.362 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:33:11.628: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8467
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 13 19:33:15.781: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 13 19:33:15.784: INFO: Pod pod-with-prestop-http-hook still exists
May 13 19:33:17.784: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 13 19:33:17.787: INFO: Pod pod-with-prestop-http-hook still exists
May 13 19:33:19.784: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 13 19:33:19.787: INFO: Pod pod-with-prestop-http-hook still exists
May 13 19:33:21.784: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 13 19:33:21.787: INFO: Pod pod-with-prestop-http-hook still exists
May 13 19:33:23.784: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 13 19:33:23.787: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:33:23.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8467" for this suite.
May 13 19:33:45.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:33:45.852: INFO: namespace container-lifecycle-hook-8467 deletion completed in 22.058665836s

• [SLOW TEST:34.224 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:33:45.852: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 13 19:33:50.006: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 19:33:50.008: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 19:33:52.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 19:33:52.010: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 19:33:54.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 19:33:54.010: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 19:33:56.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 19:33:56.011: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 19:33:58.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 19:33:58.011: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 19:34:00.009: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 19:34:00.023: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 19:34:02.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 19:34:02.010: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 19:34:04.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 19:34:04.010: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 19:34:06.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 19:34:06.010: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 19:34:08.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 19:34:08.010: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 19:34:10.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 19:34:10.010: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 19:34:12.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 19:34:12.011: INFO: Pod pod-with-poststart-exec-hook still exists
May 13 19:34:14.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 13 19:34:14.010: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:34:14.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6986" for this suite.
May 13 19:34:36.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:34:36.074: INFO: namespace container-lifecycle-hook-6986 deletion completed in 22.061874352s

• [SLOW TEST:50.222 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:34:36.074: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 13 19:34:36.202: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9484,SelfLink:/api/v1/namespaces/watch-9484/configmaps/e2e-watch-test-configmap-a,UID:241ee205-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6201,Generation:0,CreationTimestamp:2019-05-13 19:34:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 19:34:36 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}}],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 13 19:34:36.203: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9484,SelfLink:/api/v1/namespaces/watch-9484/configmaps/e2e-watch-test-configmap-a,UID:241ee205-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6201,Generation:0,CreationTimestamp:2019-05-13 19:34:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 19:34:36 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}}],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 13 19:34:46.208: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9484,SelfLink:/api/v1/namespaces/watch-9484/configmaps/e2e-watch-test-configmap-a,UID:241ee205-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6215,Generation:0,CreationTimestamp:2019-05-13 19:34:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 19:34:36 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}} {e2e.test Update v1 2019-05-13 19:34:46 +0000 UTC &Fields{Map:map[string]Fields{f:data: {map[.:{map[]} f:mutation:{map[]}]},},}}],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 13 19:34:46.208: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9484,SelfLink:/api/v1/namespaces/watch-9484/configmaps/e2e-watch-test-configmap-a,UID:241ee205-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6215,Generation:0,CreationTimestamp:2019-05-13 19:34:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 19:34:36 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}} {e2e.test Update v1 2019-05-13 19:34:46 +0000 UTC &Fields{Map:map[string]Fields{f:data: {map[.:{map[]} f:mutation:{map[]}]},},}}],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 13 19:34:56.213: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9484,SelfLink:/api/v1/namespaces/watch-9484/configmaps/e2e-watch-test-configmap-a,UID:241ee205-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6230,Generation:0,CreationTimestamp:2019-05-13 19:34:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 19:34:36 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}} {e2e.test Update v1 2019-05-13 19:34:46 +0000 UTC &Fields{Map:map[string]Fields{f:data: {map[]},},}} {e2e.test Update v1 2019-05-13 19:34:56 +0000 UTC &Fields{Map:map[string]Fields{f:data: {map[f:mutation:{map[]}]},},}}],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 13 19:34:56.213: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9484,SelfLink:/api/v1/namespaces/watch-9484/configmaps/e2e-watch-test-configmap-a,UID:241ee205-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6230,Generation:0,CreationTimestamp:2019-05-13 19:34:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 19:34:36 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}} {e2e.test Update v1 2019-05-13 19:34:46 +0000 UTC &Fields{Map:map[string]Fields{f:data: {map[]},},}} {e2e.test Update v1 2019-05-13 19:34:56 +0000 UTC &Fields{Map:map[string]Fields{f:data: {map[f:mutation:{map[]}]},},}}],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 13 19:35:06.217: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9484,SelfLink:/api/v1/namespaces/watch-9484/configmaps/e2e-watch-test-configmap-a,UID:241ee205-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6245,Generation:0,CreationTimestamp:2019-05-13 19:34:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 19:34:36 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}} {e2e.test Update v1 2019-05-13 19:34:46 +0000 UTC &Fields{Map:map[string]Fields{f:data: {map[]},},}} {e2e.test Update v1 2019-05-13 19:34:56 +0000 UTC &Fields{Map:map[string]Fields{f:data: {map[f:mutation:{map[]}]},},}}],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 13 19:35:06.217: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9484,SelfLink:/api/v1/namespaces/watch-9484/configmaps/e2e-watch-test-configmap-a,UID:241ee205-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6245,Generation:0,CreationTimestamp:2019-05-13 19:34:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 19:34:36 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}} {e2e.test Update v1 2019-05-13 19:34:46 +0000 UTC &Fields{Map:map[string]Fields{f:data: {map[]},},}} {e2e.test Update v1 2019-05-13 19:34:56 +0000 UTC &Fields{Map:map[string]Fields{f:data: {map[f:mutation:{map[]}]},},}}],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 13 19:35:16.221: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9484,SelfLink:/api/v1/namespaces/watch-9484/configmaps/e2e-watch-test-configmap-b,UID:3bf91a52-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6259,Generation:0,CreationTimestamp:2019-05-13 19:35:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 19:35:16 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}}],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 13 19:35:16.221: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9484,SelfLink:/api/v1/namespaces/watch-9484/configmaps/e2e-watch-test-configmap-b,UID:3bf91a52-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6259,Generation:0,CreationTimestamp:2019-05-13 19:35:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 19:35:16 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}}],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 13 19:35:26.225: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9484,SelfLink:/api/v1/namespaces/watch-9484/configmaps/e2e-watch-test-configmap-b,UID:3bf91a52-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6273,Generation:0,CreationTimestamp:2019-05-13 19:35:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 19:35:16 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}}],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 13 19:35:26.225: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9484,SelfLink:/api/v1/namespaces/watch-9484/configmaps/e2e-watch-test-configmap-b,UID:3bf91a52-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6273,Generation:0,CreationTimestamp:2019-05-13 19:35:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 19:35:16 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}}],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:35:36.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9484" for this suite.
May 13 19:35:42.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:35:42.287: INFO: namespace watch-9484 deletion completed in 6.058917958s

• [SLOW TEST:66.213 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:35:42.288: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 19:35:42.417: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4b95b6db-75b6-11e9-9153-920e960bc5b9" in namespace "downward-api-230" to be "success or failure"
May 13 19:35:42.419: INFO: Pod "downwardapi-volume-4b95b6db-75b6-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.115777ms
May 13 19:35:44.422: INFO: Pod "downwardapi-volume-4b95b6db-75b6-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004676572s
May 13 19:35:46.425: INFO: Pod "downwardapi-volume-4b95b6db-75b6-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00734389s
STEP: Saw pod success
May 13 19:35:46.425: INFO: Pod "downwardapi-volume-4b95b6db-75b6-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:35:46.426: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downwardapi-volume-4b95b6db-75b6-11e9-9153-920e960bc5b9 container client-container: <nil>
STEP: delete the pod
May 13 19:35:46.438: INFO: Waiting for pod downwardapi-volume-4b95b6db-75b6-11e9-9153-920e960bc5b9 to disappear
May 13 19:35:46.440: INFO: Pod downwardapi-volume-4b95b6db-75b6-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:35:46.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-230" for this suite.
May 13 19:35:52.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:35:52.503: INFO: namespace downward-api-230 deletion completed in 6.06034013s

• [SLOW TEST:10.215 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:35:52.503: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9809
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-9809
May 13 19:35:54.639: INFO: Started pod liveness-exec in namespace container-probe-9809
STEP: checking the pod's current state and verifying that restartCount is present
May 13 19:35:54.641: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:39:54.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9809" for this suite.
May 13 19:40:00.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:40:01.049: INFO: namespace container-probe-9809 deletion completed in 6.072223731s

• [SLOW TEST:248.546 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:40:01.049: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-799
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 13 19:40:01.176: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:40:04.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-799" for this suite.
May 13 19:40:10.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:40:10.194: INFO: namespace init-container-799 deletion completed in 6.059626864s

• [SLOW TEST:9.145 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:40:10.194: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2171
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 19:40:10.321: INFO: Creating deployment "nginx-deployment"
May 13 19:40:10.325: INFO: Waiting for observed generation 1
May 13 19:40:12.330: INFO: Waiting for all required pods to come up
May 13 19:40:12.334: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
May 13 19:40:18.343: INFO: Waiting for deployment "nginx-deployment" to complete
May 13 19:40:18.347: INFO: Updating deployment "nginx-deployment" with a non-existent image
May 13 19:40:18.354: INFO: Updating deployment nginx-deployment
May 13 19:40:18.354: INFO: Waiting for observed generation 2
May 13 19:40:20.358: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 13 19:40:20.360: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 13 19:40:20.362: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 13 19:40:20.367: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 13 19:40:20.367: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 13 19:40:20.381: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 13 19:40:20.386: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
May 13 19:40:20.386: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
May 13 19:40:20.404: INFO: Updating deployment nginx-deployment
May 13 19:40:20.404: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
May 13 19:40:20.418: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 13 19:40:20.422: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 13 19:40:20.443: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-2171,SelfLink:/apis/apps/v1/namespaces/deployment-2171/deployments/nginx-deployment,UID:eb45c5fc-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6968,Generation:3,CreationTimestamp:2019-05-13 19:40:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update apps/v1 2019-05-13 19:40:10 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:name:{map[]}]}]},f:spec: {map[f:progressDeadlineSeconds:{map[]} f:revisionHistoryLimit:{map[]} f:selector:{map[f:matchLabels:{map[.:{map[]} f:name:{map[]}]}]} f:strategy:{map[f:rollingUpdate:{map[.:{map[]} f:maxSurge:{map[]} f:maxUnavailable:{map[]}]} f:type:{map[]}]} f:template:{map[f:metadata:{map[f:labels:{map[.:{map[]} f:name:{map[]}]}]} f:spec:{map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]}]}]} f:dnsPolicy:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:terminationGracePeriodSeconds:{map[]}]}]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 19:40:10 +0000 UTC &Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[]}]},f:status: {map[f:conditions:{map[.:{map[]} k:{"type":"Available"}:{map[.:{map[]} f:type:{map[]}]} k:{"type":"Progressing"}:{map[.:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]}]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 19:40:15 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"Available"}:{map[f:lastTransitionTime:{map[]} f:lastUpdateTime:{map[]} f:message:{map[]} f:reason:{map[]} f:status:{map[]}]}]}]},},}} {e2e.test Update apps/v1 2019-05-13 19:40:18 +0000 UTC &Fields{Map:map[string]Fields{f:spec: {map[f:template:{map[f:spec:{map[f:containers:{map[k:{"name":"nginx"}:{map[f:image:{map[]}]}]}]}]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 19:40:18 +0000 UTC &Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[f:deployment.kubernetes.io/revision:{map[]}]}]},f:status: {map[f:availableReplicas:{map[]} f:conditions:{map[k:{"type":"Progressing"}:{map[f:lastUpdateTime:{map[]} f:message:{map[]} f:reason:{map[]}]}]} f:observedGeneration:{map[]} f:readyReplicas:{map[]} f:replicas:{map[]} f:unavailableReplicas:{map[]} f:updatedReplicas:{map[]}]},},}} {e2e.test Update apps/v1 2019-05-13 19:40:20 +0000 UTC &Fields{Map:map[string]Fields{f:spec: {map[f:replicas:{map[]}]},},}}],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Available True 2019-05-13 19:40:15 +0000 UTC 2019-05-13 19:40:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-13 19:40:18 +0000 UTC 2019-05-13 19:40:10 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

May 13 19:40:20.460: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-2171,SelfLink:/apis/apps/v1/namespaces/deployment-2171/replicasets/nginx-deployment-5f9595f595,UID:f00f8b01-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6971,Generation:3,CreationTimestamp:2019-05-13 19:40:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment eb45c5fc-75b6-11e9-ac1e-0215dc200466 0xc002823427 0xc002823428}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update apps/v1 2019-05-13 19:40:18 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:deployment.kubernetes.io/revision:{map[]}]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"eb45c5fc-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:selector:{map[f:matchLabels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]}]} f:template:{map[f:metadata:{map[f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]}]} f:spec:{map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]}]}]} f:dnsPolicy:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:terminationGracePeriodSeconds:{map[]}]}]}]},f:status: {map[f:fullyLabeledReplicas:{map[]} f:observedGeneration:{map[]} f:replicas:{map[]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 19:40:20 +0000 UTC &Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[f:deployment.kubernetes.io/desired-replicas:{map[]} f:deployment.kubernetes.io/max-replicas:{map[]}]}]},f:spec: {map[f:replicas:{map[]}]},},}}],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 13 19:40:20.460: INFO: All old ReplicaSets of Deployment "nginx-deployment":
May 13 19:40:20.461: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-2171,SelfLink:/apis/apps/v1/namespaces/deployment-2171/replicasets/nginx-deployment-6f478d8d8,UID:eb4672e9-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6969,Generation:3,CreationTimestamp:2019-05-13 19:40:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment eb45c5fc-75b6-11e9-ac1e-0215dc200466 0xc002823677 0xc002823678}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update apps/v1 2019-05-13 19:40:10 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:deployment.kubernetes.io/revision:{map[]}]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"eb45c5fc-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:selector:{map[f:matchLabels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]}]} f:template:{map[f:metadata:{map[f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]}]} f:spec:{map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]}]}]} f:dnsPolicy:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:terminationGracePeriodSeconds:{map[]}]}]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 19:40:18 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:availableReplicas:{map[]} f:fullyLabeledReplicas:{map[]} f:observedGeneration:{map[]} f:readyReplicas:{map[]} f:replicas:{map[]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 19:40:20 +0000 UTC &Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[f:deployment.kubernetes.io/desired-replicas:{map[]} f:deployment.kubernetes.io/max-replicas:{map[]}]}]},f:spec: {map[f:replicas:{map[]}]},},}}],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
May 13 19:40:20.536: INFO: Pod "nginx-deployment-5f9595f595-22xz8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-22xz8,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-5f9595f595-22xz8,UID:f153f188-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6993,Generation:0,CreationTimestamp:2019-05-13 19:40:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 f00f8b01-75b6-11e9-ac1e-0215dc200466 0xc001e2bf50 0xc001e2bf51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:20 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"f00f8b01-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.536: INFO: Pod "nginx-deployment-5f9595f595-9n9jh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-9n9jh,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-5f9595f595-9n9jh,UID:f14aab66-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6975,Generation:0,CreationTimestamp:2019-05-13 19:40:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 f00f8b01-75b6-11e9-ac1e-0215dc200466 0xc0020d01c7 0xc0020d01c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:20 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"f00f8b01-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:20 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.536: INFO: Pod "nginx-deployment-5f9595f595-g27kc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-g27kc,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-5f9595f595-g27kc,UID:f012c610-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6935,Generation:0,CreationTimestamp:2019-05-13 19:40:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 f00f8b01-75b6-11e9-ac1e-0215dc200466 0xc0020d03f0 0xc0020d03f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:18 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"f00f8b01-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}} {kubelet Update v1 2019-05-13 19:40:18 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:message:{map[]} f:reason:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Initialized"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Ready"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:message:{map[]} f:reason:{map[]} f:status:{map[]} f:type:{map[]}]}]} f:containerStatuses:{map[]} f:hostIP:{map[]} f:startTime:{map[]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.248,PodIP:,StartTime:2019-05-13 19:40:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.537: INFO: Pod "nginx-deployment-5f9595f595-g5rm9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-g5rm9,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-5f9595f595-g5rm9,UID:f14bab78-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6979,Generation:0,CreationTimestamp:2019-05-13 19:40:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 f00f8b01-75b6-11e9-ac1e-0215dc200466 0xc0020d0760 0xc0020d0761}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:20 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"f00f8b01-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:20 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.537: INFO: Pod "nginx-deployment-5f9595f595-hjtdj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-hjtdj,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-5f9595f595-hjtdj,UID:f152d51d-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6991,Generation:0,CreationTimestamp:2019-05-13 19:40:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 f00f8b01-75b6-11e9-ac1e-0215dc200466 0xc0020d0980 0xc0020d0981}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:20 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"f00f8b01-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.537: INFO: Pod "nginx-deployment-5f9595f595-hrn42" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-hrn42,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-5f9595f595-hrn42,UID:f0130425-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6947,Generation:0,CreationTimestamp:2019-05-13 19:40:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 f00f8b01-75b6-11e9-ac1e-0215dc200466 0xc0020d0b87 0xc0020d0b88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:18 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"f00f8b01-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}} {kubelet Update v1 2019-05-13 19:40:18 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:message:{map[]} f:reason:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Initialized"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Ready"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:message:{map[]} f:reason:{map[]} f:status:{map[]} f:type:{map[]}]}]} f:containerStatuses:{map[]} f:hostIP:{map[]} f:startTime:{map[]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.248,PodIP:,StartTime:2019-05-13 19:40:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.538: INFO: Pod "nginx-deployment-5f9595f595-k98wl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-k98wl,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-5f9595f595-k98wl,UID:f011ba90-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6923,Generation:0,CreationTimestamp:2019-05-13 19:40:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 f00f8b01-75b6-11e9-ac1e-0215dc200466 0xc0020d0ec0 0xc0020d0ec1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:18 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"f00f8b01-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}} {kubelet Update v1 2019-05-13 19:40:18 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:message:{map[]} f:reason:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Initialized"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Ready"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:message:{map[]} f:reason:{map[]} f:status:{map[]} f:type:{map[]}]}]} f:containerStatuses:{map[]} f:hostIP:{map[]} f:startTime:{map[]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.248,PodIP:,StartTime:2019-05-13 19:40:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.538: INFO: Pod "nginx-deployment-5f9595f595-l6trt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-l6trt,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-5f9595f595-l6trt,UID:f01fabd3-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6962,Generation:0,CreationTimestamp:2019-05-13 19:40:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 f00f8b01-75b6-11e9-ac1e-0215dc200466 0xc0020d11f0 0xc0020d11f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:18 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"f00f8b01-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}} {kubelet Update v1 2019-05-13 19:40:19 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:message:{map[]} f:reason:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Initialized"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Ready"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:message:{map[]} f:reason:{map[]} f:status:{map[]} f:type:{map[]}]}]} f:containerStatuses:{map[]} f:hostIP:{map[]} f:startTime:{map[]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.248,PodIP:,StartTime:2019-05-13 19:40:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.538: INFO: Pod "nginx-deployment-5f9595f595-nkcv4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-nkcv4,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-5f9595f595-nkcv4,UID:f01c5042-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6955,Generation:0,CreationTimestamp:2019-05-13 19:40:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 f00f8b01-75b6-11e9-ac1e-0215dc200466 0xc0020d1520 0xc0020d1521}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:18 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"f00f8b01-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}} {kubelet Update v1 2019-05-13 19:40:19 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:message:{map[]} f:reason:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Initialized"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Ready"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:message:{map[]} f:reason:{map[]} f:status:{map[]} f:type:{map[]}]}]} f:containerStatuses:{map[]} f:hostIP:{map[]} f:startTime:{map[]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:18 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.248,PodIP:,StartTime:2019-05-13 19:40:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.538: INFO: Pod "nginx-deployment-5f9595f595-sn22f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-sn22f,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-5f9595f595-sn22f,UID:f1513b01-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6986,Generation:0,CreationTimestamp:2019-05-13 19:40:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 f00f8b01-75b6-11e9-ac1e-0215dc200466 0xc0020d1850 0xc0020d1851}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:20 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"f00f8b01-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.539: INFO: Pod "nginx-deployment-5f9595f595-vd5w5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-vd5w5,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-5f9595f595-vd5w5,UID:f14dba4c-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6996,Generation:0,CreationTimestamp:2019-05-13 19:40:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 f00f8b01-75b6-11e9-ac1e-0215dc200466 0xc0020d1a57 0xc0020d1a58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:20 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"f00f8b01-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:20 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.539: INFO: Pod "nginx-deployment-5f9595f595-wv6ll" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-wv6ll,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-5f9595f595-wv6ll,UID:f15490e5-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6997,Generation:0,CreationTimestamp:2019-05-13 19:40:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 f00f8b01-75b6-11e9-ac1e-0215dc200466 0xc0020d1cb0 0xc0020d1cb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:20 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"f00f8b01-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.539: INFO: Pod "nginx-deployment-6f478d8d8-24xjb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-24xjb,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-6f478d8d8-24xjb,UID:eb4a5dd2-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6868,Generation:0,CreationTimestamp:2019-05-13 19:40:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 eb4672e9-75b6-11e9-ac1e-0215dc200466 0xc0020d1eb7 0xc0020d1eb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:10 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"eb4672e9-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}} {kubelet Update v1 2019-05-13 19:40:10 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]} k:{"type":"Initialized"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Ready"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]}]} f:hostIP:{map[]} f:startTime:{map[]}]},},}} {kubelet Update v1 2019-05-13 19:40:14 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]} k:{"type":"Ready"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]}]} f:containerStatuses:{map[]} f:phase:{map[]} f:podIP:{map[]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:10 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.248,PodIP:10.244.84.110,StartTime:2019-05-13 19:40:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-13 19:40:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://47bc4f0187068fa162f88391fcfb56450b3934549ace01f6d730cffd9fe613f8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.539: INFO: Pod "nginx-deployment-6f478d8d8-7bfvg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7bfvg,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-6f478d8d8-7bfvg,UID:f14a143a-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6974,Generation:0,CreationTimestamp:2019-05-13 19:40:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 eb4672e9-75b6-11e9-ac1e-0215dc200466 0xc002288310 0xc002288311}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:20 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"eb4672e9-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:20 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.540: INFO: Pod "nginx-deployment-6f478d8d8-7d778" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7d778,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-6f478d8d8-7d778,UID:eb4a5439-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6862,Generation:0,CreationTimestamp:2019-05-13 19:40:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 eb4672e9-75b6-11e9-ac1e-0215dc200466 0xc0022886d0 0xc0022886d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:10 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"eb4672e9-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}} {kubelet Update v1 2019-05-13 19:40:10 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]} k:{"type":"Initialized"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Ready"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]}]} f:hostIP:{map[]} f:startTime:{map[]}]},},}} {kubelet Update v1 2019-05-13 19:40:14 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]} k:{"type":"Ready"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]}]} f:containerStatuses:{map[]} f:phase:{map[]} f:podIP:{map[]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:10 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.248,PodIP:10.244.84.104,StartTime:2019-05-13 19:40:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-13 19:40:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://af3a05e231cb560c50e8791cfa37df36a0fe003a8571e37e78ac3bf570058c0f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.540: INFO: Pod "nginx-deployment-6f478d8d8-8pm2v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-8pm2v,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-6f478d8d8-8pm2v,UID:f1537fa5-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6992,Generation:0,CreationTimestamp:2019-05-13 19:40:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 eb4672e9-75b6-11e9-ac1e-0215dc200466 0xc002288c00 0xc002288c01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:20 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"eb4672e9-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.541: INFO: Pod "nginx-deployment-6f478d8d8-8w6s6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-8w6s6,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-6f478d8d8-8w6s6,UID:eb4d271a-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6846,Generation:0,CreationTimestamp:2019-05-13 19:40:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 eb4672e9-75b6-11e9-ac1e-0215dc200466 0xc002289017 0xc002289018}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:10 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"eb4672e9-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}} {kubelet Update v1 2019-05-13 19:40:11 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]} k:{"type":"Initialized"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Ready"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]}]} f:hostIP:{map[]} f:startTime:{map[]}]},},}} {kubelet Update v1 2019-05-13 19:40:13 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]} k:{"type":"Ready"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]}]} f:containerStatuses:{map[]} f:phase:{map[]} f:podIP:{map[]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:10 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.248,PodIP:10.244.84.108,StartTime:2019-05-13 19:40:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-13 19:40:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://86b0b86b6cad2b7e64a9a2ae7431dd2ff2d00702ba89a19bc6e63a1ff6f9d8e3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.541: INFO: Pod "nginx-deployment-6f478d8d8-9rwv7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-9rwv7,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-6f478d8d8-9rwv7,UID:f14e849c-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6994,Generation:0,CreationTimestamp:2019-05-13 19:40:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 eb4672e9-75b6-11e9-ac1e-0215dc200466 0xc002289520 0xc002289521}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:20 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"eb4672e9-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:20 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.541: INFO: Pod "nginx-deployment-6f478d8d8-bfqwp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-bfqwp,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-6f478d8d8-bfqwp,UID:f152498f-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6988,Generation:0,CreationTimestamp:2019-05-13 19:40:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 eb4672e9-75b6-11e9-ac1e-0215dc200466 0xc0022897c0 0xc0022897c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:20 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"eb4672e9-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.541: INFO: Pod "nginx-deployment-6f478d8d8-bmbb7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-bmbb7,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-6f478d8d8-bmbb7,UID:eb4ab8e8-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6879,Generation:0,CreationTimestamp:2019-05-13 19:40:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 eb4672e9-75b6-11e9-ac1e-0215dc200466 0xc002289af7 0xc002289af8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:10 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"eb4672e9-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}} {kubelet Update v1 2019-05-13 19:40:10 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]} k:{"type":"Initialized"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Ready"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]}]} f:hostIP:{map[]} f:startTime:{map[]}]},},}} {kubelet Update v1 2019-05-13 19:40:15 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]} k:{"type":"Ready"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]}]} f:containerStatuses:{map[]} f:phase:{map[]} f:podIP:{map[]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:10 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.248,PodIP:10.244.84.106,StartTime:2019-05-13 19:40:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-13 19:40:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://448fa97329019e73eb73255b022c0985ddd9574f5ccf5138c4f3fa99ba23b0f3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.542: INFO: Pod "nginx-deployment-6f478d8d8-c79v5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-c79v5,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-6f478d8d8-c79v5,UID:f151dd03-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6987,Generation:0,CreationTimestamp:2019-05-13 19:40:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 eb4672e9-75b6-11e9-ac1e-0215dc200466 0xc002289e20 0xc002289e21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:20 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"eb4672e9-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.542: INFO: Pod "nginx-deployment-6f478d8d8-ctdgf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ctdgf,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-6f478d8d8-ctdgf,UID:eb48d8b7-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6889,Generation:0,CreationTimestamp:2019-05-13 19:40:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 eb4672e9-75b6-11e9-ac1e-0215dc200466 0xc000fb6037 0xc000fb6038}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:10 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"eb4672e9-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}} {kubelet Update v1 2019-05-13 19:40:10 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]} k:{"type":"Initialized"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Ready"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]}]} f:hostIP:{map[]} f:startTime:{map[]}]},},}} {kubelet Update v1 2019-05-13 19:40:14 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]} k:{"type":"Ready"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]}]} f:phase:{map[]} f:podIP:{map[]}]},},}} {kubelet Update v1 2019-05-13 19:40:16 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:containerStatuses:{map[]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:10 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.248,PodIP:10.244.84.102,StartTime:2019-05-13 19:40:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-13 19:40:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://277a7c94f9a1fd8abeffec513452f400fb97266a853862da414283163e0deadb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.542: INFO: Pod "nginx-deployment-6f478d8d8-gc8k9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-gc8k9,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-6f478d8d8-gc8k9,UID:f14e4f43-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6985,Generation:0,CreationTimestamp:2019-05-13 19:40:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 eb4672e9-75b6-11e9-ac1e-0215dc200466 0xc000fb63a0 0xc000fb63a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:20 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"eb4672e9-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.543: INFO: Pod "nginx-deployment-6f478d8d8-gq2dn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-gq2dn,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-6f478d8d8-gq2dn,UID:eb48daea-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6852,Generation:0,CreationTimestamp:2019-05-13 19:40:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 eb4672e9-75b6-11e9-ac1e-0215dc200466 0xc000fb6597 0xc000fb6598}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:10 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"eb4672e9-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}} {kubelet Update v1 2019-05-13 19:40:10 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]} k:{"type":"Initialized"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Ready"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]}]} f:hostIP:{map[]} f:startTime:{map[]}]},},}} {kubelet Update v1 2019-05-13 19:40:13 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]} k:{"type":"Ready"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]}]} f:containerStatuses:{map[]} f:phase:{map[]} f:podIP:{map[]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:10 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.248,PodIP:10.244.84.105,StartTime:2019-05-13 19:40:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-13 19:40:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b80183333c1995cc179b5a8153507839ba50ed175fc00adb5717ea4f670462a8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.543: INFO: Pod "nginx-deployment-6f478d8d8-h684q" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-h684q,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-6f478d8d8-h684q,UID:eb47e86c-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6840,Generation:0,CreationTimestamp:2019-05-13 19:40:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 eb4672e9-75b6-11e9-ac1e-0215dc200466 0xc000fb68c0 0xc000fb68c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:10 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"eb4672e9-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}} {kubelet Update v1 2019-05-13 19:40:10 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]} k:{"type":"Initialized"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Ready"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]}]} f:hostIP:{map[]} f:startTime:{map[]}]},},}} {kubelet Update v1 2019-05-13 19:40:12 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]} k:{"type":"Ready"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]}]} f:containerStatuses:{map[]} f:phase:{map[]} f:podIP:{map[]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:10 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.248,PodIP:10.244.84.103,StartTime:2019-05-13 19:40:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-13 19:40:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://258451fd1b9eca375b0ec2a78f068df93bd265f79e995968e42bdd354f496beb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.544: INFO: Pod "nginx-deployment-6f478d8d8-v784t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-v784t,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-6f478d8d8-v784t,UID:f1543ac6-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6995,Generation:0,CreationTimestamp:2019-05-13 19:40:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 eb4672e9-75b6-11e9-ac1e-0215dc200466 0xc000fb6bf0 0xc000fb6bf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:20 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"eb4672e9-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 13 19:40:20.544: INFO: Pod "nginx-deployment-6f478d8d8-wm5sl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-wm5sl,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2171,SelfLink:/api/v1/namespaces/deployment-2171/pods/nginx-deployment-6f478d8d8-wm5sl,UID:eb4d8af2-75b6-11e9-ac1e-0215dc200466,ResourceVersion:6884,Generation:0,CreationTimestamp:2019-05-13 19:40:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 eb4672e9-75b6-11e9-ac1e-0215dc200466 0xc000fb6de7 0xc000fb6de8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 19:40:10 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"eb4672e9-75b6-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-w299f"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}} {kubelet Update v1 2019-05-13 19:40:12 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]} k:{"type":"Initialized"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Ready"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]}]} f:hostIP:{map[]} f:startTime:{map[]}]},},}} {kubelet Update v1 2019-05-13 19:40:16 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]} k:{"type":"Ready"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]}]} f:containerStatuses:{map[]} f:phase:{map[]} f:podIP:{map[]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-w299f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-w299f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-w299f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:10 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.248,PodIP:10.244.84.107,StartTime:2019-05-13 19:40:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-13 19:40:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://7144ec2e03697207193eb662374a8a3021b350b6fd72757545cd79756bd5a068}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:40:20.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2171" for this suite.
May 13 19:40:26.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:40:26.842: INFO: namespace deployment-2171 deletion completed in 6.280605269s

• [SLOW TEST:16.648 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:40:26.842: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6621
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 13 19:40:35.016: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-f5362ac5-75b6-11e9-9153-920e960bc5b9,GenerateName:,Namespace:events-6621,SelfLink:/api/v1/namespaces/events-6621/pods/send-events-f5362ac5-75b6-11e9-9153-920e960bc5b9,UID:f536e7c3-75b6-11e9-ac1e-0215dc200466,ResourceVersion:7261,Generation:0,CreationTimestamp:2019-05-13 19:40:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 997632136,},Annotations:map[string]string{kubernetes.io/psp: coredns,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 19:40:27 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:labels:{map[.:{map[]} f:name:{map[]} f:time:{map[]}]}]},f:spec: {map[f:containers:{map[k:{"name":"p"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:ports:{map[.:{map[]} k:{"containerPort":80,"protocol":"TCP"}:{map[.:{map[]} f:containerPort:{map[]} f:protocol:{map[]}]}]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-hpx5d"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}} {kubelet Update v1 2019-05-13 19:40:34 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Initialized"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Ready"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]}]} f:containerStatuses:{map[]} f:hostIP:{map[]} f:phase:{map[]} f:podIP:{map[]} f:startTime:{map[]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-hpx5d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hpx5d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-hpx5d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:40:27 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.248,PodIP:10.244.84.132,StartTime:2019-05-13 19:40:27 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-05-13 19:40:29 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://e3740bcae2be7eb71ea5201b2e306722846cb600d2cedc8b2874c6cb51d93bc1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
May 13 19:40:37.019: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 13 19:40:55.022: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:40:55.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6621" for this suite.
May 13 19:41:35.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:41:35.107: INFO: namespace events-6621 deletion completed in 40.074175804s

• [SLOW TEST:68.265 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:41:35.107: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4582
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 13 19:41:35.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4582'
May 13 19:41:35.513: INFO: stderr: ""
May 13 19:41:35.513: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
May 13 19:41:35.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 delete pods e2e-test-nginx-pod --namespace=kubectl-4582'
May 13 19:41:43.547: INFO: stderr: ""
May 13 19:41:43.547: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:41:43.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4582" for this suite.
May 13 19:41:49.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:41:49.617: INFO: namespace kubectl-4582 deletion completed in 6.066841944s

• [SLOW TEST:14.510 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:41:49.617: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3502
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
May 13 19:41:49.752: INFO: Waiting up to 5m0s for pod "pod-26887b55-75b7-11e9-9153-920e960bc5b9" in namespace "emptydir-3502" to be "success or failure"
May 13 19:41:49.754: INFO: Pod "pod-26887b55-75b7-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.158839ms
May 13 19:41:51.757: INFO: Pod "pod-26887b55-75b7-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004726415s
STEP: Saw pod success
May 13 19:41:51.757: INFO: Pod "pod-26887b55-75b7-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:41:51.759: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-26887b55-75b7-11e9-9153-920e960bc5b9 container test-container: <nil>
STEP: delete the pod
May 13 19:41:51.774: INFO: Waiting for pod pod-26887b55-75b7-11e9-9153-920e960bc5b9 to disappear
May 13 19:41:51.786: INFO: Pod pod-26887b55-75b7-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:41:51.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3502" for this suite.
May 13 19:41:57.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:41:57.863: INFO: namespace emptydir-3502 deletion completed in 6.073656526s

• [SLOW TEST:8.245 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:41:57.863: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-575
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 13 19:41:58.015: INFO: Number of nodes with available pods: 0
May 13 19:41:58.015: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 19:41:59.020: INFO: Number of nodes with available pods: 0
May 13 19:41:59.020: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 19:42:00.027: INFO: Number of nodes with available pods: 1
May 13 19:42:00.027: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 13 19:42:00.072: INFO: Number of nodes with available pods: 0
May 13 19:42:00.072: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 19:42:01.078: INFO: Number of nodes with available pods: 0
May 13 19:42:01.078: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 19:42:02.076: INFO: Number of nodes with available pods: 1
May 13 19:42:02.076: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-575, will wait for the garbage collector to delete the pods
May 13 19:42:02.135: INFO: Deleting DaemonSet.extensions daemon-set took: 4.09159ms
May 13 19:42:02.535: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.244551ms
May 13 19:42:13.638: INFO: Number of nodes with available pods: 0
May 13 19:42:13.638: INFO: Number of running nodes: 0, number of available pods: 0
May 13 19:42:13.639: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-575/daemonsets","resourceVersion":"7512"},"items":null}

May 13 19:42:13.641: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-575/pods","resourceVersion":"7512"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:42:13.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-575" for this suite.
May 13 19:42:19.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:42:19.714: INFO: namespace daemonsets-575 deletion completed in 6.067612733s

• [SLOW TEST:21.851 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:42:19.714: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7461
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-38792524-75b7-11e9-9153-920e960bc5b9
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:42:23.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7461" for this suite.
May 13 19:42:45.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:42:45.933: INFO: namespace configmap-7461 deletion completed in 22.059215034s

• [SLOW TEST:26.218 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:42:45.933: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5142
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-2cf7
STEP: Creating a pod to test atomic-volume-subpath
May 13 19:42:46.066: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-2cf7" in namespace "subpath-5142" to be "success or failure"
May 13 19:42:46.069: INFO: Pod "pod-subpath-test-secret-2cf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.681835ms
May 13 19:42:48.071: INFO: Pod "pod-subpath-test-secret-2cf7": Phase="Running", Reason="", readiness=true. Elapsed: 2.005422613s
May 13 19:42:50.074: INFO: Pod "pod-subpath-test-secret-2cf7": Phase="Running", Reason="", readiness=true. Elapsed: 4.0081252s
May 13 19:42:52.077: INFO: Pod "pod-subpath-test-secret-2cf7": Phase="Running", Reason="", readiness=true. Elapsed: 6.010748716s
May 13 19:42:54.079: INFO: Pod "pod-subpath-test-secret-2cf7": Phase="Running", Reason="", readiness=true. Elapsed: 8.013275773s
May 13 19:42:56.082: INFO: Pod "pod-subpath-test-secret-2cf7": Phase="Running", Reason="", readiness=true. Elapsed: 10.015822054s
May 13 19:42:58.084: INFO: Pod "pod-subpath-test-secret-2cf7": Phase="Running", Reason="", readiness=true. Elapsed: 12.01832828s
May 13 19:43:00.088: INFO: Pod "pod-subpath-test-secret-2cf7": Phase="Running", Reason="", readiness=true. Elapsed: 14.022164564s
May 13 19:43:02.091: INFO: Pod "pod-subpath-test-secret-2cf7": Phase="Running", Reason="", readiness=true. Elapsed: 16.024825653s
May 13 19:43:04.094: INFO: Pod "pod-subpath-test-secret-2cf7": Phase="Running", Reason="", readiness=true. Elapsed: 18.027499484s
May 13 19:43:06.096: INFO: Pod "pod-subpath-test-secret-2cf7": Phase="Running", Reason="", readiness=true. Elapsed: 20.030110235s
May 13 19:43:08.099: INFO: Pod "pod-subpath-test-secret-2cf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.032962325s
STEP: Saw pod success
May 13 19:43:08.099: INFO: Pod "pod-subpath-test-secret-2cf7" satisfied condition "success or failure"
May 13 19:43:08.101: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-subpath-test-secret-2cf7 container test-container-subpath-secret-2cf7: <nil>
STEP: delete the pod
May 13 19:43:08.116: INFO: Waiting for pod pod-subpath-test-secret-2cf7 to disappear
May 13 19:43:08.117: INFO: Pod pod-subpath-test-secret-2cf7 no longer exists
STEP: Deleting pod pod-subpath-test-secret-2cf7
May 13 19:43:08.117: INFO: Deleting pod "pod-subpath-test-secret-2cf7" in namespace "subpath-5142"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:43:08.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5142" for this suite.
May 13 19:43:14.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:43:14.180: INFO: namespace subpath-5142 deletion completed in 6.059779884s

• [SLOW TEST:28.247 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:43:14.180: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-2238
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 13 19:43:20.329: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2238 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 19:43:20.329: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
May 13 19:43:20.444: INFO: Exec stderr: ""
May 13 19:43:20.444: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2238 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 19:43:20.444: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
May 13 19:43:20.590: INFO: Exec stderr: ""
May 13 19:43:20.590: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2238 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 19:43:20.590: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
May 13 19:43:20.700: INFO: Exec stderr: ""
May 13 19:43:20.700: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2238 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 19:43:20.700: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
May 13 19:43:20.795: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 13 19:43:20.795: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2238 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 19:43:20.795: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
May 13 19:43:20.888: INFO: Exec stderr: ""
May 13 19:43:20.888: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2238 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 19:43:20.888: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
May 13 19:43:20.971: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 13 19:43:20.971: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2238 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 19:43:20.971: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
May 13 19:43:21.053: INFO: Exec stderr: ""
May 13 19:43:21.053: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2238 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 19:43:21.053: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
May 13 19:43:21.150: INFO: Exec stderr: ""
May 13 19:43:21.150: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2238 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 19:43:21.150: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
May 13 19:43:21.233: INFO: Exec stderr: ""
May 13 19:43:21.233: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2238 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 19:43:21.233: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
May 13 19:43:21.319: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:43:21.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2238" for this suite.
May 13 19:44:05.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:44:05.391: INFO: namespace e2e-kubelet-etc-hosts-2238 deletion completed in 44.068667211s

• [SLOW TEST:51.210 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:44:05.391: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-7877
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:44:07.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7877" for this suite.
May 13 19:44:13.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:44:13.614: INFO: namespace emptydir-wrapper-7877 deletion completed in 6.059572604s

• [SLOW TEST:8.223 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:44:13.614: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-9906
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
May 13 19:44:14.003: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 13 19:44:16.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693373454, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693373454, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693373454, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693373453, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 13 19:44:18.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693373454, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693373454, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693373454, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693373453, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 13 19:44:20.885: INFO: Waited 824.446707ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:44:21.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9906" for this suite.
May 13 19:44:27.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:44:27.590: INFO: namespace aggregator-9906 deletion completed in 6.157053227s

• [SLOW TEST:13.976 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:44:27.591: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5725
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5725
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 13 19:44:27.744: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 13 19:44:47.782: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.84.144:8080/dial?request=hostName&protocol=http&host=10.244.84.143&port=8080&tries=1'] Namespace:pod-network-test-5725 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 19:44:47.782: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
May 13 19:44:47.883: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:44:47.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5725" for this suite.
May 13 19:45:09.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:45:09.945: INFO: namespace pod-network-test-5725 deletion completed in 22.059491164s

• [SLOW TEST:42.354 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:45:09.945: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6148
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
May 13 19:45:10.075: INFO: Waiting up to 5m0s for pod "client-containers-9def8379-75b7-11e9-9153-920e960bc5b9" in namespace "containers-6148" to be "success or failure"
May 13 19:45:10.079: INFO: Pod "client-containers-9def8379-75b7-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.900439ms
May 13 19:45:12.081: INFO: Pod "client-containers-9def8379-75b7-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006352425s
May 13 19:45:14.084: INFO: Pod "client-containers-9def8379-75b7-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009102526s
STEP: Saw pod success
May 13 19:45:14.084: INFO: Pod "client-containers-9def8379-75b7-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:45:14.086: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod client-containers-9def8379-75b7-11e9-9153-920e960bc5b9 container test-container: <nil>
STEP: delete the pod
May 13 19:45:14.098: INFO: Waiting for pod client-containers-9def8379-75b7-11e9-9153-920e960bc5b9 to disappear
May 13 19:45:14.100: INFO: Pod client-containers-9def8379-75b7-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:45:14.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6148" for this suite.
May 13 19:45:20.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:45:20.198: INFO: namespace containers-6148 deletion completed in 6.095923966s

• [SLOW TEST:10.253 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:45:20.198: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7736
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 13 19:45:20.352: INFO: Waiting up to 5m0s for pod "downward-api-a40e9c76-75b7-11e9-9153-920e960bc5b9" in namespace "downward-api-7736" to be "success or failure"
May 13 19:45:20.358: INFO: Pod "downward-api-a40e9c76-75b7-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027332ms
May 13 19:45:22.361: INFO: Pod "downward-api-a40e9c76-75b7-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008700781s
STEP: Saw pod success
May 13 19:45:22.361: INFO: Pod "downward-api-a40e9c76-75b7-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:45:22.363: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downward-api-a40e9c76-75b7-11e9-9153-920e960bc5b9 container dapi-container: <nil>
STEP: delete the pod
May 13 19:45:22.378: INFO: Waiting for pod downward-api-a40e9c76-75b7-11e9-9153-920e960bc5b9 to disappear
May 13 19:45:22.380: INFO: Pod downward-api-a40e9c76-75b7-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:45:22.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7736" for this suite.
May 13 19:45:28.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:45:28.443: INFO: namespace downward-api-7736 deletion completed in 6.059823416s

• [SLOW TEST:8.245 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:45:28.443: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1470
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-a8f6c6a0-75b7-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume secrets
May 13 19:45:28.580: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a8f728c5-75b7-11e9-9153-920e960bc5b9" in namespace "projected-1470" to be "success or failure"
May 13 19:45:28.583: INFO: Pod "pod-projected-secrets-a8f728c5-75b7-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.871853ms
May 13 19:45:30.587: INFO: Pod "pod-projected-secrets-a8f728c5-75b7-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007263968s
May 13 19:45:32.589: INFO: Pod "pod-projected-secrets-a8f728c5-75b7-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00990841s
STEP: Saw pod success
May 13 19:45:32.590: INFO: Pod "pod-projected-secrets-a8f728c5-75b7-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:45:32.591: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-projected-secrets-a8f728c5-75b7-11e9-9153-920e960bc5b9 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 13 19:45:32.605: INFO: Waiting for pod pod-projected-secrets-a8f728c5-75b7-11e9-9153-920e960bc5b9 to disappear
May 13 19:45:32.606: INFO: Pod pod-projected-secrets-a8f728c5-75b7-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:45:32.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1470" for this suite.
May 13 19:45:38.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:45:38.668: INFO: namespace projected-1470 deletion completed in 6.059577329s

• [SLOW TEST:10.225 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:45:38.668: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 19:45:38.791: INFO: Creating ReplicaSet my-hostname-basic-af0e1a13-75b7-11e9-9153-920e960bc5b9
May 13 19:45:38.796: INFO: Pod name my-hostname-basic-af0e1a13-75b7-11e9-9153-920e960bc5b9: Found 0 pods out of 1
May 13 19:45:43.799: INFO: Pod name my-hostname-basic-af0e1a13-75b7-11e9-9153-920e960bc5b9: Found 1 pods out of 1
May 13 19:45:43.799: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-af0e1a13-75b7-11e9-9153-920e960bc5b9" is running
May 13 19:45:43.801: INFO: Pod "my-hostname-basic-af0e1a13-75b7-11e9-9153-920e960bc5b9-6tc5b" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-13 19:45:38 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-13 19:45:40 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-13 19:45:40 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-13 19:45:38 +0000 UTC Reason: Message:}])
May 13 19:45:43.801: INFO: Trying to dial the pod
May 13 19:45:48.807: INFO: Controller my-hostname-basic-af0e1a13-75b7-11e9-9153-920e960bc5b9: Got expected result from replica 1 [my-hostname-basic-af0e1a13-75b7-11e9-9153-920e960bc5b9-6tc5b]: "my-hostname-basic-af0e1a13-75b7-11e9-9153-920e960bc5b9-6tc5b", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:45:48.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2158" for this suite.
May 13 19:45:54.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:45:54.870: INFO: namespace replicaset-2158 deletion completed in 6.060261963s

• [SLOW TEST:16.202 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:45:54.870: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5218
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
May 13 19:45:54.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 --namespace=kubectl-5218 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
May 13 19:45:56.313: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
May 13 19:45:56.313: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:45:58.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5218" for this suite.
May 13 19:46:04.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:46:04.380: INFO: namespace kubectl-5218 deletion completed in 6.060292401s

• [SLOW TEST:9.510 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:46:04.380: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1874
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0513 19:46:14.586280      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 13 19:46:14.586: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:46:14.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1874" for this suite.
May 13 19:46:20.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:46:20.665: INFO: namespace gc-1874 deletion completed in 6.077755995s

• [SLOW TEST:16.285 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:46:20.666: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9409
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 19:46:20.814: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c818b680-75b7-11e9-9153-920e960bc5b9" in namespace "projected-9409" to be "success or failure"
May 13 19:46:20.820: INFO: Pod "downwardapi-volume-c818b680-75b7-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.131926ms
May 13 19:46:22.830: INFO: Pod "downwardapi-volume-c818b680-75b7-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016438194s
STEP: Saw pod success
May 13 19:46:22.830: INFO: Pod "downwardapi-volume-c818b680-75b7-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:46:22.832: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downwardapi-volume-c818b680-75b7-11e9-9153-920e960bc5b9 container client-container: <nil>
STEP: delete the pod
May 13 19:46:22.846: INFO: Waiting for pod downwardapi-volume-c818b680-75b7-11e9-9153-920e960bc5b9 to disappear
May 13 19:46:22.847: INFO: Pod downwardapi-volume-c818b680-75b7-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:46:22.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9409" for this suite.
May 13 19:46:28.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:46:28.908: INFO: namespace projected-9409 deletion completed in 6.059033851s

• [SLOW TEST:8.242 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:46:28.908: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5365
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
May 13 19:46:29.045: INFO: Waiting up to 5m0s for pod "pod-cd016a10-75b7-11e9-9153-920e960bc5b9" in namespace "emptydir-5365" to be "success or failure"
May 13 19:46:29.048: INFO: Pod "pod-cd016a10-75b7-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.324331ms
May 13 19:46:31.050: INFO: Pod "pod-cd016a10-75b7-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004644037s
STEP: Saw pod success
May 13 19:46:31.050: INFO: Pod "pod-cd016a10-75b7-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:46:31.052: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-cd016a10-75b7-11e9-9153-920e960bc5b9 container test-container: <nil>
STEP: delete the pod
May 13 19:46:31.065: INFO: Waiting for pod pod-cd016a10-75b7-11e9-9153-920e960bc5b9 to disappear
May 13 19:46:31.066: INFO: Pod pod-cd016a10-75b7-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:46:31.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5365" for this suite.
May 13 19:46:37.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:46:37.132: INFO: namespace emptydir-5365 deletion completed in 6.063304726s

• [SLOW TEST:8.224 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:46:37.132: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4347
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:46:37.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4347" for this suite.
May 13 19:46:59.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:46:59.343: INFO: namespace kubelet-test-4347 deletion completed in 22.061862683s

• [SLOW TEST:22.211 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:46:59.343: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2436
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 13 19:46:59.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2436'
May 13 19:46:59.547: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 13 19:46:59.547: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
May 13 19:47:01.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 delete deployment e2e-test-nginx-deployment --namespace=kubectl-2436'
May 13 19:47:01.624: INFO: stderr: ""
May 13 19:47:01.624: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:47:01.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2436" for this suite.
May 13 19:47:23.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:47:23.686: INFO: namespace kubectl-2436 deletion completed in 22.059239433s

• [SLOW TEST:24.343 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:47:23.686: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8546
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
May 13 19:47:23.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 cluster-info'
May 13 19:47:23.874: INFO: stderr: ""
May 13 19:47:23.874: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.100.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.100.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.100.0.1:443/api/v1/namespaces/kube-system/services/kube-dns-worker:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:47:23.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8546" for this suite.
May 13 19:47:29.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:47:29.937: INFO: namespace kubectl-8546 deletion completed in 6.060999304s

• [SLOW TEST:6.251 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:47:29.937: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4155
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 19:47:30.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 version --client'
May 13 19:47:30.110: INFO: stderr: ""
May 13 19:47:30.110: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:53:57Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
May 13 19:47:30.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 create -f - --namespace=kubectl-4155'
May 13 19:47:30.316: INFO: stderr: ""
May 13 19:47:30.316: INFO: stdout: "replicationcontroller/redis-master created\n"
May 13 19:47:30.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 create -f - --namespace=kubectl-4155'
May 13 19:47:30.535: INFO: stderr: ""
May 13 19:47:30.535: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
May 13 19:47:31.539: INFO: Selector matched 1 pods for map[app:redis]
May 13 19:47:31.539: INFO: Found 0 / 1
May 13 19:47:32.538: INFO: Selector matched 1 pods for map[app:redis]
May 13 19:47:32.538: INFO: Found 1 / 1
May 13 19:47:32.538: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 13 19:47:32.540: INFO: Selector matched 1 pods for map[app:redis]
May 13 19:47:32.540: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 13 19:47:32.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 describe pod redis-master-q9hpm --namespace=kubectl-4155'
May 13 19:47:32.615: INFO: stderr: ""
May 13 19:47:32.615: INFO: stdout: "Name:           redis-master-q9hpm\nNamespace:      kubectl-4155\nNode:           ip-10-0-0-248.ec2.internal/10.0.0.248\nStart Time:     Mon, 13 May 2019 19:47:30 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:         Running\nIP:             10.244.84.163\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://1d3b398a5f3b22936000ce9ed8caac7ed18766150d73a6e65bc19a8c1c3eb9ce\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 13 May 2019 19:47:31 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vww84 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-vww84:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-vww84\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     <none>\nEvents:\n  Type    Reason     Age   From                                 Message\n  ----    ------     ----  ----                                 -------\n  Normal  Scheduled  2s    default-scheduler                    Successfully assigned kubectl-4155/redis-master-q9hpm to ip-10-0-0-248.ec2.internal\n  Normal  Pulling    2s    kubelet, ip-10-0-0-248.ec2.internal  Pulling image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Pulled     1s    kubelet, ip-10-0-0-248.ec2.internal  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Created    1s    kubelet, ip-10-0-0-248.ec2.internal  Created container redis-master\n  Normal  Started    1s    kubelet, ip-10-0-0-248.ec2.internal  Started container redis-master\n"
May 13 19:47:32.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 describe rc redis-master --namespace=kubectl-4155'
May 13 19:47:32.695: INFO: stderr: ""
May 13 19:47:32.695: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-4155\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-q9hpm\n"
May 13 19:47:32.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 describe service redis-master --namespace=kubectl-4155'
May 13 19:47:32.770: INFO: stderr: ""
May 13 19:47:32.770: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-4155\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.100.242.245\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.84.163:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 13 19:47:32.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 describe node ip-10-0-0-248.ec2.internal'
May 13 19:47:32.875: INFO: stderr: ""
May 13 19:47:32.875: INFO: stdout: "Name:               ip-10-0-0-248.ec2.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=c4.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1a\n                    gravitational.io/advertise-ip=10.0.0.248\n                    gravitational.io/k8s-role=master\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-0-248.ec2.internal\n                    kubernetes.io/os=linux\n                    role=node\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 13 May 2019 19:03:49 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 13 May 2019 19:47:19 +0000   Mon, 13 May 2019 19:03:49 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 13 May 2019 19:47:19 +0000   Mon, 13 May 2019 19:03:49 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 13 May 2019 19:47:19 +0000   Mon, 13 May 2019 19:03:49 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 13 May 2019 19:47:19 +0000   Mon, 13 May 2019 19:03:59 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.0.248\n  ExternalIP:   52.54.55.223\n  InternalDNS:  ip-10-0-0-248.ec2.internal\n  Hostname:     ip-10-0-0-248.ec2.internal\n  ExternalDNS:  ec2-52-54-55-223.compute-1.amazonaws.com\nCapacity:\n attachable-volumes-aws-ebs:  39\n cpu:                         4\n ephemeral-storage:           515928320Ki\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      7492936Ki\n pods:                        110\nAllocatable:\n attachable-volumes-aws-ebs:  39\n cpu:                         4\n ephemeral-storage:           501895069303\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      7492936Ki\n pods:                        110\nSystem Info:\n Machine ID:                 833e0926ee21aed71ec075d726cbcfe0\n System UUID:                EC2D77C6-1647-F602-32E6-D68302DCA362\n Boot ID:                    9fd191f7-d575-451a-abc1-dd00b6c1f0c0\n Kernel Version:             3.10.0-693.11.6.el7.x86_64\n OS Image:                   Debian GNU/Linux 9 (stretch)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.5\n Kubelet Version:            v1.14.0\n Kube-Proxy Version:         v1.14.0\nProviderID:                  aws:///us-east-1a/i-0d75e4697edcfa79e\nNon-terminated Pods:         (16 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         32m\n  heptio-sonobuoy            sonobuoy-e2e-job-c3529c9c402c4d54                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         32m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-e496b0b34872467e-9g8fv    0 (0%)        0 (0%)      0 (0%)           0 (0%)         32m\n  kube-system                bandwagon-58d4b44589-s7qp9                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\n  kube-system                coredns-x2prj                                              100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     42m\n  kube-system                gravity-site-pzzq5                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         41m\n  kube-system                log-collector-547858d4ff-2pqdk                             100m (2%)     500m (12%)  200Mi (2%)       600Mi (8%)     42m\n  kube-system                log-forwarder-jlhbm                                        100m (2%)     500m (12%)  200Mi (2%)       600Mi (8%)     42m\n  kube-system                tiller-deploy-5f5d5b74f9-6l5nq                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         41m\n  kubectl-4155               redis-master-q9hpm                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  monitoring                 grafana-776fb79c76-v98m9                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\n  monitoring                 heapster-7b9959b6d6-xrxl2                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\n  monitoring                 influxdb-545989ddbc-9h6pl                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\n  monitoring                 kapacitor-55d57f7756-fdx56                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\n  monitoring                 telegraf-5c4699b87-tnhbp                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\n  monitoring                 telegraf-node-l2r62                                        50m (1%)      100m (2%)   50Mi (0%)        100Mi (1%)     42m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         350m (8%)   1100m (27%)\n  memory                      520Mi (7%)  1470Mi (20%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:\n  Type    Reason                   Age                From                                    Message\n  ----    ------                   ----               ----                                    -------\n  Normal  Starting                 43m                kube-proxy, ip-10-0-0-248.ec2.internal  Starting kube-proxy.\n  Normal  NodeReady                43m                kubelet, ip-10-0-0-248.ec2.internal     Node ip-10-0-0-248.ec2.internal status is now: NodeReady\n  Normal  Starting                 33m                kube-proxy, ip-10-0-0-248.ec2.internal  Starting kube-proxy.\n  Normal  Starting                 33m                kubelet, ip-10-0-0-248.ec2.internal     Starting kubelet.\n  Normal  NodeHasSufficientMemory  33m (x2 over 33m)  kubelet, ip-10-0-0-248.ec2.internal     Node ip-10-0-0-248.ec2.internal status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    33m (x2 over 33m)  kubelet, ip-10-0-0-248.ec2.internal     Node ip-10-0-0-248.ec2.internal status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     33m (x2 over 33m)  kubelet, ip-10-0-0-248.ec2.internal     Node ip-10-0-0-248.ec2.internal status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  33m                kubelet, ip-10-0-0-248.ec2.internal     Updated Node Allocatable limit across pods\n"
May 13 19:47:32.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 describe namespace kubectl-4155'
May 13 19:47:32.945: INFO: stderr: ""
May 13 19:47:32.945: INFO: stdout: "Name:         kubectl-4155\nLabels:       e2e-framework=kubectl\n              e2e-run=6fb6ff37-75b3-11e9-9153-920e960bc5b9\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:47:32.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4155" for this suite.
May 13 19:47:54.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:47:55.006: INFO: namespace kubectl-4155 deletion completed in 22.059098048s

• [SLOW TEST:25.069 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:47:55.006: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5028
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 19:47:55.136: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0051b8e8-75b8-11e9-9153-920e960bc5b9" in namespace "downward-api-5028" to be "success or failure"
May 13 19:47:55.138: INFO: Pod "downwardapi-volume-0051b8e8-75b8-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.65239ms
May 13 19:47:57.141: INFO: Pod "downwardapi-volume-0051b8e8-75b8-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005475979s
STEP: Saw pod success
May 13 19:47:57.141: INFO: Pod "downwardapi-volume-0051b8e8-75b8-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:47:57.143: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downwardapi-volume-0051b8e8-75b8-11e9-9153-920e960bc5b9 container client-container: <nil>
STEP: delete the pod
May 13 19:47:57.157: INFO: Waiting for pod downwardapi-volume-0051b8e8-75b8-11e9-9153-920e960bc5b9 to disappear
May 13 19:47:57.159: INFO: Pod downwardapi-volume-0051b8e8-75b8-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:47:57.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5028" for this suite.
May 13 19:48:03.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:48:03.222: INFO: namespace downward-api-5028 deletion completed in 6.060521121s

• [SLOW TEST:8.215 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:48:03.222: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3828
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
May 13 19:48:03.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 create -f - --namespace=kubectl-3828'
May 13 19:48:03.491: INFO: stderr: ""
May 13 19:48:03.491: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 13 19:48:03.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3828'
May 13 19:48:03.635: INFO: stderr: ""
May 13 19:48:03.635: INFO: stdout: "update-demo-nautilus-49mbw update-demo-nautilus-jn2ch "
May 13 19:48:03.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-49mbw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3828'
May 13 19:48:03.732: INFO: stderr: ""
May 13 19:48:03.732: INFO: stdout: ""
May 13 19:48:03.732: INFO: update-demo-nautilus-49mbw is created but not running
May 13 19:48:08.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3828'
May 13 19:48:08.801: INFO: stderr: ""
May 13 19:48:08.801: INFO: stdout: "update-demo-nautilus-49mbw update-demo-nautilus-jn2ch "
May 13 19:48:08.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-49mbw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3828'
May 13 19:48:08.864: INFO: stderr: ""
May 13 19:48:08.864: INFO: stdout: "true"
May 13 19:48:08.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-49mbw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3828'
May 13 19:48:08.927: INFO: stderr: ""
May 13 19:48:08.927: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 19:48:08.927: INFO: validating pod update-demo-nautilus-49mbw
May 13 19:48:08.929: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 19:48:08.930: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 19:48:08.930: INFO: update-demo-nautilus-49mbw is verified up and running
May 13 19:48:08.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-jn2ch -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3828'
May 13 19:48:08.995: INFO: stderr: ""
May 13 19:48:08.995: INFO: stdout: "true"
May 13 19:48:08.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-jn2ch -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3828'
May 13 19:48:09.060: INFO: stderr: ""
May 13 19:48:09.060: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 19:48:09.060: INFO: validating pod update-demo-nautilus-jn2ch
May 13 19:48:09.078: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 19:48:09.078: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 19:48:09.078: INFO: update-demo-nautilus-jn2ch is verified up and running
STEP: rolling-update to new replication controller
May 13 19:48:09.080: INFO: scanned /root for discovery docs: <nil>
May 13 19:48:09.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-3828'
May 13 19:48:31.409: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 13 19:48:31.409: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 13 19:48:31.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3828'
May 13 19:48:31.484: INFO: stderr: ""
May 13 19:48:31.484: INFO: stdout: "update-demo-kitten-9cdww update-demo-kitten-thczf "
May 13 19:48:31.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-kitten-9cdww -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3828'
May 13 19:48:31.556: INFO: stderr: ""
May 13 19:48:31.556: INFO: stdout: "true"
May 13 19:48:31.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-kitten-9cdww -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3828'
May 13 19:48:31.619: INFO: stderr: ""
May 13 19:48:31.619: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 13 19:48:31.619: INFO: validating pod update-demo-kitten-9cdww
May 13 19:48:31.621: INFO: got data: {
  "image": "kitten.jpg"
}

May 13 19:48:31.621: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 13 19:48:31.621: INFO: update-demo-kitten-9cdww is verified up and running
May 13 19:48:31.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-kitten-thczf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3828'
May 13 19:48:31.685: INFO: stderr: ""
May 13 19:48:31.685: INFO: stdout: "true"
May 13 19:48:31.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-kitten-thczf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3828'
May 13 19:48:31.749: INFO: stderr: ""
May 13 19:48:31.749: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 13 19:48:31.749: INFO: validating pod update-demo-kitten-thczf
May 13 19:48:31.752: INFO: got data: {
  "image": "kitten.jpg"
}

May 13 19:48:31.752: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 13 19:48:31.752: INFO: update-demo-kitten-thczf is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:48:31.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3828" for this suite.
May 13 19:48:53.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:48:53.813: INFO: namespace kubectl-3828 deletion completed in 22.05893918s

• [SLOW TEST:50.591 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:48:53.813: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-8004
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-8004
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8004
STEP: Deleting pre-stop pod
May 13 19:49:04.969: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:49:04.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8004" for this suite.
May 13 19:49:48.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:49:49.043: INFO: namespace prestop-8004 deletion completed in 44.065987684s

• [SLOW TEST:55.229 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:49:49.043: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2693
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-444b7c3d-75b8-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume configMaps
May 13 19:49:49.182: INFO: Waiting up to 5m0s for pod "pod-configmaps-444bd66f-75b8-11e9-9153-920e960bc5b9" in namespace "configmap-2693" to be "success or failure"
May 13 19:49:49.184: INFO: Pod "pod-configmaps-444bd66f-75b8-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.627946ms
May 13 19:49:51.187: INFO: Pod "pod-configmaps-444bd66f-75b8-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00443755s
STEP: Saw pod success
May 13 19:49:51.187: INFO: Pod "pod-configmaps-444bd66f-75b8-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:49:51.188: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-configmaps-444bd66f-75b8-11e9-9153-920e960bc5b9 container configmap-volume-test: <nil>
STEP: delete the pod
May 13 19:49:51.202: INFO: Waiting for pod pod-configmaps-444bd66f-75b8-11e9-9153-920e960bc5b9 to disappear
May 13 19:49:51.204: INFO: Pod pod-configmaps-444bd66f-75b8-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:49:51.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2693" for this suite.
May 13 19:49:57.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:49:57.265: INFO: namespace configmap-2693 deletion completed in 6.058769931s

• [SLOW TEST:8.222 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:49:57.265: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1021
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1021
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1021
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1021
May 13 19:49:57.399: INFO: Found 0 stateful pods, waiting for 1
May 13 19:50:07.402: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 13 19:50:07.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-1021 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 19:50:07.555: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 13 19:50:07.555: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 19:50:07.555: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 13 19:50:07.559: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 13 19:50:17.561: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 13 19:50:17.561: INFO: Waiting for statefulset status.replicas updated to 0
May 13 19:50:17.571: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999233s
May 13 19:50:18.573: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997829353s
May 13 19:50:19.576: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.995070175s
May 13 19:50:20.586: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.990387143s
May 13 19:50:21.589: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982517987s
May 13 19:50:22.591: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979761455s
May 13 19:50:23.594: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.977031657s
May 13 19:50:24.608: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.974052638s
May 13 19:50:25.610: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.960794291s
May 13 19:50:26.613: INFO: Verifying statefulset ss doesn't scale past 1 for another 958.061066ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1021
May 13 19:50:27.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-1021 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:50:27.764: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 13 19:50:27.764: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 13 19:50:27.764: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 13 19:50:27.766: INFO: Found 1 stateful pods, waiting for 3
May 13 19:50:37.770: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 13 19:50:37.770: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 13 19:50:37.770: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 13 19:50:37.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-1021 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 19:50:37.952: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 13 19:50:37.952: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 19:50:37.952: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 13 19:50:37.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-1021 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 19:50:38.122: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 13 19:50:38.122: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 19:50:38.122: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 13 19:50:38.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-1021 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 19:50:38.301: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 13 19:50:38.301: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 19:50:38.301: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 13 19:50:38.301: INFO: Waiting for statefulset status.replicas updated to 0
May 13 19:50:38.303: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 13 19:50:48.309: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 13 19:50:48.309: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 13 19:50:48.309: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 13 19:50:48.318: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999902s
May 13 19:50:49.321: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997154901s
May 13 19:50:50.329: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990276037s
May 13 19:50:51.332: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986244954s
May 13 19:50:52.335: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983094425s
May 13 19:50:53.338: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.979663782s
May 13 19:50:54.342: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976566304s
May 13 19:50:55.345: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973113813s
May 13 19:50:56.348: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.970014834s
May 13 19:50:57.351: INFO: Verifying statefulset ss doesn't scale past 3 for another 966.911589ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1021
May 13 19:50:58.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-1021 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:50:58.497: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 13 19:50:58.497: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 13 19:50:58.497: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 13 19:50:58.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-1021 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:50:58.660: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 13 19:50:58.660: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 13 19:50:58.660: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 13 19:50:58.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-1021 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:50:58.816: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 13 19:50:58.816: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 13 19:50:58.816: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 13 19:50:58.816: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 13 19:51:28.833: INFO: Deleting all statefulset in ns statefulset-1021
May 13 19:51:28.834: INFO: Scaling statefulset ss to 0
May 13 19:51:28.842: INFO: Waiting for statefulset status.replicas updated to 0
May 13 19:51:28.844: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:51:28.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1021" for this suite.
May 13 19:51:34.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:51:34.911: INFO: namespace statefulset-1021 deletion completed in 6.059564379s

• [SLOW TEST:97.646 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:51:34.912: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4352
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 19:51:35.043: INFO: Conformance test suite needs a cluster with at least 2 nodes.
May 13 19:51:35.043: INFO: Create a RollingUpdate DaemonSet
May 13 19:51:35.046: INFO: Check that daemon pods launch on every node of the cluster
May 13 19:51:35.051: INFO: Number of nodes with available pods: 0
May 13 19:51:35.051: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 19:51:36.055: INFO: Number of nodes with available pods: 0
May 13 19:51:36.055: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 19:51:37.055: INFO: Number of nodes with available pods: 1
May 13 19:51:37.055: INFO: Number of running nodes: 1, number of available pods: 1
May 13 19:51:37.055: INFO: Update the DaemonSet to trigger a rollout
May 13 19:51:37.061: INFO: Updating DaemonSet daemon-set
May 13 19:51:44.068: INFO: Roll back the DaemonSet before rollout is complete
May 13 19:51:44.076: INFO: Updating DaemonSet daemon-set
May 13 19:51:44.076: INFO: Make sure DaemonSet rollback is complete
May 13 19:51:44.078: INFO: Wrong image for pod: daemon-set-fwxs7. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 13 19:51:44.078: INFO: Pod daemon-set-fwxs7 is not available
May 13 19:51:45.083: INFO: Wrong image for pod: daemon-set-fwxs7. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 13 19:51:45.083: INFO: Pod daemon-set-fwxs7 is not available
May 13 19:51:46.083: INFO: Pod daemon-set-snc6m is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4352, will wait for the garbage collector to delete the pods
May 13 19:51:46.144: INFO: Deleting DaemonSet.extensions daemon-set took: 4.228087ms
May 13 19:51:46.545: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.233083ms
May 13 19:51:47.847: INFO: Number of nodes with available pods: 0
May 13 19:51:47.847: INFO: Number of running nodes: 0, number of available pods: 0
May 13 19:51:47.848: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4352/daemonsets","resourceVersion":"9736"},"items":null}

May 13 19:51:47.850: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4352/pods","resourceVersion":"9736"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:51:47.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4352" for this suite.
May 13 19:51:53.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:51:53.915: INFO: namespace daemonsets-4352 deletion completed in 6.058884776s

• [SLOW TEST:19.003 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:51:53.915: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8272
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 13 19:51:54.045: INFO: Waiting up to 5m0s for pod "pod-8eb86f2c-75b8-11e9-9153-920e960bc5b9" in namespace "emptydir-8272" to be "success or failure"
May 13 19:51:54.047: INFO: Pod "pod-8eb86f2c-75b8-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.17424ms
May 13 19:51:56.052: INFO: Pod "pod-8eb86f2c-75b8-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007465865s
STEP: Saw pod success
May 13 19:51:56.052: INFO: Pod "pod-8eb86f2c-75b8-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:51:56.055: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-8eb86f2c-75b8-11e9-9153-920e960bc5b9 container test-container: <nil>
STEP: delete the pod
May 13 19:51:56.070: INFO: Waiting for pod pod-8eb86f2c-75b8-11e9-9153-920e960bc5b9 to disappear
May 13 19:51:56.073: INFO: Pod pod-8eb86f2c-75b8-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:51:56.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8272" for this suite.
May 13 19:52:02.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:52:02.134: INFO: namespace emptydir-8272 deletion completed in 6.058897093s

• [SLOW TEST:8.219 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:52:02.134: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-557
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 19:52:02.260: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:52:04.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-557" for this suite.
May 13 19:52:42.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:52:42.486: INFO: namespace pods-557 deletion completed in 38.060295426s

• [SLOW TEST:40.352 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:52:42.486: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7997
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 19:52:42.617: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 13 19:52:42.625: INFO: Number of nodes with available pods: 0
May 13 19:52:42.625: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 19:52:43.630: INFO: Number of nodes with available pods: 0
May 13 19:52:43.630: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 19:52:44.630: INFO: Number of nodes with available pods: 1
May 13 19:52:44.630: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 13 19:52:44.648: INFO: Wrong image for pod: daemon-set-d4hwl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 13 19:52:45.656: INFO: Wrong image for pod: daemon-set-d4hwl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 13 19:52:46.656: INFO: Wrong image for pod: daemon-set-d4hwl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 13 19:52:47.656: INFO: Wrong image for pod: daemon-set-d4hwl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 13 19:52:47.656: INFO: Pod daemon-set-d4hwl is not available
May 13 19:52:48.656: INFO: Pod daemon-set-b9cgr is not available
STEP: Check that daemon pods are still running on every node of the cluster.
May 13 19:52:48.661: INFO: Number of nodes with available pods: 0
May 13 19:52:48.661: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 19:52:49.666: INFO: Number of nodes with available pods: 0
May 13 19:52:49.666: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 19:52:50.666: INFO: Number of nodes with available pods: 1
May 13 19:52:50.666: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7997, will wait for the garbage collector to delete the pods
May 13 19:52:50.735: INFO: Deleting DaemonSet.extensions daemon-set took: 7.173764ms
May 13 19:52:51.135: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.217973ms
May 13 19:52:54.037: INFO: Number of nodes with available pods: 0
May 13 19:52:54.037: INFO: Number of running nodes: 0, number of available pods: 0
May 13 19:52:54.039: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7997/daemonsets","resourceVersion":"9964"},"items":null}

May 13 19:52:54.040: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7997/pods","resourceVersion":"9964"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:52:54.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7997" for this suite.
May 13 19:53:00.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:53:00.112: INFO: namespace daemonsets-7997 deletion completed in 6.065948428s

• [SLOW TEST:17.626 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:53:00.113: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7526
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 13 19:53:02.768: INFO: Successfully updated pod "pod-update-b62e04d9-75b8-11e9-9153-920e960bc5b9"
STEP: verifying the updated pod is in kubernetes
May 13 19:53:02.772: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:53:02.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7526" for this suite.
May 13 19:53:24.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:53:24.833: INFO: namespace pods-7526 deletion completed in 22.05831963s

• [SLOW TEST:24.720 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:53:24.833: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3304
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:53:26.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3304" for this suite.
May 13 19:54:04.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:54:05.053: INFO: namespace kubelet-test-3304 deletion completed in 38.076432089s

• [SLOW TEST:40.220 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:54:05.053: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0513 19:54:11.196976      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 13 19:54:11.197: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:54:11.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1158" for this suite.
May 13 19:54:17.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:54:17.257: INFO: namespace gc-1158 deletion completed in 6.058318544s

• [SLOW TEST:12.203 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:54:17.257: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6140
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 19:54:17.386: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e42895e6-75b8-11e9-9153-920e960bc5b9" in namespace "downward-api-6140" to be "success or failure"
May 13 19:54:17.388: INFO: Pod "downwardapi-volume-e42895e6-75b8-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.240226ms
May 13 19:54:19.391: INFO: Pod "downwardapi-volume-e42895e6-75b8-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005000522s
STEP: Saw pod success
May 13 19:54:19.391: INFO: Pod "downwardapi-volume-e42895e6-75b8-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:54:19.393: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downwardapi-volume-e42895e6-75b8-11e9-9153-920e960bc5b9 container client-container: <nil>
STEP: delete the pod
May 13 19:54:19.409: INFO: Waiting for pod downwardapi-volume-e42895e6-75b8-11e9-9153-920e960bc5b9 to disappear
May 13 19:54:19.412: INFO: Pod downwardapi-volume-e42895e6-75b8-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:54:19.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6140" for this suite.
May 13 19:54:25.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:54:25.481: INFO: namespace downward-api-6140 deletion completed in 6.066875091s

• [SLOW TEST:8.224 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:54:25.481: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7373
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
May 13 19:54:25.611: INFO: Waiting up to 5m0s for pod "var-expansion-e90fa9a4-75b8-11e9-9153-920e960bc5b9" in namespace "var-expansion-7373" to be "success or failure"
May 13 19:54:25.613: INFO: Pod "var-expansion-e90fa9a4-75b8-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078962ms
May 13 19:54:27.616: INFO: Pod "var-expansion-e90fa9a4-75b8-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004893448s
STEP: Saw pod success
May 13 19:54:27.616: INFO: Pod "var-expansion-e90fa9a4-75b8-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:54:27.617: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod var-expansion-e90fa9a4-75b8-11e9-9153-920e960bc5b9 container dapi-container: <nil>
STEP: delete the pod
May 13 19:54:27.630: INFO: Waiting for pod var-expansion-e90fa9a4-75b8-11e9-9153-920e960bc5b9 to disappear
May 13 19:54:27.631: INFO: Pod var-expansion-e90fa9a4-75b8-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:54:27.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7373" for this suite.
May 13 19:54:33.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:54:33.694: INFO: namespace var-expansion-7373 deletion completed in 6.060343724s

• [SLOW TEST:8.213 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:54:33.694: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8894
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 13 19:54:33.823: INFO: Waiting up to 5m0s for pod "pod-edf4b4c2-75b8-11e9-9153-920e960bc5b9" in namespace "emptydir-8894" to be "success or failure"
May 13 19:54:33.825: INFO: Pod "pod-edf4b4c2-75b8-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.125341ms
May 13 19:54:35.828: INFO: Pod "pod-edf4b4c2-75b8-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005186s
STEP: Saw pod success
May 13 19:54:35.828: INFO: Pod "pod-edf4b4c2-75b8-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 19:54:35.830: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-edf4b4c2-75b8-11e9-9153-920e960bc5b9 container test-container: <nil>
STEP: delete the pod
May 13 19:54:35.848: INFO: Waiting for pod pod-edf4b4c2-75b8-11e9-9153-920e960bc5b9 to disappear
May 13 19:54:35.850: INFO: Pod pod-edf4b4c2-75b8-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 19:54:35.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8894" for this suite.
May 13 19:54:41.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 19:54:41.913: INFO: namespace emptydir-8894 deletion completed in 6.06022811s

• [SLOW TEST:8.218 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 19:54:41.913: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5088
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5088
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-5088
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5088
May 13 19:54:42.057: INFO: Found 0 stateful pods, waiting for 1
May 13 19:54:52.060: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 13 19:54:52.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 19:54:52.211: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 13 19:54:52.211: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 19:54:52.211: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 13 19:54:52.213: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 13 19:55:02.216: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 13 19:55:02.216: INFO: Waiting for statefulset status.replicas updated to 0
May 13 19:55:02.226: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
May 13 19:55:02.226: INFO: ss-0  ip-10-0-0-248.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:54:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:54:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:54:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:54:42 +0000 UTC  }]
May 13 19:55:02.226: INFO: 
May 13 19:55:02.226: INFO: StatefulSet ss has not reached scale 3, at 1
May 13 19:55:03.229: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997543541s
May 13 19:55:04.232: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994407292s
May 13 19:55:05.236: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.99109642s
May 13 19:55:06.238: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.987805513s
May 13 19:55:07.242: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.984891337s
May 13 19:55:08.245: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.98168246s
May 13 19:55:09.248: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.978547996s
May 13 19:55:10.251: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.975430603s
May 13 19:55:11.254: INFO: Verifying statefulset ss doesn't scale past 3 for another 972.344132ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5088
May 13 19:55:12.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:55:12.406: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 13 19:55:12.406: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 13 19:55:12.406: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 13 19:55:12.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:55:12.555: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 13 19:55:12.555: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 13 19:55:12.555: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 13 19:55:12.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:55:12.713: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 13 19:55:12.713: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 13 19:55:12.713: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 13 19:55:12.716: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
May 13 19:55:22.719: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 13 19:55:22.719: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 13 19:55:22.719: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 13 19:55:22.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 19:55:22.872: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 13 19:55:22.872: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 19:55:22.872: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 13 19:55:22.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 19:55:23.027: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 13 19:55:23.027: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 19:55:23.027: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 13 19:55:23.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 19:55:23.211: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 13 19:55:23.211: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 19:55:23.211: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 13 19:55:23.211: INFO: Waiting for statefulset status.replicas updated to 0
May 13 19:55:23.213: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May 13 19:55:33.218: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 13 19:55:33.219: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 13 19:55:33.219: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 13 19:55:33.228: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
May 13 19:55:33.228: INFO: ss-0  ip-10-0-0-248.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:54:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:54:42 +0000 UTC  }]
May 13 19:55:33.228: INFO: ss-1  ip-10-0-0-248.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:33.228: INFO: ss-2  ip-10-0-0-248.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:33.228: INFO: 
May 13 19:55:33.228: INFO: StatefulSet ss has not reached scale 0, at 3
May 13 19:55:34.231: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
May 13 19:55:34.231: INFO: ss-0  ip-10-0-0-248.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:54:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:54:42 +0000 UTC  }]
May 13 19:55:34.231: INFO: ss-1  ip-10-0-0-248.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:34.231: INFO: ss-2  ip-10-0-0-248.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:34.231: INFO: 
May 13 19:55:34.231: INFO: StatefulSet ss has not reached scale 0, at 3
May 13 19:55:35.235: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
May 13 19:55:35.235: INFO: ss-0  ip-10-0-0-248.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:54:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:54:42 +0000 UTC  }]
May 13 19:55:35.235: INFO: ss-1  ip-10-0-0-248.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:35.235: INFO: ss-2  ip-10-0-0-248.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:35.235: INFO: 
May 13 19:55:35.235: INFO: StatefulSet ss has not reached scale 0, at 3
May 13 19:55:36.238: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
May 13 19:55:36.238: INFO: ss-0  ip-10-0-0-248.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:54:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:54:42 +0000 UTC  }]
May 13 19:55:36.238: INFO: ss-1  ip-10-0-0-248.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:36.238: INFO: ss-2  ip-10-0-0-248.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:36.238: INFO: 
May 13 19:55:36.238: INFO: StatefulSet ss has not reached scale 0, at 3
May 13 19:55:37.241: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
May 13 19:55:37.241: INFO: ss-1  ip-10-0-0-248.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:37.241: INFO: ss-2  ip-10-0-0-248.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:37.241: INFO: 
May 13 19:55:37.241: INFO: StatefulSet ss has not reached scale 0, at 2
May 13 19:55:38.244: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
May 13 19:55:38.244: INFO: ss-1  ip-10-0-0-248.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:38.244: INFO: ss-2  ip-10-0-0-248.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:38.244: INFO: 
May 13 19:55:38.244: INFO: StatefulSet ss has not reached scale 0, at 2
May 13 19:55:39.247: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
May 13 19:55:39.247: INFO: ss-1  ip-10-0-0-248.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:39.247: INFO: ss-2  ip-10-0-0-248.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:39.247: INFO: 
May 13 19:55:39.247: INFO: StatefulSet ss has not reached scale 0, at 2
May 13 19:55:40.251: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
May 13 19:55:40.251: INFO: ss-1  ip-10-0-0-248.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:40.251: INFO: ss-2  ip-10-0-0-248.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:40.251: INFO: 
May 13 19:55:40.251: INFO: StatefulSet ss has not reached scale 0, at 2
May 13 19:55:41.254: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
May 13 19:55:41.254: INFO: ss-1  ip-10-0-0-248.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:41.254: INFO: ss-2  ip-10-0-0-248.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:41.254: INFO: 
May 13 19:55:41.254: INFO: StatefulSet ss has not reached scale 0, at 2
May 13 19:55:42.257: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
May 13 19:55:42.257: INFO: ss-1  ip-10-0-0-248.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:42.257: INFO: ss-2  ip-10-0-0-248.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 19:55:02 +0000 UTC  }]
May 13 19:55:42.257: INFO: 
May 13 19:55:42.257: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5088
May 13 19:55:43.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:55:43.347: INFO: rc: 1
May 13 19:55:43.347: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc001a66e10 exit status 1 <nil> <nil> true [0xc001957e20 0xc001957e38 0xc001957e50] [0xc001957e20 0xc001957e38 0xc001957e50] [0xc001957e30 0xc001957e48] [0x9bf9f0 0x9bf9f0] 0xc001e9c600 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

May 13 19:55:53.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:55:53.405: INFO: rc: 1
May 13 19:55:53.405: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001a67140 exit status 1 <nil> <nil> true [0xc001957e58 0xc001957e70 0xc001957e88] [0xc001957e58 0xc001957e70 0xc001957e88] [0xc001957e68 0xc001957e80] [0x9bf9f0 0x9bf9f0] 0xc001801380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:56:03.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:56:03.463: INFO: rc: 1
May 13 19:56:03.463: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001a67620 exit status 1 <nil> <nil> true [0xc001957e90 0xc001957ea8 0xc001957ec0] [0xc001957e90 0xc001957ea8 0xc001957ec0] [0xc001957ea0 0xc001957eb8] [0x9bf9f0 0x9bf9f0] 0xc00141af60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:56:13.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:56:13.520: INFO: rc: 1
May 13 19:56:13.520: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc003385da0 exit status 1 <nil> <nil> true [0xc0033e1e30 0xc0033e1e48 0xc0033e1e60] [0xc0033e1e30 0xc0033e1e48 0xc0033e1e60] [0xc0033e1e40 0xc0033e1e58] [0x9bf9f0 0x9bf9f0] 0xc00185be60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:56:23.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:56:23.581: INFO: rc: 1
May 13 19:56:23.581: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001a67950 exit status 1 <nil> <nil> true [0xc001957ec8 0xc001957ee0 0xc001957ef8] [0xc001957ec8 0xc001957ee0 0xc001957ef8] [0xc001957ed8 0xc001957ef0] [0x9bf9f0 0x9bf9f0] 0xc0018e76e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:56:33.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:56:33.639: INFO: rc: 1
May 13 19:56:33.639: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0023883f0 exit status 1 <nil> <nil> true [0xc003244008 0xc003244020 0xc003244038] [0xc003244008 0xc003244020 0xc003244038] [0xc003244018 0xc003244030] [0x9bf9f0 0x9bf9f0] 0xc0032ca2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:56:43.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:56:43.698: INFO: rc: 1
May 13 19:56:43.698: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002388720 exit status 1 <nil> <nil> true [0xc003244040 0xc003244058 0xc003244070] [0xc003244040 0xc003244058 0xc003244070] [0xc003244050 0xc003244068] [0x9bf9f0 0x9bf9f0] 0xc0032ca660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:56:53.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:56:53.754: INFO: rc: 1
May 13 19:56:53.754: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000b8a450 exit status 1 <nil> <nil> true [0xc00325a000 0xc00325a018 0xc00325a030] [0xc00325a000 0xc00325a018 0xc00325a030] [0xc00325a010 0xc00325a028] [0x9bf9f0 0x9bf9f0] 0xc001547260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:57:03.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:57:03.813: INFO: rc: 1
May 13 19:57:03.813: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000b8a7e0 exit status 1 <nil> <nil> true [0xc00325a038 0xc00325a050 0xc00325a068] [0xc00325a038 0xc00325a050 0xc00325a068] [0xc00325a048 0xc00325a060] [0x9bf9f0 0x9bf9f0] 0xc001908fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:57:13.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:57:13.869: INFO: rc: 1
May 13 19:57:13.869: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002356360 exit status 1 <nil> <nil> true [0xc00218c000 0xc00218c038 0xc00218c068] [0xc00218c000 0xc00218c038 0xc00218c068] [0xc00218c010 0xc00218c050] [0x9bf9f0 0x9bf9f0] 0xc001e9c780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:57:23.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:57:23.929: INFO: rc: 1
May 13 19:57:23.929: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0023567b0 exit status 1 <nil> <nil> true [0xc00218c070 0xc00218c090 0xc00218c0d0] [0xc00218c070 0xc00218c090 0xc00218c0d0] [0xc00218c080 0xc00218c0b8] [0x9bf9f0 0x9bf9f0] 0xc001ff18c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:57:33.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:57:33.987: INFO: rc: 1
May 13 19:57:33.987: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000b8ab70 exit status 1 <nil> <nil> true [0xc00325a070 0xc00325a088 0xc00325a0a0] [0xc00325a070 0xc00325a088 0xc00325a0a0] [0xc00325a080 0xc00325a098] [0x9bf9f0 0x9bf9f0] 0xc00185b740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:57:43.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:57:44.047: INFO: rc: 1
May 13 19:57:44.047: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002356b40 exit status 1 <nil> <nil> true [0xc00218c0d8 0xc00218c110 0xc00218c138] [0xc00218c0d8 0xc00218c110 0xc00218c138] [0xc00218c100 0xc00218c130] [0x9bf9f0 0x9bf9f0] 0xc001c9d380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:57:54.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:57:54.111: INFO: rc: 1
May 13 19:57:54.111: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002356ea0 exit status 1 <nil> <nil> true [0xc00218c160 0xc00218c1a8 0xc00218c1e8] [0xc00218c160 0xc00218c1a8 0xc00218c1e8] [0xc00218c190 0xc00218c1e0] [0x9bf9f0 0x9bf9f0] 0xc001bda8a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:58:04.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:58:04.169: INFO: rc: 1
May 13 19:58:04.169: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002357200 exit status 1 <nil> <nil> true [0xc00218c200 0xc00218c230 0xc00218c280] [0xc00218c200 0xc00218c230 0xc00218c280] [0xc00218c210 0xc00218c260] [0x9bf9f0 0x9bf9f0] 0xc001da87e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:58:14.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:58:14.228: INFO: rc: 1
May 13 19:58:14.228: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002357590 exit status 1 <nil> <nil> true [0xc00218c298 0xc00218c2b0 0xc00218c2d8] [0xc00218c298 0xc00218c2b0 0xc00218c2d8] [0xc00218c2a8 0xc00218c2c0] [0x9bf9f0 0x9bf9f0] 0xc001ff4900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:58:24.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:58:24.287: INFO: rc: 1
May 13 19:58:24.287: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0023578f0 exit status 1 <nil> <nil> true [0xc00218c2f0 0xc00218c310 0xc00218c360] [0xc00218c2f0 0xc00218c310 0xc00218c360] [0xc00218c300 0xc00218c348] [0x9bf9f0 0x9bf9f0] 0xc0020a9920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:58:34.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:58:34.345: INFO: rc: 1
May 13 19:58:34.345: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d2c210 exit status 1 <nil> <nil> true [0xc00325a008 0xc00325a020 0xc00325a038] [0xc00325a008 0xc00325a020 0xc00325a038] [0xc00325a018 0xc00325a030] [0x9bf9f0 0x9bf9f0] 0xc001da87e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:58:44.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:58:44.408: INFO: rc: 1
May 13 19:58:44.408: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d2c570 exit status 1 <nil> <nil> true [0xc00325a040 0xc00325a058 0xc00325a070] [0xc00325a040 0xc00325a058 0xc00325a070] [0xc00325a050 0xc00325a068] [0x9bf9f0 0x9bf9f0] 0xc001bda360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:58:54.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:58:54.467: INFO: rc: 1
May 13 19:58:54.467: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d2c900 exit status 1 <nil> <nil> true [0xc00325a078 0xc00325a090 0xc00325a0a8] [0xc00325a078 0xc00325a090 0xc00325a0a8] [0xc00325a088 0xc00325a0a0] [0x9bf9f0 0x9bf9f0] 0xc001bdbd40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:59:04.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:59:04.525: INFO: rc: 1
May 13 19:59:04.525: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002388420 exit status 1 <nil> <nil> true [0xc003244000 0xc003244018 0xc003244030] [0xc003244000 0xc003244018 0xc003244030] [0xc003244010 0xc003244028] [0x9bf9f0 0x9bf9f0] 0xc001ff18c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:59:14.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:59:14.584: INFO: rc: 1
May 13 19:59:14.584: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0023887b0 exit status 1 <nil> <nil> true [0xc003244038 0xc003244050 0xc003244068] [0xc003244038 0xc003244050 0xc003244068] [0xc003244048 0xc003244060] [0x9bf9f0 0x9bf9f0] 0xc001d7bc80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:59:24.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:59:24.643: INFO: rc: 1
May 13 19:59:24.644: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d2cc90 exit status 1 <nil> <nil> true [0xc00325a0b0 0xc00325a0c8 0xc00325a0e0] [0xc00325a0b0 0xc00325a0c8 0xc00325a0e0] [0xc00325a0c0 0xc00325a0d8] [0x9bf9f0 0x9bf9f0] 0xc001c9d7a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:59:34.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:59:34.701: INFO: rc: 1
May 13 19:59:34.701: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d2cfc0 exit status 1 <nil> <nil> true [0xc00325a0e8 0xc00325a100 0xc00325a118] [0xc00325a0e8 0xc00325a100 0xc00325a118] [0xc00325a0f8 0xc00325a110] [0x9bf9f0 0x9bf9f0] 0xc0014872c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:59:44.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:59:44.758: INFO: rc: 1
May 13 19:59:44.758: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d2d320 exit status 1 <nil> <nil> true [0xc00325a120 0xc00325a138 0xc00325a150] [0xc00325a120 0xc00325a138 0xc00325a150] [0xc00325a130 0xc00325a148] [0x9bf9f0 0x9bf9f0] 0xc00156bbc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 19:59:54.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 19:59:54.821: INFO: rc: 1
May 13 19:59:54.821: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002d2d650 exit status 1 <nil> <nil> true [0xc00325a158 0xc00325a170 0xc00325a188] [0xc00325a158 0xc00325a170 0xc00325a188] [0xc00325a168 0xc00325a180] [0x9bf9f0 0x9bf9f0] 0xc00185b740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 20:00:04.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 20:00:04.881: INFO: rc: 1
May 13 20:00:04.881: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000b8a600 exit status 1 <nil> <nil> true [0xc00218c000 0xc00218c038 0xc00218c068] [0xc00218c000 0xc00218c038 0xc00218c068] [0xc00218c010 0xc00218c050] [0x9bf9f0 0x9bf9f0] 0xc0032ca2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 20:00:14.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 20:00:14.942: INFO: rc: 1
May 13 20:00:14.942: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000b8a9c0 exit status 1 <nil> <nil> true [0xc00218c070 0xc00218c090 0xc00218c0d0] [0xc00218c070 0xc00218c090 0xc00218c0d0] [0xc00218c080 0xc00218c0b8] [0x9bf9f0 0x9bf9f0] 0xc0032ca660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 20:00:24.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 20:00:25.002: INFO: rc: 1
May 13 20:00:25.002: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000b8ad20 exit status 1 <nil> <nil> true [0xc00218c0d8 0xc00218c110 0xc00218c138] [0xc00218c0d8 0xc00218c110 0xc00218c138] [0xc00218c100 0xc00218c130] [0x9bf9f0 0x9bf9f0] 0xc0032caa20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 20:00:35.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 20:00:35.063: INFO: rc: 1
May 13 20:00:35.063: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0023882d0 exit status 1 <nil> <nil> true [0xc003244008 0xc003244020 0xc003244038] [0xc003244008 0xc003244020 0xc003244038] [0xc003244018 0xc003244030] [0x9bf9f0 0x9bf9f0] 0xc001547260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 13 20:00:45.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-5088 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 20:00:45.120: INFO: rc: 1
May 13 20:00:45.120: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
May 13 20:00:45.120: INFO: Scaling statefulset ss to 0
May 13 20:00:45.128: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 13 20:00:45.130: INFO: Deleting all statefulset in ns statefulset-5088
May 13 20:00:45.132: INFO: Scaling statefulset ss to 0
May 13 20:00:45.139: INFO: Waiting for statefulset status.replicas updated to 0
May 13 20:00:45.141: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:00:45.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5088" for this suite.
May 13 20:00:51.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:00:51.215: INFO: namespace statefulset-5088 deletion completed in 6.065293488s

• [SLOW TEST:369.303 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:00:51.215: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1586
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0513 20:00:52.375251      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 13 20:00:52.375: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:00:52.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1586" for this suite.
May 13 20:00:58.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:00:58.436: INFO: namespace gc-1586 deletion completed in 6.059760129s

• [SLOW TEST:7.221 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:00:58.437: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5022
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 20:00:58.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 version'
May 13 20:00:58.629: INFO: stderr: ""
May 13 20:00:58.629: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:53:57Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:45:25Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:00:58.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5022" for this suite.
May 13 20:01:04.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:01:04.691: INFO: namespace kubectl-5022 deletion completed in 6.059562104s

• [SLOW TEST:6.255 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:01:04.691: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9913
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-d7026fdd-75b9-11e9-9153-920e960bc5b9
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-d7026fdd-75b9-11e9-9153-920e960bc5b9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:01:08.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9913" for this suite.
May 13 20:01:30.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:01:30.917: INFO: namespace configmap-9913 deletion completed in 22.060094252s

• [SLOW TEST:26.226 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:01:30.917: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-808
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-808.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-808.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 13 20:01:35.065: INFO: Unable to read wheezy_udp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:35.067: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:35.073: INFO: Unable to read jessie_udp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:35.074: INFO: Unable to read jessie_tcp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:35.074: INFO: Lookups using dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

May 13 20:01:40.083: INFO: Unable to read wheezy_udp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:40.085: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:40.091: INFO: Unable to read jessie_udp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:40.093: INFO: Unable to read jessie_tcp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:40.093: INFO: Lookups using dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

May 13 20:01:45.081: INFO: Unable to read wheezy_udp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:45.082: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:45.088: INFO: Unable to read jessie_udp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:45.089: INFO: Unable to read jessie_tcp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:45.090: INFO: Lookups using dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

May 13 20:01:50.081: INFO: Unable to read wheezy_udp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:50.083: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:50.088: INFO: Unable to read jessie_udp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:50.090: INFO: Unable to read jessie_tcp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:50.090: INFO: Lookups using dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

May 13 20:01:55.081: INFO: Unable to read wheezy_udp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:55.082: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:55.088: INFO: Unable to read jessie_udp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:55.090: INFO: Unable to read jessie_tcp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:01:55.090: INFO: Lookups using dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

May 13 20:02:00.082: INFO: Unable to read wheezy_udp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:02:00.084: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:02:00.090: INFO: Unable to read jessie_udp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:02:00.092: INFO: Unable to read jessie_tcp@PodARecord from pod dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9)
May 13 20:02:00.092: INFO: Lookups using dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

May 13 20:02:05.091: INFO: DNS probes using dns-808/dns-test-e6a46a7d-75b9-11e9-9153-920e960bc5b9 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:02:05.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-808" for this suite.
May 13 20:02:11.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:02:11.198: INFO: namespace dns-808 deletion completed in 6.087029619s

• [SLOW TEST:40.281 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:02:11.199: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4871
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
May 13 20:02:11.323: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-742566056 proxy --unix-socket=/tmp/kubectl-proxy-unix746665995/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:02:11.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4871" for this suite.
May 13 20:02:17.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:02:17.438: INFO: namespace kubectl-4871 deletion completed in 6.058343103s

• [SLOW TEST:6.240 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:02:17.438: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8237
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:02:19.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8237" for this suite.
May 13 20:03:09.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:03:09.639: INFO: namespace kubelet-test-8237 deletion completed in 50.058473046s

• [SLOW TEST:52.200 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:03:09.639: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1928
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 13 20:03:09.773: INFO: Waiting up to 5m0s for pod "pod-217c6f16-75ba-11e9-9153-920e960bc5b9" in namespace "emptydir-1928" to be "success or failure"
May 13 20:03:09.775: INFO: Pod "pod-217c6f16-75ba-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.8821ms
May 13 20:03:11.778: INFO: Pod "pod-217c6f16-75ba-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004501262s
May 13 20:03:13.781: INFO: Pod "pod-217c6f16-75ba-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007181548s
STEP: Saw pod success
May 13 20:03:13.781: INFO: Pod "pod-217c6f16-75ba-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:03:13.782: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-217c6f16-75ba-11e9-9153-920e960bc5b9 container test-container: <nil>
STEP: delete the pod
May 13 20:03:13.795: INFO: Waiting for pod pod-217c6f16-75ba-11e9-9153-920e960bc5b9 to disappear
May 13 20:03:13.797: INFO: Pod pod-217c6f16-75ba-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:03:13.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1928" for this suite.
May 13 20:03:19.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:03:19.858: INFO: namespace emptydir-1928 deletion completed in 6.058695233s

• [SLOW TEST:10.219 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:03:19.858: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-8625
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8625
I0513 20:03:19.985615      16 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8625, replica count: 1
I0513 20:03:21.039447      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0513 20:03:22.039692      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0513 20:03:23.039907      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 13 20:03:23.147: INFO: Created: latency-svc-2l6q9
May 13 20:03:23.149: INFO: Got endpoints: latency-svc-2l6q9 [9.80342ms]
May 13 20:03:23.160: INFO: Created: latency-svc-g9p6h
May 13 20:03:23.164: INFO: Got endpoints: latency-svc-g9p6h [14.420805ms]
May 13 20:03:23.167: INFO: Created: latency-svc-mnbgg
May 13 20:03:23.171: INFO: Got endpoints: latency-svc-mnbgg [20.949414ms]
May 13 20:03:23.171: INFO: Created: latency-svc-xv4vt
May 13 20:03:23.175: INFO: Got endpoints: latency-svc-xv4vt [25.062816ms]
May 13 20:03:23.175: INFO: Created: latency-svc-j4vcn
May 13 20:03:23.181: INFO: Got endpoints: latency-svc-j4vcn [30.830204ms]
May 13 20:03:23.182: INFO: Created: latency-svc-nc6ht
May 13 20:03:23.184: INFO: Got endpoints: latency-svc-nc6ht [33.466419ms]
May 13 20:03:23.188: INFO: Created: latency-svc-7xrvm
May 13 20:03:23.194: INFO: Created: latency-svc-x5jt4
May 13 20:03:23.194: INFO: Got endpoints: latency-svc-7xrvm [43.20313ms]
May 13 20:03:23.197: INFO: Got endpoints: latency-svc-x5jt4 [46.702877ms]
May 13 20:03:23.203: INFO: Created: latency-svc-2rs9f
May 13 20:03:23.205: INFO: Got endpoints: latency-svc-2rs9f [54.001918ms]
May 13 20:03:23.207: INFO: Created: latency-svc-kcbzp
May 13 20:03:23.213: INFO: Got endpoints: latency-svc-kcbzp [61.714827ms]
May 13 20:03:23.213: INFO: Created: latency-svc-ktqpn
May 13 20:03:23.216: INFO: Got endpoints: latency-svc-ktqpn [65.297657ms]
May 13 20:03:23.216: INFO: Created: latency-svc-v6b4w
May 13 20:03:23.221: INFO: Got endpoints: latency-svc-v6b4w [70.303232ms]
May 13 20:03:23.223: INFO: Created: latency-svc-2zjbb
May 13 20:03:23.226: INFO: Got endpoints: latency-svc-2zjbb [75.427656ms]
May 13 20:03:23.230: INFO: Created: latency-svc-4lg2j
May 13 20:03:23.234: INFO: Got endpoints: latency-svc-4lg2j [83.265678ms]
May 13 20:03:23.237: INFO: Created: latency-svc-5smqf
May 13 20:03:23.242: INFO: Created: latency-svc-rwm74
May 13 20:03:23.242: INFO: Got endpoints: latency-svc-5smqf [90.791664ms]
May 13 20:03:23.246: INFO: Created: latency-svc-f775h
May 13 20:03:23.246: INFO: Got endpoints: latency-svc-rwm74 [94.957188ms]
May 13 20:03:23.248: INFO: Got endpoints: latency-svc-f775h [84.31765ms]
May 13 20:03:23.252: INFO: Created: latency-svc-mkhwz
May 13 20:03:23.257: INFO: Created: latency-svc-n5657
May 13 20:03:23.257: INFO: Got endpoints: latency-svc-mkhwz [86.05779ms]
May 13 20:03:23.261: INFO: Created: latency-svc-zlvsk
May 13 20:03:23.261: INFO: Got endpoints: latency-svc-n5657 [86.41398ms]
May 13 20:03:23.267: INFO: Created: latency-svc-cxvph
May 13 20:03:23.267: INFO: Got endpoints: latency-svc-zlvsk [85.830046ms]
May 13 20:03:23.270: INFO: Created: latency-svc-l2jzb
May 13 20:03:23.274: INFO: Got endpoints: latency-svc-l2jzb [79.629627ms]
May 13 20:03:23.274: INFO: Got endpoints: latency-svc-cxvph [89.819179ms]
May 13 20:03:23.277: INFO: Created: latency-svc-qpm2w
May 13 20:03:23.283: INFO: Created: latency-svc-9nzk2
May 13 20:03:23.283: INFO: Got endpoints: latency-svc-qpm2w [85.802109ms]
May 13 20:03:23.284: INFO: Got endpoints: latency-svc-9nzk2 [78.813894ms]
May 13 20:03:23.289: INFO: Created: latency-svc-xq5mq
May 13 20:03:23.293: INFO: Created: latency-svc-46hjh
May 13 20:03:23.293: INFO: Got endpoints: latency-svc-xq5mq [80.712702ms]
May 13 20:03:23.298: INFO: Created: latency-svc-rzg2p
May 13 20:03:23.298: INFO: Got endpoints: latency-svc-46hjh [82.291156ms]
May 13 20:03:23.302: INFO: Got endpoints: latency-svc-rzg2p [81.655018ms]
May 13 20:03:23.305: INFO: Created: latency-svc-kt7cz
May 13 20:03:23.308: INFO: Got endpoints: latency-svc-kt7cz [81.888967ms]
May 13 20:03:23.312: INFO: Created: latency-svc-zgqt8
May 13 20:03:23.321: INFO: Created: latency-svc-kxjfc
May 13 20:03:23.321: INFO: Created: latency-svc-r8bhq
May 13 20:03:23.321: INFO: Got endpoints: latency-svc-r8bhq [79.570613ms]
May 13 20:03:23.322: INFO: Got endpoints: latency-svc-zgqt8 [87.398241ms]
May 13 20:03:23.326: INFO: Got endpoints: latency-svc-kxjfc [79.744286ms]
May 13 20:03:23.327: INFO: Created: latency-svc-wkkr4
May 13 20:03:23.332: INFO: Got endpoints: latency-svc-wkkr4 [83.835341ms]
May 13 20:03:23.333: INFO: Created: latency-svc-c98sj
May 13 20:03:23.339: INFO: Got endpoints: latency-svc-c98sj [81.754129ms]
May 13 20:03:23.344: INFO: Created: latency-svc-9gbq7
May 13 20:03:23.351: INFO: Created: latency-svc-qfddx
May 13 20:03:23.354: INFO: Got endpoints: latency-svc-9gbq7 [92.592067ms]
May 13 20:03:23.360: INFO: Created: latency-svc-blxrv
May 13 20:03:23.364: INFO: Created: latency-svc-rv78l
May 13 20:03:23.370: INFO: Created: latency-svc-lcsb4
May 13 20:03:23.374: INFO: Created: latency-svc-jzpc7
May 13 20:03:23.380: INFO: Created: latency-svc-qfwpw
May 13 20:03:23.383: INFO: Created: latency-svc-sd7bj
May 13 20:03:23.389: INFO: Created: latency-svc-hxq57
May 13 20:03:23.391: INFO: Created: latency-svc-xf9wv
May 13 20:03:23.421: INFO: Got endpoints: latency-svc-qfddx [153.737106ms]
May 13 20:03:23.433: INFO: Created: latency-svc-mkknc
May 13 20:03:23.450: INFO: Created: latency-svc-n7sgx
May 13 20:03:23.461: INFO: Created: latency-svc-xtjrg
May 13 20:03:23.461: INFO: Got endpoints: latency-svc-blxrv [187.392561ms]
May 13 20:03:23.474: INFO: Created: latency-svc-dg9hr
May 13 20:03:23.485: INFO: Created: latency-svc-khfzp
May 13 20:03:23.491: INFO: Created: latency-svc-bzgtb
May 13 20:03:23.501: INFO: Created: latency-svc-khbwg
May 13 20:03:23.509: INFO: Created: latency-svc-5l2rz
May 13 20:03:23.514: INFO: Got endpoints: latency-svc-rv78l [239.987911ms]
May 13 20:03:23.537: INFO: Created: latency-svc-rsxcz
May 13 20:03:23.553: INFO: Got endpoints: latency-svc-lcsb4 [270.22743ms]
May 13 20:03:23.575: INFO: Created: latency-svc-tvchl
May 13 20:03:23.599: INFO: Got endpoints: latency-svc-jzpc7 [315.524802ms]
May 13 20:03:23.606: INFO: Created: latency-svc-h7f6q
May 13 20:03:23.649: INFO: Got endpoints: latency-svc-qfwpw [355.395702ms]
May 13 20:03:23.656: INFO: Created: latency-svc-zwk2l
May 13 20:03:23.699: INFO: Got endpoints: latency-svc-sd7bj [400.322594ms]
May 13 20:03:23.706: INFO: Created: latency-svc-5nxmz
May 13 20:03:23.749: INFO: Got endpoints: latency-svc-hxq57 [446.688122ms]
May 13 20:03:23.756: INFO: Created: latency-svc-ft54g
May 13 20:03:23.799: INFO: Got endpoints: latency-svc-xf9wv [490.557083ms]
May 13 20:03:23.805: INFO: Created: latency-svc-6d46n
May 13 20:03:23.851: INFO: Got endpoints: latency-svc-mkknc [529.955965ms]
May 13 20:03:23.859: INFO: Created: latency-svc-2zwmk
May 13 20:03:23.899: INFO: Got endpoints: latency-svc-n7sgx [577.294923ms]
May 13 20:03:23.906: INFO: Created: latency-svc-2cfpc
May 13 20:03:23.949: INFO: Got endpoints: latency-svc-xtjrg [622.901114ms]
May 13 20:03:23.957: INFO: Created: latency-svc-2smnc
May 13 20:03:23.999: INFO: Got endpoints: latency-svc-dg9hr [666.185273ms]
May 13 20:03:24.005: INFO: Created: latency-svc-b6nk5
May 13 20:03:24.048: INFO: Got endpoints: latency-svc-khfzp [709.487188ms]
May 13 20:03:24.056: INFO: Created: latency-svc-k86dz
May 13 20:03:24.099: INFO: Got endpoints: latency-svc-bzgtb [745.171829ms]
May 13 20:03:24.109: INFO: Created: latency-svc-ndftt
May 13 20:03:24.148: INFO: Got endpoints: latency-svc-khbwg [727.544265ms]
May 13 20:03:24.157: INFO: Created: latency-svc-bgckj
May 13 20:03:24.199: INFO: Got endpoints: latency-svc-5l2rz [737.947995ms]
May 13 20:03:24.206: INFO: Created: latency-svc-gzrts
May 13 20:03:24.249: INFO: Got endpoints: latency-svc-rsxcz [734.621201ms]
May 13 20:03:24.254: INFO: Created: latency-svc-gpjfp
May 13 20:03:24.299: INFO: Got endpoints: latency-svc-tvchl [745.04771ms]
May 13 20:03:24.307: INFO: Created: latency-svc-j2z94
May 13 20:03:24.348: INFO: Got endpoints: latency-svc-h7f6q [749.010669ms]
May 13 20:03:24.355: INFO: Created: latency-svc-hjhgj
May 13 20:03:24.399: INFO: Got endpoints: latency-svc-zwk2l [749.808753ms]
May 13 20:03:24.404: INFO: Created: latency-svc-gf6bz
May 13 20:03:24.449: INFO: Got endpoints: latency-svc-5nxmz [749.712238ms]
May 13 20:03:24.455: INFO: Created: latency-svc-8bt4m
May 13 20:03:24.498: INFO: Got endpoints: latency-svc-ft54g [749.467135ms]
May 13 20:03:24.505: INFO: Created: latency-svc-wlr4n
May 13 20:03:24.549: INFO: Got endpoints: latency-svc-6d46n [749.88228ms]
May 13 20:03:24.554: INFO: Created: latency-svc-594nz
May 13 20:03:24.599: INFO: Got endpoints: latency-svc-2zwmk [747.175006ms]
May 13 20:03:24.604: INFO: Created: latency-svc-f5d7w
May 13 20:03:24.649: INFO: Got endpoints: latency-svc-2cfpc [749.744028ms]
May 13 20:03:24.655: INFO: Created: latency-svc-v28zb
May 13 20:03:24.699: INFO: Got endpoints: latency-svc-2smnc [750.307165ms]
May 13 20:03:24.704: INFO: Created: latency-svc-hrrbc
May 13 20:03:24.749: INFO: Got endpoints: latency-svc-b6nk5 [749.927142ms]
May 13 20:03:24.754: INFO: Created: latency-svc-94tgj
May 13 20:03:24.799: INFO: Got endpoints: latency-svc-k86dz [750.310281ms]
May 13 20:03:24.804: INFO: Created: latency-svc-t2b85
May 13 20:03:24.849: INFO: Got endpoints: latency-svc-ndftt [749.307946ms]
May 13 20:03:24.854: INFO: Created: latency-svc-xnjsz
May 13 20:03:24.899: INFO: Got endpoints: latency-svc-bgckj [750.549471ms]
May 13 20:03:24.904: INFO: Created: latency-svc-lvkbg
May 13 20:03:24.948: INFO: Got endpoints: latency-svc-gzrts [749.402319ms]
May 13 20:03:24.954: INFO: Created: latency-svc-vb9hp
May 13 20:03:24.999: INFO: Got endpoints: latency-svc-gpjfp [750.05335ms]
May 13 20:03:25.004: INFO: Created: latency-svc-g7mrl
May 13 20:03:25.048: INFO: Got endpoints: latency-svc-j2z94 [749.295557ms]
May 13 20:03:25.054: INFO: Created: latency-svc-cs8z8
May 13 20:03:25.098: INFO: Got endpoints: latency-svc-hjhgj [750.16044ms]
May 13 20:03:25.104: INFO: Created: latency-svc-8q27n
May 13 20:03:25.148: INFO: Got endpoints: latency-svc-gf6bz [749.635165ms]
May 13 20:03:25.155: INFO: Created: latency-svc-qhmpn
May 13 20:03:25.198: INFO: Got endpoints: latency-svc-8bt4m [749.63373ms]
May 13 20:03:25.204: INFO: Created: latency-svc-q47br
May 13 20:03:25.250: INFO: Got endpoints: latency-svc-wlr4n [751.224015ms]
May 13 20:03:25.256: INFO: Created: latency-svc-nvmrx
May 13 20:03:25.299: INFO: Got endpoints: latency-svc-594nz [749.785139ms]
May 13 20:03:25.306: INFO: Created: latency-svc-swckv
May 13 20:03:25.349: INFO: Got endpoints: latency-svc-f5d7w [749.928652ms]
May 13 20:03:25.355: INFO: Created: latency-svc-c2t4t
May 13 20:03:25.400: INFO: Got endpoints: latency-svc-v28zb [751.424046ms]
May 13 20:03:25.406: INFO: Created: latency-svc-rt5tx
May 13 20:03:25.450: INFO: Got endpoints: latency-svc-hrrbc [750.870886ms]
May 13 20:03:25.457: INFO: Created: latency-svc-qltzh
May 13 20:03:25.499: INFO: Got endpoints: latency-svc-94tgj [750.231307ms]
May 13 20:03:25.505: INFO: Created: latency-svc-vx7f5
May 13 20:03:25.549: INFO: Got endpoints: latency-svc-t2b85 [749.786987ms]
May 13 20:03:25.555: INFO: Created: latency-svc-659nl
May 13 20:03:25.598: INFO: Got endpoints: latency-svc-xnjsz [749.757001ms]
May 13 20:03:25.604: INFO: Created: latency-svc-fbqkl
May 13 20:03:25.649: INFO: Got endpoints: latency-svc-lvkbg [749.969398ms]
May 13 20:03:25.655: INFO: Created: latency-svc-8g8mz
May 13 20:03:25.700: INFO: Got endpoints: latency-svc-vb9hp [751.372843ms]
May 13 20:03:25.706: INFO: Created: latency-svc-hfw62
May 13 20:03:25.750: INFO: Got endpoints: latency-svc-g7mrl [750.918858ms]
May 13 20:03:25.758: INFO: Created: latency-svc-prh7s
May 13 20:03:25.800: INFO: Got endpoints: latency-svc-cs8z8 [751.762672ms]
May 13 20:03:25.811: INFO: Created: latency-svc-5fjn8
May 13 20:03:25.849: INFO: Got endpoints: latency-svc-8q27n [750.201022ms]
May 13 20:03:25.854: INFO: Created: latency-svc-bq8jk
May 13 20:03:25.898: INFO: Got endpoints: latency-svc-qhmpn [750.068123ms]
May 13 20:03:25.904: INFO: Created: latency-svc-4dfc7
May 13 20:03:25.949: INFO: Got endpoints: latency-svc-q47br [750.233963ms]
May 13 20:03:25.954: INFO: Created: latency-svc-tfqq6
May 13 20:03:25.998: INFO: Got endpoints: latency-svc-nvmrx [748.528322ms]
May 13 20:03:26.004: INFO: Created: latency-svc-v2wqk
May 13 20:03:26.050: INFO: Got endpoints: latency-svc-swckv [750.974423ms]
May 13 20:03:26.058: INFO: Created: latency-svc-dhg4x
May 13 20:03:26.099: INFO: Got endpoints: latency-svc-c2t4t [750.58741ms]
May 13 20:03:26.105: INFO: Created: latency-svc-vgs56
May 13 20:03:26.150: INFO: Got endpoints: latency-svc-rt5tx [749.842797ms]
May 13 20:03:26.157: INFO: Created: latency-svc-qf8c9
May 13 20:03:26.199: INFO: Got endpoints: latency-svc-qltzh [749.372348ms]
May 13 20:03:26.205: INFO: Created: latency-svc-86gv8
May 13 20:03:26.249: INFO: Got endpoints: latency-svc-vx7f5 [750.145821ms]
May 13 20:03:26.255: INFO: Created: latency-svc-hndj7
May 13 20:03:26.299: INFO: Got endpoints: latency-svc-659nl [750.377594ms]
May 13 20:03:26.314: INFO: Created: latency-svc-7cn78
May 13 20:03:26.348: INFO: Got endpoints: latency-svc-fbqkl [749.969103ms]
May 13 20:03:26.356: INFO: Created: latency-svc-c2hdf
May 13 20:03:26.400: INFO: Got endpoints: latency-svc-8g8mz [750.802805ms]
May 13 20:03:26.406: INFO: Created: latency-svc-pc6jh
May 13 20:03:26.449: INFO: Got endpoints: latency-svc-hfw62 [748.924715ms]
May 13 20:03:26.456: INFO: Created: latency-svc-6kt4r
May 13 20:03:26.499: INFO: Got endpoints: latency-svc-prh7s [749.372554ms]
May 13 20:03:26.506: INFO: Created: latency-svc-mbhr6
May 13 20:03:26.549: INFO: Got endpoints: latency-svc-5fjn8 [749.206252ms]
May 13 20:03:26.555: INFO: Created: latency-svc-mw7jw
May 13 20:03:26.598: INFO: Got endpoints: latency-svc-bq8jk [749.763483ms]
May 13 20:03:26.605: INFO: Created: latency-svc-c2wrh
May 13 20:03:26.649: INFO: Got endpoints: latency-svc-4dfc7 [750.420492ms]
May 13 20:03:26.655: INFO: Created: latency-svc-str94
May 13 20:03:26.698: INFO: Got endpoints: latency-svc-tfqq6 [749.895057ms]
May 13 20:03:26.707: INFO: Created: latency-svc-n7lwx
May 13 20:03:26.749: INFO: Got endpoints: latency-svc-v2wqk [750.557444ms]
May 13 20:03:26.756: INFO: Created: latency-svc-mnz79
May 13 20:03:26.799: INFO: Got endpoints: latency-svc-dhg4x [749.228164ms]
May 13 20:03:26.805: INFO: Created: latency-svc-xhzgj
May 13 20:03:26.849: INFO: Got endpoints: latency-svc-vgs56 [749.536089ms]
May 13 20:03:26.855: INFO: Created: latency-svc-pq9l6
May 13 20:03:26.899: INFO: Got endpoints: latency-svc-qf8c9 [748.990311ms]
May 13 20:03:26.905: INFO: Created: latency-svc-r562x
May 13 20:03:26.949: INFO: Got endpoints: latency-svc-86gv8 [749.413865ms]
May 13 20:03:26.955: INFO: Created: latency-svc-b2wzq
May 13 20:03:26.999: INFO: Got endpoints: latency-svc-hndj7 [750.088105ms]
May 13 20:03:27.005: INFO: Created: latency-svc-v4g2f
May 13 20:03:27.048: INFO: Got endpoints: latency-svc-7cn78 [749.134339ms]
May 13 20:03:27.055: INFO: Created: latency-svc-slmqj
May 13 20:03:27.099: INFO: Got endpoints: latency-svc-c2hdf [750.850265ms]
May 13 20:03:27.107: INFO: Created: latency-svc-4vh9w
May 13 20:03:27.149: INFO: Got endpoints: latency-svc-pc6jh [749.30449ms]
May 13 20:03:27.155: INFO: Created: latency-svc-nnf88
May 13 20:03:27.198: INFO: Got endpoints: latency-svc-6kt4r [749.526334ms]
May 13 20:03:27.205: INFO: Created: latency-svc-4q6gx
May 13 20:03:27.249: INFO: Got endpoints: latency-svc-mbhr6 [749.935111ms]
May 13 20:03:27.255: INFO: Created: latency-svc-shbjd
May 13 20:03:27.299: INFO: Got endpoints: latency-svc-mw7jw [749.809819ms]
May 13 20:03:27.306: INFO: Created: latency-svc-rt2vc
May 13 20:03:27.350: INFO: Got endpoints: latency-svc-c2wrh [751.221907ms]
May 13 20:03:27.356: INFO: Created: latency-svc-mt8dh
May 13 20:03:27.399: INFO: Got endpoints: latency-svc-str94 [750.107565ms]
May 13 20:03:27.406: INFO: Created: latency-svc-j5mj6
May 13 20:03:27.450: INFO: Got endpoints: latency-svc-n7lwx [751.259647ms]
May 13 20:03:27.455: INFO: Created: latency-svc-hwx7k
May 13 20:03:27.499: INFO: Got endpoints: latency-svc-mnz79 [749.613289ms]
May 13 20:03:27.506: INFO: Created: latency-svc-695ht
May 13 20:03:27.551: INFO: Got endpoints: latency-svc-xhzgj [752.472312ms]
May 13 20:03:27.561: INFO: Created: latency-svc-4vwcf
May 13 20:03:27.599: INFO: Got endpoints: latency-svc-pq9l6 [750.210387ms]
May 13 20:03:27.606: INFO: Created: latency-svc-nn2t7
May 13 20:03:27.649: INFO: Got endpoints: latency-svc-r562x [750.104075ms]
May 13 20:03:27.656: INFO: Created: latency-svc-xbg4m
May 13 20:03:27.699: INFO: Got endpoints: latency-svc-b2wzq [749.814028ms]
May 13 20:03:27.706: INFO: Created: latency-svc-8ztj5
May 13 20:03:27.749: INFO: Got endpoints: latency-svc-v4g2f [749.383919ms]
May 13 20:03:27.756: INFO: Created: latency-svc-gl2ll
May 13 20:03:27.799: INFO: Got endpoints: latency-svc-slmqj [751.052853ms]
May 13 20:03:27.805: INFO: Created: latency-svc-ftvlv
May 13 20:03:27.849: INFO: Got endpoints: latency-svc-4vh9w [749.861644ms]
May 13 20:03:27.856: INFO: Created: latency-svc-85wbl
May 13 20:03:27.899: INFO: Got endpoints: latency-svc-nnf88 [750.129547ms]
May 13 20:03:27.906: INFO: Created: latency-svc-rhqhk
May 13 20:03:27.949: INFO: Got endpoints: latency-svc-4q6gx [750.485974ms]
May 13 20:03:27.956: INFO: Created: latency-svc-dsfdw
May 13 20:03:28.000: INFO: Got endpoints: latency-svc-shbjd [750.81239ms]
May 13 20:03:28.007: INFO: Created: latency-svc-sgp65
May 13 20:03:28.049: INFO: Got endpoints: latency-svc-rt2vc [750.588138ms]
May 13 20:03:28.056: INFO: Created: latency-svc-7vfsw
May 13 20:03:28.098: INFO: Got endpoints: latency-svc-mt8dh [748.622221ms]
May 13 20:03:28.106: INFO: Created: latency-svc-mfmdt
May 13 20:03:28.149: INFO: Got endpoints: latency-svc-j5mj6 [749.534171ms]
May 13 20:03:28.156: INFO: Created: latency-svc-nffw8
May 13 20:03:28.199: INFO: Got endpoints: latency-svc-hwx7k [749.279972ms]
May 13 20:03:28.206: INFO: Created: latency-svc-t8jrk
May 13 20:03:28.249: INFO: Got endpoints: latency-svc-695ht [750.317071ms]
May 13 20:03:28.255: INFO: Created: latency-svc-vds6t
May 13 20:03:28.299: INFO: Got endpoints: latency-svc-4vwcf [747.575844ms]
May 13 20:03:28.307: INFO: Created: latency-svc-fd7x6
May 13 20:03:28.349: INFO: Got endpoints: latency-svc-nn2t7 [750.367726ms]
May 13 20:03:28.356: INFO: Created: latency-svc-w92qc
May 13 20:03:28.399: INFO: Got endpoints: latency-svc-xbg4m [750.039086ms]
May 13 20:03:28.405: INFO: Created: latency-svc-jz55k
May 13 20:03:28.449: INFO: Got endpoints: latency-svc-8ztj5 [750.236631ms]
May 13 20:03:28.458: INFO: Created: latency-svc-sp8lx
May 13 20:03:28.499: INFO: Got endpoints: latency-svc-gl2ll [750.274175ms]
May 13 20:03:28.505: INFO: Created: latency-svc-chhwk
May 13 20:03:28.548: INFO: Got endpoints: latency-svc-ftvlv [748.993974ms]
May 13 20:03:28.555: INFO: Created: latency-svc-xzdpt
May 13 20:03:28.599: INFO: Got endpoints: latency-svc-85wbl [749.412922ms]
May 13 20:03:28.606: INFO: Created: latency-svc-mnq8s
May 13 20:03:28.650: INFO: Got endpoints: latency-svc-rhqhk [750.411ms]
May 13 20:03:28.657: INFO: Created: latency-svc-wm24n
May 13 20:03:28.699: INFO: Got endpoints: latency-svc-dsfdw [749.617233ms]
May 13 20:03:28.705: INFO: Created: latency-svc-q45xm
May 13 20:03:28.749: INFO: Got endpoints: latency-svc-sgp65 [748.896262ms]
May 13 20:03:28.756: INFO: Created: latency-svc-ddz4k
May 13 20:03:28.799: INFO: Got endpoints: latency-svc-7vfsw [749.612973ms]
May 13 20:03:28.805: INFO: Created: latency-svc-njfcw
May 13 20:03:28.849: INFO: Got endpoints: latency-svc-mfmdt [750.420343ms]
May 13 20:03:28.856: INFO: Created: latency-svc-5f4p8
May 13 20:03:28.899: INFO: Got endpoints: latency-svc-nffw8 [750.221424ms]
May 13 20:03:28.905: INFO: Created: latency-svc-526pv
May 13 20:03:28.950: INFO: Got endpoints: latency-svc-t8jrk [750.870309ms]
May 13 20:03:28.956: INFO: Created: latency-svc-kcsvv
May 13 20:03:28.999: INFO: Got endpoints: latency-svc-vds6t [749.56671ms]
May 13 20:03:29.006: INFO: Created: latency-svc-zrtk8
May 13 20:03:29.048: INFO: Got endpoints: latency-svc-fd7x6 [749.029273ms]
May 13 20:03:29.056: INFO: Created: latency-svc-b4zfm
May 13 20:03:29.099: INFO: Got endpoints: latency-svc-w92qc [749.504808ms]
May 13 20:03:29.104: INFO: Created: latency-svc-v5nwh
May 13 20:03:29.149: INFO: Got endpoints: latency-svc-jz55k [749.696263ms]
May 13 20:03:29.155: INFO: Created: latency-svc-8qnd2
May 13 20:03:29.205: INFO: Got endpoints: latency-svc-sp8lx [756.097462ms]
May 13 20:03:29.214: INFO: Created: latency-svc-x9m6t
May 13 20:03:29.248: INFO: Got endpoints: latency-svc-chhwk [749.580418ms]
May 13 20:03:29.256: INFO: Created: latency-svc-xbnrc
May 13 20:03:29.299: INFO: Got endpoints: latency-svc-xzdpt [750.873075ms]
May 13 20:03:29.305: INFO: Created: latency-svc-wnlbc
May 13 20:03:29.349: INFO: Got endpoints: latency-svc-mnq8s [750.72452ms]
May 13 20:03:29.356: INFO: Created: latency-svc-fpgv6
May 13 20:03:29.400: INFO: Got endpoints: latency-svc-wm24n [749.620044ms]
May 13 20:03:29.407: INFO: Created: latency-svc-gg5sj
May 13 20:03:29.450: INFO: Got endpoints: latency-svc-q45xm [751.644423ms]
May 13 20:03:29.457: INFO: Created: latency-svc-zw7hl
May 13 20:03:29.500: INFO: Got endpoints: latency-svc-ddz4k [750.899971ms]
May 13 20:03:29.506: INFO: Created: latency-svc-2kj5b
May 13 20:03:29.550: INFO: Got endpoints: latency-svc-njfcw [750.928173ms]
May 13 20:03:29.563: INFO: Created: latency-svc-95lz2
May 13 20:03:29.599: INFO: Got endpoints: latency-svc-5f4p8 [749.861193ms]
May 13 20:03:29.605: INFO: Created: latency-svc-rbftc
May 13 20:03:29.649: INFO: Got endpoints: latency-svc-526pv [749.644302ms]
May 13 20:03:29.654: INFO: Created: latency-svc-d8q5v
May 13 20:03:29.699: INFO: Got endpoints: latency-svc-kcsvv [748.713397ms]
May 13 20:03:29.706: INFO: Created: latency-svc-g82p5
May 13 20:03:29.749: INFO: Got endpoints: latency-svc-zrtk8 [750.03544ms]
May 13 20:03:29.755: INFO: Created: latency-svc-pzsb6
May 13 20:03:29.799: INFO: Got endpoints: latency-svc-b4zfm [750.652805ms]
May 13 20:03:29.806: INFO: Created: latency-svc-vh86b
May 13 20:03:29.849: INFO: Got endpoints: latency-svc-v5nwh [749.874784ms]
May 13 20:03:29.856: INFO: Created: latency-svc-nghvp
May 13 20:03:29.899: INFO: Got endpoints: latency-svc-8qnd2 [749.616719ms]
May 13 20:03:29.905: INFO: Created: latency-svc-czgsv
May 13 20:03:29.950: INFO: Got endpoints: latency-svc-x9m6t [744.68963ms]
May 13 20:03:29.956: INFO: Created: latency-svc-wfzc2
May 13 20:03:30.000: INFO: Got endpoints: latency-svc-xbnrc [751.86902ms]
May 13 20:03:30.008: INFO: Created: latency-svc-p5r5x
May 13 20:03:30.049: INFO: Got endpoints: latency-svc-wnlbc [749.51398ms]
May 13 20:03:30.055: INFO: Created: latency-svc-6gghx
May 13 20:03:30.099: INFO: Got endpoints: latency-svc-fpgv6 [749.505043ms]
May 13 20:03:30.105: INFO: Created: latency-svc-ztdkh
May 13 20:03:30.150: INFO: Got endpoints: latency-svc-gg5sj [750.483882ms]
May 13 20:03:30.157: INFO: Created: latency-svc-lqsx7
May 13 20:03:30.199: INFO: Got endpoints: latency-svc-zw7hl [748.646312ms]
May 13 20:03:30.207: INFO: Created: latency-svc-2kvvn
May 13 20:03:30.250: INFO: Got endpoints: latency-svc-2kj5b [749.841703ms]
May 13 20:03:30.255: INFO: Created: latency-svc-7qpbt
May 13 20:03:30.300: INFO: Got endpoints: latency-svc-95lz2 [749.381294ms]
May 13 20:03:30.310: INFO: Created: latency-svc-dmccg
May 13 20:03:30.349: INFO: Got endpoints: latency-svc-rbftc [750.679694ms]
May 13 20:03:30.358: INFO: Created: latency-svc-kpmz9
May 13 20:03:30.399: INFO: Got endpoints: latency-svc-d8q5v [750.346837ms]
May 13 20:03:30.404: INFO: Created: latency-svc-6l92r
May 13 20:03:30.449: INFO: Got endpoints: latency-svc-g82p5 [749.925602ms]
May 13 20:03:30.455: INFO: Created: latency-svc-6blwp
May 13 20:03:30.499: INFO: Got endpoints: latency-svc-pzsb6 [750.168112ms]
May 13 20:03:30.505: INFO: Created: latency-svc-sc5sz
May 13 20:03:30.549: INFO: Got endpoints: latency-svc-vh86b [750.679257ms]
May 13 20:03:30.555: INFO: Created: latency-svc-mnbbr
May 13 20:03:30.599: INFO: Got endpoints: latency-svc-nghvp [750.234627ms]
May 13 20:03:30.606: INFO: Created: latency-svc-s92m6
May 13 20:03:30.649: INFO: Got endpoints: latency-svc-czgsv [749.875791ms]
May 13 20:03:30.657: INFO: Created: latency-svc-c8bps
May 13 20:03:30.699: INFO: Got endpoints: latency-svc-wfzc2 [748.985317ms]
May 13 20:03:30.705: INFO: Created: latency-svc-qlg29
May 13 20:03:30.749: INFO: Got endpoints: latency-svc-p5r5x [748.112264ms]
May 13 20:03:30.755: INFO: Created: latency-svc-mhgxj
May 13 20:03:30.799: INFO: Got endpoints: latency-svc-6gghx [750.044669ms]
May 13 20:03:30.805: INFO: Created: latency-svc-mqhls
May 13 20:03:30.850: INFO: Got endpoints: latency-svc-ztdkh [751.222212ms]
May 13 20:03:30.861: INFO: Created: latency-svc-8n5tm
May 13 20:03:30.899: INFO: Got endpoints: latency-svc-lqsx7 [748.683897ms]
May 13 20:03:30.907: INFO: Created: latency-svc-bn9jm
May 13 20:03:30.949: INFO: Got endpoints: latency-svc-2kvvn [750.075521ms]
May 13 20:03:30.955: INFO: Created: latency-svc-f7jv9
May 13 20:03:31.000: INFO: Got endpoints: latency-svc-7qpbt [750.156492ms]
May 13 20:03:31.048: INFO: Got endpoints: latency-svc-dmccg [748.770995ms]
May 13 20:03:31.100: INFO: Got endpoints: latency-svc-kpmz9 [750.305159ms]
May 13 20:03:31.150: INFO: Got endpoints: latency-svc-6l92r [750.695082ms]
May 13 20:03:31.199: INFO: Got endpoints: latency-svc-6blwp [750.245795ms]
May 13 20:03:31.248: INFO: Got endpoints: latency-svc-sc5sz [749.467114ms]
May 13 20:03:31.299: INFO: Got endpoints: latency-svc-mnbbr [749.748597ms]
May 13 20:03:31.349: INFO: Got endpoints: latency-svc-s92m6 [750.184338ms]
May 13 20:03:31.400: INFO: Got endpoints: latency-svc-c8bps [750.93388ms]
May 13 20:03:31.450: INFO: Got endpoints: latency-svc-qlg29 [750.747828ms]
May 13 20:03:31.499: INFO: Got endpoints: latency-svc-mhgxj [750.637589ms]
May 13 20:03:31.549: INFO: Got endpoints: latency-svc-mqhls [750.454729ms]
May 13 20:03:31.599: INFO: Got endpoints: latency-svc-8n5tm [748.609904ms]
May 13 20:03:31.649: INFO: Got endpoints: latency-svc-bn9jm [750.296453ms]
May 13 20:03:31.700: INFO: Got endpoints: latency-svc-f7jv9 [750.439075ms]
May 13 20:03:31.700: INFO: Latencies: [14.420805ms 20.949414ms 25.062816ms 30.830204ms 33.466419ms 43.20313ms 46.702877ms 54.001918ms 61.714827ms 65.297657ms 70.303232ms 75.427656ms 78.813894ms 79.570613ms 79.629627ms 79.744286ms 80.712702ms 81.655018ms 81.754129ms 81.888967ms 82.291156ms 83.265678ms 83.835341ms 84.31765ms 85.802109ms 85.830046ms 86.05779ms 86.41398ms 87.398241ms 89.819179ms 90.791664ms 92.592067ms 94.957188ms 153.737106ms 187.392561ms 239.987911ms 270.22743ms 315.524802ms 355.395702ms 400.322594ms 446.688122ms 490.557083ms 529.955965ms 577.294923ms 622.901114ms 666.185273ms 709.487188ms 727.544265ms 734.621201ms 737.947995ms 744.68963ms 745.04771ms 745.171829ms 747.175006ms 747.575844ms 748.112264ms 748.528322ms 748.609904ms 748.622221ms 748.646312ms 748.683897ms 748.713397ms 748.770995ms 748.896262ms 748.924715ms 748.985317ms 748.990311ms 748.993974ms 749.010669ms 749.029273ms 749.134339ms 749.206252ms 749.228164ms 749.279972ms 749.295557ms 749.30449ms 749.307946ms 749.372348ms 749.372554ms 749.381294ms 749.383919ms 749.402319ms 749.412922ms 749.413865ms 749.467114ms 749.467135ms 749.504808ms 749.505043ms 749.51398ms 749.526334ms 749.534171ms 749.536089ms 749.56671ms 749.580418ms 749.612973ms 749.613289ms 749.616719ms 749.617233ms 749.620044ms 749.63373ms 749.635165ms 749.644302ms 749.696263ms 749.712238ms 749.744028ms 749.748597ms 749.757001ms 749.763483ms 749.785139ms 749.786987ms 749.808753ms 749.809819ms 749.814028ms 749.841703ms 749.842797ms 749.861193ms 749.861644ms 749.874784ms 749.875791ms 749.88228ms 749.895057ms 749.925602ms 749.927142ms 749.928652ms 749.935111ms 749.969103ms 749.969398ms 750.03544ms 750.039086ms 750.044669ms 750.05335ms 750.068123ms 750.075521ms 750.088105ms 750.104075ms 750.107565ms 750.129547ms 750.145821ms 750.156492ms 750.16044ms 750.168112ms 750.184338ms 750.201022ms 750.210387ms 750.221424ms 750.231307ms 750.233963ms 750.234627ms 750.236631ms 750.245795ms 750.274175ms 750.296453ms 750.305159ms 750.307165ms 750.310281ms 750.317071ms 750.346837ms 750.367726ms 750.377594ms 750.411ms 750.420343ms 750.420492ms 750.439075ms 750.454729ms 750.483882ms 750.485974ms 750.549471ms 750.557444ms 750.58741ms 750.588138ms 750.637589ms 750.652805ms 750.679257ms 750.679694ms 750.695082ms 750.72452ms 750.747828ms 750.802805ms 750.81239ms 750.850265ms 750.870309ms 750.870886ms 750.873075ms 750.899971ms 750.918858ms 750.928173ms 750.93388ms 750.974423ms 751.052853ms 751.221907ms 751.222212ms 751.224015ms 751.259647ms 751.372843ms 751.424046ms 751.644423ms 751.762672ms 751.86902ms 752.472312ms 756.097462ms]
May 13 20:03:31.700: INFO: 50 %ile: 749.635165ms
May 13 20:03:31.700: INFO: 90 %ile: 750.870309ms
May 13 20:03:31.700: INFO: 99 %ile: 752.472312ms
May 13 20:03:31.700: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:03:31.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8625" for this suite.
May 13 20:03:45.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:03:45.762: INFO: namespace svc-latency-8625 deletion completed in 14.060126799s

• [SLOW TEST:25.904 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:03:45.763: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1202
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 13 20:03:45.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1202'
May 13 20:03:46.148: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 13 20:03:46.148: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
May 13 20:03:46.159: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-z5dsc]
May 13 20:03:46.159: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-z5dsc" in namespace "kubectl-1202" to be "running and ready"
May 13 20:03:46.161: INFO: Pod "e2e-test-nginx-rc-z5dsc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.5579ms
May 13 20:03:48.164: INFO: Pod "e2e-test-nginx-rc-z5dsc": Phase="Running", Reason="", readiness=true. Elapsed: 2.005068319s
May 13 20:03:48.164: INFO: Pod "e2e-test-nginx-rc-z5dsc" satisfied condition "running and ready"
May 13 20:03:48.164: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-z5dsc]
May 13 20:03:48.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 logs rc/e2e-test-nginx-rc --namespace=kubectl-1202'
May 13 20:03:48.243: INFO: stderr: ""
May 13 20:03:48.243: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
May 13 20:03:48.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 delete rc e2e-test-nginx-rc --namespace=kubectl-1202'
May 13 20:03:48.311: INFO: stderr: ""
May 13 20:03:48.311: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:03:48.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1202" for this suite.
May 13 20:03:54.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:03:54.375: INFO: namespace kubectl-1202 deletion completed in 6.061736189s

• [SLOW TEST:8.613 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:03:54.376: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8969
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
May 13 20:03:54.506: INFO: Waiting up to 5m0s for pod "var-expansion-3c25e85a-75ba-11e9-9153-920e960bc5b9" in namespace "var-expansion-8969" to be "success or failure"
May 13 20:03:54.509: INFO: Pod "var-expansion-3c25e85a-75ba-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.812656ms
May 13 20:03:56.512: INFO: Pod "var-expansion-3c25e85a-75ba-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006435289s
STEP: Saw pod success
May 13 20:03:56.512: INFO: Pod "var-expansion-3c25e85a-75ba-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:03:56.514: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod var-expansion-3c25e85a-75ba-11e9-9153-920e960bc5b9 container dapi-container: <nil>
STEP: delete the pod
May 13 20:03:56.526: INFO: Waiting for pod var-expansion-3c25e85a-75ba-11e9-9153-920e960bc5b9 to disappear
May 13 20:03:56.530: INFO: Pod var-expansion-3c25e85a-75ba-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:03:56.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8969" for this suite.
May 13 20:04:02.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:04:02.593: INFO: namespace var-expansion-8969 deletion completed in 6.061391007s

• [SLOW TEST:8.218 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:04:02.593: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3332
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-3332/configmap-test-410bff1f-75ba-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume configMaps
May 13 20:04:02.726: INFO: Waiting up to 5m0s for pod "pod-configmaps-410c5e9a-75ba-11e9-9153-920e960bc5b9" in namespace "configmap-3332" to be "success or failure"
May 13 20:04:02.728: INFO: Pod "pod-configmaps-410c5e9a-75ba-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.668339ms
May 13 20:04:04.731: INFO: Pod "pod-configmaps-410c5e9a-75ba-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005116466s
STEP: Saw pod success
May 13 20:04:04.731: INFO: Pod "pod-configmaps-410c5e9a-75ba-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:04:04.733: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-configmaps-410c5e9a-75ba-11e9-9153-920e960bc5b9 container env-test: <nil>
STEP: delete the pod
May 13 20:04:04.746: INFO: Waiting for pod pod-configmaps-410c5e9a-75ba-11e9-9153-920e960bc5b9 to disappear
May 13 20:04:04.749: INFO: Pod pod-configmaps-410c5e9a-75ba-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:04:04.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3332" for this suite.
May 13 20:04:10.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:04:10.811: INFO: namespace configmap-3332 deletion completed in 6.059934365s

• [SLOW TEST:8.217 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:04:10.811: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2596
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 20:04:10.945: INFO: Waiting up to 5m0s for pod "downwardapi-volume-45f1e1e9-75ba-11e9-9153-920e960bc5b9" in namespace "projected-2596" to be "success or failure"
May 13 20:04:10.964: INFO: Pod "downwardapi-volume-45f1e1e9-75ba-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 19.206445ms
May 13 20:04:12.967: INFO: Pod "downwardapi-volume-45f1e1e9-75ba-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021904833s
STEP: Saw pod success
May 13 20:04:12.967: INFO: Pod "downwardapi-volume-45f1e1e9-75ba-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:04:12.969: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downwardapi-volume-45f1e1e9-75ba-11e9-9153-920e960bc5b9 container client-container: <nil>
STEP: delete the pod
May 13 20:04:12.984: INFO: Waiting for pod downwardapi-volume-45f1e1e9-75ba-11e9-9153-920e960bc5b9 to disappear
May 13 20:04:12.986: INFO: Pod downwardapi-volume-45f1e1e9-75ba-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:04:12.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2596" for this suite.
May 13 20:04:18.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:04:19.054: INFO: namespace projected-2596 deletion completed in 6.065710964s

• [SLOW TEST:8.243 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:04:19.054: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3822
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-4adb91f2-75ba-11e9-9153-920e960bc5b9
May 13 20:04:19.183: INFO: Pod name my-hostname-basic-4adb91f2-75ba-11e9-9153-920e960bc5b9: Found 0 pods out of 1
May 13 20:04:24.185: INFO: Pod name my-hostname-basic-4adb91f2-75ba-11e9-9153-920e960bc5b9: Found 1 pods out of 1
May 13 20:04:24.186: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-4adb91f2-75ba-11e9-9153-920e960bc5b9" are running
May 13 20:04:24.187: INFO: Pod "my-hostname-basic-4adb91f2-75ba-11e9-9153-920e960bc5b9-qg9kt" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-13 20:04:19 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-13 20:04:21 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-13 20:04:21 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-13 20:04:19 +0000 UTC Reason: Message:}])
May 13 20:04:24.187: INFO: Trying to dial the pod
May 13 20:04:29.196: INFO: Controller my-hostname-basic-4adb91f2-75ba-11e9-9153-920e960bc5b9: Got expected result from replica 1 [my-hostname-basic-4adb91f2-75ba-11e9-9153-920e960bc5b9-qg9kt]: "my-hostname-basic-4adb91f2-75ba-11e9-9153-920e960bc5b9-qg9kt", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:04:29.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3822" for this suite.
May 13 20:04:35.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:04:35.261: INFO: namespace replication-controller-3822 deletion completed in 6.062039073s

• [SLOW TEST:16.206 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:04:35.261: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8401
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
May 13 20:04:37.409: INFO: Pod pod-hostip-54849dbf-75ba-11e9-9153-920e960bc5b9 has hostIP: 10.0.0.248
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:04:37.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8401" for this suite.
May 13 20:04:59.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:04:59.470: INFO: namespace pods-8401 deletion completed in 22.058970093s

• [SLOW TEST:24.209 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:04:59.470: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5003
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-62f2749b-75ba-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume secrets
May 13 20:04:59.601: INFO: Waiting up to 5m0s for pod "pod-secrets-62f2d1b6-75ba-11e9-9153-920e960bc5b9" in namespace "secrets-5003" to be "success or failure"
May 13 20:04:59.604: INFO: Pod "pod-secrets-62f2d1b6-75ba-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.317142ms
May 13 20:05:01.607: INFO: Pod "pod-secrets-62f2d1b6-75ba-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005904285s
May 13 20:05:03.609: INFO: Pod "pod-secrets-62f2d1b6-75ba-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00815978s
STEP: Saw pod success
May 13 20:05:03.609: INFO: Pod "pod-secrets-62f2d1b6-75ba-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:05:03.611: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-secrets-62f2d1b6-75ba-11e9-9153-920e960bc5b9 container secret-volume-test: <nil>
STEP: delete the pod
May 13 20:05:03.634: INFO: Waiting for pod pod-secrets-62f2d1b6-75ba-11e9-9153-920e960bc5b9 to disappear
May 13 20:05:03.637: INFO: Pod pod-secrets-62f2d1b6-75ba-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:05:03.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5003" for this suite.
May 13 20:05:09.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:05:09.698: INFO: namespace secrets-5003 deletion completed in 6.059602142s

• [SLOW TEST:10.229 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:05:09.699: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1936
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-1936
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1936 to expose endpoints map[]
May 13 20:05:09.837: INFO: successfully validated that service endpoint-test2 in namespace services-1936 exposes endpoints map[] (3.570088ms elapsed)
STEP: Creating pod pod1 in namespace services-1936
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1936 to expose endpoints map[pod1:[80]]
May 13 20:05:11.863: INFO: successfully validated that service endpoint-test2 in namespace services-1936 exposes endpoints map[pod1:[80]] (2.017673342s elapsed)
STEP: Creating pod pod2 in namespace services-1936
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1936 to expose endpoints map[pod1:[80] pod2:[80]]
May 13 20:05:14.893: INFO: successfully validated that service endpoint-test2 in namespace services-1936 exposes endpoints map[pod1:[80] pod2:[80]] (3.026060222s elapsed)
STEP: Deleting pod pod1 in namespace services-1936
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1936 to expose endpoints map[pod2:[80]]
May 13 20:05:15.910: INFO: successfully validated that service endpoint-test2 in namespace services-1936 exposes endpoints map[pod2:[80]] (1.012506855s elapsed)
STEP: Deleting pod pod2 in namespace services-1936
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1936 to expose endpoints map[]
May 13 20:05:16.918: INFO: successfully validated that service endpoint-test2 in namespace services-1936 exposes endpoints map[] (1.003973843s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:05:16.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1936" for this suite.
May 13 20:05:38.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:05:38.989: INFO: namespace services-1936 deletion completed in 22.060573812s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:29.290 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:05:38.989: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5720
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-7a80c1fb-75ba-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume configMaps
May 13 20:05:39.123: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7a811ac3-75ba-11e9-9153-920e960bc5b9" in namespace "projected-5720" to be "success or failure"
May 13 20:05:39.129: INFO: Pod "pod-projected-configmaps-7a811ac3-75ba-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.44358ms
May 13 20:05:41.132: INFO: Pod "pod-projected-configmaps-7a811ac3-75ba-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009036521s
STEP: Saw pod success
May 13 20:05:41.132: INFO: Pod "pod-projected-configmaps-7a811ac3-75ba-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:05:41.133: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-projected-configmaps-7a811ac3-75ba-11e9-9153-920e960bc5b9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 13 20:05:41.146: INFO: Waiting for pod pod-projected-configmaps-7a811ac3-75ba-11e9-9153-920e960bc5b9 to disappear
May 13 20:05:41.148: INFO: Pod pod-projected-configmaps-7a811ac3-75ba-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:05:41.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5720" for this suite.
May 13 20:05:47.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:05:47.209: INFO: namespace projected-5720 deletion completed in 6.059153789s

• [SLOW TEST:8.220 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:05:47.209: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6222
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 13 20:05:47.338: INFO: Waiting up to 5m0s for pod "pod-7f66e9b1-75ba-11e9-9153-920e960bc5b9" in namespace "emptydir-6222" to be "success or failure"
May 13 20:05:47.341: INFO: Pod "pod-7f66e9b1-75ba-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.538474ms
May 13 20:05:49.343: INFO: Pod "pod-7f66e9b1-75ba-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004824909s
STEP: Saw pod success
May 13 20:05:49.343: INFO: Pod "pod-7f66e9b1-75ba-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:05:49.345: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-7f66e9b1-75ba-11e9-9153-920e960bc5b9 container test-container: <nil>
STEP: delete the pod
May 13 20:05:49.357: INFO: Waiting for pod pod-7f66e9b1-75ba-11e9-9153-920e960bc5b9 to disappear
May 13 20:05:49.359: INFO: Pod pod-7f66e9b1-75ba-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:05:49.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6222" for this suite.
May 13 20:05:55.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:05:55.420: INFO: namespace emptydir-6222 deletion completed in 6.059030058s

• [SLOW TEST:8.211 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:05:55.420: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7531
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 13 20:05:58.075: INFO: Successfully updated pod "labelsupdate844bd825-75ba-11e9-9153-920e960bc5b9"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:06:00.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7531" for this suite.
May 13 20:06:22.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:06:22.159: INFO: namespace projected-7531 deletion completed in 22.060334681s

• [SLOW TEST:26.739 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:06:22.159: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4410
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-943c2fa1-75ba-11e9-9153-920e960bc5b9
STEP: Creating secret with name s-test-opt-upd-943c3003-75ba-11e9-9153-920e960bc5b9
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-943c2fa1-75ba-11e9-9153-920e960bc5b9
STEP: Updating secret s-test-opt-upd-943c3003-75ba-11e9-9153-920e960bc5b9
STEP: Creating secret with name s-test-opt-create-943c301e-75ba-11e9-9153-920e960bc5b9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:06:30.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4410" for this suite.
May 13 20:06:52.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:06:52.413: INFO: namespace secrets-4410 deletion completed in 22.058953485s

• [SLOW TEST:30.254 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:06:52.414: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8906
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0513 20:07:02.551678      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 13 20:07:02.551: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:07:02.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8906" for this suite.
May 13 20:07:08.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:07:08.612: INFO: namespace gc-8906 deletion completed in 6.05888208s

• [SLOW TEST:16.198 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:07:08.612: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-365
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-365
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
May 13 20:07:08.746: INFO: Found 0 stateful pods, waiting for 3
May 13 20:07:18.749: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 13 20:07:18.749: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 13 20:07:18.749: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 13 20:07:18.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-365 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 20:07:18.916: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 13 20:07:18.916: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 20:07:18.916: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 13 20:07:28.943: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 13 20:07:38.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-365 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 20:07:39.109: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 13 20:07:39.109: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 13 20:07:39.109: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 13 20:07:59.122: INFO: Waiting for StatefulSet statefulset-365/ss2 to complete update
STEP: Rolling back to a previous revision
May 13 20:08:09.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-365 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 13 20:08:09.301: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 13 20:08:09.301: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 13 20:08:09.301: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 13 20:08:19.331: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 13 20:08:29.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 exec --namespace=statefulset-365 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 13 20:08:29.492: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 13 20:08:29.492: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 13 20:08:29.492: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 13 20:08:49.506: INFO: Waiting for StatefulSet statefulset-365/ss2 to complete update
May 13 20:08:49.506: INFO: Waiting for Pod statefulset-365/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 13 20:08:59.511: INFO: Deleting all statefulset in ns statefulset-365
May 13 20:08:59.513: INFO: Scaling statefulset ss2 to 0
May 13 20:09:29.525: INFO: Waiting for statefulset status.replicas updated to 0
May 13 20:09:29.527: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:09:29.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-365" for this suite.
May 13 20:09:35.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:09:35.608: INFO: namespace statefulset-365 deletion completed in 6.06911476s

• [SLOW TEST:146.996 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:09:35.608: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-173
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
May 13 20:09:35.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 create -f - --namespace=kubectl-173'
May 13 20:09:35.925: INFO: stderr: ""
May 13 20:09:35.925: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 13 20:09:35.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-173'
May 13 20:09:36.014: INFO: stderr: ""
May 13 20:09:36.014: INFO: stdout: "update-demo-nautilus-99sc8 update-demo-nautilus-grp7d "
May 13 20:09:36.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-99sc8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-173'
May 13 20:09:36.087: INFO: stderr: ""
May 13 20:09:36.087: INFO: stdout: ""
May 13 20:09:36.087: INFO: update-demo-nautilus-99sc8 is created but not running
May 13 20:09:41.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-173'
May 13 20:09:41.157: INFO: stderr: ""
May 13 20:09:41.157: INFO: stdout: "update-demo-nautilus-99sc8 update-demo-nautilus-grp7d "
May 13 20:09:41.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-99sc8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-173'
May 13 20:09:41.221: INFO: stderr: ""
May 13 20:09:41.221: INFO: stdout: "true"
May 13 20:09:41.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-99sc8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-173'
May 13 20:09:41.300: INFO: stderr: ""
May 13 20:09:41.300: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 20:09:41.300: INFO: validating pod update-demo-nautilus-99sc8
May 13 20:09:41.303: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 20:09:41.303: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 20:09:41.303: INFO: update-demo-nautilus-99sc8 is verified up and running
May 13 20:09:41.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-grp7d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-173'
May 13 20:09:41.366: INFO: stderr: ""
May 13 20:09:41.366: INFO: stdout: "true"
May 13 20:09:41.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-grp7d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-173'
May 13 20:09:41.428: INFO: stderr: ""
May 13 20:09:41.428: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 20:09:41.428: INFO: validating pod update-demo-nautilus-grp7d
May 13 20:09:41.431: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 20:09:41.431: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 20:09:41.431: INFO: update-demo-nautilus-grp7d is verified up and running
STEP: using delete to clean up resources
May 13 20:09:41.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 delete --grace-period=0 --force -f - --namespace=kubectl-173'
May 13 20:09:41.496: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 20:09:41.497: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 13 20:09:41.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-173'
May 13 20:09:41.578: INFO: stderr: "No resources found.\n"
May 13 20:09:41.578: INFO: stdout: ""
May 13 20:09:41.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods -l name=update-demo --namespace=kubectl-173 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 13 20:09:41.649: INFO: stderr: ""
May 13 20:09:41.649: INFO: stdout: "update-demo-nautilus-99sc8\nupdate-demo-nautilus-grp7d\n"
May 13 20:09:42.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-173'
May 13 20:09:42.223: INFO: stderr: "No resources found.\n"
May 13 20:09:42.223: INFO: stdout: ""
May 13 20:09:42.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods -l name=update-demo --namespace=kubectl-173 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 13 20:09:42.289: INFO: stderr: ""
May 13 20:09:42.289: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:09:42.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-173" for this suite.
May 13 20:10:04.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:10:04.352: INFO: namespace kubectl-173 deletion completed in 22.060476614s

• [SLOW TEST:28.744 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:10:04.352: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9359
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-18abd3e7-75bb-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume configMaps
May 13 20:10:04.483: INFO: Waiting up to 5m0s for pod "pod-configmaps-18ac396a-75bb-11e9-9153-920e960bc5b9" in namespace "configmap-9359" to be "success or failure"
May 13 20:10:04.485: INFO: Pod "pod-configmaps-18ac396a-75bb-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.794742ms
May 13 20:10:06.488: INFO: Pod "pod-configmaps-18ac396a-75bb-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004673363s
STEP: Saw pod success
May 13 20:10:06.488: INFO: Pod "pod-configmaps-18ac396a-75bb-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:10:06.490: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-configmaps-18ac396a-75bb-11e9-9153-920e960bc5b9 container configmap-volume-test: <nil>
STEP: delete the pod
May 13 20:10:06.508: INFO: Waiting for pod pod-configmaps-18ac396a-75bb-11e9-9153-920e960bc5b9 to disappear
May 13 20:10:06.510: INFO: Pod pod-configmaps-18ac396a-75bb-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:10:06.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9359" for this suite.
May 13 20:10:12.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:10:12.574: INFO: namespace configmap-9359 deletion completed in 6.061239444s

• [SLOW TEST:8.221 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:10:12.574: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3321
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:11:12.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3321" for this suite.
May 13 20:11:34.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:11:34.769: INFO: namespace container-probe-3321 deletion completed in 22.060380323s

• [SLOW TEST:82.195 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:11:34.769: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8333
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-8333
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8333 to expose endpoints map[]
May 13 20:11:34.923: INFO: successfully validated that service multi-endpoint-test in namespace services-8333 exposes endpoints map[] (2.787313ms elapsed)
STEP: Creating pod pod1 in namespace services-8333
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8333 to expose endpoints map[pod1:[100]]
May 13 20:11:36.950: INFO: successfully validated that service multi-endpoint-test in namespace services-8333 exposes endpoints map[pod1:[100]] (2.019470624s elapsed)
STEP: Creating pod pod2 in namespace services-8333
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8333 to expose endpoints map[pod1:[100] pod2:[101]]
May 13 20:11:38.985: INFO: successfully validated that service multi-endpoint-test in namespace services-8333 exposes endpoints map[pod1:[100] pod2:[101]] (2.029968662s elapsed)
STEP: Deleting pod pod1 in namespace services-8333
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8333 to expose endpoints map[pod2:[101]]
May 13 20:11:38.997: INFO: successfully validated that service multi-endpoint-test in namespace services-8333 exposes endpoints map[pod2:[101]] (8.692262ms elapsed)
STEP: Deleting pod pod2 in namespace services-8333
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8333 to expose endpoints map[]
May 13 20:11:40.017: INFO: successfully validated that service multi-endpoint-test in namespace services-8333 exposes endpoints map[] (1.015122802s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:11:40.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8333" for this suite.
May 13 20:12:02.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:12:02.135: INFO: namespace services-8333 deletion completed in 22.059429599s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:27.366 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:12:02.136: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7010
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
May 13 20:12:02.265: INFO: Waiting up to 5m0s for pod "pod-5ee02db5-75bb-11e9-9153-920e960bc5b9" in namespace "emptydir-7010" to be "success or failure"
May 13 20:12:02.267: INFO: Pod "pod-5ee02db5-75bb-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.864857ms
May 13 20:12:04.270: INFO: Pod "pod-5ee02db5-75bb-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004508895s
STEP: Saw pod success
May 13 20:12:04.270: INFO: Pod "pod-5ee02db5-75bb-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:12:04.272: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-5ee02db5-75bb-11e9-9153-920e960bc5b9 container test-container: <nil>
STEP: delete the pod
May 13 20:12:04.286: INFO: Waiting for pod pod-5ee02db5-75bb-11e9-9153-920e960bc5b9 to disappear
May 13 20:12:04.293: INFO: Pod pod-5ee02db5-75bb-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:12:04.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7010" for this suite.
May 13 20:12:10.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:12:10.374: INFO: namespace emptydir-7010 deletion completed in 6.073834543s

• [SLOW TEST:8.239 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:12:10.375: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3304
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-63c96c10-75bb-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume secrets
May 13 20:12:10.506: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-63c9ca1f-75bb-11e9-9153-920e960bc5b9" in namespace "projected-3304" to be "success or failure"
May 13 20:12:10.508: INFO: Pod "pod-projected-secrets-63c9ca1f-75bb-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.1293ms
May 13 20:12:12.511: INFO: Pod "pod-projected-secrets-63c9ca1f-75bb-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004972462s
STEP: Saw pod success
May 13 20:12:12.511: INFO: Pod "pod-projected-secrets-63c9ca1f-75bb-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:12:12.513: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-projected-secrets-63c9ca1f-75bb-11e9-9153-920e960bc5b9 container secret-volume-test: <nil>
STEP: delete the pod
May 13 20:12:12.527: INFO: Waiting for pod pod-projected-secrets-63c9ca1f-75bb-11e9-9153-920e960bc5b9 to disappear
May 13 20:12:12.529: INFO: Pod pod-projected-secrets-63c9ca1f-75bb-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:12:12.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3304" for this suite.
May 13 20:12:18.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:12:18.613: INFO: namespace projected-3304 deletion completed in 6.081753567s

• [SLOW TEST:8.238 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:12:18.613: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1305
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-68b26e80-75bb-11e9-9153-920e960bc5b9
STEP: Creating secret with name secret-projected-all-test-volume-68b26e61-75bb-11e9-9153-920e960bc5b9
STEP: Creating a pod to test Check all projections for projected volume plugin
May 13 20:12:18.747: INFO: Waiting up to 5m0s for pod "projected-volume-68b26e19-75bb-11e9-9153-920e960bc5b9" in namespace "projected-1305" to be "success or failure"
May 13 20:12:18.750: INFO: Pod "projected-volume-68b26e19-75bb-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.366124ms
May 13 20:12:20.752: INFO: Pod "projected-volume-68b26e19-75bb-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005150237s
STEP: Saw pod success
May 13 20:12:20.752: INFO: Pod "projected-volume-68b26e19-75bb-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:12:20.754: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod projected-volume-68b26e19-75bb-11e9-9153-920e960bc5b9 container projected-all-volume-test: <nil>
STEP: delete the pod
May 13 20:12:20.783: INFO: Waiting for pod projected-volume-68b26e19-75bb-11e9-9153-920e960bc5b9 to disappear
May 13 20:12:20.785: INFO: Pod projected-volume-68b26e19-75bb-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:12:20.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1305" for this suite.
May 13 20:12:26.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:12:26.849: INFO: namespace projected-1305 deletion completed in 6.062398911s

• [SLOW TEST:8.236 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:12:26.850: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5293
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 13 20:12:26.973: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 13 20:12:26.977: INFO: Waiting for terminating namespaces to be deleted...
May 13 20:12:26.979: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-0-248.ec2.internal before test
May 13 20:12:26.996: INFO: telegraf-node-l2r62 from monitoring started at 2019-05-13 19:05:15 +0000 UTC (1 container statuses recorded)
May 13 20:12:26.996: INFO: 	Container telegraf ready: true, restart count 0
May 13 20:12:26.996: INFO: tiller-deploy-5f5d5b74f9-6l5nq from kube-system started at 2019-05-13 19:05:39 +0000 UTC (1 container statuses recorded)
May 13 20:12:26.996: INFO: 	Container tiller ready: true, restart count 0
May 13 20:12:26.996: INFO: sonobuoy-e2e-job-c3529c9c402c4d54 from heptio-sonobuoy started at 2019-05-13 19:15:02 +0000 UTC (2 container statuses recorded)
May 13 20:12:26.996: INFO: 	Container e2e ready: true, restart count 0
May 13 20:12:26.996: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 13 20:12:26.996: INFO: influxdb-545989ddbc-9h6pl from monitoring started at 2019-05-13 19:05:14 +0000 UTC (2 container statuses recorded)
May 13 20:12:26.996: INFO: 	Container influxdb ready: true, restart count 0
May 13 20:12:26.996: INFO: 	Container watcher ready: true, restart count 1
May 13 20:12:26.996: INFO: telegraf-5c4699b87-tnhbp from monitoring started at 2019-05-13 19:05:15 +0000 UTC (1 container statuses recorded)
May 13 20:12:26.996: INFO: 	Container telegraf ready: true, restart count 0
May 13 20:12:26.996: INFO: gravity-site-pzzq5 from kube-system started at 2019-05-13 19:06:12 +0000 UTC (1 container statuses recorded)
May 13 20:12:26.996: INFO: 	Container gravity-site ready: true, restart count 0
May 13 20:12:26.996: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-13 19:15:00 +0000 UTC (1 container statuses recorded)
May 13 20:12:26.996: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 13 20:12:26.996: INFO: coredns-x2prj from kube-system started at 2019-05-13 19:04:41 +0000 UTC (1 container statuses recorded)
May 13 20:12:26.996: INFO: 	Container coredns ready: true, restart count 0
May 13 20:12:26.996: INFO: log-forwarder-jlhbm from kube-system started at 2019-05-13 19:04:56 +0000 UTC (1 container statuses recorded)
May 13 20:12:26.996: INFO: 	Container log-forwarder ready: true, restart count 0
May 13 20:12:26.996: INFO: kapacitor-55d57f7756-fdx56 from monitoring started at 2019-05-13 19:05:14 +0000 UTC (3 container statuses recorded)
May 13 20:12:26.996: INFO: 	Container alert-loader ready: true, restart count 0
May 13 20:12:26.996: INFO: 	Container kapacitor ready: true, restart count 0
May 13 20:12:26.996: INFO: 	Container watcher ready: true, restart count 1
May 13 20:12:26.996: INFO: bandwagon-58d4b44589-s7qp9 from kube-system started at 2019-05-13 19:04:48 +0000 UTC (1 container statuses recorded)
May 13 20:12:26.996: INFO: 	Container bandwagon ready: true, restart count 0
May 13 20:12:26.996: INFO: log-collector-547858d4ff-2pqdk from kube-system started at 2019-05-13 19:04:55 +0000 UTC (1 container statuses recorded)
May 13 20:12:26.996: INFO: 	Container log-collector ready: true, restart count 0
May 13 20:12:26.996: INFO: heapster-7b9959b6d6-xrxl2 from monitoring started at 2019-05-13 19:05:14 +0000 UTC (1 container statuses recorded)
May 13 20:12:26.996: INFO: 	Container heapster ready: true, restart count 0
May 13 20:12:26.996: INFO: sonobuoy-systemd-logs-daemon-set-e496b0b34872467e-9g8fv from heptio-sonobuoy started at 2019-05-13 19:15:25 +0000 UTC (2 container statuses recorded)
May 13 20:12:26.996: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 13 20:12:26.996: INFO: 	Container systemd-logs ready: true, restart count 0
May 13 20:12:26.996: INFO: grafana-776fb79c76-v98m9 from monitoring started at 2019-05-13 19:05:14 +0000 UTC (2 container statuses recorded)
May 13 20:12:26.996: INFO: 	Container grafana ready: true, restart count 0
May 13 20:12:26.996: INFO: 	Container watcher ready: true, restart count 1
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node ip-10-0-0-248.ec2.internal
May 13 20:12:27.089: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-0-248.ec2.internal
May 13 20:12:27.089: INFO: Pod sonobuoy-e2e-job-c3529c9c402c4d54 requesting resource cpu=0m on Node ip-10-0-0-248.ec2.internal
May 13 20:12:27.089: INFO: Pod sonobuoy-systemd-logs-daemon-set-e496b0b34872467e-9g8fv requesting resource cpu=0m on Node ip-10-0-0-248.ec2.internal
May 13 20:12:27.089: INFO: Pod bandwagon-58d4b44589-s7qp9 requesting resource cpu=0m on Node ip-10-0-0-248.ec2.internal
May 13 20:12:27.089: INFO: Pod coredns-x2prj requesting resource cpu=100m on Node ip-10-0-0-248.ec2.internal
May 13 20:12:27.089: INFO: Pod gravity-site-pzzq5 requesting resource cpu=0m on Node ip-10-0-0-248.ec2.internal
May 13 20:12:27.089: INFO: Pod log-collector-547858d4ff-2pqdk requesting resource cpu=100m on Node ip-10-0-0-248.ec2.internal
May 13 20:12:27.089: INFO: Pod log-forwarder-jlhbm requesting resource cpu=100m on Node ip-10-0-0-248.ec2.internal
May 13 20:12:27.089: INFO: Pod tiller-deploy-5f5d5b74f9-6l5nq requesting resource cpu=0m on Node ip-10-0-0-248.ec2.internal
May 13 20:12:27.089: INFO: Pod grafana-776fb79c76-v98m9 requesting resource cpu=0m on Node ip-10-0-0-248.ec2.internal
May 13 20:12:27.089: INFO: Pod heapster-7b9959b6d6-xrxl2 requesting resource cpu=0m on Node ip-10-0-0-248.ec2.internal
May 13 20:12:27.089: INFO: Pod influxdb-545989ddbc-9h6pl requesting resource cpu=0m on Node ip-10-0-0-248.ec2.internal
May 13 20:12:27.089: INFO: Pod kapacitor-55d57f7756-fdx56 requesting resource cpu=0m on Node ip-10-0-0-248.ec2.internal
May 13 20:12:27.089: INFO: Pod telegraf-5c4699b87-tnhbp requesting resource cpu=0m on Node ip-10-0-0-248.ec2.internal
May 13 20:12:27.089: INFO: Pod telegraf-node-l2r62 requesting resource cpu=50m on Node ip-10-0-0-248.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6daceef7-75bb-11e9-9153-920e960bc5b9.159e57273af8df59], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5293/filler-pod-6daceef7-75bb-11e9-9153-920e960bc5b9 to ip-10-0-0-248.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6daceef7-75bb-11e9-9153-920e960bc5b9.159e57275f81dd1c], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6daceef7-75bb-11e9-9153-920e960bc5b9.159e572775d24a5b], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6daceef7-75bb-11e9-9153-920e960bc5b9.159e572777bbe4c8], Reason = [Created], Message = [Created container filler-pod-6daceef7-75bb-11e9-9153-920e960bc5b9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6daceef7-75bb-11e9-9153-920e960bc5b9.159e57277dfb4081], Reason = [Started], Message = [Started container filler-pod-6daceef7-75bb-11e9-9153-920e960bc5b9]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.159e5727b26b83f9], Reason = [FailedScheduling], Message = [0/1 nodes are available: 1 Insufficient cpu.]
STEP: removing the label node off the node ip-10-0-0-248.ec2.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:12:30.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5293" for this suite.
May 13 20:12:36.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:12:36.207: INFO: namespace sched-pred-5293 deletion completed in 6.060366156s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:9.357 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:12:36.207: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9724
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:12:39.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9724" for this suite.
May 13 20:13:01.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:13:01.418: INFO: namespace replication-controller-9724 deletion completed in 22.06354053s

• [SLOW TEST:25.211 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:13:01.418: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6786
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6002
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6247
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:13:25.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6786" for this suite.
May 13 20:13:31.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:13:31.861: INFO: namespace namespaces-6786 deletion completed in 6.059369066s
STEP: Destroying namespace "nsdeletetest-6002" for this suite.
May 13 20:13:31.863: INFO: Namespace nsdeletetest-6002 was already deleted
STEP: Destroying namespace "nsdeletetest-6247" for this suite.
May 13 20:13:37.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:13:37.938: INFO: namespace nsdeletetest-6247 deletion completed in 6.075059292s

• [SLOW TEST:36.520 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:13:37.938: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5108
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 13 20:13:38.125: INFO: Waiting up to 5m0s for pod "downward-api-9802c97c-75bb-11e9-9153-920e960bc5b9" in namespace "downward-api-5108" to be "success or failure"
May 13 20:13:38.128: INFO: Pod "downward-api-9802c97c-75bb-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.723358ms
May 13 20:13:40.130: INFO: Pod "downward-api-9802c97c-75bb-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005501766s
STEP: Saw pod success
May 13 20:13:40.130: INFO: Pod "downward-api-9802c97c-75bb-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:13:40.133: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downward-api-9802c97c-75bb-11e9-9153-920e960bc5b9 container dapi-container: <nil>
STEP: delete the pod
May 13 20:13:40.156: INFO: Waiting for pod downward-api-9802c97c-75bb-11e9-9153-920e960bc5b9 to disappear
May 13 20:13:40.160: INFO: Pod downward-api-9802c97c-75bb-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:13:40.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5108" for this suite.
May 13 20:13:46.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:13:46.234: INFO: namespace downward-api-5108 deletion completed in 6.071790676s

• [SLOW TEST:8.296 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:13:46.234: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8002
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 13 20:13:48.883: INFO: Successfully updated pod "labelsupdate9cec5e7d-75bb-11e9-9153-920e960bc5b9"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:13:50.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8002" for this suite.
May 13 20:14:12.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:14:12.963: INFO: namespace downward-api-8002 deletion completed in 22.06571914s

• [SLOW TEST:26.729 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:14:12.964: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3124
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
May 13 20:14:15.604: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3124 pod-service-account-ad283f02-75bb-11e9-9153-920e960bc5b9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 13 20:14:15.753: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3124 pod-service-account-ad283f02-75bb-11e9-9153-920e960bc5b9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 13 20:14:15.895: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3124 pod-service-account-ad283f02-75bb-11e9-9153-920e960bc5b9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:14:16.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3124" for this suite.
May 13 20:14:22.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:14:22.100: INFO: namespace svcaccounts-3124 deletion completed in 6.059188651s

• [SLOW TEST:9.136 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:14:22.100: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1578
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 20:14:24.245: INFO: Waiting up to 5m0s for pod "client-envvars-b3808955-75bb-11e9-9153-920e960bc5b9" in namespace "pods-1578" to be "success or failure"
May 13 20:14:24.248: INFO: Pod "client-envvars-b3808955-75bb-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.130784ms
May 13 20:14:26.251: INFO: Pod "client-envvars-b3808955-75bb-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005643627s
STEP: Saw pod success
May 13 20:14:26.251: INFO: Pod "client-envvars-b3808955-75bb-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:14:26.253: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod client-envvars-b3808955-75bb-11e9-9153-920e960bc5b9 container env3cont: <nil>
STEP: delete the pod
May 13 20:14:26.266: INFO: Waiting for pod client-envvars-b3808955-75bb-11e9-9153-920e960bc5b9 to disappear
May 13 20:14:26.268: INFO: Pod client-envvars-b3808955-75bb-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:14:26.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1578" for this suite.
May 13 20:15:16.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:15:16.328: INFO: namespace pods-1578 deletion completed in 50.058956455s

• [SLOW TEST:54.228 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:15:16.329: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1360
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 20:15:16.458: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 13 20:15:21.461: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 13 20:15:21.461: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 13 20:15:21.474: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-1360,SelfLink:/apis/apps/v1/namespaces/deployment-1360/deployments/test-cleanup-deployment,UID:d59ca52f-75bb-11e9-ac1e-0215dc200466,ResourceVersion:15200,Generation:1,CreationTimestamp:2019-05-13 20:15:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update apps/v1 2019-05-13 20:15:21 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:name:{map[]}]}]},f:spec: {map[f:progressDeadlineSeconds:{map[]} f:replicas:{map[]} f:revisionHistoryLimit:{map[]} f:selector:{map[f:matchLabels:{map[.:{map[]} f:name:{map[]}]}]} f:strategy:{map[f:rollingUpdate:{map[.:{map[]} f:maxSurge:{map[]} f:maxUnavailable:{map[]}]} f:type:{map[]}]} f:template:{map[f:metadata:{map[f:labels:{map[.:{map[]} f:name:{map[]}]}]} f:spec:{map[f:containers:{map[k:{"name":"redis"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]}]}]} f:dnsPolicy:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:terminationGracePeriodSeconds:{map[]}]}]}]},},}}],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

May 13 20:15:21.478: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
May 13 20:15:21.478: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 13 20:15:21.478: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-1360,SelfLink:/apis/apps/v1/namespaces/deployment-1360/replicasets/test-cleanup-controller,UID:d29ff574-75bb-11e9-ac1e-0215dc200466,ResourceVersion:15201,Generation:1,CreationTimestamp:2019-05-13 20:15:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment d59ca52f-75bb-11e9-ac1e-0215dc200466 0xc002fc11a7 0xc002fc11a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update apps/v1 2019-05-13 20:15:16 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:name:{map[]} f:pod:{map[]}]}]},f:spec: {map[f:replicas:{map[]} f:selector:{map[f:matchLabels:{map[.:{map[]} f:name:{map[]} f:pod:{map[]}]}]} f:template:{map[f:metadata:{map[f:labels:{map[.:{map[]} f:name:{map[]} f:pod:{map[]}]}]} f:spec:{map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]}]}]} f:dnsPolicy:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:terminationGracePeriodSeconds:{map[]}]}]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 20:15:16 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:fullyLabeledReplicas:{map[]} f:observedGeneration:{map[]} f:replicas:{map[]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 20:15:18 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:availableReplicas:{map[]} f:readyReplicas:{map[]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 20:15:21 +0000 UTC &Fields{Map:map[string]Fields{f:metadata: {map[f:ownerReferences:{map[.:{map[]} k:{"uid":"d59ca52f-75bb-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},},}}],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 13 20:15:21.481: INFO: Pod "test-cleanup-controller-hj4l7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-hj4l7,GenerateName:test-cleanup-controller-,Namespace:deployment-1360,SelfLink:/api/v1/namespaces/deployment-1360/pods/test-cleanup-controller-hj4l7,UID:d2a0dced-75bb-11e9-ac1e-0215dc200466,ResourceVersion:15192,Generation:0,CreationTimestamp:2019-05-13 20:15:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller d29ff574-75bb-11e9-ac1e-0215dc200466 0xc002814667 0xc002814668}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 20:15:16 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"d29ff574-75bb-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-dbw5b"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}} {kubelet Update v1 2019-05-13 20:15:16 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]} k:{"type":"Initialized"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Ready"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]}]} f:hostIP:{map[]} f:startTime:{map[]}]},},}} {kubelet Update v1 2019-05-13 20:15:18 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]} k:{"type":"Ready"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]}]} f:containerStatuses:{map[]} f:phase:{map[]} f:podIP:{map[]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-dbw5b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dbw5b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dbw5b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 20:15:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 20:15:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 20:15:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 20:15:16 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.248,PodIP:10.244.84.248,StartTime:2019-05-13 20:15:16 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-13 20:15:17 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f8e57bf4a52c16af56b91bcd12adf4f53b95e9d3971ccebcaac079d8cd94e7e2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:15:21.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1360" for this suite.
May 13 20:15:27.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:15:27.560: INFO: namespace deployment-1360 deletion completed in 6.07131257s

• [SLOW TEST:11.231 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:15:27.560: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2338
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 13 20:15:27.690: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2338,SelfLink:/api/v1/namespaces/watch-2338/configmaps/e2e-watch-test-watch-closed,UID:d951a50d-75bb-11e9-ac1e-0215dc200466,ResourceVersion:15246,Generation:0,CreationTimestamp:2019-05-13 20:15:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 20:15:27 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}}],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 13 20:15:27.690: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2338,SelfLink:/api/v1/namespaces/watch-2338/configmaps/e2e-watch-test-watch-closed,UID:d951a50d-75bb-11e9-ac1e-0215dc200466,ResourceVersion:15247,Generation:0,CreationTimestamp:2019-05-13 20:15:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 20:15:27 +0000 UTC Fields{Map:map[string]Fields{f:data: {map[.:{map[]} f:mutation:{map[]}]},f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}}],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 13 20:15:27.697: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2338,SelfLink:/api/v1/namespaces/watch-2338/configmaps/e2e-watch-test-watch-closed,UID:d951a50d-75bb-11e9-ac1e-0215dc200466,ResourceVersion:15248,Generation:0,CreationTimestamp:2019-05-13 20:15:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 20:15:27 +0000 UTC Fields{Map:map[string]Fields{f:data: {map[.:{map[]} f:mutation:{map[]}]},f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}}],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 13 20:15:27.697: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2338,SelfLink:/api/v1/namespaces/watch-2338/configmaps/e2e-watch-test-watch-closed,UID:d951a50d-75bb-11e9-ac1e-0215dc200466,ResourceVersion:15249,Generation:0,CreationTimestamp:2019-05-13 20:15:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 20:15:27 +0000 UTC Fields{Map:map[string]Fields{f:data: {map[.:{map[]} f:mutation:{map[]}]},f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}}],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:15:27.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2338" for this suite.
May 13 20:15:33.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:15:33.760: INFO: namespace watch-2338 deletion completed in 6.060688412s

• [SLOW TEST:6.200 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:15:33.760: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2280
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 13 20:15:33.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-2280'
May 13 20:15:34.150: INFO: stderr: ""
May 13 20:15:34.150: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
May 13 20:15:39.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pod e2e-test-nginx-pod --namespace=kubectl-2280 -o json'
May 13 20:15:39.297: INFO: stderr: ""
May 13 20:15:39.297: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"coredns\"\n        },\n        \"creationTimestamp\": \"2019-05-13T20:15:34Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fields\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": null,\n                            \"f:kubernetes.io/psp\": null\n                        },\n                        \"f:labels\": {\n                            \".\": null,\n                            \"f:run\": null\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-nginx-pod\\\"}\": {\n                                \".\": null,\n                                \"f:image\": null,\n                                \"f:imagePullPolicy\": null,\n                                \"f:name\": null,\n                                \"f:resources\": null,\n                                \"f:terminationMessagePath\": null,\n                                \"f:terminationMessagePolicy\": null,\n                                \"f:volumeMounts\": {\n                                    \".\": null,\n                                    \"k:{\\\"mountPath\\\":\\\"/var/run/secrets/kubernetes.io/serviceaccount\\\"}\": {\n                                        \".\": null,\n                                        \"f:mountPath\": null,\n                                        \"f:name\": null,\n                                        \"f:readOnly\": null\n                                    }\n                                }\n                            }\n                        },\n                        \"f:dnsPolicy\": null,\n                        \"f:enableServiceLinks\": null,\n                        \"f:restartPolicy\": null,\n                        \"f:schedulerName\": null,\n                        \"f:securityContext\": null,\n                        \"f:serviceAccount\": null,\n                        \"f:serviceAccountName\": null,\n                        \"f:terminationGracePeriodSeconds\": null,\n                        \"f:volumes\": {\n                            \".\": null,\n                            \"k:{\\\"name\\\":\\\"default-token-vrn54\\\"}\": {\n                                \".\": null,\n                                \"f:name\": null,\n                                \"f:secret\": {\n                                    \".\": null,\n                                    \"f:secretName\": null\n                                }\n                            }\n                        }\n                    }\n                },\n                \"manager\": \"kubectl\",\n                \"operation\": \"Update\",\n                \"time\": \"2019-05-13T20:15:34Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fields\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": null,\n                                \"f:lastProbeTime\": null,\n                                \"f:type\": null\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": null,\n                                \"f:lastProbeTime\": null,\n                                \"f:lastTransitionTime\": null,\n                                \"f:status\": null,\n                                \"f:type\": null\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": null,\n                                \"f:lastProbeTime\": null,\n                                \"f:type\": null\n                            }\n                        },\n                        \"f:hostIP\": null,\n                        \"f:startTime\": null\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2019-05-13T20:15:34Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fields\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \"f:lastTransitionTime\": null,\n                                \"f:status\": null\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \"f:lastTransitionTime\": null,\n                                \"f:status\": null\n                            }\n                        },\n                        \"f:containerStatuses\": null,\n                        \"f:phase\": null,\n                        \"f:podIP\": null\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2019-05-13T20:15:35Z\"\n            }\n        ],\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-2280\",\n        \"resourceVersion\": \"15279\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2280/pods/e2e-test-nginx-pod\",\n        \"uid\": \"dd2a0dff-75bb-11e9-ac1e-0215dc200466\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"Always\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-vrn54\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-0-248.ec2.internal\",\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"volumes\": [\n            {\n                \"name\": \"default-token-vrn54\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-vrn54\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-13T20:15:34Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-13T20:15:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-13T20:15:35Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-13T20:15:34Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://a9edb4f9f4dadc1192a33fb6471bfeb4b43ccd418e4764a3a7865f3e1f72690a\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-05-13T20:15:35Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.0.248\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.84.249\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-05-13T20:15:34Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 13 20:15:39.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 replace -f - --namespace=kubectl-2280'
May 13 20:15:39.455: INFO: stderr: ""
May 13 20:15:39.455: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
May 13 20:15:39.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 delete pods e2e-test-nginx-pod --namespace=kubectl-2280'
May 13 20:15:41.696: INFO: stderr: ""
May 13 20:15:41.696: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:15:41.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2280" for this suite.
May 13 20:15:47.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:15:47.780: INFO: namespace kubectl-2280 deletion completed in 6.069931295s

• [SLOW TEST:14.020 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:15:47.780: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1228
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 13 20:15:47.909: INFO: PodSpec: initContainers in spec.initContainers
May 13 20:16:35.627: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-e55f496a-75bb-11e9-9153-920e960bc5b9", GenerateName:"", Namespace:"init-container-1228", SelfLink:"/api/v1/namespaces/init-container-1228/pods/pod-init-e55f496a-75bb-11e9-9153-920e960bc5b9", UID:"e5603b4f-75bb-11e9-ac1e-0215dc200466", ResourceVersion:"15433", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63693375347, loc:(*time.Location)(0x89f10e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"907217198"}, Annotations:map[string]string{"kubernetes.io/psp":"coredns"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001966440), Fields:(*v1.Fields)(0xc001eb8958)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001966460), Fields:(*v1.Fields)(0xc001eb9180)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001966480), Fields:(*v1.Fields)(0xc001eb95d8)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0019664a0), Fields:(*v1.Fields)(0xc001eb9640)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-f6hgt", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00183f300), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-f6hgt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-f6hgt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-f6hgt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003461828), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-0-248.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0026549c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration(nil), HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00346188c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693375347, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693375347, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693375347, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693375347, loc:(*time.Location)(0x89f10e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.0.248", PodIP:"10.244.84.250", StartTime:(*v1.Time)(0xc0019664c0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008d88c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008d8930)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://7e8736da75528dd7e65235c7be8e950ffef83fe35aa359001aeebb2195ab134a"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001966500), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0019664e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:16:35.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1228" for this suite.
May 13 20:16:57.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:16:57.704: INFO: namespace init-container-1228 deletion completed in 22.073898575s

• [SLOW TEST:69.924 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:16:57.704: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1117
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1117.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1117.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1117.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1117.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1117.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1117.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 13 20:17:01.874: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:01.876: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:01.881: INFO: Unable to read jessie_udp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:01.883: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:01.883: INFO: Lookups using dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

May 13 20:17:06.890: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:06.892: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:06.898: INFO: Unable to read jessie_udp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:06.900: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:06.900: INFO: Lookups using dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

May 13 20:17:11.890: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:11.892: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:11.897: INFO: Unable to read jessie_udp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:11.899: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:11.899: INFO: Lookups using dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

May 13 20:17:16.890: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:16.892: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:16.897: INFO: Unable to read jessie_udp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:16.899: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:16.899: INFO: Lookups using dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

May 13 20:17:21.891: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:21.893: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:21.899: INFO: Unable to read jessie_udp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:21.901: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:21.901: INFO: Lookups using dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

May 13 20:17:26.890: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:26.892: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:26.897: INFO: Unable to read jessie_udp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:26.899: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9: the server could not find the requested resource (get pods dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9)
May 13 20:17:26.899: INFO: Lookups using dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

May 13 20:17:31.899: INFO: DNS probes using dns-1117/dns-test-0f0f9e67-75bc-11e9-9153-920e960bc5b9 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:17:31.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1117" for this suite.
May 13 20:17:37.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:17:38.012: INFO: namespace dns-1117 deletion completed in 6.102823992s

• [SLOW TEST:40.307 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:17:38.012: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2285
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 20:17:38.192: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:17:39.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2285" for this suite.
May 13 20:17:45.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:17:45.319: INFO: namespace custom-resource-definition-2285 deletion completed in 6.0605166s

• [SLOW TEST:7.307 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:17:45.320: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7262
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
May 13 20:17:45.452: INFO: Waiting up to 5m0s for pod "pod-2b6e1801-75bc-11e9-9153-920e960bc5b9" in namespace "emptydir-7262" to be "success or failure"
May 13 20:17:45.456: INFO: Pod "pod-2b6e1801-75bc-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.692267ms
May 13 20:17:47.458: INFO: Pod "pod-2b6e1801-75bc-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006210176s
STEP: Saw pod success
May 13 20:17:47.458: INFO: Pod "pod-2b6e1801-75bc-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:17:47.460: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-2b6e1801-75bc-11e9-9153-920e960bc5b9 container test-container: <nil>
STEP: delete the pod
May 13 20:17:47.485: INFO: Waiting for pod pod-2b6e1801-75bc-11e9-9153-920e960bc5b9 to disappear
May 13 20:17:47.487: INFO: Pod pod-2b6e1801-75bc-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:17:47.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7262" for this suite.
May 13 20:17:53.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:17:53.555: INFO: namespace emptydir-7262 deletion completed in 6.066317695s

• [SLOW TEST:8.235 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:17:53.555: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5994
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 20:17:53.724: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"3058e718-75bc-11e9-ac1e-0215dc200466", Controller:(*bool)(0xc0020d0636), BlockOwnerDeletion:(*bool)(0xc0020d0637)}}
May 13 20:17:53.731: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3056e471-75bc-11e9-ac1e-0215dc200466", Controller:(*bool)(0xc003548bb6), BlockOwnerDeletion:(*bool)(0xc003548bb7)}}
May 13 20:17:53.741: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"305782cf-75bc-11e9-ac1e-0215dc200466", Controller:(*bool)(0xc002923da6), BlockOwnerDeletion:(*bool)(0xc002923da7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:17:58.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5994" for this suite.
May 13 20:18:04.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:18:04.816: INFO: namespace gc-5994 deletion completed in 6.064026s

• [SLOW TEST:11.261 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:18:04.817: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4101
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-4101
May 13 20:18:06.952: INFO: Started pod liveness-exec in namespace container-probe-4101
STEP: checking the pod's current state and verifying that restartCount is present
May 13 20:18:06.954: INFO: Initial restart count of pod liveness-exec is 0
May 13 20:18:57.026: INFO: Restart count of pod container-probe-4101/liveness-exec is now 1 (50.071793659s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:18:57.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4101" for this suite.
May 13 20:19:03.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:19:03.102: INFO: namespace container-probe-4101 deletion completed in 6.066659341s

• [SLOW TEST:58.286 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:19:03.102: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1688
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 20:19:03.231: INFO: Waiting up to 5m0s for pod "downwardapi-volume-59ca9bd0-75bc-11e9-9153-920e960bc5b9" in namespace "projected-1688" to be "success or failure"
May 13 20:19:03.233: INFO: Pod "downwardapi-volume-59ca9bd0-75bc-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.945507ms
May 13 20:19:05.239: INFO: Pod "downwardapi-volume-59ca9bd0-75bc-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007404114s
STEP: Saw pod success
May 13 20:19:05.239: INFO: Pod "downwardapi-volume-59ca9bd0-75bc-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:19:05.241: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downwardapi-volume-59ca9bd0-75bc-11e9-9153-920e960bc5b9 container client-container: <nil>
STEP: delete the pod
May 13 20:19:05.256: INFO: Waiting for pod downwardapi-volume-59ca9bd0-75bc-11e9-9153-920e960bc5b9 to disappear
May 13 20:19:05.260: INFO: Pod downwardapi-volume-59ca9bd0-75bc-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:19:05.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1688" for this suite.
May 13 20:19:11.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:19:11.321: INFO: namespace projected-1688 deletion completed in 6.059155473s

• [SLOW TEST:8.219 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:19:11.321: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2018
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 13 20:19:11.444: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:19:15.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2018" for this suite.
May 13 20:19:37.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:19:37.163: INFO: namespace init-container-2018 deletion completed in 22.065480983s

• [SLOW TEST:25.842 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:19:37.163: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9084
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
May 13 20:19:37.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 create -f - --namespace=kubectl-9084'
May 13 20:19:37.484: INFO: stderr: ""
May 13 20:19:37.484: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
May 13 20:19:38.487: INFO: Selector matched 1 pods for map[app:redis]
May 13 20:19:38.487: INFO: Found 0 / 1
May 13 20:19:39.486: INFO: Selector matched 1 pods for map[app:redis]
May 13 20:19:39.486: INFO: Found 0 / 1
May 13 20:19:40.487: INFO: Selector matched 1 pods for map[app:redis]
May 13 20:19:40.487: INFO: Found 1 / 1
May 13 20:19:40.487: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 13 20:19:40.489: INFO: Selector matched 1 pods for map[app:redis]
May 13 20:19:40.489: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
May 13 20:19:40.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 logs redis-master-clkfg redis-master --namespace=kubectl-9084'
May 13 20:19:40.593: INFO: stderr: ""
May 13 20:19:40.593: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 May 20:19:38.712 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 May 20:19:38.712 # Server started, Redis version 3.2.12\n1:M 13 May 20:19:38.712 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 May 20:19:38.713 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
May 13 20:19:40.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 log redis-master-clkfg redis-master --namespace=kubectl-9084 --tail=1'
May 13 20:19:40.696: INFO: stderr: ""
May 13 20:19:40.696: INFO: stdout: "1:M 13 May 20:19:38.713 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
May 13 20:19:40.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 log redis-master-clkfg redis-master --namespace=kubectl-9084 --limit-bytes=1'
May 13 20:19:40.806: INFO: stderr: ""
May 13 20:19:40.806: INFO: stdout: " "
STEP: exposing timestamps
May 13 20:19:40.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 log redis-master-clkfg redis-master --namespace=kubectl-9084 --tail=1 --timestamps'
May 13 20:19:40.914: INFO: stderr: ""
May 13 20:19:40.914: INFO: stdout: "2019-05-13T20:19:38.71329945Z 1:M 13 May 20:19:38.713 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
May 13 20:19:43.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 log redis-master-clkfg redis-master --namespace=kubectl-9084 --since=1s'
May 13 20:19:43.490: INFO: stderr: ""
May 13 20:19:43.490: INFO: stdout: ""
May 13 20:19:43.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 log redis-master-clkfg redis-master --namespace=kubectl-9084 --since=24h'
May 13 20:19:43.581: INFO: stderr: ""
May 13 20:19:43.581: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 May 20:19:38.712 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 May 20:19:38.712 # Server started, Redis version 3.2.12\n1:M 13 May 20:19:38.712 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 May 20:19:38.713 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
May 13 20:19:43.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 delete --grace-period=0 --force -f - --namespace=kubectl-9084'
May 13 20:19:43.648: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 20:19:43.648: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
May 13 20:19:43.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get rc,svc -l name=nginx --no-headers --namespace=kubectl-9084'
May 13 20:19:43.717: INFO: stderr: "No resources found.\n"
May 13 20:19:43.717: INFO: stdout: ""
May 13 20:19:43.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods -l name=nginx --namespace=kubectl-9084 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 13 20:19:43.781: INFO: stderr: ""
May 13 20:19:43.781: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:19:43.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9084" for this suite.
May 13 20:19:49.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:19:49.848: INFO: namespace kubectl-9084 deletion completed in 6.065166257s

• [SLOW TEST:12.685 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:19:49.848: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-998
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-998
May 13 20:19:51.993: INFO: Started pod liveness-http in namespace container-probe-998
STEP: checking the pod's current state and verifying that restartCount is present
May 13 20:19:51.995: INFO: Initial restart count of pod liveness-http is 0
May 13 20:20:12.045: INFO: Restart count of pod container-probe-998/liveness-http is now 1 (20.049556945s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:20:12.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-998" for this suite.
May 13 20:20:18.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:20:18.130: INFO: namespace container-probe-998 deletion completed in 6.075278712s

• [SLOW TEST:28.282 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:20:18.130: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-591
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 13 20:20:18.266: INFO: Waiting up to 5m0s for pod "downward-api-8683a5ca-75bc-11e9-9153-920e960bc5b9" in namespace "downward-api-591" to be "success or failure"
May 13 20:20:18.272: INFO: Pod "downward-api-8683a5ca-75bc-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.168831ms
May 13 20:20:20.282: INFO: Pod "downward-api-8683a5ca-75bc-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016414509s
STEP: Saw pod success
May 13 20:20:20.282: INFO: Pod "downward-api-8683a5ca-75bc-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:20:20.286: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downward-api-8683a5ca-75bc-11e9-9153-920e960bc5b9 container dapi-container: <nil>
STEP: delete the pod
May 13 20:20:20.326: INFO: Waiting for pod downward-api-8683a5ca-75bc-11e9-9153-920e960bc5b9 to disappear
May 13 20:20:20.329: INFO: Pod downward-api-8683a5ca-75bc-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:20:20.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-591" for this suite.
May 13 20:20:26.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:20:26.394: INFO: namespace downward-api-591 deletion completed in 6.062057028s

• [SLOW TEST:8.263 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:20:26.394: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4914
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 20:20:26.518: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:20:28.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4914" for this suite.
May 13 20:21:06.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:21:06.605: INFO: namespace pods-4914 deletion completed in 38.061020091s

• [SLOW TEST:40.211 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:21:06.605: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6854
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:21:06.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6854" for this suite.
May 13 20:21:28.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:21:28.813: INFO: namespace pods-6854 deletion completed in 22.063642109s

• [SLOW TEST:22.208 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:21:28.813: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8349
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-b0a44c0f-75bc-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume configMaps
May 13 20:21:28.945: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b0a4acc6-75bc-11e9-9153-920e960bc5b9" in namespace "projected-8349" to be "success or failure"
May 13 20:21:28.948: INFO: Pod "pod-projected-configmaps-b0a4acc6-75bc-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.338187ms
May 13 20:21:30.951: INFO: Pod "pod-projected-configmaps-b0a4acc6-75bc-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005910955s
STEP: Saw pod success
May 13 20:21:30.951: INFO: Pod "pod-projected-configmaps-b0a4acc6-75bc-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:21:30.952: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-projected-configmaps-b0a4acc6-75bc-11e9-9153-920e960bc5b9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 13 20:21:30.967: INFO: Waiting for pod pod-projected-configmaps-b0a4acc6-75bc-11e9-9153-920e960bc5b9 to disappear
May 13 20:21:30.969: INFO: Pod pod-projected-configmaps-b0a4acc6-75bc-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:21:30.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8349" for this suite.
May 13 20:21:36.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:21:37.044: INFO: namespace projected-8349 deletion completed in 6.072788018s

• [SLOW TEST:8.231 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:21:37.044: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-260
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-b58cbe6a-75bc-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume configMaps
May 13 20:21:37.187: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b58d3d27-75bc-11e9-9153-920e960bc5b9" in namespace "projected-260" to be "success or failure"
May 13 20:21:37.189: INFO: Pod "pod-projected-configmaps-b58d3d27-75bc-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.228672ms
May 13 20:21:39.192: INFO: Pod "pod-projected-configmaps-b58d3d27-75bc-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00487779s
STEP: Saw pod success
May 13 20:21:39.192: INFO: Pod "pod-projected-configmaps-b58d3d27-75bc-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:21:39.193: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-projected-configmaps-b58d3d27-75bc-11e9-9153-920e960bc5b9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 13 20:21:39.207: INFO: Waiting for pod pod-projected-configmaps-b58d3d27-75bc-11e9-9153-920e960bc5b9 to disappear
May 13 20:21:39.209: INFO: Pod pod-projected-configmaps-b58d3d27-75bc-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:21:39.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-260" for this suite.
May 13 20:21:45.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:21:45.271: INFO: namespace projected-260 deletion completed in 6.060493512s

• [SLOW TEST:8.227 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:21:45.272: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7042
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-ba7415a4-75bc-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume configMaps
May 13 20:21:45.405: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba746d75-75bc-11e9-9153-920e960bc5b9" in namespace "configmap-7042" to be "success or failure"
May 13 20:21:45.407: INFO: Pod "pod-configmaps-ba746d75-75bc-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.693906ms
May 13 20:21:47.410: INFO: Pod "pod-configmaps-ba746d75-75bc-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004405656s
STEP: Saw pod success
May 13 20:21:47.410: INFO: Pod "pod-configmaps-ba746d75-75bc-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:21:47.411: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-configmaps-ba746d75-75bc-11e9-9153-920e960bc5b9 container configmap-volume-test: <nil>
STEP: delete the pod
May 13 20:21:47.423: INFO: Waiting for pod pod-configmaps-ba746d75-75bc-11e9-9153-920e960bc5b9 to disappear
May 13 20:21:47.425: INFO: Pod pod-configmaps-ba746d75-75bc-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:21:47.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7042" for this suite.
May 13 20:21:53.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:21:53.487: INFO: namespace configmap-7042 deletion completed in 6.060363794s

• [SLOW TEST:8.215 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:21:53.488: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3181
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-7318
STEP: Creating secret with name secret-test-bf5961ad-75bc-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume secrets
May 13 20:21:53.742: INFO: Waiting up to 5m0s for pod "pod-secrets-bf6c7cfb-75bc-11e9-9153-920e960bc5b9" in namespace "secrets-3181" to be "success or failure"
May 13 20:21:53.744: INFO: Pod "pod-secrets-bf6c7cfb-75bc-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.846564ms
May 13 20:21:55.746: INFO: Pod "pod-secrets-bf6c7cfb-75bc-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004453345s
STEP: Saw pod success
May 13 20:21:55.746: INFO: Pod "pod-secrets-bf6c7cfb-75bc-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:21:55.748: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-secrets-bf6c7cfb-75bc-11e9-9153-920e960bc5b9 container secret-volume-test: <nil>
STEP: delete the pod
May 13 20:21:55.759: INFO: Waiting for pod pod-secrets-bf6c7cfb-75bc-11e9-9153-920e960bc5b9 to disappear
May 13 20:21:55.760: INFO: Pod pod-secrets-bf6c7cfb-75bc-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:21:55.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3181" for this suite.
May 13 20:22:01.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:22:01.825: INFO: namespace secrets-3181 deletion completed in 6.062314009s
STEP: Destroying namespace "secret-namespace-7318" for this suite.
May 13 20:22:07.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:22:07.886: INFO: namespace secret-namespace-7318 deletion completed in 6.060618284s

• [SLOW TEST:14.398 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:22:07.886: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1365
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
May 13 20:22:08.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 create -f - --namespace=kubectl-1365'
May 13 20:22:08.160: INFO: stderr: ""
May 13 20:22:08.160: INFO: stdout: "pod/pause created\n"
May 13 20:22:08.160: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 13 20:22:08.160: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1365" to be "running and ready"
May 13 20:22:08.164: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.55294ms
May 13 20:22:10.166: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006042927s
May 13 20:22:12.169: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.008621928s
May 13 20:22:12.169: INFO: Pod "pause" satisfied condition "running and ready"
May 13 20:22:12.169: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
May 13 20:22:12.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 label pods pause testing-label=testing-label-value --namespace=kubectl-1365'
May 13 20:22:12.245: INFO: stderr: ""
May 13 20:22:12.245: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 13 20:22:12.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pod pause -L testing-label --namespace=kubectl-1365'
May 13 20:22:12.311: INFO: stderr: ""
May 13 20:22:12.311: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 13 20:22:12.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 label pods pause testing-label- --namespace=kubectl-1365'
May 13 20:22:12.389: INFO: stderr: ""
May 13 20:22:12.389: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 13 20:22:12.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pod pause -L testing-label --namespace=kubectl-1365'
May 13 20:22:12.454: INFO: stderr: ""
May 13 20:22:12.454: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
May 13 20:22:12.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 delete --grace-period=0 --force -f - --namespace=kubectl-1365'
May 13 20:22:12.524: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 20:22:12.524: INFO: stdout: "pod \"pause\" force deleted\n"
May 13 20:22:12.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get rc,svc -l name=pause --no-headers --namespace=kubectl-1365'
May 13 20:22:12.601: INFO: stderr: "No resources found.\n"
May 13 20:22:12.601: INFO: stdout: ""
May 13 20:22:12.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods -l name=pause --namespace=kubectl-1365 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 13 20:22:12.674: INFO: stderr: ""
May 13 20:22:12.674: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:22:12.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1365" for this suite.
May 13 20:22:18.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:22:18.738: INFO: namespace kubectl-1365 deletion completed in 6.061669462s

• [SLOW TEST:10.852 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:22:18.738: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1685
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
May 13 20:22:18.863: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

May 13 20:22:18.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 create -f - --namespace=kubectl-1685'
May 13 20:22:19.010: INFO: stderr: ""
May 13 20:22:19.010: INFO: stdout: "service/redis-slave created\n"
May 13 20:22:19.011: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

May 13 20:22:19.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 create -f - --namespace=kubectl-1685'
May 13 20:22:19.162: INFO: stderr: ""
May 13 20:22:19.162: INFO: stdout: "service/redis-master created\n"
May 13 20:22:19.162: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 13 20:22:19.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 create -f - --namespace=kubectl-1685'
May 13 20:22:19.327: INFO: stderr: ""
May 13 20:22:19.327: INFO: stdout: "service/frontend created\n"
May 13 20:22:19.327: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

May 13 20:22:19.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 create -f - --namespace=kubectl-1685'
May 13 20:22:19.482: INFO: stderr: ""
May 13 20:22:19.482: INFO: stdout: "deployment.apps/frontend created\n"
May 13 20:22:19.482: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 13 20:22:19.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 create -f - --namespace=kubectl-1685'
May 13 20:22:19.669: INFO: stderr: ""
May 13 20:22:19.669: INFO: stdout: "deployment.apps/redis-master created\n"
May 13 20:22:19.671: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

May 13 20:22:19.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 create -f - --namespace=kubectl-1685'
May 13 20:22:19.834: INFO: stderr: ""
May 13 20:22:19.834: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
May 13 20:22:19.834: INFO: Waiting for all frontend pods to be Running.
May 13 20:22:34.885: INFO: Waiting for frontend to serve content.
May 13 20:22:35.907: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

May 13 20:22:40.928: INFO: Trying to add a new entry to the guestbook.
May 13 20:22:40.965: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 13 20:22:40.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 delete --grace-period=0 --force -f - --namespace=kubectl-1685'
May 13 20:22:41.096: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 20:22:41.096: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
May 13 20:22:41.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 delete --grace-period=0 --force -f - --namespace=kubectl-1685'
May 13 20:22:41.176: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 20:22:41.176: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 13 20:22:41.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 delete --grace-period=0 --force -f - --namespace=kubectl-1685'
May 13 20:22:41.254: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 20:22:41.254: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 13 20:22:41.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 delete --grace-period=0 --force -f - --namespace=kubectl-1685'
May 13 20:22:41.328: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 20:22:41.328: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 13 20:22:41.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 delete --grace-period=0 --force -f - --namespace=kubectl-1685'
May 13 20:22:41.400: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 20:22:41.400: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 13 20:22:41.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 delete --grace-period=0 --force -f - --namespace=kubectl-1685'
May 13 20:22:41.467: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 20:22:41.467: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:22:41.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1685" for this suite.
May 13 20:23:25.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:23:25.531: INFO: namespace kubectl-1685 deletion completed in 44.062205103s

• [SLOW TEST:66.793 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:23:25.531: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5246
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 20:23:25.714: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 13 20:23:25.719: INFO: Pod name sample-pod: Found 0 pods out of 1
May 13 20:23:30.722: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 13 20:23:30.722: INFO: Creating deployment "test-rolling-update-deployment"
May 13 20:23:30.725: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 13 20:23:30.729: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 13 20:23:32.733: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 13 20:23:32.735: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 13 20:23:32.740: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-5246,SelfLink:/apis/apps/v1/namespaces/deployment-5246/deployments/test-rolling-update-deployment,UID:f93b75c0-75bc-11e9-ac1e-0215dc200466,ResourceVersion:16864,Generation:1,CreationTimestamp:2019-05-13 20:23:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update apps/v1 2019-05-13 20:23:30 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:name:{map[]}]}]},f:spec: {map[f:progressDeadlineSeconds:{map[]} f:replicas:{map[]} f:revisionHistoryLimit:{map[]} f:selector:{map[f:matchLabels:{map[.:{map[]} f:name:{map[]}]}]} f:strategy:{map[f:rollingUpdate:{map[.:{map[]} f:maxSurge:{map[]} f:maxUnavailable:{map[]}]} f:type:{map[]}]} f:template:{map[f:metadata:{map[f:labels:{map[.:{map[]} f:name:{map[]}]}]} f:spec:{map[f:containers:{map[k:{"name":"redis"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]}]}]} f:dnsPolicy:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:terminationGracePeriodSeconds:{map[]}]}]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 20:23:30 +0000 UTC &Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:deployment.kubernetes.io/revision:{map[]}]}]},f:status: {map[f:conditions:{map[.:{map[]} k:{"type":"Available"}:{map[.:{map[]} f:lastTransitionTime:{map[]} f:lastUpdateTime:{map[]} f:message:{map[]} f:reason:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Progressing"}:{map[.:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]}]} f:observedGeneration:{map[]} f:updatedReplicas:{map[]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 20:23:32 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:availableReplicas:{map[]} f:conditions:{map[k:{"type":"Progressing"}:{map[f:lastUpdateTime:{map[]} f:message:{map[]} f:reason:{map[]}]}]} f:readyReplicas:{map[]} f:replicas:{map[]}]},},}}],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-13 20:23:30 +0000 UTC 2019-05-13 20:23:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-13 20:23:32 +0000 UTC 2019-05-13 20:23:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 13 20:23:32.742: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-5246,SelfLink:/apis/apps/v1/namespaces/deployment-5246/replicasets/test-rolling-update-deployment-67599b4d9,UID:f93d40e5-75bc-11e9-ac1e-0215dc200466,ResourceVersion:16853,Generation:1,CreationTimestamp:2019-05-13 20:23:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment f93b75c0-75bc-11e9-ac1e-0215dc200466 0xc003252f60 0xc003252f61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update apps/v1 2019-05-13 20:23:30 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:deployment.kubernetes.io/desired-replicas:{map[]} f:deployment.kubernetes.io/max-replicas:{map[]} f:deployment.kubernetes.io/revision:{map[]}]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"f93b75c0-75bc-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:replicas:{map[]} f:selector:{map[f:matchLabels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]}]} f:template:{map[f:metadata:{map[f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]}]} f:spec:{map[f:containers:{map[k:{"name":"redis"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]}]}]} f:dnsPolicy:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:terminationGracePeriodSeconds:{map[]}]}]}]},f:status: {map[f:fullyLabeledReplicas:{map[]} f:observedGeneration:{map[]} f:replicas:{map[]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 20:23:32 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:availableReplicas:{map[]} f:readyReplicas:{map[]}]},},}}],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 13 20:23:32.742: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 13 20:23:32.743: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-5246,SelfLink:/apis/apps/v1/namespaces/deployment-5246/replicasets/test-rolling-update-controller,UID:f63f5470-75bc-11e9-ac1e-0215dc200466,ResourceVersion:16862,Generation:2,CreationTimestamp:2019-05-13 20:23:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment f93b75c0-75bc-11e9-ac1e-0215dc200466 0xc003252c17 0xc003252c18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update apps/v1 2019-05-13 20:23:25 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:deployment.kubernetes.io/revision:{map[]}]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod:{map[]}]}]},f:spec: {map[f:selector:{map[f:matchLabels:{map[.:{map[]} f:name:{map[]} f:pod:{map[]}]}]} f:template:{map[f:metadata:{map[f:labels:{map[.:{map[]} f:name:{map[]} f:pod:{map[]}]}]} f:spec:{map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]}]}]} f:dnsPolicy:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:terminationGracePeriodSeconds:{map[]}]}]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 20:23:30 +0000 UTC &Fields{Map:map[string]Fields{f:metadata: {map[f:ownerReferences:{map[.:{map[]} k:{"uid":"f93b75c0-75bc-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 20:23:32 +0000 UTC &Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[f:deployment.kubernetes.io/desired-replicas:{map[]} f:deployment.kubernetes.io/max-replicas:{map[]}]}]},f:spec: {map[f:replicas:{map[]}]},f:status: {map[f:observedGeneration:{map[]} f:replicas:{map[]}]},},}}],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 13 20:23:32.745: INFO: Pod "test-rolling-update-deployment-67599b4d9-554qb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-554qb,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-5246,SelfLink:/api/v1/namespaces/deployment-5246/pods/test-rolling-update-deployment-67599b4d9-554qb,UID:f93dce8a-75bc-11e9-ac1e-0215dc200466,ResourceVersion:16852,Generation:0,CreationTimestamp:2019-05-13 20:23:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 f93d40e5-75bc-11e9-ac1e-0215dc200466 0xc0032539e0 0xc0032539e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 20:23:30 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"f93d40e5-75bc-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"redis"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-6qwlg"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}} {kubelet Update v1 2019-05-13 20:23:30 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]} k:{"type":"Initialized"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Ready"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]}]} f:hostIP:{map[]} f:startTime:{map[]}]},},}} {kubelet Update v1 2019-05-13 20:23:32 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]} k:{"type":"Ready"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]}]} f:containerStatuses:{map[]} f:phase:{map[]} f:podIP:{map[]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-6qwlg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6qwlg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-6qwlg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 20:23:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 20:23:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 20:23:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 20:23:30 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.248,PodIP:10.244.84.37,StartTime:2019-05-13 20:23:30 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-13 20:23:31 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://0d2487257151969a8c035fff100821b89c9c63b7b11f809b18a641a5487522c4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:23:32.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5246" for this suite.
May 13 20:23:38.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:23:38.806: INFO: namespace deployment-5246 deletion completed in 6.059188921s

• [SLOW TEST:13.274 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:23:38.806: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7375
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 13 20:23:38.929: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:23:42.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7375" for this suite.
May 13 20:23:48.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:23:48.642: INFO: namespace init-container-7375 deletion completed in 6.062313644s

• [SLOW TEST:9.835 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:23:48.642: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7977
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:23:48.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7977" for this suite.
May 13 20:23:54.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:23:54.841: INFO: namespace services-7977 deletion completed in 6.071631919s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.199 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:23:54.841: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9222
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 13 20:23:54.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-9222'
May 13 20:23:55.037: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 13 20:23:55.037: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
May 13 20:23:55.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 delete jobs e2e-test-nginx-job --namespace=kubectl-9222'
May 13 20:23:55.118: INFO: stderr: ""
May 13 20:23:55.118: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:23:55.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9222" for this suite.
May 13 20:24:01.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:24:01.186: INFO: namespace kubectl-9222 deletion completed in 6.065985598s

• [SLOW TEST:6.345 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:24:01.186: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-7103
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7148
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9433
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:24:07.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7103" for this suite.
May 13 20:24:13.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:24:13.641: INFO: namespace namespaces-7103 deletion completed in 6.07248567s
STEP: Destroying namespace "nsdeletetest-7148" for this suite.
May 13 20:24:13.642: INFO: Namespace nsdeletetest-7148 was already deleted
STEP: Destroying namespace "nsdeletetest-9433" for this suite.
May 13 20:24:19.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:24:19.707: INFO: namespace nsdeletetest-9433 deletion completed in 6.065192219s

• [SLOW TEST:18.521 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:24:19.708: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-464
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-1680c76a-75bd-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume configMaps
May 13 20:24:19.839: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-16812029-75bd-11e9-9153-920e960bc5b9" in namespace "projected-464" to be "success or failure"
May 13 20:24:19.841: INFO: Pod "pod-projected-configmaps-16812029-75bd-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.346654ms
May 13 20:24:21.844: INFO: Pod "pod-projected-configmaps-16812029-75bd-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00515307s
STEP: Saw pod success
May 13 20:24:21.844: INFO: Pod "pod-projected-configmaps-16812029-75bd-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:24:21.846: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-projected-configmaps-16812029-75bd-11e9-9153-920e960bc5b9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 13 20:24:21.862: INFO: Waiting for pod pod-projected-configmaps-16812029-75bd-11e9-9153-920e960bc5b9 to disappear
May 13 20:24:21.864: INFO: Pod pod-projected-configmaps-16812029-75bd-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:24:21.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-464" for this suite.
May 13 20:24:27.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:24:27.929: INFO: namespace projected-464 deletion completed in 6.062633485s

• [SLOW TEST:8.221 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:24:27.929: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8805
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-1b676deb-75bd-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume secrets
May 13 20:24:28.062: INFO: Waiting up to 5m0s for pod "pod-secrets-1b67d6a2-75bd-11e9-9153-920e960bc5b9" in namespace "secrets-8805" to be "success or failure"
May 13 20:24:28.064: INFO: Pod "pod-secrets-1b67d6a2-75bd-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.958739ms
May 13 20:24:30.067: INFO: Pod "pod-secrets-1b67d6a2-75bd-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004610956s
STEP: Saw pod success
May 13 20:24:30.067: INFO: Pod "pod-secrets-1b67d6a2-75bd-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:24:30.068: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-secrets-1b67d6a2-75bd-11e9-9153-920e960bc5b9 container secret-volume-test: <nil>
STEP: delete the pod
May 13 20:24:30.083: INFO: Waiting for pod pod-secrets-1b67d6a2-75bd-11e9-9153-920e960bc5b9 to disappear
May 13 20:24:30.085: INFO: Pod pod-secrets-1b67d6a2-75bd-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:24:30.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8805" for this suite.
May 13 20:24:36.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:24:36.146: INFO: namespace secrets-8805 deletion completed in 6.059232043s

• [SLOW TEST:8.217 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:24:36.146: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4948
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-4948
May 13 20:24:38.281: INFO: Started pod liveness-http in namespace container-probe-4948
STEP: checking the pod's current state and verifying that restartCount is present
May 13 20:24:38.282: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:28:38.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4948" for this suite.
May 13 20:28:44.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:28:44.711: INFO: namespace container-probe-4948 deletion completed in 6.062528064s

• [SLOW TEST:248.564 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:28:44.711: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5209
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:28:48.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5209" for this suite.
May 13 20:28:54.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:28:54.914: INFO: namespace kubelet-test-5209 deletion completed in 6.060011704s

• [SLOW TEST:10.203 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:28:54.915: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3667
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-ba8a5a28-75bd-11e9-9153-920e960bc5b9
STEP: Creating configMap with name cm-test-opt-upd-ba8a5a69-75bd-11e9-9153-920e960bc5b9
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-ba8a5a28-75bd-11e9-9153-920e960bc5b9
STEP: Updating configmap cm-test-opt-upd-ba8a5a69-75bd-11e9-9153-920e960bc5b9
STEP: Creating configMap with name cm-test-opt-create-ba8a5a81-75bd-11e9-9153-920e960bc5b9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:29:01.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3667" for this suite.
May 13 20:29:17.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:29:17.163: INFO: namespace projected-3667 deletion completed in 16.060327365s

• [SLOW TEST:22.249 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:29:17.164: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5331
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 13 20:29:17.294: INFO: Waiting up to 5m0s for pod "downward-api-c7ccfe73-75bd-11e9-9153-920e960bc5b9" in namespace "downward-api-5331" to be "success or failure"
May 13 20:29:17.297: INFO: Pod "downward-api-c7ccfe73-75bd-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.884223ms
May 13 20:29:19.300: INFO: Pod "downward-api-c7ccfe73-75bd-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005727438s
STEP: Saw pod success
May 13 20:29:19.300: INFO: Pod "downward-api-c7ccfe73-75bd-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:29:19.302: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downward-api-c7ccfe73-75bd-11e9-9153-920e960bc5b9 container dapi-container: <nil>
STEP: delete the pod
May 13 20:29:19.318: INFO: Waiting for pod downward-api-c7ccfe73-75bd-11e9-9153-920e960bc5b9 to disappear
May 13 20:29:19.320: INFO: Pod downward-api-c7ccfe73-75bd-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:29:19.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5331" for this suite.
May 13 20:29:25.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:29:25.383: INFO: namespace downward-api-5331 deletion completed in 6.059869874s

• [SLOW TEST:8.219 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:29:25.383: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-305
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-ccb2eedc-75bd-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume configMaps
May 13 20:29:25.514: INFO: Waiting up to 5m0s for pod "pod-configmaps-ccb34204-75bd-11e9-9153-920e960bc5b9" in namespace "configmap-305" to be "success or failure"
May 13 20:29:25.515: INFO: Pod "pod-configmaps-ccb34204-75bd-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.717058ms
May 13 20:29:27.518: INFO: Pod "pod-configmaps-ccb34204-75bd-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004382417s
STEP: Saw pod success
May 13 20:29:27.518: INFO: Pod "pod-configmaps-ccb34204-75bd-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:29:27.520: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-configmaps-ccb34204-75bd-11e9-9153-920e960bc5b9 container configmap-volume-test: <nil>
STEP: delete the pod
May 13 20:29:27.542: INFO: Waiting for pod pod-configmaps-ccb34204-75bd-11e9-9153-920e960bc5b9 to disappear
May 13 20:29:27.544: INFO: Pod pod-configmaps-ccb34204-75bd-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:29:27.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-305" for this suite.
May 13 20:29:33.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:29:33.614: INFO: namespace configmap-305 deletion completed in 6.068011735s

• [SLOW TEST:8.231 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:29:33.614: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-372
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-d19b31d2-75bd-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume configMaps
May 13 20:29:33.747: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d19b870d-75bd-11e9-9153-920e960bc5b9" in namespace "projected-372" to be "success or failure"
May 13 20:29:33.749: INFO: Pod "pod-projected-configmaps-d19b870d-75bd-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.691583ms
May 13 20:29:35.752: INFO: Pod "pod-projected-configmaps-d19b870d-75bd-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005484351s
May 13 20:29:37.755: INFO: Pod "pod-projected-configmaps-d19b870d-75bd-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008562263s
STEP: Saw pod success
May 13 20:29:37.755: INFO: Pod "pod-projected-configmaps-d19b870d-75bd-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:29:37.757: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-projected-configmaps-d19b870d-75bd-11e9-9153-920e960bc5b9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 13 20:29:37.776: INFO: Waiting for pod pod-projected-configmaps-d19b870d-75bd-11e9-9153-920e960bc5b9 to disappear
May 13 20:29:37.780: INFO: Pod pod-projected-configmaps-d19b870d-75bd-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:29:37.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-372" for this suite.
May 13 20:29:43.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:29:43.847: INFO: namespace projected-372 deletion completed in 6.064114274s

• [SLOW TEST:10.232 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:29:43.847: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8888
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 20:29:43.978: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d7b49bdc-75bd-11e9-9153-920e960bc5b9" in namespace "projected-8888" to be "success or failure"
May 13 20:29:43.980: INFO: Pod "downwardapi-volume-d7b49bdc-75bd-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.976087ms
May 13 20:29:45.983: INFO: Pod "downwardapi-volume-d7b49bdc-75bd-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004628205s
May 13 20:29:47.985: INFO: Pod "downwardapi-volume-d7b49bdc-75bd-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00722347s
STEP: Saw pod success
May 13 20:29:47.985: INFO: Pod "downwardapi-volume-d7b49bdc-75bd-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:29:47.987: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downwardapi-volume-d7b49bdc-75bd-11e9-9153-920e960bc5b9 container client-container: <nil>
STEP: delete the pod
May 13 20:29:48.000: INFO: Waiting for pod downwardapi-volume-d7b49bdc-75bd-11e9-9153-920e960bc5b9 to disappear
May 13 20:29:48.003: INFO: Pod downwardapi-volume-d7b49bdc-75bd-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:29:48.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8888" for this suite.
May 13 20:29:54.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:29:54.064: INFO: namespace projected-8888 deletion completed in 6.05983394s

• [SLOW TEST:10.218 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:29:54.065: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2137
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-ddcbe5ea-75bd-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume secrets
May 13 20:29:54.198: INFO: Waiting up to 5m0s for pod "pod-secrets-ddcc4ca7-75bd-11e9-9153-920e960bc5b9" in namespace "secrets-2137" to be "success or failure"
May 13 20:29:54.199: INFO: Pod "pod-secrets-ddcc4ca7-75bd-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.754649ms
May 13 20:29:56.202: INFO: Pod "pod-secrets-ddcc4ca7-75bd-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004323789s
May 13 20:29:58.205: INFO: Pod "pod-secrets-ddcc4ca7-75bd-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007331182s
STEP: Saw pod success
May 13 20:29:58.205: INFO: Pod "pod-secrets-ddcc4ca7-75bd-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:29:58.207: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-secrets-ddcc4ca7-75bd-11e9-9153-920e960bc5b9 container secret-volume-test: <nil>
STEP: delete the pod
May 13 20:29:58.224: INFO: Waiting for pod pod-secrets-ddcc4ca7-75bd-11e9-9153-920e960bc5b9 to disappear
May 13 20:29:58.225: INFO: Pod pod-secrets-ddcc4ca7-75bd-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:29:58.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2137" for this suite.
May 13 20:30:04.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:30:04.289: INFO: namespace secrets-2137 deletion completed in 6.061513128s

• [SLOW TEST:10.224 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:30:04.289: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5141
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 13 20:30:04.420: INFO: Waiting up to 5m0s for pod "downward-api-e3e3ec32-75bd-11e9-9153-920e960bc5b9" in namespace "downward-api-5141" to be "success or failure"
May 13 20:30:04.421: INFO: Pod "downward-api-e3e3ec32-75bd-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.856136ms
May 13 20:30:06.424: INFO: Pod "downward-api-e3e3ec32-75bd-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004589096s
STEP: Saw pod success
May 13 20:30:06.424: INFO: Pod "downward-api-e3e3ec32-75bd-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:30:06.426: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downward-api-e3e3ec32-75bd-11e9-9153-920e960bc5b9 container dapi-container: <nil>
STEP: delete the pod
May 13 20:30:06.439: INFO: Waiting for pod downward-api-e3e3ec32-75bd-11e9-9153-920e960bc5b9 to disappear
May 13 20:30:06.443: INFO: Pod downward-api-e3e3ec32-75bd-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:30:06.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5141" for this suite.
May 13 20:30:12.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:30:12.504: INFO: namespace downward-api-5141 deletion completed in 6.0597708s

• [SLOW TEST:8.215 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:30:12.505: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9087
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-e8c9365c-75bd-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume configMaps
May 13 20:30:12.635: INFO: Waiting up to 5m0s for pod "pod-configmaps-e8c997a1-75bd-11e9-9153-920e960bc5b9" in namespace "configmap-9087" to be "success or failure"
May 13 20:30:12.639: INFO: Pod "pod-configmaps-e8c997a1-75bd-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.17703ms
May 13 20:30:14.644: INFO: Pod "pod-configmaps-e8c997a1-75bd-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008875163s
STEP: Saw pod success
May 13 20:30:14.644: INFO: Pod "pod-configmaps-e8c997a1-75bd-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:30:14.647: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-configmaps-e8c997a1-75bd-11e9-9153-920e960bc5b9 container configmap-volume-test: <nil>
STEP: delete the pod
May 13 20:30:14.674: INFO: Waiting for pod pod-configmaps-e8c997a1-75bd-11e9-9153-920e960bc5b9 to disappear
May 13 20:30:14.675: INFO: Pod pod-configmaps-e8c997a1-75bd-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:30:14.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9087" for this suite.
May 13 20:30:20.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:30:20.770: INFO: namespace configmap-9087 deletion completed in 6.091659348s

• [SLOW TEST:8.266 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:30:20.770: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5183
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 13 20:30:20.909: INFO: Pod name pod-release: Found 0 pods out of 1
May 13 20:30:25.911: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:30:26.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5183" for this suite.
May 13 20:30:32.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:30:33.004: INFO: namespace replication-controller-5183 deletion completed in 6.076382634s

• [SLOW TEST:12.234 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:30:33.005: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4045
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 13 20:30:33.135: INFO: Waiting up to 5m0s for pod "pod-f5016a10-75bd-11e9-9153-920e960bc5b9" in namespace "emptydir-4045" to be "success or failure"
May 13 20:30:33.139: INFO: Pod "pod-f5016a10-75bd-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.161144ms
May 13 20:30:35.141: INFO: Pod "pod-f5016a10-75bd-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005736769s
STEP: Saw pod success
May 13 20:30:35.141: INFO: Pod "pod-f5016a10-75bd-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:30:35.143: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-f5016a10-75bd-11e9-9153-920e960bc5b9 container test-container: <nil>
STEP: delete the pod
May 13 20:30:35.157: INFO: Waiting for pod pod-f5016a10-75bd-11e9-9153-920e960bc5b9 to disappear
May 13 20:30:35.158: INFO: Pod pod-f5016a10-75bd-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:30:35.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4045" for this suite.
May 13 20:30:41.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:30:41.250: INFO: namespace emptydir-4045 deletion completed in 6.089621809s

• [SLOW TEST:8.245 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:30:41.250: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9306
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 13 20:30:41.377: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 13 20:30:41.381: INFO: Waiting for terminating namespaces to be deleted...
May 13 20:30:41.383: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-0-248.ec2.internal before test
May 13 20:30:41.400: INFO: gravity-site-pzzq5 from kube-system started at 2019-05-13 19:06:12 +0000 UTC (1 container statuses recorded)
May 13 20:30:41.400: INFO: 	Container gravity-site ready: true, restart count 0
May 13 20:30:41.400: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-13 19:15:00 +0000 UTC (1 container statuses recorded)
May 13 20:30:41.400: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 13 20:30:41.400: INFO: coredns-x2prj from kube-system started at 2019-05-13 19:04:41 +0000 UTC (1 container statuses recorded)
May 13 20:30:41.400: INFO: 	Container coredns ready: true, restart count 0
May 13 20:30:41.400: INFO: log-forwarder-jlhbm from kube-system started at 2019-05-13 19:04:56 +0000 UTC (1 container statuses recorded)
May 13 20:30:41.400: INFO: 	Container log-forwarder ready: true, restart count 0
May 13 20:30:41.400: INFO: kapacitor-55d57f7756-fdx56 from monitoring started at 2019-05-13 19:05:14 +0000 UTC (3 container statuses recorded)
May 13 20:30:41.400: INFO: 	Container alert-loader ready: true, restart count 0
May 13 20:30:41.400: INFO: 	Container kapacitor ready: true, restart count 0
May 13 20:30:41.400: INFO: 	Container watcher ready: true, restart count 1
May 13 20:30:41.400: INFO: bandwagon-58d4b44589-s7qp9 from kube-system started at 2019-05-13 19:04:48 +0000 UTC (1 container statuses recorded)
May 13 20:30:41.400: INFO: 	Container bandwagon ready: true, restart count 0
May 13 20:30:41.400: INFO: log-collector-547858d4ff-2pqdk from kube-system started at 2019-05-13 19:04:55 +0000 UTC (1 container statuses recorded)
May 13 20:30:41.400: INFO: 	Container log-collector ready: true, restart count 0
May 13 20:30:41.400: INFO: heapster-7b9959b6d6-xrxl2 from monitoring started at 2019-05-13 19:05:14 +0000 UTC (1 container statuses recorded)
May 13 20:30:41.400: INFO: 	Container heapster ready: true, restart count 0
May 13 20:30:41.400: INFO: sonobuoy-systemd-logs-daemon-set-e496b0b34872467e-9g8fv from heptio-sonobuoy started at 2019-05-13 19:15:25 +0000 UTC (2 container statuses recorded)
May 13 20:30:41.400: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 13 20:30:41.400: INFO: 	Container systemd-logs ready: true, restart count 1
May 13 20:30:41.400: INFO: grafana-776fb79c76-v98m9 from monitoring started at 2019-05-13 19:05:14 +0000 UTC (2 container statuses recorded)
May 13 20:30:41.400: INFO: 	Container grafana ready: true, restart count 0
May 13 20:30:41.400: INFO: 	Container watcher ready: true, restart count 1
May 13 20:30:41.400: INFO: telegraf-node-l2r62 from monitoring started at 2019-05-13 19:05:15 +0000 UTC (1 container statuses recorded)
May 13 20:30:41.400: INFO: 	Container telegraf ready: true, restart count 0
May 13 20:30:41.400: INFO: tiller-deploy-5f5d5b74f9-6l5nq from kube-system started at 2019-05-13 19:05:39 +0000 UTC (1 container statuses recorded)
May 13 20:30:41.400: INFO: 	Container tiller ready: true, restart count 0
May 13 20:30:41.400: INFO: sonobuoy-e2e-job-c3529c9c402c4d54 from heptio-sonobuoy started at 2019-05-13 19:15:02 +0000 UTC (2 container statuses recorded)
May 13 20:30:41.400: INFO: 	Container e2e ready: true, restart count 0
May 13 20:30:41.400: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 13 20:30:41.400: INFO: influxdb-545989ddbc-9h6pl from monitoring started at 2019-05-13 19:05:14 +0000 UTC (2 container statuses recorded)
May 13 20:30:41.400: INFO: 	Container influxdb ready: true, restart count 0
May 13 20:30:41.400: INFO: 	Container watcher ready: true, restart count 1
May 13 20:30:41.400: INFO: telegraf-5c4699b87-tnhbp from monitoring started at 2019-05-13 19:05:15 +0000 UTC (1 container statuses recorded)
May 13 20:30:41.400: INFO: 	Container telegraf ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.159e582603f78a5f], Reason = [FailedScheduling], Message = [0/1 nodes are available: 1 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:30:42.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9306" for this suite.
May 13 20:30:48.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:30:48.484: INFO: namespace sched-pred-9306 deletion completed in 6.064090288s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.234 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:30:48.485: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4284
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-strj
STEP: Creating a pod to test atomic-volume-subpath
May 13 20:30:48.621: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-strj" in namespace "subpath-4284" to be "success or failure"
May 13 20:30:48.628: INFO: Pod "pod-subpath-test-projected-strj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.36055ms
May 13 20:30:50.630: INFO: Pod "pod-subpath-test-projected-strj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009075845s
May 13 20:30:52.633: INFO: Pod "pod-subpath-test-projected-strj": Phase="Running", Reason="", readiness=true. Elapsed: 4.012072766s
May 13 20:30:54.636: INFO: Pod "pod-subpath-test-projected-strj": Phase="Running", Reason="", readiness=true. Elapsed: 6.015191058s
May 13 20:30:56.639: INFO: Pod "pod-subpath-test-projected-strj": Phase="Running", Reason="", readiness=true. Elapsed: 8.017897004s
May 13 20:30:58.641: INFO: Pod "pod-subpath-test-projected-strj": Phase="Running", Reason="", readiness=true. Elapsed: 10.020314264s
May 13 20:31:00.645: INFO: Pod "pod-subpath-test-projected-strj": Phase="Running", Reason="", readiness=true. Elapsed: 12.023474384s
May 13 20:31:02.647: INFO: Pod "pod-subpath-test-projected-strj": Phase="Running", Reason="", readiness=true. Elapsed: 14.026238782s
May 13 20:31:04.650: INFO: Pod "pod-subpath-test-projected-strj": Phase="Running", Reason="", readiness=true. Elapsed: 16.02894615s
May 13 20:31:06.653: INFO: Pod "pod-subpath-test-projected-strj": Phase="Running", Reason="", readiness=true. Elapsed: 18.031796381s
May 13 20:31:08.656: INFO: Pod "pod-subpath-test-projected-strj": Phase="Running", Reason="", readiness=true. Elapsed: 20.034393718s
May 13 20:31:10.658: INFO: Pod "pod-subpath-test-projected-strj": Phase="Running", Reason="", readiness=true. Elapsed: 22.036982762s
May 13 20:31:12.661: INFO: Pod "pod-subpath-test-projected-strj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.039669667s
STEP: Saw pod success
May 13 20:31:12.661: INFO: Pod "pod-subpath-test-projected-strj" satisfied condition "success or failure"
May 13 20:31:12.663: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-subpath-test-projected-strj container test-container-subpath-projected-strj: <nil>
STEP: delete the pod
May 13 20:31:12.675: INFO: Waiting for pod pod-subpath-test-projected-strj to disappear
May 13 20:31:12.681: INFO: Pod pod-subpath-test-projected-strj no longer exists
STEP: Deleting pod pod-subpath-test-projected-strj
May 13 20:31:12.681: INFO: Deleting pod "pod-subpath-test-projected-strj" in namespace "subpath-4284"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:31:12.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4284" for this suite.
May 13 20:31:18.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:31:18.743: INFO: namespace subpath-4284 deletion completed in 6.05878071s

• [SLOW TEST:30.259 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:31:18.743: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6377
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 20:31:18.873: INFO: Waiting up to 5m0s for pod "downwardapi-volume-104499ec-75be-11e9-9153-920e960bc5b9" in namespace "downward-api-6377" to be "success or failure"
May 13 20:31:18.875: INFO: Pod "downwardapi-volume-104499ec-75be-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.225229ms
May 13 20:31:20.882: INFO: Pod "downwardapi-volume-104499ec-75be-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009160247s
STEP: Saw pod success
May 13 20:31:20.882: INFO: Pod "downwardapi-volume-104499ec-75be-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:31:20.884: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downwardapi-volume-104499ec-75be-11e9-9153-920e960bc5b9 container client-container: <nil>
STEP: delete the pod
May 13 20:31:20.901: INFO: Waiting for pod downwardapi-volume-104499ec-75be-11e9-9153-920e960bc5b9 to disappear
May 13 20:31:20.903: INFO: Pod downwardapi-volume-104499ec-75be-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:31:20.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6377" for this suite.
May 13 20:31:26.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:31:26.967: INFO: namespace downward-api-6377 deletion completed in 6.061469608s

• [SLOW TEST:8.223 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:31:26.967: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2987
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 20:31:27.097: INFO: Waiting up to 5m0s for pod "downwardapi-volume-152b6c85-75be-11e9-9153-920e960bc5b9" in namespace "projected-2987" to be "success or failure"
May 13 20:31:27.100: INFO: Pod "downwardapi-volume-152b6c85-75be-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.916826ms
May 13 20:31:29.103: INFO: Pod "downwardapi-volume-152b6c85-75be-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005636221s
STEP: Saw pod success
May 13 20:31:29.103: INFO: Pod "downwardapi-volume-152b6c85-75be-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:31:29.105: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downwardapi-volume-152b6c85-75be-11e9-9153-920e960bc5b9 container client-container: <nil>
STEP: delete the pod
May 13 20:31:29.117: INFO: Waiting for pod downwardapi-volume-152b6c85-75be-11e9-9153-920e960bc5b9 to disappear
May 13 20:31:29.119: INFO: Pod downwardapi-volume-152b6c85-75be-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:31:29.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2987" for this suite.
May 13 20:31:35.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:31:35.184: INFO: namespace projected-2987 deletion completed in 6.061927868s

• [SLOW TEST:8.217 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:31:35.184: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6189
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 13 20:31:35.317: INFO: Waiting up to 5m0s for pod "pod-1a11be50-75be-11e9-9153-920e960bc5b9" in namespace "emptydir-6189" to be "success or failure"
May 13 20:31:35.319: INFO: Pod "pod-1a11be50-75be-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.893104ms
May 13 20:31:37.322: INFO: Pod "pod-1a11be50-75be-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004841161s
STEP: Saw pod success
May 13 20:31:37.322: INFO: Pod "pod-1a11be50-75be-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:31:37.324: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-1a11be50-75be-11e9-9153-920e960bc5b9 container test-container: <nil>
STEP: delete the pod
May 13 20:31:37.338: INFO: Waiting for pod pod-1a11be50-75be-11e9-9153-920e960bc5b9 to disappear
May 13 20:31:37.341: INFO: Pod pod-1a11be50-75be-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:31:37.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6189" for this suite.
May 13 20:31:43.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:31:43.403: INFO: namespace emptydir-6189 deletion completed in 6.060396063s

• [SLOW TEST:8.220 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:31:43.404: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7418
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 13 20:31:43.527: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 13 20:31:43.547: INFO: Waiting for terminating namespaces to be deleted...
May 13 20:31:43.549: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-0-248.ec2.internal before test
May 13 20:31:43.576: INFO: kapacitor-55d57f7756-fdx56 from monitoring started at 2019-05-13 19:05:14 +0000 UTC (3 container statuses recorded)
May 13 20:31:43.576: INFO: 	Container alert-loader ready: true, restart count 0
May 13 20:31:43.576: INFO: 	Container kapacitor ready: true, restart count 0
May 13 20:31:43.576: INFO: 	Container watcher ready: true, restart count 1
May 13 20:31:43.576: INFO: coredns-x2prj from kube-system started at 2019-05-13 19:04:41 +0000 UTC (1 container statuses recorded)
May 13 20:31:43.576: INFO: 	Container coredns ready: true, restart count 0
May 13 20:31:43.576: INFO: log-forwarder-jlhbm from kube-system started at 2019-05-13 19:04:56 +0000 UTC (1 container statuses recorded)
May 13 20:31:43.576: INFO: 	Container log-forwarder ready: true, restart count 0
May 13 20:31:43.576: INFO: bandwagon-58d4b44589-s7qp9 from kube-system started at 2019-05-13 19:04:48 +0000 UTC (1 container statuses recorded)
May 13 20:31:43.576: INFO: 	Container bandwagon ready: true, restart count 0
May 13 20:31:43.576: INFO: log-collector-547858d4ff-2pqdk from kube-system started at 2019-05-13 19:04:55 +0000 UTC (1 container statuses recorded)
May 13 20:31:43.576: INFO: 	Container log-collector ready: true, restart count 0
May 13 20:31:43.576: INFO: heapster-7b9959b6d6-xrxl2 from monitoring started at 2019-05-13 19:05:14 +0000 UTC (1 container statuses recorded)
May 13 20:31:43.576: INFO: 	Container heapster ready: true, restart count 0
May 13 20:31:43.576: INFO: sonobuoy-systemd-logs-daemon-set-e496b0b34872467e-9g8fv from heptio-sonobuoy started at 2019-05-13 19:15:25 +0000 UTC (2 container statuses recorded)
May 13 20:31:43.577: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 13 20:31:43.577: INFO: 	Container systemd-logs ready: true, restart count 1
May 13 20:31:43.577: INFO: grafana-776fb79c76-v98m9 from monitoring started at 2019-05-13 19:05:14 +0000 UTC (2 container statuses recorded)
May 13 20:31:43.577: INFO: 	Container grafana ready: true, restart count 0
May 13 20:31:43.577: INFO: 	Container watcher ready: true, restart count 1
May 13 20:31:43.577: INFO: sonobuoy-e2e-job-c3529c9c402c4d54 from heptio-sonobuoy started at 2019-05-13 19:15:02 +0000 UTC (2 container statuses recorded)
May 13 20:31:43.577: INFO: 	Container e2e ready: true, restart count 0
May 13 20:31:43.577: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 13 20:31:43.577: INFO: telegraf-node-l2r62 from monitoring started at 2019-05-13 19:05:15 +0000 UTC (1 container statuses recorded)
May 13 20:31:43.577: INFO: 	Container telegraf ready: true, restart count 0
May 13 20:31:43.577: INFO: tiller-deploy-5f5d5b74f9-6l5nq from kube-system started at 2019-05-13 19:05:39 +0000 UTC (1 container statuses recorded)
May 13 20:31:43.577: INFO: 	Container tiller ready: true, restart count 0
May 13 20:31:43.577: INFO: influxdb-545989ddbc-9h6pl from monitoring started at 2019-05-13 19:05:14 +0000 UTC (2 container statuses recorded)
May 13 20:31:43.577: INFO: 	Container influxdb ready: true, restart count 0
May 13 20:31:43.577: INFO: 	Container watcher ready: true, restart count 1
May 13 20:31:43.577: INFO: telegraf-5c4699b87-tnhbp from monitoring started at 2019-05-13 19:05:15 +0000 UTC (1 container statuses recorded)
May 13 20:31:43.577: INFO: 	Container telegraf ready: true, restart count 0
May 13 20:31:43.577: INFO: gravity-site-pzzq5 from kube-system started at 2019-05-13 19:06:12 +0000 UTC (1 container statuses recorded)
May 13 20:31:43.577: INFO: 	Container gravity-site ready: true, restart count 0
May 13 20:31:43.577: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-13 19:15:00 +0000 UTC (1 container statuses recorded)
May 13 20:31:43.577: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-2165908c-75be-11e9-9153-920e960bc5b9 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-2165908c-75be-11e9-9153-920e960bc5b9 off the node ip-10-0-0-248.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2165908c-75be-11e9-9153-920e960bc5b9
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:31:49.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7418" for this suite.
May 13 20:32:07.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:32:07.716: INFO: namespace sched-pred-7418 deletion completed in 18.070230217s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:24.313 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:32:07.716: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2139
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 13 20:32:10.379: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2d77b630-75be-11e9-9153-920e960bc5b9"
May 13 20:32:10.379: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2d77b630-75be-11e9-9153-920e960bc5b9" in namespace "pods-2139" to be "terminated due to deadline exceeded"
May 13 20:32:10.381: INFO: Pod "pod-update-activedeadlineseconds-2d77b630-75be-11e9-9153-920e960bc5b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.228446ms
May 13 20:32:12.383: INFO: Pod "pod-update-activedeadlineseconds-2d77b630-75be-11e9-9153-920e960bc5b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.004895556s
May 13 20:32:14.386: INFO: Pod "pod-update-activedeadlineseconds-2d77b630-75be-11e9-9153-920e960bc5b9": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.007437645s
May 13 20:32:14.386: INFO: Pod "pod-update-activedeadlineseconds-2d77b630-75be-11e9-9153-920e960bc5b9" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:32:14.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2139" for this suite.
May 13 20:32:20.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:32:20.454: INFO: namespace pods-2139 deletion completed in 6.065961429s

• [SLOW TEST:12.738 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:32:20.454: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9589
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
May 13 20:32:20.606: INFO: Waiting up to 5m0s for pod "client-containers-350ee396-75be-11e9-9153-920e960bc5b9" in namespace "containers-9589" to be "success or failure"
May 13 20:32:20.614: INFO: Pod "client-containers-350ee396-75be-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.405647ms
May 13 20:32:22.618: INFO: Pod "client-containers-350ee396-75be-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012253175s
May 13 20:32:24.621: INFO: Pod "client-containers-350ee396-75be-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014817243s
STEP: Saw pod success
May 13 20:32:24.621: INFO: Pod "client-containers-350ee396-75be-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:32:24.622: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod client-containers-350ee396-75be-11e9-9153-920e960bc5b9 container test-container: <nil>
STEP: delete the pod
May 13 20:32:24.635: INFO: Waiting for pod client-containers-350ee396-75be-11e9-9153-920e960bc5b9 to disappear
May 13 20:32:24.637: INFO: Pod client-containers-350ee396-75be-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:32:24.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9589" for this suite.
May 13 20:32:30.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:32:30.700: INFO: namespace containers-9589 deletion completed in 6.061699389s

• [SLOW TEST:10.246 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:32:30.700: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-6217
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
May 13 20:32:30.856: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-6217" to be "success or failure"
May 13 20:32:30.861: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 5.42851ms
May 13 20:32:32.865: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008781297s
STEP: Saw pod success
May 13 20:32:32.865: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
May 13 20:32:32.867: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
May 13 20:32:32.883: INFO: Waiting for pod pod-host-path-test to disappear
May 13 20:32:32.886: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:32:32.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-6217" for this suite.
May 13 20:32:38.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:32:38.963: INFO: namespace hostpath-6217 deletion completed in 6.074723418s

• [SLOW TEST:8.263 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:32:38.964: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8117
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 20:32:57.149: INFO: Container started at 2019-05-13 20:32:40 +0000 UTC, pod became ready at 2019-05-13 20:32:56 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:32:57.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8117" for this suite.
May 13 20:33:19.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:33:19.211: INFO: namespace container-probe-8117 deletion completed in 22.059953143s

• [SLOW TEST:40.248 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:33:19.211: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9054
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
May 13 20:33:19.341: INFO: Waiting up to 5m0s for pod "client-containers-58128a27-75be-11e9-9153-920e960bc5b9" in namespace "containers-9054" to be "success or failure"
May 13 20:33:19.343: INFO: Pod "client-containers-58128a27-75be-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.759002ms
May 13 20:33:21.345: INFO: Pod "client-containers-58128a27-75be-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004235833s
May 13 20:33:23.348: INFO: Pod "client-containers-58128a27-75be-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006960757s
STEP: Saw pod success
May 13 20:33:23.348: INFO: Pod "client-containers-58128a27-75be-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:33:23.350: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod client-containers-58128a27-75be-11e9-9153-920e960bc5b9 container test-container: <nil>
STEP: delete the pod
May 13 20:33:23.361: INFO: Waiting for pod client-containers-58128a27-75be-11e9-9153-920e960bc5b9 to disappear
May 13 20:33:23.363: INFO: Pod client-containers-58128a27-75be-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:33:23.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9054" for this suite.
May 13 20:33:29.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:33:29.426: INFO: namespace containers-9054 deletion completed in 6.060888417s

• [SLOW TEST:10.215 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:33:29.427: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9845
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 20:33:29.556: INFO: (0) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 3.665439ms)
May 13 20:33:29.558: INFO: (1) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 1.947921ms)
May 13 20:33:29.560: INFO: (2) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 1.886174ms)
May 13 20:33:29.562: INFO: (3) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 1.941809ms)
May 13 20:33:29.564: INFO: (4) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 2.040448ms)
May 13 20:33:29.566: INFO: (5) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 1.988783ms)
May 13 20:33:29.568: INFO: (6) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 2.087494ms)
May 13 20:33:29.570: INFO: (7) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 2.07288ms)
May 13 20:33:29.572: INFO: (8) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 1.957682ms)
May 13 20:33:29.575: INFO: (9) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 2.467272ms)
May 13 20:33:29.577: INFO: (10) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 2.134085ms)
May 13 20:33:29.579: INFO: (11) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 1.875506ms)
May 13 20:33:29.581: INFO: (12) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 2.039841ms)
May 13 20:33:29.583: INFO: (13) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 2.054793ms)
May 13 20:33:29.585: INFO: (14) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 1.897348ms)
May 13 20:33:29.587: INFO: (15) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 1.958832ms)
May 13 20:33:29.589: INFO: (16) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 1.84202ms)
May 13 20:33:29.591: INFO: (17) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 1.970692ms)
May 13 20:33:29.593: INFO: (18) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 2.072086ms)
May 13 20:33:29.595: INFO: (19) /api/v1/nodes/ip-10-0-0-248.ec2.internal/proxy/logs/: <pre>
<a href="apiserver/">apiserver/</a>
<a href="btmp">btmp</a>
<a href="containers/">container... (200; 2.054492ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:33:29.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9845" for this suite.
May 13 20:33:35.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:33:35.655: INFO: namespace proxy-9845 deletion completed in 6.058621858s

• [SLOW TEST:6.229 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:33:35.655: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9065
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0513 20:34:15.800097      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 13 20:34:15.800: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:34:15.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9065" for this suite.
May 13 20:34:21.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:34:21.865: INFO: namespace gc-9065 deletion completed in 6.06368152s

• [SLOW TEST:46.210 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:34:21.865: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8403
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
May 13 20:34:21.995: INFO: Waiting up to 5m0s for pod "pod-7d6ab19b-75be-11e9-9153-920e960bc5b9" in namespace "emptydir-8403" to be "success or failure"
May 13 20:34:21.997: INFO: Pod "pod-7d6ab19b-75be-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059368ms
May 13 20:34:24.001: INFO: Pod "pod-7d6ab19b-75be-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005760348s
STEP: Saw pod success
May 13 20:34:24.001: INFO: Pod "pod-7d6ab19b-75be-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:34:24.003: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-7d6ab19b-75be-11e9-9153-920e960bc5b9 container test-container: <nil>
STEP: delete the pod
May 13 20:34:24.016: INFO: Waiting for pod pod-7d6ab19b-75be-11e9-9153-920e960bc5b9 to disappear
May 13 20:34:24.018: INFO: Pod pod-7d6ab19b-75be-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:34:24.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8403" for this suite.
May 13 20:34:30.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:34:30.091: INFO: namespace emptydir-8403 deletion completed in 6.070899817s

• [SLOW TEST:8.225 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:34:30.091: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9233
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-82520117-75be-11e9-9153-920e960bc5b9
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-82520117-75be-11e9-9153-920e960bc5b9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:34:34.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9233" for this suite.
May 13 20:34:56.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:34:56.312: INFO: namespace projected-9233 deletion completed in 22.062204896s

• [SLOW TEST:26.221 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:34:56.312: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4764
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-bw4r
STEP: Creating a pod to test atomic-volume-subpath
May 13 20:34:56.446: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-bw4r" in namespace "subpath-4764" to be "success or failure"
May 13 20:34:56.449: INFO: Pod "pod-subpath-test-configmap-bw4r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.338629ms
May 13 20:34:58.451: INFO: Pod "pod-subpath-test-configmap-bw4r": Phase="Running", Reason="", readiness=true. Elapsed: 2.004976173s
May 13 20:35:00.454: INFO: Pod "pod-subpath-test-configmap-bw4r": Phase="Running", Reason="", readiness=true. Elapsed: 4.007969919s
May 13 20:35:02.457: INFO: Pod "pod-subpath-test-configmap-bw4r": Phase="Running", Reason="", readiness=true. Elapsed: 6.010730798s
May 13 20:35:04.460: INFO: Pod "pod-subpath-test-configmap-bw4r": Phase="Running", Reason="", readiness=true. Elapsed: 8.01332293s
May 13 20:35:06.463: INFO: Pod "pod-subpath-test-configmap-bw4r": Phase="Running", Reason="", readiness=true. Elapsed: 10.016036715s
May 13 20:35:08.465: INFO: Pod "pod-subpath-test-configmap-bw4r": Phase="Running", Reason="", readiness=true. Elapsed: 12.01867327s
May 13 20:35:10.468: INFO: Pod "pod-subpath-test-configmap-bw4r": Phase="Running", Reason="", readiness=true. Elapsed: 14.021190902s
May 13 20:35:12.470: INFO: Pod "pod-subpath-test-configmap-bw4r": Phase="Running", Reason="", readiness=true. Elapsed: 16.023818989s
May 13 20:35:14.473: INFO: Pod "pod-subpath-test-configmap-bw4r": Phase="Running", Reason="", readiness=true. Elapsed: 18.026494639s
May 13 20:35:16.476: INFO: Pod "pod-subpath-test-configmap-bw4r": Phase="Running", Reason="", readiness=true. Elapsed: 20.029262027s
May 13 20:35:18.478: INFO: Pod "pod-subpath-test-configmap-bw4r": Phase="Running", Reason="", readiness=true. Elapsed: 22.031892116s
May 13 20:35:20.481: INFO: Pod "pod-subpath-test-configmap-bw4r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.034794157s
STEP: Saw pod success
May 13 20:35:20.481: INFO: Pod "pod-subpath-test-configmap-bw4r" satisfied condition "success or failure"
May 13 20:35:20.483: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-subpath-test-configmap-bw4r container test-container-subpath-configmap-bw4r: <nil>
STEP: delete the pod
May 13 20:35:20.522: INFO: Waiting for pod pod-subpath-test-configmap-bw4r to disappear
May 13 20:35:20.525: INFO: Pod pod-subpath-test-configmap-bw4r no longer exists
STEP: Deleting pod pod-subpath-test-configmap-bw4r
May 13 20:35:20.525: INFO: Deleting pod "pod-subpath-test-configmap-bw4r" in namespace "subpath-4764"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:35:20.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4764" for this suite.
May 13 20:35:26.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:35:26.590: INFO: namespace subpath-4764 deletion completed in 6.061198001s

• [SLOW TEST:30.278 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:35:26.590: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2139
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-2139
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 13 20:35:26.714: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 13 20:35:48.752: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.84.79:8080/dial?request=hostName&protocol=udp&host=10.244.84.78&port=8081&tries=1'] Namespace:pod-network-test-2139 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 20:35:48.752: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
May 13 20:35:48.855: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:35:48.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2139" for this suite.
May 13 20:36:10.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:36:10.928: INFO: namespace pod-network-test-2139 deletion completed in 22.068408979s

• [SLOW TEST:44.338 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:36:10.928: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7970
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-be6f4bc2-75be-11e9-9153-920e960bc5b9
STEP: Creating secret with name s-test-opt-upd-be6f4c0c-75be-11e9-9153-920e960bc5b9
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-be6f4bc2-75be-11e9-9153-920e960bc5b9
STEP: Updating secret s-test-opt-upd-be6f4c0c-75be-11e9-9153-920e960bc5b9
STEP: Creating secret with name s-test-opt-create-be6f4c28-75be-11e9-9153-920e960bc5b9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:37:21.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7970" for this suite.
May 13 20:37:43.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:37:43.397: INFO: namespace projected-7970 deletion completed in 22.059250379s

• [SLOW TEST:92.469 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:37:43.397: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-322
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
May 13 20:37:43.530: INFO: Waiting up to 5m0s for pod "client-containers-f589f8bf-75be-11e9-9153-920e960bc5b9" in namespace "containers-322" to be "success or failure"
May 13 20:37:43.532: INFO: Pod "client-containers-f589f8bf-75be-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.210784ms
May 13 20:37:45.535: INFO: Pod "client-containers-f589f8bf-75be-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005426135s
STEP: Saw pod success
May 13 20:37:45.535: INFO: Pod "client-containers-f589f8bf-75be-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:37:45.537: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod client-containers-f589f8bf-75be-11e9-9153-920e960bc5b9 container test-container: <nil>
STEP: delete the pod
May 13 20:37:45.550: INFO: Waiting for pod client-containers-f589f8bf-75be-11e9-9153-920e960bc5b9 to disappear
May 13 20:37:45.552: INFO: Pod client-containers-f589f8bf-75be-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:37:45.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-322" for this suite.
May 13 20:37:51.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:37:51.615: INFO: namespace containers-322 deletion completed in 6.061450253s

• [SLOW TEST:8.218 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:37:51.616: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5045
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 13 20:37:54.269: INFO: Successfully updated pod "annotationupdatefa701ff9-75be-11e9-9153-920e960bc5b9"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:37:56.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5045" for this suite.
May 13 20:38:18.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:38:18.355: INFO: namespace downward-api-5045 deletion completed in 22.072093895s

• [SLOW TEST:26.739 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:38:18.355: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6624
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 13 20:38:18.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6624'
May 13 20:38:18.735: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 13 20:38:18.735: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
May 13 20:38:18.742: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
May 13 20:38:18.747: INFO: scanned /root for discovery docs: <nil>
May 13 20:38:18.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-6624'
May 13 20:38:34.486: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 13 20:38:34.486: INFO: stdout: "Created e2e-test-nginx-rc-17e528426d34f66fd00d371d62df0e53\nScaling up e2e-test-nginx-rc-17e528426d34f66fd00d371d62df0e53 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-17e528426d34f66fd00d371d62df0e53 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-17e528426d34f66fd00d371d62df0e53 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
May 13 20:38:34.486: INFO: stdout: "Created e2e-test-nginx-rc-17e528426d34f66fd00d371d62df0e53\nScaling up e2e-test-nginx-rc-17e528426d34f66fd00d371d62df0e53 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-17e528426d34f66fd00d371d62df0e53 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-17e528426d34f66fd00d371d62df0e53 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
May 13 20:38:34.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-6624'
May 13 20:38:34.559: INFO: stderr: ""
May 13 20:38:34.559: INFO: stdout: "e2e-test-nginx-rc-17e528426d34f66fd00d371d62df0e53-9gj66 "
May 13 20:38:34.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods e2e-test-nginx-rc-17e528426d34f66fd00d371d62df0e53-9gj66 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6624'
May 13 20:38:34.624: INFO: stderr: ""
May 13 20:38:34.624: INFO: stdout: "true"
May 13 20:38:34.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods e2e-test-nginx-rc-17e528426d34f66fd00d371d62df0e53-9gj66 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6624'
May 13 20:38:34.687: INFO: stderr: ""
May 13 20:38:34.687: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
May 13 20:38:34.687: INFO: e2e-test-nginx-rc-17e528426d34f66fd00d371d62df0e53-9gj66 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
May 13 20:38:34.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 delete rc e2e-test-nginx-rc --namespace=kubectl-6624'
May 13 20:38:34.753: INFO: stderr: ""
May 13 20:38:34.753: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:38:34.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6624" for this suite.
May 13 20:38:56.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:38:56.827: INFO: namespace kubectl-6624 deletion completed in 22.071639782s

• [SLOW TEST:38.473 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:38:56.828: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5997
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-214ff2af-75bf-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume configMaps
May 13 20:38:56.969: INFO: Waiting up to 5m0s for pod "pod-configmaps-21505ffe-75bf-11e9-9153-920e960bc5b9" in namespace "configmap-5997" to be "success or failure"
May 13 20:38:56.972: INFO: Pod "pod-configmaps-21505ffe-75bf-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.271161ms
May 13 20:38:58.975: INFO: Pod "pod-configmaps-21505ffe-75bf-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00603706s
STEP: Saw pod success
May 13 20:38:58.975: INFO: Pod "pod-configmaps-21505ffe-75bf-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:38:58.976: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-configmaps-21505ffe-75bf-11e9-9153-920e960bc5b9 container configmap-volume-test: <nil>
STEP: delete the pod
May 13 20:38:58.989: INFO: Waiting for pod pod-configmaps-21505ffe-75bf-11e9-9153-920e960bc5b9 to disappear
May 13 20:38:58.991: INFO: Pod pod-configmaps-21505ffe-75bf-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:38:58.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5997" for this suite.
May 13 20:39:04.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:39:05.084: INFO: namespace configmap-5997 deletion completed in 6.090961673s

• [SLOW TEST:8.256 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:39:05.084: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1801
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 13 20:39:07.759: INFO: Successfully updated pod "annotationupdate263d3a43-75bf-11e9-9153-920e960bc5b9"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:39:09.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1801" for this suite.
May 13 20:39:31.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:39:31.833: INFO: namespace projected-1801 deletion completed in 22.061429821s

• [SLOW TEST:26.749 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:39:31.834: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1429
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 13 20:39:31.974: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1429,SelfLink:/api/v1/namespaces/watch-1429/configmaps/e2e-watch-test-resource-version,UID:362c9fbe-75bf-11e9-ac1e-0215dc200466,ResourceVersion:19754,Generation:0,CreationTimestamp:2019-05-13 20:39:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 20:39:31 +0000 UTC Fields{Map:map[string]Fields{f:data: {map[.:{map[]} f:mutation:{map[]}]},f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}}],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 13 20:39:31.974: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1429,SelfLink:/api/v1/namespaces/watch-1429/configmaps/e2e-watch-test-resource-version,UID:362c9fbe-75bf-11e9-ac1e-0215dc200466,ResourceVersion:19755,Generation:0,CreationTimestamp:2019-05-13 20:39:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 20:39:31 +0000 UTC Fields{Map:map[string]Fields{f:data: {map[.:{map[]} f:mutation:{map[]}]},f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}}],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:39:31.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1429" for this suite.
May 13 20:39:37.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:39:38.070: INFO: namespace watch-1429 deletion completed in 6.093572441s

• [SLOW TEST:6.236 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:39:38.070: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 20:39:38.217: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 13 20:39:38.225: INFO: Number of nodes with available pods: 0
May 13 20:39:38.225: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 13 20:39:38.246: INFO: Number of nodes with available pods: 0
May 13 20:39:38.246: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 20:39:39.249: INFO: Number of nodes with available pods: 0
May 13 20:39:39.249: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 20:39:40.249: INFO: Number of nodes with available pods: 1
May 13 20:39:40.249: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 13 20:39:40.270: INFO: Number of nodes with available pods: 1
May 13 20:39:40.270: INFO: Number of running nodes: 0, number of available pods: 1
May 13 20:39:41.273: INFO: Number of nodes with available pods: 0
May 13 20:39:41.273: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 13 20:39:41.280: INFO: Number of nodes with available pods: 0
May 13 20:39:41.280: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 20:39:42.283: INFO: Number of nodes with available pods: 0
May 13 20:39:42.283: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 20:39:43.283: INFO: Number of nodes with available pods: 0
May 13 20:39:43.283: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 20:39:44.283: INFO: Number of nodes with available pods: 0
May 13 20:39:44.283: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 20:39:45.283: INFO: Number of nodes with available pods: 0
May 13 20:39:45.283: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 20:39:46.283: INFO: Number of nodes with available pods: 0
May 13 20:39:46.283: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 20:39:47.283: INFO: Number of nodes with available pods: 0
May 13 20:39:47.283: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 20:39:48.283: INFO: Number of nodes with available pods: 0
May 13 20:39:48.283: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 20:39:49.283: INFO: Number of nodes with available pods: 0
May 13 20:39:49.283: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 20:39:50.283: INFO: Number of nodes with available pods: 0
May 13 20:39:50.283: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 20:39:51.283: INFO: Number of nodes with available pods: 0
May 13 20:39:51.283: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 20:39:52.283: INFO: Number of nodes with available pods: 0
May 13 20:39:52.283: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 20:39:53.283: INFO: Number of nodes with available pods: 0
May 13 20:39:53.283: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 20:39:54.283: INFO: Number of nodes with available pods: 0
May 13 20:39:54.283: INFO: Node ip-10-0-0-248.ec2.internal is running more than one daemon pod
May 13 20:39:55.283: INFO: Number of nodes with available pods: 1
May 13 20:39:55.283: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-909, will wait for the garbage collector to delete the pods
May 13 20:39:55.343: INFO: Deleting DaemonSet.extensions daemon-set took: 4.515721ms
May 13 20:39:55.743: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.233127ms
May 13 20:40:03.645: INFO: Number of nodes with available pods: 0
May 13 20:40:03.645: INFO: Number of running nodes: 0, number of available pods: 0
May 13 20:40:03.647: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-909/daemonsets","resourceVersion":"19864"},"items":null}

May 13 20:40:03.648: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-909/pods","resourceVersion":"19864"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:40:03.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-909" for this suite.
May 13 20:40:09.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:40:09.723: INFO: namespace daemonsets-909 deletion completed in 6.058764483s

• [SLOW TEST:31.653 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:40:09.723: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3835
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 13 20:40:09.857: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3835,SelfLink:/api/v1/namespaces/watch-3835/configmaps/e2e-watch-test-label-changed,UID:4cc1f218-75bf-11e9-ac1e-0215dc200466,ResourceVersion:19901,Generation:0,CreationTimestamp:2019-05-13 20:40:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 20:40:09 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}}],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 13 20:40:09.858: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3835,SelfLink:/api/v1/namespaces/watch-3835/configmaps/e2e-watch-test-label-changed,UID:4cc1f218-75bf-11e9-ac1e-0215dc200466,ResourceVersion:19902,Generation:0,CreationTimestamp:2019-05-13 20:40:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 20:40:09 +0000 UTC Fields{Map:map[string]Fields{f:data: {map[.:{map[]} f:mutation:{map[]}]},f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}}],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 13 20:40:09.858: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3835,SelfLink:/api/v1/namespaces/watch-3835/configmaps/e2e-watch-test-label-changed,UID:4cc1f218-75bf-11e9-ac1e-0215dc200466,ResourceVersion:19903,Generation:0,CreationTimestamp:2019-05-13 20:40:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 20:40:09 +0000 UTC Fields{Map:map[string]Fields{f:data: {map[.:{map[]} f:mutation:{map[]}]},f:metadata: {map[f:labels:{map[.:{map[]} f:watch-this-configmap:{map[]}]}]},},}}],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 13 20:40:19.872: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3835,SelfLink:/api/v1/namespaces/watch-3835/configmaps/e2e-watch-test-label-changed,UID:4cc1f218-75bf-11e9-ac1e-0215dc200466,ResourceVersion:19918,Generation:0,CreationTimestamp:2019-05-13 20:40:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 20:40:09 +0000 UTC Fields{Map:map[string]Fields{f:data: {map[.:{map[]} f:mutation:{map[]}]},f:metadata: {map[f:labels:{map[]}]},},}} {e2e.test Update v1 2019-05-13 20:40:19 +0000 UTC &Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[f:watch-this-configmap:{map[]}]}]},},}}],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 13 20:40:19.873: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3835,SelfLink:/api/v1/namespaces/watch-3835/configmaps/e2e-watch-test-label-changed,UID:4cc1f218-75bf-11e9-ac1e-0215dc200466,ResourceVersion:19919,Generation:0,CreationTimestamp:2019-05-13 20:40:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 20:40:09 +0000 UTC Fields{Map:map[string]Fields{f:data: {map[]},f:metadata: {map[f:labels:{map[]}]},},}} {e2e.test Update v1 2019-05-13 20:40:19 +0000 UTC &Fields{Map:map[string]Fields{f:data: {map[f:mutation:{map[]}]},f:metadata: {map[f:labels:{map[f:watch-this-configmap:{map[]}]}]},},}}],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
May 13 20:40:19.873: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3835,SelfLink:/api/v1/namespaces/watch-3835/configmaps/e2e-watch-test-label-changed,UID:4cc1f218-75bf-11e9-ac1e-0215dc200466,ResourceVersion:19920,Generation:0,CreationTimestamp:2019-05-13 20:40:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update v1 2019-05-13 20:40:09 +0000 UTC Fields{Map:map[string]Fields{f:data: {map[]},f:metadata: {map[f:labels:{map[]}]},},}} {e2e.test Update v1 2019-05-13 20:40:19 +0000 UTC &Fields{Map:map[string]Fields{f:data: {map[f:mutation:{map[]}]},f:metadata: {map[f:labels:{map[f:watch-this-configmap:{map[]}]}]},},}}],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:40:19.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3835" for this suite.
May 13 20:40:25.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:40:25.947: INFO: namespace watch-3835 deletion completed in 6.072160004s

• [SLOW TEST:16.224 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:40:25.947: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6908
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-f7hrl in namespace proxy-6908
I0513 20:40:26.087930      16 runners.go:184] Created replication controller with name: proxy-service-f7hrl, namespace: proxy-6908, replica count: 1
I0513 20:40:27.138259      16 runners.go:184] proxy-service-f7hrl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0513 20:40:28.138475      16 runners.go:184] proxy-service-f7hrl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0513 20:40:29.138710      16 runners.go:184] proxy-service-f7hrl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0513 20:40:30.138915      16 runners.go:184] proxy-service-f7hrl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0513 20:40:31.139127      16 runners.go:184] proxy-service-f7hrl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0513 20:40:32.139263      16 runners.go:184] proxy-service-f7hrl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0513 20:40:33.139468      16 runners.go:184] proxy-service-f7hrl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0513 20:40:34.139700      16 runners.go:184] proxy-service-f7hrl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0513 20:40:35.139940      16 runners.go:184] proxy-service-f7hrl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0513 20:40:36.140154      16 runners.go:184] proxy-service-f7hrl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0513 20:40:37.140294      16 runners.go:184] proxy-service-f7hrl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0513 20:40:38.140415      16 runners.go:184] proxy-service-f7hrl Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 13 20:40:38.142: INFO: setup took 12.067588984s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 13 20:40:38.151: INFO: (0) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 8.531582ms)
May 13 20:40:38.155: INFO: (0) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 10.787048ms)
May 13 20:40:38.172: INFO: (0) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 28.654054ms)
May 13 20:40:38.172: INFO: (0) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 29.553638ms)
May 13 20:40:38.172: INFO: (0) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 28.644035ms)
May 13 20:40:38.172: INFO: (0) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 29.43449ms)
May 13 20:40:38.173: INFO: (0) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 30.00831ms)
May 13 20:40:38.174: INFO: (0) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 30.493852ms)
May 13 20:40:38.177: INFO: (0) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 32.754709ms)
May 13 20:40:38.180: INFO: (0) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 37.28303ms)
May 13 20:40:38.184: INFO: (0) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 41.832669ms)
May 13 20:40:38.185: INFO: (0) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 42.469524ms)
May 13 20:40:38.186: INFO: (0) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 42.334779ms)
May 13 20:40:38.186: INFO: (0) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 41.800496ms)
May 13 20:40:38.186: INFO: (0) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 42.318006ms)
May 13 20:40:38.186: INFO: (0) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 42.808766ms)
May 13 20:40:38.198: INFO: (1) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 11.354211ms)
May 13 20:40:38.201: INFO: (1) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 13.000478ms)
May 13 20:40:38.206: INFO: (1) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 18.776749ms)
May 13 20:40:38.206: INFO: (1) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 19.309622ms)
May 13 20:40:38.210: INFO: (1) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 22.997333ms)
May 13 20:40:38.210: INFO: (1) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 22.458356ms)
May 13 20:40:38.210: INFO: (1) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 23.169416ms)
May 13 20:40:38.210: INFO: (1) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 22.425496ms)
May 13 20:40:38.210: INFO: (1) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 23.31654ms)
May 13 20:40:38.210: INFO: (1) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 23.471639ms)
May 13 20:40:38.214: INFO: (1) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 26.413087ms)
May 13 20:40:38.216: INFO: (1) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 28.896721ms)
May 13 20:40:38.216: INFO: (1) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 28.856826ms)
May 13 20:40:38.218: INFO: (1) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 30.362299ms)
May 13 20:40:38.222: INFO: (1) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 34.99976ms)
May 13 20:40:38.222: INFO: (1) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 34.928541ms)
May 13 20:40:38.232: INFO: (2) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 9.496582ms)
May 13 20:40:38.236: INFO: (2) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 12.735046ms)
May 13 20:40:38.236: INFO: (2) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 12.676349ms)
May 13 20:40:38.236: INFO: (2) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 12.288816ms)
May 13 20:40:38.236: INFO: (2) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 12.403075ms)
May 13 20:40:38.236: INFO: (2) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 12.507141ms)
May 13 20:40:38.239: INFO: (2) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 15.492173ms)
May 13 20:40:38.239: INFO: (2) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 15.93459ms)
May 13 20:40:38.239: INFO: (2) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 15.637474ms)
May 13 20:40:38.239: INFO: (2) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 15.857497ms)
May 13 20:40:38.239: INFO: (2) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 16.794799ms)
May 13 20:40:38.241: INFO: (2) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 17.045391ms)
May 13 20:40:38.241: INFO: (2) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 17.527612ms)
May 13 20:40:38.242: INFO: (2) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 18.514034ms)
May 13 20:40:38.242: INFO: (2) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 19.185315ms)
May 13 20:40:38.243: INFO: (2) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 19.974829ms)
May 13 20:40:38.250: INFO: (3) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 6.63708ms)
May 13 20:40:38.258: INFO: (3) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 14.73994ms)
May 13 20:40:38.258: INFO: (3) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 14.628362ms)
May 13 20:40:38.258: INFO: (3) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 14.614722ms)
May 13 20:40:38.258: INFO: (3) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 14.366722ms)
May 13 20:40:38.258: INFO: (3) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 14.741359ms)
May 13 20:40:38.258: INFO: (3) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 14.419511ms)
May 13 20:40:38.258: INFO: (3) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 14.553872ms)
May 13 20:40:38.259: INFO: (3) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 15.514866ms)
May 13 20:40:38.261: INFO: (3) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 18.030065ms)
May 13 20:40:38.263: INFO: (3) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 19.361139ms)
May 13 20:40:38.263: INFO: (3) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 19.488835ms)
May 13 20:40:38.263: INFO: (3) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 19.697951ms)
May 13 20:40:38.263: INFO: (3) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 19.824073ms)
May 13 20:40:38.263: INFO: (3) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 19.492826ms)
May 13 20:40:38.263: INFO: (3) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 19.272484ms)
May 13 20:40:38.274: INFO: (4) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 10.097656ms)
May 13 20:40:38.278: INFO: (4) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 14.902625ms)
May 13 20:40:38.278: INFO: (4) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 14.581048ms)
May 13 20:40:38.278: INFO: (4) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 14.414764ms)
May 13 20:40:38.278: INFO: (4) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 14.492787ms)
May 13 20:40:38.278: INFO: (4) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 14.784306ms)
May 13 20:40:38.279: INFO: (4) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 14.465347ms)
May 13 20:40:38.279: INFO: (4) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 14.65185ms)
May 13 20:40:38.280: INFO: (4) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 16.476533ms)
May 13 20:40:38.280: INFO: (4) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 16.673052ms)
May 13 20:40:38.281: INFO: (4) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 17.264297ms)
May 13 20:40:38.281: INFO: (4) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 17.515265ms)
May 13 20:40:38.281: INFO: (4) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 17.247703ms)
May 13 20:40:38.282: INFO: (4) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 17.571039ms)
May 13 20:40:38.282: INFO: (4) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 17.782455ms)
May 13 20:40:38.285: INFO: (4) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 21.809599ms)
May 13 20:40:38.300: INFO: (5) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 14.005181ms)
May 13 20:40:38.300: INFO: (5) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 14.241162ms)
May 13 20:40:38.301: INFO: (5) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 16.420333ms)
May 13 20:40:38.302: INFO: (5) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 16.025882ms)
May 13 20:40:38.302: INFO: (5) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 15.981324ms)
May 13 20:40:38.302: INFO: (5) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 16.577996ms)
May 13 20:40:38.302: INFO: (5) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 16.50109ms)
May 13 20:40:38.303: INFO: (5) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 17.516991ms)
May 13 20:40:38.304: INFO: (5) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 18.199844ms)
May 13 20:40:38.304: INFO: (5) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 18.141707ms)
May 13 20:40:38.304: INFO: (5) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 18.081351ms)
May 13 20:40:38.305: INFO: (5) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 18.888639ms)
May 13 20:40:38.305: INFO: (5) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 20.036042ms)
May 13 20:40:38.306: INFO: (5) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 20.298853ms)
May 13 20:40:38.306: INFO: (5) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 20.996947ms)
May 13 20:40:38.307: INFO: (5) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 21.789964ms)
May 13 20:40:38.320: INFO: (6) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 12.887985ms)
May 13 20:40:38.320: INFO: (6) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 12.995201ms)
May 13 20:40:38.321: INFO: (6) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 14.067218ms)
May 13 20:40:38.321: INFO: (6) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 13.663133ms)
May 13 20:40:38.321: INFO: (6) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 13.894919ms)
May 13 20:40:38.321: INFO: (6) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 13.864919ms)
May 13 20:40:38.322: INFO: (6) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 14.558309ms)
May 13 20:40:38.322: INFO: (6) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 14.913627ms)
May 13 20:40:38.323: INFO: (6) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 15.31932ms)
May 13 20:40:38.323: INFO: (6) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 15.445705ms)
May 13 20:40:38.323: INFO: (6) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 15.320379ms)
May 13 20:40:38.327: INFO: (6) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 19.588947ms)
May 13 20:40:38.327: INFO: (6) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 19.592055ms)
May 13 20:40:38.327: INFO: (6) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 20.350429ms)
May 13 20:40:38.327: INFO: (6) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 20.018623ms)
May 13 20:40:38.344: INFO: (6) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 37.009667ms)
May 13 20:40:38.355: INFO: (7) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 10.749029ms)
May 13 20:40:38.365: INFO: (7) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 20.473237ms)
May 13 20:40:38.365: INFO: (7) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 20.46339ms)
May 13 20:40:38.367: INFO: (7) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 22.416217ms)
May 13 20:40:38.367: INFO: (7) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 21.695873ms)
May 13 20:40:38.367: INFO: (7) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 21.547938ms)
May 13 20:40:38.367: INFO: (7) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 22.094149ms)
May 13 20:40:38.367: INFO: (7) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 22.034905ms)
May 13 20:40:38.367: INFO: (7) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 21.751139ms)
May 13 20:40:38.367: INFO: (7) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 22.26744ms)
May 13 20:40:38.367: INFO: (7) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 22.186237ms)
May 13 20:40:38.367: INFO: (7) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 22.974125ms)
May 13 20:40:38.367: INFO: (7) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 23.14082ms)
May 13 20:40:38.367: INFO: (7) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 22.900102ms)
May 13 20:40:38.367: INFO: (7) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 22.145514ms)
May 13 20:40:38.367: INFO: (7) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 21.827073ms)
May 13 20:40:38.373: INFO: (8) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 5.354343ms)
May 13 20:40:38.379: INFO: (8) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 10.283316ms)
May 13 20:40:38.381: INFO: (8) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 12.833546ms)
May 13 20:40:38.382: INFO: (8) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 13.038989ms)
May 13 20:40:38.382: INFO: (8) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 13.461596ms)
May 13 20:40:38.382: INFO: (8) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 13.724255ms)
May 13 20:40:38.383: INFO: (8) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 14.787639ms)
May 13 20:40:38.383: INFO: (8) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 14.274874ms)
May 13 20:40:38.383: INFO: (8) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 15.186541ms)
May 13 20:40:38.383: INFO: (8) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 15.79674ms)
May 13 20:40:38.383: INFO: (8) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 15.17376ms)
May 13 20:40:38.383: INFO: (8) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 15.56717ms)
May 13 20:40:38.383: INFO: (8) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 15.093183ms)
May 13 20:40:38.384: INFO: (8) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 16.155324ms)
May 13 20:40:38.384: INFO: (8) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 16.207868ms)
May 13 20:40:38.384: INFO: (8) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 16.371479ms)
May 13 20:40:38.395: INFO: (9) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 10.763793ms)
May 13 20:40:38.396: INFO: (9) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 11.622081ms)
May 13 20:40:38.397: INFO: (9) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 12.10838ms)
May 13 20:40:38.397: INFO: (9) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 12.341475ms)
May 13 20:40:38.398: INFO: (9) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 12.968575ms)
May 13 20:40:38.398: INFO: (9) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 13.162792ms)
May 13 20:40:38.398: INFO: (9) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 13.590519ms)
May 13 20:40:38.398: INFO: (9) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 13.645246ms)
May 13 20:40:38.398: INFO: (9) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 13.613646ms)
May 13 20:40:38.399: INFO: (9) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 13.906241ms)
May 13 20:40:38.399: INFO: (9) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 13.809364ms)
May 13 20:40:38.399: INFO: (9) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 14.490507ms)
May 13 20:40:38.399: INFO: (9) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 14.71876ms)
May 13 20:40:38.399: INFO: (9) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 14.842521ms)
May 13 20:40:38.400: INFO: (9) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 14.975276ms)
May 13 20:40:38.400: INFO: (9) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 14.875289ms)
May 13 20:40:38.403: INFO: (10) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 3.449383ms)
May 13 20:40:38.409: INFO: (10) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 8.800778ms)
May 13 20:40:38.409: INFO: (10) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 8.631815ms)
May 13 20:40:38.409: INFO: (10) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 8.794832ms)
May 13 20:40:38.409: INFO: (10) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 8.921172ms)
May 13 20:40:38.409: INFO: (10) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 8.890955ms)
May 13 20:40:38.410: INFO: (10) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 10.357878ms)
May 13 20:40:38.411: INFO: (10) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 10.787508ms)
May 13 20:40:38.411: INFO: (10) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 10.825793ms)
May 13 20:40:38.411: INFO: (10) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 11.143971ms)
May 13 20:40:38.411: INFO: (10) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 10.953066ms)
May 13 20:40:38.411: INFO: (10) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 11.157833ms)
May 13 20:40:38.411: INFO: (10) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 11.070868ms)
May 13 20:40:38.411: INFO: (10) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 11.215204ms)
May 13 20:40:38.411: INFO: (10) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 11.137457ms)
May 13 20:40:38.411: INFO: (10) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 11.46051ms)
May 13 20:40:38.419: INFO: (11) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 7.467742ms)
May 13 20:40:38.419: INFO: (11) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 7.094279ms)
May 13 20:40:38.419: INFO: (11) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 7.759836ms)
May 13 20:40:38.428: INFO: (11) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 16.097969ms)
May 13 20:40:38.428: INFO: (11) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 15.803763ms)
May 13 20:40:38.428: INFO: (11) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 15.628493ms)
May 13 20:40:38.428: INFO: (11) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 16.302953ms)
May 13 20:40:38.428: INFO: (11) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 15.740231ms)
May 13 20:40:38.432: INFO: (11) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 20.455883ms)
May 13 20:40:38.432: INFO: (11) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 20.286557ms)
May 13 20:40:38.432: INFO: (11) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 19.991357ms)
May 13 20:40:38.433: INFO: (11) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 20.515401ms)
May 13 20:40:38.433: INFO: (11) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 20.600528ms)
May 13 20:40:38.433: INFO: (11) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 20.458018ms)
May 13 20:40:38.433: INFO: (11) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 20.441465ms)
May 13 20:40:38.433: INFO: (11) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 21.148183ms)
May 13 20:40:38.444: INFO: (12) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 10.861089ms)
May 13 20:40:38.444: INFO: (12) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 11.08463ms)
May 13 20:40:38.445: INFO: (12) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 10.947154ms)
May 13 20:40:38.445: INFO: (12) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 11.224278ms)
May 13 20:40:38.445: INFO: (12) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 11.223129ms)
May 13 20:40:38.445: INFO: (12) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 11.36782ms)
May 13 20:40:38.445: INFO: (12) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 11.201768ms)
May 13 20:40:38.445: INFO: (12) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 11.741333ms)
May 13 20:40:38.445: INFO: (12) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 11.740929ms)
May 13 20:40:38.446: INFO: (12) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 12.387ms)
May 13 20:40:38.446: INFO: (12) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 12.72871ms)
May 13 20:40:38.447: INFO: (12) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 13.053155ms)
May 13 20:40:38.447: INFO: (12) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 13.217895ms)
May 13 20:40:38.447: INFO: (12) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 13.817512ms)
May 13 20:40:38.447: INFO: (12) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 13.987096ms)
May 13 20:40:38.447: INFO: (12) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 13.847594ms)
May 13 20:40:38.454: INFO: (13) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 6.749467ms)
May 13 20:40:38.454: INFO: (13) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 5.949862ms)
May 13 20:40:38.454: INFO: (13) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 6.436848ms)
May 13 20:40:38.454: INFO: (13) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 5.914847ms)
May 13 20:40:38.460: INFO: (13) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 11.796175ms)
May 13 20:40:38.460: INFO: (13) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 11.677627ms)
May 13 20:40:38.460: INFO: (13) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 12.319114ms)
May 13 20:40:38.460: INFO: (13) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 11.52007ms)
May 13 20:40:38.460: INFO: (13) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 12.239205ms)
May 13 20:40:38.460: INFO: (13) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 12.697563ms)
May 13 20:40:38.460: INFO: (13) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 12.780542ms)
May 13 20:40:38.460: INFO: (13) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 11.868494ms)
May 13 20:40:38.460: INFO: (13) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 11.626315ms)
May 13 20:40:38.460: INFO: (13) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 12.224481ms)
May 13 20:40:38.461: INFO: (13) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 12.834349ms)
May 13 20:40:38.461: INFO: (13) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 12.047776ms)
May 13 20:40:38.466: INFO: (14) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 5.277552ms)
May 13 20:40:38.473: INFO: (14) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 11.606047ms)
May 13 20:40:38.474: INFO: (14) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 12.461707ms)
May 13 20:40:38.474: INFO: (14) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 12.871963ms)
May 13 20:40:38.474: INFO: (14) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 13.546608ms)
May 13 20:40:38.475: INFO: (14) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 13.717891ms)
May 13 20:40:38.475: INFO: (14) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 14.083614ms)
May 13 20:40:38.476: INFO: (14) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 15.523715ms)
May 13 20:40:38.477: INFO: (14) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 15.575986ms)
May 13 20:40:38.477: INFO: (14) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 15.482776ms)
May 13 20:40:38.480: INFO: (14) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 18.572485ms)
May 13 20:40:38.480: INFO: (14) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 18.774572ms)
May 13 20:40:38.480: INFO: (14) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 19.18397ms)
May 13 20:40:38.480: INFO: (14) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 19.308198ms)
May 13 20:40:38.480: INFO: (14) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 18.999521ms)
May 13 20:40:38.480: INFO: (14) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 18.967333ms)
May 13 20:40:38.486: INFO: (15) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 6.212486ms)
May 13 20:40:38.488: INFO: (15) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 7.236269ms)
May 13 20:40:38.488: INFO: (15) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 7.179665ms)
May 13 20:40:38.495: INFO: (15) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 14.187298ms)
May 13 20:40:38.495: INFO: (15) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 14.506785ms)
May 13 20:40:38.496: INFO: (15) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 13.912341ms)
May 13 20:40:38.496: INFO: (15) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 14.50995ms)
May 13 20:40:38.496: INFO: (15) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 14.78853ms)
May 13 20:40:38.496: INFO: (15) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 14.133909ms)
May 13 20:40:38.496: INFO: (15) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 14.900987ms)
May 13 20:40:38.496: INFO: (15) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 15.059634ms)
May 13 20:40:38.497: INFO: (15) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 15.998797ms)
May 13 20:40:38.497: INFO: (15) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 16.275365ms)
May 13 20:40:38.497: INFO: (15) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 16.473866ms)
May 13 20:40:38.497: INFO: (15) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 16.872145ms)
May 13 20:40:38.497: INFO: (15) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 16.78846ms)
May 13 20:40:38.502: INFO: (16) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 5.190249ms)
May 13 20:40:38.505: INFO: (16) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 7.209606ms)
May 13 20:40:38.508: INFO: (16) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 10.190178ms)
May 13 20:40:38.511: INFO: (16) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 12.709957ms)
May 13 20:40:38.511: INFO: (16) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 13.095333ms)
May 13 20:40:38.511: INFO: (16) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 13.05613ms)
May 13 20:40:38.511: INFO: (16) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 12.564894ms)
May 13 20:40:38.512: INFO: (16) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 14.125178ms)
May 13 20:40:38.512: INFO: (16) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 14.263026ms)
May 13 20:40:38.515: INFO: (16) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 17.997139ms)
May 13 20:40:38.516: INFO: (16) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 17.359503ms)
May 13 20:40:38.516: INFO: (16) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 17.994426ms)
May 13 20:40:38.516: INFO: (16) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 18.240995ms)
May 13 20:40:38.517: INFO: (16) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 18.63683ms)
May 13 20:40:38.517: INFO: (16) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 19.102588ms)
May 13 20:40:38.518: INFO: (16) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 20.01301ms)
May 13 20:40:38.531: INFO: (17) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 12.228454ms)
May 13 20:40:38.532: INFO: (17) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 13.547433ms)
May 13 20:40:38.532: INFO: (17) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 12.820436ms)
May 13 20:40:38.532: INFO: (17) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 12.99553ms)
May 13 20:40:38.532: INFO: (17) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 13.144545ms)
May 13 20:40:38.533: INFO: (17) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 13.949227ms)
May 13 20:40:38.533: INFO: (17) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 13.883093ms)
May 13 20:40:38.533: INFO: (17) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 14.634201ms)
May 13 20:40:38.533: INFO: (17) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 15.318721ms)
May 13 20:40:38.535: INFO: (17) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 16.270855ms)
May 13 20:40:38.535: INFO: (17) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 16.04963ms)
May 13 20:40:38.535: INFO: (17) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 16.080687ms)
May 13 20:40:38.535: INFO: (17) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 16.357844ms)
May 13 20:40:38.535: INFO: (17) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 16.967438ms)
May 13 20:40:38.536: INFO: (17) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 17.416198ms)
May 13 20:40:38.536: INFO: (17) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 17.356725ms)
May 13 20:40:38.543: INFO: (18) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 6.64448ms)
May 13 20:40:38.544: INFO: (18) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 7.778134ms)
May 13 20:40:38.544: INFO: (18) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 7.595324ms)
May 13 20:40:38.545: INFO: (18) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 8.059224ms)
May 13 20:40:38.545: INFO: (18) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 8.397548ms)
May 13 20:40:38.546: INFO: (18) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 10.312864ms)
May 13 20:40:38.547: INFO: (18) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 10.176206ms)
May 13 20:40:38.547: INFO: (18) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 10.783852ms)
May 13 20:40:38.549: INFO: (18) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 12.605943ms)
May 13 20:40:38.550: INFO: (18) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 14.413583ms)
May 13 20:40:38.550: INFO: (18) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 14.053398ms)
May 13 20:40:38.550: INFO: (18) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 14.31408ms)
May 13 20:40:38.551: INFO: (18) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 13.863981ms)
May 13 20:40:38.551: INFO: (18) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 13.833291ms)
May 13 20:40:38.551: INFO: (18) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 14.78489ms)
May 13 20:40:38.551: INFO: (18) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 14.987673ms)
May 13 20:40:38.567: INFO: (19) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 15.387299ms)
May 13 20:40:38.567: INFO: (19) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">... (200; 14.844433ms)
May 13 20:40:38.568: INFO: (19) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname2/proxy/: bar (200; 16.016561ms)
May 13 20:40:38.568: INFO: (19) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p/proxy/rewriteme">test</a> (200; 16.181029ms)
May 13 20:40:38.568: INFO: (19) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname1/proxy/: tls baz (200; 16.504282ms)
May 13 20:40:38.568: INFO: (19) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:460/proxy/: tls baz (200; 16.208757ms)
May 13 20:40:38.568: INFO: (19) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:1080/proxy/rewriteme">test<... (200; 16.136009ms)
May 13 20:40:38.568: INFO: (19) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:160/proxy/: foo (200; 16.41291ms)
May 13 20:40:38.568: INFO: (19) /api/v1/namespaces/proxy-6908/services/http:proxy-service-f7hrl:portname1/proxy/: foo (200; 16.266383ms)
May 13 20:40:38.568: INFO: (19) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/: <a href="/api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:443/proxy/tlsrewritem... (200; 16.119152ms)
May 13 20:40:38.568: INFO: (19) /api/v1/namespaces/proxy-6908/pods/proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 16.586799ms)
May 13 20:40:38.568: INFO: (19) /api/v1/namespaces/proxy-6908/pods/http:proxy-service-f7hrl-dq97p:162/proxy/: bar (200; 16.771085ms)
May 13 20:40:38.568: INFO: (19) /api/v1/namespaces/proxy-6908/services/https:proxy-service-f7hrl:tlsportname2/proxy/: tls qux (200; 16.711405ms)
May 13 20:40:38.568: INFO: (19) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname1/proxy/: foo (200; 16.573487ms)
May 13 20:40:38.568: INFO: (19) /api/v1/namespaces/proxy-6908/services/proxy-service-f7hrl:portname2/proxy/: bar (200; 16.683915ms)
May 13 20:40:38.569: INFO: (19) /api/v1/namespaces/proxy-6908/pods/https:proxy-service-f7hrl-dq97p:462/proxy/: tls qux (200; 16.98249ms)
STEP: deleting ReplicationController proxy-service-f7hrl in namespace proxy-6908, will wait for the garbage collector to delete the pods
May 13 20:40:38.625: INFO: Deleting ReplicationController proxy-service-f7hrl took: 4.351822ms
May 13 20:40:39.026: INFO: Terminating ReplicationController proxy-service-f7hrl pods took: 400.236529ms
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:40:40.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6908" for this suite.
May 13 20:40:46.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:40:46.398: INFO: namespace proxy-6908 deletion completed in 6.069422145s

• [SLOW TEST:20.451 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:40:46.398: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-650
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 13 20:40:49.543: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:40:50.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-650" for this suite.
May 13 20:41:12.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:41:12.633: INFO: namespace replicaset-650 deletion completed in 22.060508875s

• [SLOW TEST:26.235 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:41:12.633: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8265
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 20:41:12.772: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7242452f-75bf-11e9-9153-920e960bc5b9" in namespace "downward-api-8265" to be "success or failure"
May 13 20:41:12.775: INFO: Pod "downwardapi-volume-7242452f-75bf-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.255534ms
May 13 20:41:14.777: INFO: Pod "downwardapi-volume-7242452f-75bf-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004880702s
STEP: Saw pod success
May 13 20:41:14.777: INFO: Pod "downwardapi-volume-7242452f-75bf-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:41:14.779: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downwardapi-volume-7242452f-75bf-11e9-9153-920e960bc5b9 container client-container: <nil>
STEP: delete the pod
May 13 20:41:14.795: INFO: Waiting for pod downwardapi-volume-7242452f-75bf-11e9-9153-920e960bc5b9 to disappear
May 13 20:41:14.797: INFO: Pod downwardapi-volume-7242452f-75bf-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:41:14.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8265" for this suite.
May 13 20:41:20.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:41:20.880: INFO: namespace downward-api-8265 deletion completed in 6.081544232s

• [SLOW TEST:8.247 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:41:20.881: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3576
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-772e4912-75bf-11e9-9153-920e960bc5b9
STEP: Creating configMap with name cm-test-opt-upd-772e496a-75bf-11e9-9153-920e960bc5b9
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-772e4912-75bf-11e9-9153-920e960bc5b9
STEP: Updating configmap cm-test-opt-upd-772e496a-75bf-11e9-9153-920e960bc5b9
STEP: Creating configMap with name cm-test-opt-create-772e498f-75bf-11e9-9153-920e960bc5b9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:41:29.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3576" for this suite.
May 13 20:41:51.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:41:51.158: INFO: namespace configmap-3576 deletion completed in 22.06077863s

• [SLOW TEST:30.278 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:41:51.158: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5529
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:42:11.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5529" for this suite.
May 13 20:42:17.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:42:17.493: INFO: namespace container-runtime-5529 deletion completed in 6.06254584s

• [SLOW TEST:26.335 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:42:17.493: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1881
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 20:42:17.679: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98f205a9-75bf-11e9-9153-920e960bc5b9" in namespace "projected-1881" to be "success or failure"
May 13 20:42:17.684: INFO: Pod "downwardapi-volume-98f205a9-75bf-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.75626ms
May 13 20:42:19.686: INFO: Pod "downwardapi-volume-98f205a9-75bf-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007625667s
STEP: Saw pod success
May 13 20:42:19.686: INFO: Pod "downwardapi-volume-98f205a9-75bf-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:42:19.688: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downwardapi-volume-98f205a9-75bf-11e9-9153-920e960bc5b9 container client-container: <nil>
STEP: delete the pod
May 13 20:42:19.700: INFO: Waiting for pod downwardapi-volume-98f205a9-75bf-11e9-9153-920e960bc5b9 to disappear
May 13 20:42:19.703: INFO: Pod downwardapi-volume-98f205a9-75bf-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:42:19.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1881" for this suite.
May 13 20:42:25.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:42:25.765: INFO: namespace projected-1881 deletion completed in 6.060044469s

• [SLOW TEST:8.272 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:42:25.765: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3468
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
May 13 20:42:25.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 create -f - --namespace=kubectl-3468'
May 13 20:42:26.034: INFO: stderr: ""
May 13 20:42:26.034: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 13 20:42:26.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3468'
May 13 20:42:26.134: INFO: stderr: ""
May 13 20:42:26.134: INFO: stdout: "update-demo-nautilus-24pdh update-demo-nautilus-tckrc "
May 13 20:42:26.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-24pdh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3468'
May 13 20:42:26.199: INFO: stderr: ""
May 13 20:42:26.199: INFO: stdout: ""
May 13 20:42:26.199: INFO: update-demo-nautilus-24pdh is created but not running
May 13 20:42:31.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3468'
May 13 20:42:31.264: INFO: stderr: ""
May 13 20:42:31.264: INFO: stdout: "update-demo-nautilus-24pdh update-demo-nautilus-tckrc "
May 13 20:42:31.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-24pdh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3468'
May 13 20:42:31.329: INFO: stderr: ""
May 13 20:42:31.329: INFO: stdout: "true"
May 13 20:42:31.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-24pdh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3468'
May 13 20:42:31.395: INFO: stderr: ""
May 13 20:42:31.395: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 20:42:31.395: INFO: validating pod update-demo-nautilus-24pdh
May 13 20:42:31.398: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 20:42:31.398: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 20:42:31.398: INFO: update-demo-nautilus-24pdh is verified up and running
May 13 20:42:31.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-tckrc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3468'
May 13 20:42:31.461: INFO: stderr: ""
May 13 20:42:31.461: INFO: stdout: "true"
May 13 20:42:31.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-tckrc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3468'
May 13 20:42:31.527: INFO: stderr: ""
May 13 20:42:31.527: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 20:42:31.527: INFO: validating pod update-demo-nautilus-tckrc
May 13 20:42:31.530: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 20:42:31.530: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 20:42:31.530: INFO: update-demo-nautilus-tckrc is verified up and running
STEP: scaling down the replication controller
May 13 20:42:31.531: INFO: scanned /root for discovery docs: <nil>
May 13 20:42:31.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-3468'
May 13 20:42:32.617: INFO: stderr: ""
May 13 20:42:32.617: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 13 20:42:32.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3468'
May 13 20:42:32.686: INFO: stderr: ""
May 13 20:42:32.686: INFO: stdout: "update-demo-nautilus-24pdh update-demo-nautilus-tckrc "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 13 20:42:37.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3468'
May 13 20:42:37.800: INFO: stderr: ""
May 13 20:42:37.800: INFO: stdout: "update-demo-nautilus-tckrc "
May 13 20:42:37.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-tckrc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3468'
May 13 20:42:37.903: INFO: stderr: ""
May 13 20:42:37.904: INFO: stdout: "true"
May 13 20:42:37.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-tckrc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3468'
May 13 20:42:37.983: INFO: stderr: ""
May 13 20:42:37.983: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 20:42:37.983: INFO: validating pod update-demo-nautilus-tckrc
May 13 20:42:37.986: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 20:42:37.986: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 20:42:37.986: INFO: update-demo-nautilus-tckrc is verified up and running
STEP: scaling up the replication controller
May 13 20:42:37.988: INFO: scanned /root for discovery docs: <nil>
May 13 20:42:37.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-3468'
May 13 20:42:39.139: INFO: stderr: ""
May 13 20:42:39.139: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 13 20:42:39.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3468'
May 13 20:42:39.207: INFO: stderr: ""
May 13 20:42:39.207: INFO: stdout: "update-demo-nautilus-jcj8n update-demo-nautilus-tckrc "
May 13 20:42:39.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-jcj8n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3468'
May 13 20:42:39.272: INFO: stderr: ""
May 13 20:42:39.272: INFO: stdout: ""
May 13 20:42:39.272: INFO: update-demo-nautilus-jcj8n is created but not running
May 13 20:42:44.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3468'
May 13 20:42:44.355: INFO: stderr: ""
May 13 20:42:44.355: INFO: stdout: "update-demo-nautilus-jcj8n update-demo-nautilus-tckrc "
May 13 20:42:44.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-jcj8n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3468'
May 13 20:42:44.429: INFO: stderr: ""
May 13 20:42:44.429: INFO: stdout: "true"
May 13 20:42:44.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-jcj8n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3468'
May 13 20:42:44.493: INFO: stderr: ""
May 13 20:42:44.493: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 20:42:44.493: INFO: validating pod update-demo-nautilus-jcj8n
May 13 20:42:44.496: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 20:42:44.496: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 20:42:44.496: INFO: update-demo-nautilus-jcj8n is verified up and running
May 13 20:42:44.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-tckrc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3468'
May 13 20:42:44.559: INFO: stderr: ""
May 13 20:42:44.559: INFO: stdout: "true"
May 13 20:42:44.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods update-demo-nautilus-tckrc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3468'
May 13 20:42:44.624: INFO: stderr: ""
May 13 20:42:44.624: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 13 20:42:44.624: INFO: validating pod update-demo-nautilus-tckrc
May 13 20:42:44.626: INFO: got data: {
  "image": "nautilus.jpg"
}

May 13 20:42:44.626: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 13 20:42:44.626: INFO: update-demo-nautilus-tckrc is verified up and running
STEP: using delete to clean up resources
May 13 20:42:44.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 delete --grace-period=0 --force -f - --namespace=kubectl-3468'
May 13 20:42:44.694: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 13 20:42:44.694: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 13 20:42:44.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3468'
May 13 20:42:44.765: INFO: stderr: "No resources found.\n"
May 13 20:42:44.765: INFO: stdout: ""
May 13 20:42:44.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods -l name=update-demo --namespace=kubectl-3468 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 13 20:42:44.833: INFO: stderr: ""
May 13 20:42:44.833: INFO: stdout: "update-demo-nautilus-jcj8n\nupdate-demo-nautilus-tckrc\n"
May 13 20:42:45.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3468'
May 13 20:42:45.405: INFO: stderr: "No resources found.\n"
May 13 20:42:45.405: INFO: stdout: ""
May 13 20:42:45.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742566056 get pods -l name=update-demo --namespace=kubectl-3468 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 13 20:42:45.473: INFO: stderr: ""
May 13 20:42:45.473: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:42:45.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3468" for this suite.
May 13 20:43:07.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:43:07.543: INFO: namespace kubectl-3468 deletion completed in 22.067287677s

• [SLOW TEST:41.778 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:43:07.543: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2157
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 20:43:07.726: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6c6bf26-75bf-11e9-9153-920e960bc5b9" in namespace "downward-api-2157" to be "success or failure"
May 13 20:43:07.728: INFO: Pod "downwardapi-volume-b6c6bf26-75bf-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138956ms
May 13 20:43:09.731: INFO: Pod "downwardapi-volume-b6c6bf26-75bf-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004970878s
STEP: Saw pod success
May 13 20:43:09.731: INFO: Pod "downwardapi-volume-b6c6bf26-75bf-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:43:09.733: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downwardapi-volume-b6c6bf26-75bf-11e9-9153-920e960bc5b9 container client-container: <nil>
STEP: delete the pod
May 13 20:43:09.747: INFO: Waiting for pod downwardapi-volume-b6c6bf26-75bf-11e9-9153-920e960bc5b9 to disappear
May 13 20:43:09.749: INFO: Pod downwardapi-volume-b6c6bf26-75bf-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:43:09.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2157" for this suite.
May 13 20:43:15.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:43:15.811: INFO: namespace downward-api-2157 deletion completed in 6.060043442s

• [SLOW TEST:8.268 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:43:15.811: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6154
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:43:17.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6154" for this suite.
May 13 20:44:07.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:44:08.023: INFO: namespace kubelet-test-6154 deletion completed in 50.067263428s

• [SLOW TEST:52.212 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:44:08.024: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8544
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-dacc84e4-75bf-11e9-9153-920e960bc5b9
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:44:08.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8544" for this suite.
May 13 20:44:14.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:44:14.223: INFO: namespace configmap-8544 deletion completed in 6.064200595s

• [SLOW TEST:6.199 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:44:14.223: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9395
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 13 20:44:14.352: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 13 20:44:19.355: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 13 20:44:19.355: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 13 20:44:21.360: INFO: Creating deployment "test-rollover-deployment"
May 13 20:44:21.391: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 13 20:44:23.399: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 13 20:44:23.403: INFO: Ensure that both replica sets have 1 created replica
May 13 20:44:23.406: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 13 20:44:23.413: INFO: Updating deployment test-rollover-deployment
May 13 20:44:23.413: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 13 20:44:25.418: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 13 20:44:25.422: INFO: Make sure deployment "test-rollover-deployment" is complete
May 13 20:44:25.426: INFO: all replica sets need to contain the pod-template-hash label
May 13 20:44:25.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377061, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377061, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377064, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377061, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 13 20:44:27.431: INFO: all replica sets need to contain the pod-template-hash label
May 13 20:44:27.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377061, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377061, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377064, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377061, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 13 20:44:29.431: INFO: all replica sets need to contain the pod-template-hash label
May 13 20:44:29.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377061, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377061, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377064, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377061, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 13 20:44:31.431: INFO: all replica sets need to contain the pod-template-hash label
May 13 20:44:31.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377061, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377061, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377064, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377061, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 13 20:44:33.430: INFO: all replica sets need to contain the pod-template-hash label
May 13 20:44:33.430: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377061, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377061, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377064, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693377061, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 13 20:44:35.431: INFO: 
May 13 20:44:35.431: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 13 20:44:35.436: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-9395,SelfLink:/apis/apps/v1/namespaces/deployment-9395/deployments/test-rollover-deployment,UID:e2acfe48-75bf-11e9-ac1e-0215dc200466,ResourceVersion:20824,Generation:2,CreationTimestamp:2019-05-13 20:44:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update apps/v1 2019-05-13 20:44:21 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:name:{map[]}]}]},f:spec: {map[f:minReadySeconds:{map[]} f:progressDeadlineSeconds:{map[]} f:replicas:{map[]} f:revisionHistoryLimit:{map[]} f:selector:{map[f:matchLabels:{map[.:{map[]} f:name:{map[]}]}]} f:strategy:{map[f:rollingUpdate:{map[.:{map[]} f:maxSurge:{map[]} f:maxUnavailable:{map[]}]} f:type:{map[]}]} f:template:{map[f:metadata:{map[f:labels:{map[.:{map[]} f:name:{map[]}]}]} f:spec:{map[f:dnsPolicy:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:terminationGracePeriodSeconds:{map[]}]}]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 20:44:21 +0000 UTC &Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[]}]},f:status: {map[f:conditions:{map[.:{map[]} k:{"type":"Available"}:{map[.:{map[]} f:lastTransitionTime:{map[]} f:lastUpdateTime:{map[]} f:message:{map[]} f:reason:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Progressing"}:{map[.:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]}]}]},},}} {e2e.test Update apps/v1 2019-05-13 20:44:23 +0000 UTC &Fields{Map:map[string]Fields{f:spec: {map[f:template:{map[f:spec:{map[f:containers:{map[k:{"name":"redis"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]}]}]}]}]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 20:44:23 +0000 UTC &Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[f:deployment.kubernetes.io/revision:{map[]}]}]},f:status: {map[f:observedGeneration:{map[]} f:updatedReplicas:{map[]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 20:44:34 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:availableReplicas:{map[]} f:conditions:{map[k:{"type":"Progressing"}:{map[f:lastUpdateTime:{map[]} f:message:{map[]} f:reason:{map[]}]}]} f:readyReplicas:{map[]} f:replicas:{map[]}]},},}}],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-13 20:44:21 +0000 UTC 2019-05-13 20:44:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-13 20:44:34 +0000 UTC 2019-05-13 20:44:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 13 20:44:35.438: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-9395,SelfLink:/apis/apps/v1/namespaces/deployment-9395/replicasets/test-rollover-deployment-766b4d6c9d,UID:e3e5197e-75bf-11e9-ac1e-0215dc200466,ResourceVersion:20813,Generation:2,CreationTimestamp:2019-05-13 20:44:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e2acfe48-75bf-11e9-ac1e-0215dc200466 0xc00293ab27 0xc00293ab28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update apps/v1 2019-05-13 20:44:23 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:deployment.kubernetes.io/desired-replicas:{map[]} f:deployment.kubernetes.io/max-replicas:{map[]} f:deployment.kubernetes.io/revision:{map[]}]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"e2acfe48-75bf-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:minReadySeconds:{map[]} f:replicas:{map[]} f:selector:{map[f:matchLabels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]}]} f:template:{map[f:metadata:{map[f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]}]} f:spec:{map[f:containers:{map[k:{"name":"redis"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]}]}]} f:dnsPolicy:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:terminationGracePeriodSeconds:{map[]}]}]}]},f:status: {map[f:fullyLabeledReplicas:{map[]} f:observedGeneration:{map[]} f:replicas:{map[]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 20:44:24 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:readyReplicas:{map[]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 20:44:34 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:availableReplicas:{map[]}]},},}}],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 13 20:44:35.438: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 13 20:44:35.438: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-9395,SelfLink:/apis/apps/v1/namespaces/deployment-9395/replicasets/test-rollover-controller,UID:de7d92d5-75bf-11e9-ac1e-0215dc200466,ResourceVersion:20823,Generation:2,CreationTimestamp:2019-05-13 20:44:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e2acfe48-75bf-11e9-ac1e-0215dc200466 0xc00293a637 0xc00293a638}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{e2e.test Update apps/v1 2019-05-13 20:44:14 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:labels:{map[.:{map[]} f:name:{map[]} f:pod:{map[]}]}]},f:spec: {map[f:selector:{map[f:matchLabels:{map[.:{map[]} f:name:{map[]} f:pod:{map[]}]}]} f:template:{map[f:metadata:{map[f:labels:{map[.:{map[]} f:name:{map[]} f:pod:{map[]}]}]} f:spec:{map[f:containers:{map[k:{"name":"nginx"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]}]}]} f:dnsPolicy:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:terminationGracePeriodSeconds:{map[]}]}]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 20:44:21 +0000 UTC &Fields{Map:map[string]Fields{f:metadata: {map[f:ownerReferences:{map[.:{map[]} k:{"uid":"e2acfe48-75bf-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 20:44:34 +0000 UTC &Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:deployment.kubernetes.io/desired-replicas:{map[]} f:deployment.kubernetes.io/max-replicas:{map[]}]}]},f:spec: {map[f:replicas:{map[]}]},f:status: {map[f:observedGeneration:{map[]} f:replicas:{map[]}]},},}}],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 13 20:44:35.439: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-9395,SelfLink:/apis/apps/v1/namespaces/deployment-9395/replicasets/test-rollover-deployment-6455657675,UID:e2b2c1bb-75bf-11e9-ac1e-0215dc200466,ResourceVersion:20780,Generation:2,CreationTimestamp:2019-05-13 20:44:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e2acfe48-75bf-11e9-ac1e-0215dc200466 0xc00293a8c7 0xc00293a8c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update apps/v1 2019-05-13 20:44:21 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:deployment.kubernetes.io/desired-replicas:{map[]} f:deployment.kubernetes.io/max-replicas:{map[]} f:deployment.kubernetes.io/revision:{map[]}]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"e2acfe48-75bf-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:minReadySeconds:{map[]} f:selector:{map[f:matchLabels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]}]} f:template:{map[f:metadata:{map[f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]}]} f:spec:{map[f:containers:{map[k:{"name":"redis-slave"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]}]}]} f:dnsPolicy:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:terminationGracePeriodSeconds:{map[]}]}]}]},},}} {kube-controller-manager Update apps/v1 2019-05-13 20:44:23 +0000 UTC &Fields{Map:map[string]Fields{f:spec: {map[f:replicas:{map[]}]},f:status: {map[f:observedGeneration:{map[]} f:replicas:{map[]}]},},}}],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 13 20:44:35.440: INFO: Pod "test-rollover-deployment-766b4d6c9d-wgbxd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-wgbxd,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-9395,SelfLink:/api/v1/namespaces/deployment-9395/pods/test-rollover-deployment-766b4d6c9d-wgbxd,UID:e3ea89d3-75bf-11e9-ac1e-0215dc200466,ResourceVersion:20797,Generation:0,CreationTimestamp:2019-05-13 20:44:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d e3e5197e-75bf-11e9-ac1e-0215dc200466 0xc00293b7c7 0xc00293b7c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[{kube-controller-manager Update v1 2019-05-13 20:44:23 +0000 UTC Fields{Map:map[string]Fields{f:metadata: {map[f:annotations:{map[.:{map[]} f:kubernetes.io/psp:{map[]}]} f:generateName:{map[]} f:labels:{map[.:{map[]} f:name:{map[]} f:pod-template-hash:{map[]}]} f:ownerReferences:{map[.:{map[]} k:{"uid":"e3e5197e-75bf-11e9-ac1e-0215dc200466"}:{map[.:{map[]} f:apiVersion:{map[]} f:blockOwnerDeletion:{map[]} f:controller:{map[]} f:kind:{map[]} f:name:{map[]} f:uid:{map[]}]}]}]},f:spec: {map[f:containers:{map[k:{"name":"redis"}:{map[.:{map[]} f:image:{map[]} f:imagePullPolicy:{map[]} f:name:{map[]} f:resources:{map[]} f:terminationMessagePath:{map[]} f:terminationMessagePolicy:{map[]} f:volumeMounts:{map[.:{map[]} k:{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}:{map[.:{map[]} f:mountPath:{map[]} f:name:{map[]} f:readOnly:{map[]}]}]}]}]} f:dnsPolicy:{map[]} f:enableServiceLinks:{map[]} f:restartPolicy:{map[]} f:schedulerName:{map[]} f:securityContext:{map[]} f:serviceAccount:{map[]} f:serviceAccountName:{map[]} f:terminationGracePeriodSeconds:{map[]} f:volumes:{map[.:{map[]} k:{"name":"default-token-h9xgb"}:{map[.:{map[]} f:name:{map[]} f:secret:{map[.:{map[]} f:secretName:{map[]}]}]}]}]},},}} {kubelet Update v1 2019-05-13 20:44:23 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]} k:{"type":"Initialized"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:lastTransitionTime:{map[]} f:status:{map[]} f:type:{map[]}]} k:{"type":"Ready"}:{map[.:{map[]} f:lastProbeTime:{map[]} f:type:{map[]}]}]} f:hostIP:{map[]} f:startTime:{map[]}]},},}} {kubelet Update v1 2019-05-13 20:44:24 +0000 UTC &Fields{Map:map[string]Fields{f:status: {map[f:conditions:{map[k:{"type":"ContainersReady"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]} k:{"type":"Ready"}:{map[f:lastTransitionTime:{map[]} f:status:{map[]}]}]} f:containerStatuses:{map[]} f:phase:{map[]} f:podIP:{map[]}]},},}}],},Spec:PodSpec{Volumes:[{default-token-h9xgb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h9xgb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-h9xgb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-0-248.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 20:44:23 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 20:44:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 20:44:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-13 20:44:23 +0000 UTC  }],Message:,Reason:,HostIP:10.0.0.248,PodIP:10.244.84.106,StartTime:2019-05-13 20:44:23 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-13 20:44:24 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://383359bab27d810d65689b4ecd4ddffc49f1ba4d0b1c8a1b89e07a43ddb39fdc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:44:35.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9395" for this suite.
May 13 20:44:41.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:44:41.504: INFO: namespace deployment-9395 deletion completed in 6.061490979s

• [SLOW TEST:27.281 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:44:41.504: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6562
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-6562/secret-test-eec0741b-75bf-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume secrets
May 13 20:44:41.637: INFO: Waiting up to 5m0s for pod "pod-configmaps-eec0cf9d-75bf-11e9-9153-920e960bc5b9" in namespace "secrets-6562" to be "success or failure"
May 13 20:44:41.642: INFO: Pod "pod-configmaps-eec0cf9d-75bf-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.538132ms
May 13 20:44:43.645: INFO: Pod "pod-configmaps-eec0cf9d-75bf-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007463104s
STEP: Saw pod success
May 13 20:44:43.645: INFO: Pod "pod-configmaps-eec0cf9d-75bf-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:44:43.647: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-configmaps-eec0cf9d-75bf-11e9-9153-920e960bc5b9 container env-test: <nil>
STEP: delete the pod
May 13 20:44:43.659: INFO: Waiting for pod pod-configmaps-eec0cf9d-75bf-11e9-9153-920e960bc5b9 to disappear
May 13 20:44:43.661: INFO: Pod pod-configmaps-eec0cf9d-75bf-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:44:43.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6562" for this suite.
May 13 20:44:49.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:44:49.725: INFO: namespace secrets-6562 deletion completed in 6.061772865s

• [SLOW TEST:8.221 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:44:49.725: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5615
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 20:44:49.904: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3ae27a6-75bf-11e9-9153-920e960bc5b9" in namespace "projected-5615" to be "success or failure"
May 13 20:44:49.908: INFO: Pod "downwardapi-volume-f3ae27a6-75bf-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.520543ms
May 13 20:44:51.910: INFO: Pod "downwardapi-volume-f3ae27a6-75bf-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006207014s
STEP: Saw pod success
May 13 20:44:51.910: INFO: Pod "downwardapi-volume-f3ae27a6-75bf-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:44:51.912: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downwardapi-volume-f3ae27a6-75bf-11e9-9153-920e960bc5b9 container client-container: <nil>
STEP: delete the pod
May 13 20:44:51.924: INFO: Waiting for pod downwardapi-volume-f3ae27a6-75bf-11e9-9153-920e960bc5b9 to disappear
May 13 20:44:51.927: INFO: Pod downwardapi-volume-f3ae27a6-75bf-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:44:51.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5615" for this suite.
May 13 20:44:57.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:44:58.013: INFO: namespace projected-5615 deletion completed in 6.082999139s

• [SLOW TEST:8.288 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:44:58.013: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8710
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-f89a0421-75bf-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume secrets
May 13 20:44:58.167: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f89abdab-75bf-11e9-9153-920e960bc5b9" in namespace "projected-8710" to be "success or failure"
May 13 20:44:58.169: INFO: Pod "pod-projected-secrets-f89abdab-75bf-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006964ms
May 13 20:45:00.172: INFO: Pod "pod-projected-secrets-f89abdab-75bf-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004826658s
STEP: Saw pod success
May 13 20:45:00.172: INFO: Pod "pod-projected-secrets-f89abdab-75bf-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:45:00.174: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-projected-secrets-f89abdab-75bf-11e9-9153-920e960bc5b9 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 13 20:45:00.188: INFO: Waiting for pod pod-projected-secrets-f89abdab-75bf-11e9-9153-920e960bc5b9 to disappear
May 13 20:45:00.190: INFO: Pod pod-projected-secrets-f89abdab-75bf-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:45:00.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8710" for this suite.
May 13 20:45:06.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:45:06.254: INFO: namespace projected-8710 deletion completed in 6.062103959s

• [SLOW TEST:8.241 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:45:06.255: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-278
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 20:45:06.435: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd88759d-75bf-11e9-9153-920e960bc5b9" in namespace "projected-278" to be "success or failure"
May 13 20:45:06.436: INFO: Pod "downwardapi-volume-fd88759d-75bf-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.698488ms
May 13 20:45:08.439: INFO: Pod "downwardapi-volume-fd88759d-75bf-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004396052s
STEP: Saw pod success
May 13 20:45:08.439: INFO: Pod "downwardapi-volume-fd88759d-75bf-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:45:08.441: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod downwardapi-volume-fd88759d-75bf-11e9-9153-920e960bc5b9 container client-container: <nil>
STEP: delete the pod
May 13 20:45:08.455: INFO: Waiting for pod downwardapi-volume-fd88759d-75bf-11e9-9153-920e960bc5b9 to disappear
May 13 20:45:08.457: INFO: Pod downwardapi-volume-fd88759d-75bf-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:45:08.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-278" for this suite.
May 13 20:45:14.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:45:14.521: INFO: namespace projected-278 deletion completed in 6.062214438s

• [SLOW TEST:8.267 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:45:14.521: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4303
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-026e154a-75c0-11e9-9153-920e960bc5b9
STEP: Creating a pod to test consume secrets
May 13 20:45:14.653: INFO: Waiting up to 5m0s for pod "pod-secrets-026e8157-75c0-11e9-9153-920e960bc5b9" in namespace "secrets-4303" to be "success or failure"
May 13 20:45:14.656: INFO: Pod "pod-secrets-026e8157-75c0-11e9-9153-920e960bc5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.067927ms
May 13 20:45:16.658: INFO: Pod "pod-secrets-026e8157-75c0-11e9-9153-920e960bc5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005697279s
STEP: Saw pod success
May 13 20:45:16.658: INFO: Pod "pod-secrets-026e8157-75c0-11e9-9153-920e960bc5b9" satisfied condition "success or failure"
May 13 20:45:16.660: INFO: Trying to get logs from node ip-10-0-0-248.ec2.internal pod pod-secrets-026e8157-75c0-11e9-9153-920e960bc5b9 container secret-volume-test: <nil>
STEP: delete the pod
May 13 20:45:16.674: INFO: Waiting for pod pod-secrets-026e8157-75c0-11e9-9153-920e960bc5b9 to disappear
May 13 20:45:16.676: INFO: Pod pod-secrets-026e8157-75c0-11e9-9153-920e960bc5b9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:45:16.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4303" for this suite.
May 13 20:45:22.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:45:22.740: INFO: namespace secrets-4303 deletion completed in 6.061456554s

• [SLOW TEST:8.219 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 20:45:22.741: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6990
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-6990
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 13 20:45:22.870: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 13 20:45:36.909: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.84.112 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6990 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 13 20:45:36.909: INFO: >>> kubeConfig: /tmp/kubeconfig-742566056
May 13 20:45:38.006: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 20:45:38.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6990" for this suite.
May 13 20:46:00.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 20:46:00.151: INFO: namespace pod-network-test-6990 deletion completed in 22.138039302s

• [SLOW TEST:37.411 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSMay 13 20:46:00.152: INFO: Running AfterSuite actions on all nodes
May 13 20:46:00.152: INFO: Running AfterSuite actions on node 1
May 13 20:46:00.152: INFO: Skipping dumping logs from cluster

Ran 204 of 3584 Specs in 5444.665 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3380 Skipped PASS

Ginkgo ran 1 suite in 1h30m45.686909414s
Test Suite Passed
