I0923 09:40:04.380676      19 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-533696960
I0923 09:40:04.380814      19 e2e.go:240] Starting e2e run "1e553bc0-dde6-11e9-93ab-0610dba1f5f1" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1569231602 - Will randomize all specs
Will run 204 of 3584 specs

Sep 23 09:40:04.526: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 09:40:04.529: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep 23 09:40:04.546: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep 23 09:40:04.576: INFO: 27 / 27 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep 23 09:40:04.576: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Sep 23 09:40:04.576: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep 23 09:40:04.583: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Sep 23 09:40:04.583: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep 23 09:40:04.583: INFO: e2e test version: v1.14.0
Sep 23 09:40:04.584: INFO: kube-apiserver version: v1.14.0
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:40:04.584: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename emptydir
Sep 23 09:40:04.618: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 23 09:40:04.625: INFO: Waiting up to 5m0s for pod "pod-1f24328c-dde6-11e9-93ab-0610dba1f5f1" in namespace "emptydir-649" to be "success or failure"
Sep 23 09:40:04.628: INFO: Pod "pod-1f24328c-dde6-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.837804ms
Sep 23 09:40:06.632: INFO: Pod "pod-1f24328c-dde6-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007084727s
STEP: Saw pod success
Sep 23 09:40:06.632: INFO: Pod "pod-1f24328c-dde6-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 09:40:06.635: INFO: Trying to get logs from node kube-node3 pod pod-1f24328c-dde6-11e9-93ab-0610dba1f5f1 container test-container: <nil>
STEP: delete the pod
Sep 23 09:40:06.758: INFO: Waiting for pod pod-1f24328c-dde6-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 09:40:06.761: INFO: Pod pod-1f24328c-dde6-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:40:06.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-649" for this suite.
Sep 23 09:40:12.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:40:12.890: INFO: namespace emptydir-649 deletion completed in 6.124893532s

• [SLOW TEST:8.306 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:40:12.891: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 23 09:40:12.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1397'
Sep 23 09:40:13.219: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 23 09:40:13.219: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Sep 23 09:40:15.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 delete deployment e2e-test-nginx-deployment --namespace=kubectl-1397'
Sep 23 09:40:15.307: INFO: stderr: ""
Sep 23 09:40:15.307: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:40:15.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1397" for this suite.
Sep 23 09:40:37.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:40:37.400: INFO: namespace kubectl-1397 deletion completed in 22.088729627s

• [SLOW TEST:24.509 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:40:37.400: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 23 09:40:37.457: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:37.458: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:37.458: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:37.460: INFO: Number of nodes with available pods: 0
Sep 23 09:40:37.460: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 09:40:38.465: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:38.465: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:38.465: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:38.469: INFO: Number of nodes with available pods: 0
Sep 23 09:40:38.469: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 09:40:39.465: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:39.465: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:39.465: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:39.468: INFO: Number of nodes with available pods: 2
Sep 23 09:40:39.468: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 09:40:40.465: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:40.465: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:40.465: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:40.468: INFO: Number of nodes with available pods: 3
Sep 23 09:40:40.468: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep 23 09:40:40.483: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:40.483: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:40.483: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:40.488: INFO: Number of nodes with available pods: 2
Sep 23 09:40:40.488: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 09:40:41.493: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:41.493: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:41.493: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:41.497: INFO: Number of nodes with available pods: 2
Sep 23 09:40:41.497: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 09:40:42.493: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:42.493: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:42.493: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:40:42.497: INFO: Number of nodes with available pods: 3
Sep 23 09:40:42.497: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6575, will wait for the garbage collector to delete the pods
Sep 23 09:40:42.561: INFO: Deleting DaemonSet.extensions daemon-set took: 6.875781ms
Sep 23 09:40:42.861: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.333255ms
Sep 23 09:40:52.265: INFO: Number of nodes with available pods: 0
Sep 23 09:40:52.265: INFO: Number of running nodes: 0, number of available pods: 0
Sep 23 09:40:52.268: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6575/daemonsets","resourceVersion":"346396"},"items":null}

Sep 23 09:40:52.271: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6575/pods","resourceVersion":"346396"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:40:52.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6575" for this suite.
Sep 23 09:40:58.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:40:58.381: INFO: namespace daemonsets-6575 deletion completed in 6.094861816s

• [SLOW TEST:20.981 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:40:58.382: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:41:18.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9175" for this suite.
Sep 23 09:41:24.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:41:24.589: INFO: namespace namespaces-9175 deletion completed in 6.083939219s
STEP: Destroying namespace "nsdeletetest-7803" for this suite.
Sep 23 09:41:24.592: INFO: Namespace nsdeletetest-7803 was already deleted
STEP: Destroying namespace "nsdeletetest-8945" for this suite.
Sep 23 09:41:30.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:41:30.676: INFO: namespace nsdeletetest-8945 deletion completed in 6.084581464s

• [SLOW TEST:32.294 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:41:30.676: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Sep 23 09:41:32.726: INFO: Pod pod-hostip-52745f2b-dde6-11e9-93ab-0610dba1f5f1 has hostIP: 172.16.151.106
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:41:32.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2178" for this suite.
Sep 23 09:41:54.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:41:54.812: INFO: namespace pods-2178 deletion completed in 22.082896737s

• [SLOW TEST:24.136 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:41:54.813: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Sep 23 09:41:54.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 --namespace=kubectl-7480 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Sep 23 09:41:56.480: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Sep 23 09:41:56.480: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:41:58.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7480" for this suite.
Sep 23 09:42:04.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:42:04.581: INFO: namespace kubectl-7480 deletion completed in 6.091211015s

• [SLOW TEST:9.768 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:42:04.581: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-7228
Sep 23 09:42:06.627: INFO: Started pod liveness-http in namespace container-probe-7228
STEP: checking the pod's current state and verifying that restartCount is present
Sep 23 09:42:06.630: INFO: Initial restart count of pod liveness-http is 0
Sep 23 09:42:26.668: INFO: Restart count of pod container-probe-7228/liveness-http is now 1 (20.037931576s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:42:26.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7228" for this suite.
Sep 23 09:42:32.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:42:32.774: INFO: namespace container-probe-7228 deletion completed in 6.091713484s

• [SLOW TEST:28.193 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:42:32.775: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Sep 23 09:42:32.812: INFO: Waiting up to 5m0s for pod "var-expansion-7777df56-dde6-11e9-93ab-0610dba1f5f1" in namespace "var-expansion-9555" to be "success or failure"
Sep 23 09:42:32.815: INFO: Pod "var-expansion-7777df56-dde6-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.564017ms
Sep 23 09:42:34.819: INFO: Pod "var-expansion-7777df56-dde6-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006265298s
STEP: Saw pod success
Sep 23 09:42:34.819: INFO: Pod "var-expansion-7777df56-dde6-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 09:42:34.822: INFO: Trying to get logs from node kube-node3 pod var-expansion-7777df56-dde6-11e9-93ab-0610dba1f5f1 container dapi-container: <nil>
STEP: delete the pod
Sep 23 09:42:34.842: INFO: Waiting for pod var-expansion-7777df56-dde6-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 09:42:34.845: INFO: Pod var-expansion-7777df56-dde6-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:42:34.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9555" for this suite.
Sep 23 09:42:40.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:42:40.943: INFO: namespace var-expansion-9555 deletion completed in 6.093620663s

• [SLOW TEST:8.168 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:42:40.943: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:42:40.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7712" for this suite.
Sep 23 09:42:47.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:42:47.095: INFO: namespace kubelet-test-7712 deletion completed in 6.091785113s

• [SLOW TEST:6.152 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:42:47.095: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-rc769 in namespace proxy-8712
I0923 09:42:47.152577      19 runners.go:184] Created replication controller with name: proxy-service-rc769, namespace: proxy-8712, replica count: 1
I0923 09:42:48.203117      19 runners.go:184] proxy-service-rc769 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 09:42:49.203331      19 runners.go:184] proxy-service-rc769 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0923 09:42:50.203693      19 runners.go:184] proxy-service-rc769 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0923 09:42:51.204062      19 runners.go:184] proxy-service-rc769 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0923 09:42:52.204345      19 runners.go:184] proxy-service-rc769 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0923 09:42:53.204535      19 runners.go:184] proxy-service-rc769 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0923 09:42:54.204755      19 runners.go:184] proxy-service-rc769 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0923 09:42:55.204996      19 runners.go:184] proxy-service-rc769 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 23 09:42:55.208: INFO: setup took 8.075273739s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep 23 09:42:55.214: INFO: (0) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 5.634693ms)
Sep 23 09:42:55.214: INFO: (0) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 5.619861ms)
Sep 23 09:42:55.214: INFO: (0) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 5.598657ms)
Sep 23 09:42:55.214: INFO: (0) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 5.714245ms)
Sep 23 09:42:55.214: INFO: (0) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 5.818158ms)
Sep 23 09:42:55.214: INFO: (0) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 6.301221ms)
Sep 23 09:42:55.215: INFO: (0) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 7.061142ms)
Sep 23 09:42:55.215: INFO: (0) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 7.211026ms)
Sep 23 09:42:55.216: INFO: (0) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 7.640495ms)
Sep 23 09:42:55.218: INFO: (0) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 10.14804ms)
Sep 23 09:42:55.219: INFO: (0) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 11.28137ms)
Sep 23 09:42:55.219: INFO: (0) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 11.607148ms)
Sep 23 09:42:55.221: INFO: (0) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 13.457096ms)
Sep 23 09:42:55.221: INFO: (0) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 13.461798ms)
Sep 23 09:42:55.225: INFO: (0) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 16.814778ms)
Sep 23 09:42:55.225: INFO: (0) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 16.83672ms)
Sep 23 09:42:55.229: INFO: (1) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 3.845578ms)
Sep 23 09:42:55.229: INFO: (1) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 4.371777ms)
Sep 23 09:42:55.229: INFO: (1) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 4.406001ms)
Sep 23 09:42:55.229: INFO: (1) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 4.509332ms)
Sep 23 09:42:55.230: INFO: (1) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 4.646959ms)
Sep 23 09:42:55.230: INFO: (1) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 4.77343ms)
Sep 23 09:42:55.230: INFO: (1) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 4.822964ms)
Sep 23 09:42:55.230: INFO: (1) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 4.697792ms)
Sep 23 09:42:55.230: INFO: (1) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 4.693693ms)
Sep 23 09:42:55.230: INFO: (1) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 4.768899ms)
Sep 23 09:42:55.236: INFO: (1) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 11.208459ms)
Sep 23 09:42:55.237: INFO: (1) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 12.077776ms)
Sep 23 09:42:55.237: INFO: (1) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 12.120094ms)
Sep 23 09:42:55.237: INFO: (1) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 12.058618ms)
Sep 23 09:42:55.237: INFO: (1) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 12.154273ms)
Sep 23 09:42:55.237: INFO: (1) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 12.218835ms)
Sep 23 09:42:55.242: INFO: (2) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 4.996772ms)
Sep 23 09:42:55.242: INFO: (2) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 5.000908ms)
Sep 23 09:42:55.243: INFO: (2) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 5.343957ms)
Sep 23 09:42:55.243: INFO: (2) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 5.354842ms)
Sep 23 09:42:55.243: INFO: (2) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 5.390925ms)
Sep 23 09:42:55.243: INFO: (2) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 5.383695ms)
Sep 23 09:42:55.243: INFO: (2) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 5.430312ms)
Sep 23 09:42:55.243: INFO: (2) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 5.446609ms)
Sep 23 09:42:55.243: INFO: (2) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 5.4961ms)
Sep 23 09:42:55.243: INFO: (2) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 5.657477ms)
Sep 23 09:42:55.243: INFO: (2) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 5.727803ms)
Sep 23 09:42:55.243: INFO: (2) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 5.821447ms)
Sep 23 09:42:55.248: INFO: (2) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 11.085513ms)
Sep 23 09:42:55.248: INFO: (2) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 11.094125ms)
Sep 23 09:42:55.248: INFO: (2) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 11.087948ms)
Sep 23 09:42:55.249: INFO: (2) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 11.531378ms)
Sep 23 09:42:55.253: INFO: (3) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 3.847235ms)
Sep 23 09:42:55.253: INFO: (3) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 4.159382ms)
Sep 23 09:42:55.254: INFO: (3) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 4.794301ms)
Sep 23 09:42:55.254: INFO: (3) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 4.840523ms)
Sep 23 09:42:55.254: INFO: (3) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 4.899271ms)
Sep 23 09:42:55.254: INFO: (3) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 5.261802ms)
Sep 23 09:42:55.254: INFO: (3) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 5.193696ms)
Sep 23 09:42:55.254: INFO: (3) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 5.262345ms)
Sep 23 09:42:55.254: INFO: (3) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 5.26122ms)
Sep 23 09:42:55.254: INFO: (3) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 5.523156ms)
Sep 23 09:42:55.255: INFO: (3) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 6.307101ms)
Sep 23 09:42:55.256: INFO: (3) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 7.482856ms)
Sep 23 09:42:55.257: INFO: (3) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 7.601036ms)
Sep 23 09:42:55.257: INFO: (3) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 7.580996ms)
Sep 23 09:42:55.257: INFO: (3) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 7.890691ms)
Sep 23 09:42:55.259: INFO: (3) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 10.115946ms)
Sep 23 09:42:55.267: INFO: (4) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 7.939073ms)
Sep 23 09:42:55.268: INFO: (4) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 9.128286ms)
Sep 23 09:42:55.268: INFO: (4) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 9.231665ms)
Sep 23 09:42:55.268: INFO: (4) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 9.218732ms)
Sep 23 09:42:55.269: INFO: (4) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 9.226694ms)
Sep 23 09:42:55.269: INFO: (4) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 9.757118ms)
Sep 23 09:42:55.269: INFO: (4) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 9.673603ms)
Sep 23 09:42:55.269: INFO: (4) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 9.695983ms)
Sep 23 09:42:55.269: INFO: (4) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 9.769438ms)
Sep 23 09:42:55.269: INFO: (4) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 9.871045ms)
Sep 23 09:42:55.269: INFO: (4) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 10.089176ms)
Sep 23 09:42:55.270: INFO: (4) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 10.421527ms)
Sep 23 09:42:55.271: INFO: (4) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 11.939071ms)
Sep 23 09:42:55.271: INFO: (4) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 11.992232ms)
Sep 23 09:42:55.271: INFO: (4) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 12.098783ms)
Sep 23 09:42:55.271: INFO: (4) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 12.017637ms)
Sep 23 09:42:55.275: INFO: (5) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 3.506733ms)
Sep 23 09:42:55.275: INFO: (5) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 3.641631ms)
Sep 23 09:42:55.275: INFO: (5) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 3.645375ms)
Sep 23 09:42:55.276: INFO: (5) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 4.070716ms)
Sep 23 09:42:55.276: INFO: (5) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 4.270008ms)
Sep 23 09:42:55.277: INFO: (5) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 5.024696ms)
Sep 23 09:42:55.277: INFO: (5) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 5.065589ms)
Sep 23 09:42:55.277: INFO: (5) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 5.427049ms)
Sep 23 09:42:55.277: INFO: (5) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 5.296348ms)
Sep 23 09:42:55.277: INFO: (5) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 5.418604ms)
Sep 23 09:42:55.280: INFO: (5) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 7.956037ms)
Sep 23 09:42:55.280: INFO: (5) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 8.184597ms)
Sep 23 09:42:55.280: INFO: (5) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 8.025262ms)
Sep 23 09:42:55.280: INFO: (5) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 8.022258ms)
Sep 23 09:42:55.280: INFO: (5) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 8.135812ms)
Sep 23 09:42:55.280: INFO: (5) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 8.216317ms)
Sep 23 09:42:55.291: INFO: (6) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 11.240209ms)
Sep 23 09:42:55.291: INFO: (6) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 11.294188ms)
Sep 23 09:42:55.291: INFO: (6) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 11.253181ms)
Sep 23 09:42:55.291: INFO: (6) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 11.280189ms)
Sep 23 09:42:55.291: INFO: (6) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 11.379129ms)
Sep 23 09:42:55.291: INFO: (6) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 11.361516ms)
Sep 23 09:42:55.291: INFO: (6) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 11.337088ms)
Sep 23 09:42:55.291: INFO: (6) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 11.406388ms)
Sep 23 09:42:55.291: INFO: (6) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 11.377075ms)
Sep 23 09:42:55.291: INFO: (6) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 11.556328ms)
Sep 23 09:42:55.292: INFO: (6) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 12.163488ms)
Sep 23 09:42:55.292: INFO: (6) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 12.218749ms)
Sep 23 09:42:55.292: INFO: (6) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 12.20926ms)
Sep 23 09:42:55.292: INFO: (6) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 12.270076ms)
Sep 23 09:42:55.292: INFO: (6) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 12.232577ms)
Sep 23 09:42:55.292: INFO: (6) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 12.502036ms)
Sep 23 09:42:55.296: INFO: (7) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 3.589838ms)
Sep 23 09:42:55.298: INFO: (7) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 5.704583ms)
Sep 23 09:42:55.298: INFO: (7) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 5.781749ms)
Sep 23 09:42:55.298: INFO: (7) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 5.814533ms)
Sep 23 09:42:55.298: INFO: (7) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 5.862317ms)
Sep 23 09:42:55.298: INFO: (7) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 5.789825ms)
Sep 23 09:42:55.298: INFO: (7) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 5.809688ms)
Sep 23 09:42:55.298: INFO: (7) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 5.872248ms)
Sep 23 09:42:55.298: INFO: (7) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 5.76814ms)
Sep 23 09:42:55.298: INFO: (7) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 5.836021ms)
Sep 23 09:42:55.299: INFO: (7) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 6.260072ms)
Sep 23 09:42:55.300: INFO: (7) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 7.397681ms)
Sep 23 09:42:55.300: INFO: (7) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 7.530755ms)
Sep 23 09:42:55.300: INFO: (7) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 7.600759ms)
Sep 23 09:42:55.300: INFO: (7) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 7.68275ms)
Sep 23 09:42:55.300: INFO: (7) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 7.58831ms)
Sep 23 09:42:55.309: INFO: (8) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 8.839711ms)
Sep 23 09:42:55.311: INFO: (8) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 10.441109ms)
Sep 23 09:42:55.311: INFO: (8) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 10.812133ms)
Sep 23 09:42:55.311: INFO: (8) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 11.152516ms)
Sep 23 09:42:55.314: INFO: (8) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 14.083466ms)
Sep 23 09:42:55.314: INFO: (8) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 14.200411ms)
Sep 23 09:42:55.314: INFO: (8) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 14.207056ms)
Sep 23 09:42:55.314: INFO: (8) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 14.309521ms)
Sep 23 09:42:55.314: INFO: (8) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 14.224962ms)
Sep 23 09:42:55.314: INFO: (8) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 14.297631ms)
Sep 23 09:42:55.314: INFO: (8) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 14.244696ms)
Sep 23 09:42:55.314: INFO: (8) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 14.26643ms)
Sep 23 09:42:55.314: INFO: (8) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 14.189819ms)
Sep 23 09:42:55.314: INFO: (8) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 14.253013ms)
Sep 23 09:42:55.315: INFO: (8) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 15.348342ms)
Sep 23 09:42:55.316: INFO: (8) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 15.897995ms)
Sep 23 09:42:55.329: INFO: (9) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 12.780083ms)
Sep 23 09:42:55.329: INFO: (9) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 13.279313ms)
Sep 23 09:42:55.329: INFO: (9) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 13.329107ms)
Sep 23 09:42:55.329: INFO: (9) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 13.336652ms)
Sep 23 09:42:55.329: INFO: (9) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 13.265637ms)
Sep 23 09:42:55.329: INFO: (9) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 13.309798ms)
Sep 23 09:42:55.329: INFO: (9) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 13.3154ms)
Sep 23 09:42:55.329: INFO: (9) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 13.329169ms)
Sep 23 09:42:55.329: INFO: (9) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 13.318558ms)
Sep 23 09:42:55.332: INFO: (9) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 15.501103ms)
Sep 23 09:42:55.332: INFO: (9) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 16.21603ms)
Sep 23 09:42:55.333: INFO: (9) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 17.062192ms)
Sep 23 09:42:55.334: INFO: (9) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 17.500699ms)
Sep 23 09:42:55.334: INFO: (9) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 17.477824ms)
Sep 23 09:42:55.334: INFO: (9) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 17.645252ms)
Sep 23 09:42:55.336: INFO: (9) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 19.573573ms)
Sep 23 09:42:55.340: INFO: (10) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 3.984267ms)
Sep 23 09:42:55.340: INFO: (10) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 4.475199ms)
Sep 23 09:42:55.340: INFO: (10) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 4.330938ms)
Sep 23 09:42:55.340: INFO: (10) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 4.398819ms)
Sep 23 09:42:55.340: INFO: (10) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 4.47647ms)
Sep 23 09:42:55.341: INFO: (10) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 4.56426ms)
Sep 23 09:42:55.341: INFO: (10) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 4.669329ms)
Sep 23 09:42:55.341: INFO: (10) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 4.808773ms)
Sep 23 09:42:55.341: INFO: (10) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 5.051606ms)
Sep 23 09:42:55.341: INFO: (10) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 4.981392ms)
Sep 23 09:42:55.341: INFO: (10) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 4.970696ms)
Sep 23 09:42:55.342: INFO: (10) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 6.251385ms)
Sep 23 09:42:55.345: INFO: (10) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 8.460153ms)
Sep 23 09:42:55.345: INFO: (10) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 8.432167ms)
Sep 23 09:42:55.345: INFO: (10) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 8.752518ms)
Sep 23 09:42:55.345: INFO: (10) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 9.009593ms)
Sep 23 09:42:55.348: INFO: (11) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 2.928989ms)
Sep 23 09:42:55.349: INFO: (11) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 3.836933ms)
Sep 23 09:42:55.349: INFO: (11) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 4.101816ms)
Sep 23 09:42:55.349: INFO: (11) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 4.114829ms)
Sep 23 09:42:55.350: INFO: (11) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 4.179264ms)
Sep 23 09:42:55.350: INFO: (11) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 4.364071ms)
Sep 23 09:42:55.350: INFO: (11) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 4.263412ms)
Sep 23 09:42:55.350: INFO: (11) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 4.703489ms)
Sep 23 09:42:55.350: INFO: (11) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 4.666275ms)
Sep 23 09:42:55.350: INFO: (11) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 4.697052ms)
Sep 23 09:42:55.351: INFO: (11) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 5.255359ms)
Sep 23 09:42:55.351: INFO: (11) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 5.311966ms)
Sep 23 09:42:55.352: INFO: (11) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 6.415409ms)
Sep 23 09:42:55.352: INFO: (11) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 6.396453ms)
Sep 23 09:42:55.352: INFO: (11) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 6.487447ms)
Sep 23 09:42:55.353: INFO: (11) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 7.119757ms)
Sep 23 09:42:55.355: INFO: (12) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 2.687292ms)
Sep 23 09:42:55.357: INFO: (12) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 4.201705ms)
Sep 23 09:42:55.358: INFO: (12) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 5.145286ms)
Sep 23 09:42:55.358: INFO: (12) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 4.749357ms)
Sep 23 09:42:55.358: INFO: (12) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 4.457447ms)
Sep 23 09:42:55.358: INFO: (12) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 4.462213ms)
Sep 23 09:42:55.358: INFO: (12) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 5.210203ms)
Sep 23 09:42:55.358: INFO: (12) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 5.078545ms)
Sep 23 09:42:55.358: INFO: (12) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 5.208553ms)
Sep 23 09:42:55.359: INFO: (12) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 4.993298ms)
Sep 23 09:42:55.359: INFO: (12) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 5.200925ms)
Sep 23 09:42:55.359: INFO: (12) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 5.580671ms)
Sep 23 09:42:55.359: INFO: (12) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 6.622258ms)
Sep 23 09:42:55.359: INFO: (12) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 5.639648ms)
Sep 23 09:42:55.359: INFO: (12) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 5.917217ms)
Sep 23 09:42:55.360: INFO: (12) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 7.112143ms)
Sep 23 09:42:55.364: INFO: (13) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 3.858989ms)
Sep 23 09:42:55.366: INFO: (13) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 5.943923ms)
Sep 23 09:42:55.366: INFO: (13) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 5.910418ms)
Sep 23 09:42:55.366: INFO: (13) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 5.918567ms)
Sep 23 09:42:55.366: INFO: (13) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 5.966114ms)
Sep 23 09:42:55.366: INFO: (13) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 6.032057ms)
Sep 23 09:42:55.366: INFO: (13) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 6.079855ms)
Sep 23 09:42:55.366: INFO: (13) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 5.942784ms)
Sep 23 09:42:55.366: INFO: (13) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 5.967175ms)
Sep 23 09:42:55.366: INFO: (13) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 5.94142ms)
Sep 23 09:42:55.367: INFO: (13) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 6.306811ms)
Sep 23 09:42:55.367: INFO: (13) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 6.477689ms)
Sep 23 09:42:55.367: INFO: (13) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 7.338383ms)
Sep 23 09:42:55.368: INFO: (13) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 7.416765ms)
Sep 23 09:42:55.368: INFO: (13) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 7.357005ms)
Sep 23 09:42:55.368: INFO: (13) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 7.436933ms)
Sep 23 09:42:55.372: INFO: (14) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 4.598928ms)
Sep 23 09:42:55.372: INFO: (14) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 4.609538ms)
Sep 23 09:42:55.373: INFO: (14) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 4.980828ms)
Sep 23 09:42:55.373: INFO: (14) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 5.030871ms)
Sep 23 09:42:55.373: INFO: (14) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 4.968754ms)
Sep 23 09:42:55.373: INFO: (14) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 5.007288ms)
Sep 23 09:42:55.373: INFO: (14) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 5.026235ms)
Sep 23 09:42:55.373: INFO: (14) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 5.045061ms)
Sep 23 09:42:55.373: INFO: (14) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 4.996797ms)
Sep 23 09:42:55.373: INFO: (14) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 4.974633ms)
Sep 23 09:42:55.373: INFO: (14) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 5.00954ms)
Sep 23 09:42:55.374: INFO: (14) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 5.843146ms)
Sep 23 09:42:55.374: INFO: (14) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 6.631997ms)
Sep 23 09:42:55.374: INFO: (14) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 6.748066ms)
Sep 23 09:42:55.375: INFO: (14) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 6.78545ms)
Sep 23 09:42:55.375: INFO: (14) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 6.88125ms)
Sep 23 09:42:55.379: INFO: (15) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 4.5099ms)
Sep 23 09:42:55.379: INFO: (15) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 4.54706ms)
Sep 23 09:42:55.379: INFO: (15) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 4.66159ms)
Sep 23 09:42:55.379: INFO: (15) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 4.51604ms)
Sep 23 09:42:55.379: INFO: (15) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 4.597975ms)
Sep 23 09:42:55.380: INFO: (15) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 5.640085ms)
Sep 23 09:42:55.380: INFO: (15) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 5.578077ms)
Sep 23 09:42:55.380: INFO: (15) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 5.524134ms)
Sep 23 09:42:55.381: INFO: (15) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 5.920291ms)
Sep 23 09:42:55.381: INFO: (15) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 6.038515ms)
Sep 23 09:42:55.381: INFO: (15) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 5.917651ms)
Sep 23 09:42:55.382: INFO: (15) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 7.209841ms)
Sep 23 09:42:55.383: INFO: (15) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 8.128727ms)
Sep 23 09:42:55.383: INFO: (15) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 8.188276ms)
Sep 23 09:42:55.383: INFO: (15) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 8.224528ms)
Sep 23 09:42:55.383: INFO: (15) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 8.273569ms)
Sep 23 09:42:55.386: INFO: (16) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 3.084572ms)
Sep 23 09:42:55.388: INFO: (16) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 4.957618ms)
Sep 23 09:42:55.388: INFO: (16) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 5.074983ms)
Sep 23 09:42:55.388: INFO: (16) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 5.247128ms)
Sep 23 09:42:55.388: INFO: (16) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 5.286502ms)
Sep 23 09:42:55.388: INFO: (16) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 5.355712ms)
Sep 23 09:42:55.389: INFO: (16) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 6.168791ms)
Sep 23 09:42:55.389: INFO: (16) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 6.242627ms)
Sep 23 09:42:55.389: INFO: (16) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 6.180219ms)
Sep 23 09:42:55.390: INFO: (16) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 6.575247ms)
Sep 23 09:42:55.390: INFO: (16) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 6.614085ms)
Sep 23 09:42:55.390: INFO: (16) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 6.818409ms)
Sep 23 09:42:55.390: INFO: (16) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 6.910337ms)
Sep 23 09:42:55.390: INFO: (16) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 6.785365ms)
Sep 23 09:42:55.390: INFO: (16) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 6.817358ms)
Sep 23 09:42:55.390: INFO: (16) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 7.415159ms)
Sep 23 09:42:55.394: INFO: (17) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 3.278126ms)
Sep 23 09:42:55.395: INFO: (17) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 4.395225ms)
Sep 23 09:42:55.396: INFO: (17) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 5.074971ms)
Sep 23 09:42:55.396: INFO: (17) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 5.080524ms)
Sep 23 09:42:55.396: INFO: (17) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 5.14764ms)
Sep 23 09:42:55.396: INFO: (17) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 5.197117ms)
Sep 23 09:42:55.396: INFO: (17) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 5.320917ms)
Sep 23 09:42:55.396: INFO: (17) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 5.317332ms)
Sep 23 09:42:55.396: INFO: (17) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 5.373836ms)
Sep 23 09:42:55.396: INFO: (17) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 5.416613ms)
Sep 23 09:42:55.397: INFO: (17) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 6.153514ms)
Sep 23 09:42:55.398: INFO: (17) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 7.237593ms)
Sep 23 09:42:55.398: INFO: (17) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 7.386336ms)
Sep 23 09:42:55.398: INFO: (17) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 7.392214ms)
Sep 23 09:42:55.398: INFO: (17) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 7.327351ms)
Sep 23 09:42:55.398: INFO: (17) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 7.797258ms)
Sep 23 09:42:55.408: INFO: (18) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 9.898591ms)
Sep 23 09:42:55.409: INFO: (18) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 10.418918ms)
Sep 23 09:42:55.409: INFO: (18) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 10.346704ms)
Sep 23 09:42:55.409: INFO: (18) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 10.404779ms)
Sep 23 09:42:55.409: INFO: (18) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 10.43443ms)
Sep 23 09:42:55.409: INFO: (18) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 10.446651ms)
Sep 23 09:42:55.409: INFO: (18) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 10.604633ms)
Sep 23 09:42:55.409: INFO: (18) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 10.535297ms)
Sep 23 09:42:55.409: INFO: (18) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 10.448453ms)
Sep 23 09:42:55.409: INFO: (18) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 10.497491ms)
Sep 23 09:42:55.409: INFO: (18) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 10.535284ms)
Sep 23 09:42:55.413: INFO: (18) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 14.424214ms)
Sep 23 09:42:55.413: INFO: (18) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 14.45297ms)
Sep 23 09:42:55.413: INFO: (18) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 14.646205ms)
Sep 23 09:42:55.413: INFO: (18) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 14.581524ms)
Sep 23 09:42:55.413: INFO: (18) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 14.590322ms)
Sep 23 09:42:55.417: INFO: (19) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:162/proxy/: bar (200; 3.356485ms)
Sep 23 09:42:55.419: INFO: (19) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:1080/proxy/rewriteme">... (200; 5.612881ms)
Sep 23 09:42:55.419: INFO: (19) /api/v1/namespaces/proxy-8712/pods/http:proxy-service-rc769-f4gjl:160/proxy/: foo (200; 5.628624ms)
Sep 23 09:42:55.419: INFO: (19) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:1080/proxy/rewriteme">test<... (200; 5.637966ms)
Sep 23 09:42:55.419: INFO: (19) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:160/proxy/: foo (200; 5.775059ms)
Sep 23 09:42:55.419: INFO: (19) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:462/proxy/: tls qux (200; 5.872493ms)
Sep 23 09:42:55.419: INFO: (19) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl/proxy/rewriteme">test</a> (200; 5.723879ms)
Sep 23 09:42:55.419: INFO: (19) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:460/proxy/: tls baz (200; 5.917056ms)
Sep 23 09:42:55.419: INFO: (19) /api/v1/namespaces/proxy-8712/pods/proxy-service-rc769-f4gjl:162/proxy/: bar (200; 5.688977ms)
Sep 23 09:42:55.419: INFO: (19) /api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/: <a href="/api/v1/namespaces/proxy-8712/pods/https:proxy-service-rc769-f4gjl:443/proxy/tlsrewritem... (200; 5.718911ms)
Sep 23 09:42:55.420: INFO: (19) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname1/proxy/: tls baz (200; 6.953356ms)
Sep 23 09:42:55.420: INFO: (19) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname1/proxy/: foo (200; 6.762023ms)
Sep 23 09:42:55.421: INFO: (19) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname2/proxy/: bar (200; 8.101722ms)
Sep 23 09:42:55.421: INFO: (19) /api/v1/namespaces/proxy-8712/services/https:proxy-service-rc769:tlsportname2/proxy/: tls qux (200; 8.12934ms)
Sep 23 09:42:55.421: INFO: (19) /api/v1/namespaces/proxy-8712/services/http:proxy-service-rc769:portname2/proxy/: bar (200; 8.093666ms)
Sep 23 09:42:55.421: INFO: (19) /api/v1/namespaces/proxy-8712/services/proxy-service-rc769:portname1/proxy/: foo (200; 8.056504ms)
STEP: deleting ReplicationController proxy-service-rc769 in namespace proxy-8712, will wait for the garbage collector to delete the pods
Sep 23 09:42:55.482: INFO: Deleting ReplicationController proxy-service-rc769 took: 7.95514ms
Sep 23 09:42:55.782: INFO: Terminating ReplicationController proxy-service-rc769 pods took: 300.212122ms
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:42:57.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8712" for this suite.
Sep 23 09:43:03.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:43:04.078: INFO: namespace proxy-8712 deletion completed in 6.090411203s

• [SLOW TEST:16.983 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:43:04.079: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-8a22d775-dde6-11e9-93ab-0610dba1f5f1
STEP: Creating secret with name s-test-opt-upd-8a22d7fc-dde6-11e9-93ab-0610dba1f5f1
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8a22d775-dde6-11e9-93ab-0610dba1f5f1
STEP: Updating secret s-test-opt-upd-8a22d7fc-dde6-11e9-93ab-0610dba1f5f1
STEP: Creating secret with name s-test-opt-create-8a22d81d-dde6-11e9-93ab-0610dba1f5f1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:43:08.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2510" for this suite.
Sep 23 09:43:30.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:43:30.331: INFO: namespace projected-2510 deletion completed in 22.096359753s

• [SLOW TEST:26.252 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:43:30.331: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 23 09:43:30.375: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99c6f605-dde6-11e9-93ab-0610dba1f5f1" in namespace "projected-4446" to be "success or failure"
Sep 23 09:43:30.379: INFO: Pod "downwardapi-volume-99c6f605-dde6-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.32067ms
Sep 23 09:43:32.383: INFO: Pod "downwardapi-volume-99c6f605-dde6-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007426799s
STEP: Saw pod success
Sep 23 09:43:32.383: INFO: Pod "downwardapi-volume-99c6f605-dde6-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 09:43:32.386: INFO: Trying to get logs from node kube-node3 pod downwardapi-volume-99c6f605-dde6-11e9-93ab-0610dba1f5f1 container client-container: <nil>
STEP: delete the pod
Sep 23 09:43:32.403: INFO: Waiting for pod downwardapi-volume-99c6f605-dde6-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 09:43:32.416: INFO: Pod downwardapi-volume-99c6f605-dde6-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:43:32.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4446" for this suite.
Sep 23 09:43:38.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:43:38.521: INFO: namespace projected-4446 deletion completed in 6.100991171s

• [SLOW TEST:8.190 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:43:38.521: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:44:38.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8275" for this suite.
Sep 23 09:45:00.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:45:00.656: INFO: namespace container-probe-8275 deletion completed in 22.088205116s

• [SLOW TEST:82.135 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:45:00.657: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-cf9cc2c3-dde6-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume secrets
Sep 23 09:45:00.697: INFO: Waiting up to 5m0s for pod "pod-secrets-cf9d344c-dde6-11e9-93ab-0610dba1f5f1" in namespace "secrets-2419" to be "success or failure"
Sep 23 09:45:00.700: INFO: Pod "pod-secrets-cf9d344c-dde6-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.842556ms
Sep 23 09:45:02.703: INFO: Pod "pod-secrets-cf9d344c-dde6-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006732453s
Sep 23 09:45:04.707: INFO: Pod "pod-secrets-cf9d344c-dde6-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010603935s
STEP: Saw pod success
Sep 23 09:45:04.707: INFO: Pod "pod-secrets-cf9d344c-dde6-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 09:45:04.710: INFO: Trying to get logs from node kube-node3 pod pod-secrets-cf9d344c-dde6-11e9-93ab-0610dba1f5f1 container secret-volume-test: <nil>
STEP: delete the pod
Sep 23 09:45:04.731: INFO: Waiting for pod pod-secrets-cf9d344c-dde6-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 09:45:04.733: INFO: Pod pod-secrets-cf9d344c-dde6-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:45:04.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2419" for this suite.
Sep 23 09:45:10.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:45:10.831: INFO: namespace secrets-2419 deletion completed in 6.094303975s

• [SLOW TEST:10.174 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:45:10.831: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-9248
Sep 23 09:45:14.881: INFO: Started pod liveness-exec in namespace container-probe-9248
STEP: checking the pod's current state and verifying that restartCount is present
Sep 23 09:45:14.883: INFO: Initial restart count of pod liveness-exec is 0
Sep 23 09:46:00.974: INFO: Restart count of pod container-probe-9248/liveness-exec is now 1 (46.091128207s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:46:00.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9248" for this suite.
Sep 23 09:46:07.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:46:07.084: INFO: namespace container-probe-9248 deletion completed in 6.09512095s

• [SLOW TEST:56.253 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:46:07.084: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0923 09:46:47.147100      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 23 09:46:47.147: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:46:47.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4919" for this suite.
Sep 23 09:46:55.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:46:55.235: INFO: namespace gc-4919 deletion completed in 8.08450983s

• [SLOW TEST:48.151 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:46:55.235: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-13e81976-dde7-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume configMaps
Sep 23 09:46:55.276: INFO: Waiting up to 5m0s for pod "pod-configmaps-13e896ed-dde7-11e9-93ab-0610dba1f5f1" in namespace "configmap-2886" to be "success or failure"
Sep 23 09:46:55.278: INFO: Pod "pod-configmaps-13e896ed-dde7-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.573409ms
Sep 23 09:46:57.282: INFO: Pod "pod-configmaps-13e896ed-dde7-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006172603s
STEP: Saw pod success
Sep 23 09:46:57.282: INFO: Pod "pod-configmaps-13e896ed-dde7-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 09:46:57.285: INFO: Trying to get logs from node kube-node3 pod pod-configmaps-13e896ed-dde7-11e9-93ab-0610dba1f5f1 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 23 09:46:57.307: INFO: Waiting for pod pod-configmaps-13e896ed-dde7-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 09:46:57.310: INFO: Pod pod-configmaps-13e896ed-dde7-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:46:57.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2886" for this suite.
Sep 23 09:47:03.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:47:03.403: INFO: namespace configmap-2886 deletion completed in 6.089267696s

• [SLOW TEST:8.167 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:47:03.403: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-18c7389d-dde7-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume configMaps
Sep 23 09:47:03.450: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-18c7be02-dde7-11e9-93ab-0610dba1f5f1" in namespace "projected-371" to be "success or failure"
Sep 23 09:47:03.453: INFO: Pod "pod-projected-configmaps-18c7be02-dde7-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.643653ms
Sep 23 09:47:05.457: INFO: Pod "pod-projected-configmaps-18c7be02-dde7-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007816866s
STEP: Saw pod success
Sep 23 09:47:05.457: INFO: Pod "pod-projected-configmaps-18c7be02-dde7-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 09:47:05.460: INFO: Trying to get logs from node kube-node3 pod pod-projected-configmaps-18c7be02-dde7-11e9-93ab-0610dba1f5f1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 23 09:47:05.478: INFO: Waiting for pod pod-projected-configmaps-18c7be02-dde7-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 09:47:05.481: INFO: Pod pod-projected-configmaps-18c7be02-dde7-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:47:05.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-371" for this suite.
Sep 23 09:47:11.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:47:11.570: INFO: namespace projected-371 deletion completed in 6.085275094s

• [SLOW TEST:8.167 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:47:11.570: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep 23 09:47:17.631: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-538 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 09:47:17.631: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 09:47:17.793: INFO: Exec stderr: ""
Sep 23 09:47:17.793: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-538 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 09:47:17.793: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 09:47:17.975: INFO: Exec stderr: ""
Sep 23 09:47:17.975: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-538 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 09:47:17.975: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 09:47:18.128: INFO: Exec stderr: ""
Sep 23 09:47:18.128: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-538 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 09:47:18.128: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 09:47:18.295: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep 23 09:47:18.295: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-538 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 09:47:18.295: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 09:47:18.450: INFO: Exec stderr: ""
Sep 23 09:47:18.450: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-538 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 09:47:18.450: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 09:47:18.630: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep 23 09:47:18.630: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-538 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 09:47:18.630: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 09:47:18.770: INFO: Exec stderr: ""
Sep 23 09:47:18.771: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-538 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 09:47:18.771: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 09:47:18.945: INFO: Exec stderr: ""
Sep 23 09:47:18.945: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-538 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 09:47:18.945: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 09:47:19.092: INFO: Exec stderr: ""
Sep 23 09:47:19.092: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-538 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 09:47:19.092: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 09:47:19.272: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:47:19.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-538" for this suite.
Sep 23 09:47:57.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:47:57.371: INFO: namespace e2e-kubelet-etc-hosts-538 deletion completed in 38.094624609s

• [SLOW TEST:45.801 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:47:57.372: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 09:47:57.444: INFO: (0) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 35.444495ms)
Sep 23 09:47:57.447: INFO: (1) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.524791ms)
Sep 23 09:47:57.451: INFO: (2) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.13701ms)
Sep 23 09:47:57.454: INFO: (3) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.944162ms)
Sep 23 09:47:57.457: INFO: (4) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.186308ms)
Sep 23 09:47:57.460: INFO: (5) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.277508ms)
Sep 23 09:47:57.463: INFO: (6) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.046933ms)
Sep 23 09:47:57.467: INFO: (7) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.293066ms)
Sep 23 09:47:57.471: INFO: (8) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.938481ms)
Sep 23 09:47:57.474: INFO: (9) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.401327ms)
Sep 23 09:47:57.477: INFO: (10) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.056229ms)
Sep 23 09:47:57.480: INFO: (11) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.226502ms)
Sep 23 09:47:57.483: INFO: (12) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.025949ms)
Sep 23 09:47:57.487: INFO: (13) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.409962ms)
Sep 23 09:47:57.490: INFO: (14) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.872441ms)
Sep 23 09:47:57.493: INFO: (15) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.046315ms)
Sep 23 09:47:57.496: INFO: (16) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.294226ms)
Sep 23 09:47:57.499: INFO: (17) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.274286ms)
Sep 23 09:47:57.502: INFO: (18) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.909084ms)
Sep 23 09:47:57.505: INFO: (19) /api/v1/nodes/kube-node1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.044143ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:47:57.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3161" for this suite.
Sep 23 09:48:03.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:48:03.608: INFO: namespace proxy-3161 deletion completed in 6.098705384s

• [SLOW TEST:6.236 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:48:03.608: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 23 09:48:03.645: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ca8e0d2-dde7-11e9-93ab-0610dba1f5f1" in namespace "projected-4581" to be "success or failure"
Sep 23 09:48:03.649: INFO: Pod "downwardapi-volume-3ca8e0d2-dde7-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.161741ms
Sep 23 09:48:05.653: INFO: Pod "downwardapi-volume-3ca8e0d2-dde7-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007371917s
STEP: Saw pod success
Sep 23 09:48:05.653: INFO: Pod "downwardapi-volume-3ca8e0d2-dde7-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 09:48:05.656: INFO: Trying to get logs from node kube-node3 pod downwardapi-volume-3ca8e0d2-dde7-11e9-93ab-0610dba1f5f1 container client-container: <nil>
STEP: delete the pod
Sep 23 09:48:05.673: INFO: Waiting for pod downwardapi-volume-3ca8e0d2-dde7-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 09:48:05.675: INFO: Pod downwardapi-volume-3ca8e0d2-dde7-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:48:05.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4581" for this suite.
Sep 23 09:48:11.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:48:11.771: INFO: namespace projected-4581 deletion completed in 6.092911151s

• [SLOW TEST:8.164 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:48:11.772: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Sep 23 09:48:11.811: INFO: Waiting up to 5m0s for pod "var-expansion-4186e2c5-dde7-11e9-93ab-0610dba1f5f1" in namespace "var-expansion-5146" to be "success or failure"
Sep 23 09:48:11.815: INFO: Pod "var-expansion-4186e2c5-dde7-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.914962ms
Sep 23 09:48:13.818: INFO: Pod "var-expansion-4186e2c5-dde7-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007660386s
STEP: Saw pod success
Sep 23 09:48:13.818: INFO: Pod "var-expansion-4186e2c5-dde7-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 09:48:13.821: INFO: Trying to get logs from node kube-node3 pod var-expansion-4186e2c5-dde7-11e9-93ab-0610dba1f5f1 container dapi-container: <nil>
STEP: delete the pod
Sep 23 09:48:13.839: INFO: Waiting for pod var-expansion-4186e2c5-dde7-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 09:48:13.842: INFO: Pod var-expansion-4186e2c5-dde7-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:48:13.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5146" for this suite.
Sep 23 09:48:19.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:48:19.939: INFO: namespace var-expansion-5146 deletion completed in 6.093036373s

• [SLOW TEST:8.167 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:48:19.939: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep 23 09:48:19.972: INFO: PodSpec: initContainers in spec.initContainers
Sep 23 09:49:04.137: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-46652843-dde7-11e9-93ab-0610dba1f5f1", GenerateName:"", Namespace:"init-container-6370", SelfLink:"/api/v1/namespaces/init-container-6370/pods/pod-init-46652843-dde7-11e9-93ab-0610dba1f5f1", UID:"46658f9e-dde7-11e9-a6ed-0cda411df060", ResourceVersion:"348450", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63704828899, loc:(*time.Location)(0x89f10e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"972928613"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.119.152/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-kgvwv", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002cfe9c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-kgvwv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-kgvwv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-kgvwv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002d02238), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kube-node3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002c53320), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002d022c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002d022e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002d022e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002d022ec)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704828899, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704828899, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704828899, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704828899, loc:(*time.Location)(0x89f10e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.151.106", PodIP:"192.168.119.152", StartTime:(*v1.Time)(0xc002f8fa00), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001e296c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001e29730)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker://sha256:758ec7f3a1ee85f8f08399b55641bfb13e8c1109287ddc5e22b68c3d653152ee", ContainerID:"docker://b412b51226b77ccf599ef373c3c2925992bf35edeadefaca949cbd6f2b5f7c5c"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f8fa40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f8fa20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:49:04.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6370" for this suite.
Sep 23 09:49:26.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:49:26.232: INFO: namespace init-container-6370 deletion completed in 22.089751751s

• [SLOW TEST:66.293 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:49:26.233: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7572
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7572
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7572
Sep 23 09:49:26.276: INFO: Found 0 stateful pods, waiting for 1
Sep 23 09:49:36.280: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep 23 09:49:36.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-7572 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 23 09:49:36.541: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 23 09:49:36.541: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 23 09:49:36.541: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 23 09:49:36.545: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 23 09:49:36.545: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 09:49:36.560: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999698s
Sep 23 09:49:37.564: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996257871s
Sep 23 09:49:38.568: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992249313s
Sep 23 09:49:39.572: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988493339s
Sep 23 09:49:40.576: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.98445541s
Sep 23 09:49:41.580: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.980744667s
Sep 23 09:49:42.583: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976593226s
Sep 23 09:49:43.588: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.973188475s
Sep 23 09:49:44.591: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.969083428s
Sep 23 09:49:45.595: INFO: Verifying statefulset ss doesn't scale past 1 for another 965.365872ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7572
Sep 23 09:49:46.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-7572 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 23 09:49:46.819: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 23 09:49:46.819: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 23 09:49:46.819: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 23 09:49:46.823: INFO: Found 1 stateful pods, waiting for 3
Sep 23 09:49:56.827: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 09:49:56.827: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 09:49:56.827: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep 23 09:49:56.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-7572 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 23 09:49:57.064: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 23 09:49:57.064: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 23 09:49:57.064: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 23 09:49:57.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-7572 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 23 09:49:57.315: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 23 09:49:57.315: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 23 09:49:57.315: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 23 09:49:57.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-7572 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 23 09:49:57.556: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 23 09:49:57.556: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 23 09:49:57.556: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 23 09:49:57.556: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 09:49:57.559: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep 23 09:50:07.566: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 23 09:50:07.566: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 23 09:50:07.566: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 23 09:50:07.576: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999715s
Sep 23 09:50:08.580: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996706349s
Sep 23 09:50:09.585: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992309317s
Sep 23 09:50:10.589: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987814077s
Sep 23 09:50:11.593: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98346313s
Sep 23 09:50:12.598: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978842373s
Sep 23 09:50:13.602: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.974471444s
Sep 23 09:50:14.606: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.970767405s
Sep 23 09:50:15.610: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96645368s
Sep 23 09:50:16.614: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.563923ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7572
Sep 23 09:50:17.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-7572 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 23 09:50:17.852: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 23 09:50:17.852: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 23 09:50:17.852: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 23 09:50:17.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-7572 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 23 09:50:18.071: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 23 09:50:18.071: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 23 09:50:18.071: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 23 09:50:18.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-7572 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 23 09:50:18.328: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 23 09:50:18.328: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 23 09:50:18.328: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 23 09:50:18.328: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep 23 09:50:38.341: INFO: Deleting all statefulset in ns statefulset-7572
Sep 23 09:50:38.344: INFO: Scaling statefulset ss to 0
Sep 23 09:50:38.351: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 09:50:38.354: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:50:38.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7572" for this suite.
Sep 23 09:50:44.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:50:44.457: INFO: namespace statefulset-7572 deletion completed in 6.089069739s

• [SLOW TEST:78.224 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:50:44.457: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-9c88d8a6-dde7-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume secrets
Sep 23 09:50:44.501: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9c897e00-dde7-11e9-93ab-0610dba1f5f1" in namespace "projected-4800" to be "success or failure"
Sep 23 09:50:44.507: INFO: Pod "pod-projected-secrets-9c897e00-dde7-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.839795ms
Sep 23 09:50:46.511: INFO: Pod "pod-projected-secrets-9c897e00-dde7-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010207023s
STEP: Saw pod success
Sep 23 09:50:46.511: INFO: Pod "pod-projected-secrets-9c897e00-dde7-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 09:50:46.514: INFO: Trying to get logs from node kube-node3 pod pod-projected-secrets-9c897e00-dde7-11e9-93ab-0610dba1f5f1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 23 09:50:46.538: INFO: Waiting for pod pod-projected-secrets-9c897e00-dde7-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 09:50:46.544: INFO: Pod pod-projected-secrets-9c897e00-dde7-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:50:46.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4800" for this suite.
Sep 23 09:50:52.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:50:52.637: INFO: namespace projected-4800 deletion completed in 6.087691709s

• [SLOW TEST:8.180 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:50:52.637: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-j5mx
STEP: Creating a pod to test atomic-volume-subpath
Sep 23 09:50:52.681: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-j5mx" in namespace "subpath-8454" to be "success or failure"
Sep 23 09:50:52.683: INFO: Pod "pod-subpath-test-downwardapi-j5mx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.170534ms
Sep 23 09:50:54.686: INFO: Pod "pod-subpath-test-downwardapi-j5mx": Phase="Running", Reason="", readiness=true. Elapsed: 2.005221923s
Sep 23 09:50:56.702: INFO: Pod "pod-subpath-test-downwardapi-j5mx": Phase="Running", Reason="", readiness=true. Elapsed: 4.021103861s
Sep 23 09:50:58.705: INFO: Pod "pod-subpath-test-downwardapi-j5mx": Phase="Running", Reason="", readiness=true. Elapsed: 6.024330885s
Sep 23 09:51:00.708: INFO: Pod "pod-subpath-test-downwardapi-j5mx": Phase="Running", Reason="", readiness=true. Elapsed: 8.027322251s
Sep 23 09:51:02.711: INFO: Pod "pod-subpath-test-downwardapi-j5mx": Phase="Running", Reason="", readiness=true. Elapsed: 10.030427405s
Sep 23 09:51:04.715: INFO: Pod "pod-subpath-test-downwardapi-j5mx": Phase="Running", Reason="", readiness=true. Elapsed: 12.034410761s
Sep 23 09:51:06.720: INFO: Pod "pod-subpath-test-downwardapi-j5mx": Phase="Running", Reason="", readiness=true. Elapsed: 14.038760631s
Sep 23 09:51:08.724: INFO: Pod "pod-subpath-test-downwardapi-j5mx": Phase="Running", Reason="", readiness=true. Elapsed: 16.042617537s
Sep 23 09:51:10.728: INFO: Pod "pod-subpath-test-downwardapi-j5mx": Phase="Running", Reason="", readiness=true. Elapsed: 18.04657865s
Sep 23 09:51:12.731: INFO: Pod "pod-subpath-test-downwardapi-j5mx": Phase="Running", Reason="", readiness=true. Elapsed: 20.050209393s
Sep 23 09:51:14.735: INFO: Pod "pod-subpath-test-downwardapi-j5mx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.053942919s
STEP: Saw pod success
Sep 23 09:51:14.735: INFO: Pod "pod-subpath-test-downwardapi-j5mx" satisfied condition "success or failure"
Sep 23 09:51:14.738: INFO: Trying to get logs from node kube-node3 pod pod-subpath-test-downwardapi-j5mx container test-container-subpath-downwardapi-j5mx: <nil>
STEP: delete the pod
Sep 23 09:51:14.758: INFO: Waiting for pod pod-subpath-test-downwardapi-j5mx to disappear
Sep 23 09:51:14.766: INFO: Pod pod-subpath-test-downwardapi-j5mx no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-j5mx
Sep 23 09:51:14.766: INFO: Deleting pod "pod-subpath-test-downwardapi-j5mx" in namespace "subpath-8454"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:51:14.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8454" for this suite.
Sep 23 09:51:20.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:51:20.875: INFO: namespace subpath-8454 deletion completed in 6.098454664s

• [SLOW TEST:28.237 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:51:20.875: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 23 09:51:20.918: INFO: Waiting up to 5m0s for pod "pod-b23e646c-dde7-11e9-93ab-0610dba1f5f1" in namespace "emptydir-9419" to be "success or failure"
Sep 23 09:51:20.921: INFO: Pod "pod-b23e646c-dde7-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.601942ms
Sep 23 09:51:22.925: INFO: Pod "pod-b23e646c-dde7-11e9-93ab-0610dba1f5f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006381066s
Sep 23 09:51:24.929: INFO: Pod "pod-b23e646c-dde7-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010575143s
STEP: Saw pod success
Sep 23 09:51:24.929: INFO: Pod "pod-b23e646c-dde7-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 09:51:24.932: INFO: Trying to get logs from node kube-node3 pod pod-b23e646c-dde7-11e9-93ab-0610dba1f5f1 container test-container: <nil>
STEP: delete the pod
Sep 23 09:51:24.948: INFO: Waiting for pod pod-b23e646c-dde7-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 09:51:24.950: INFO: Pod pod-b23e646c-dde7-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:51:24.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9419" for this suite.
Sep 23 09:51:30.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:51:31.043: INFO: namespace emptydir-9419 deletion completed in 6.088985063s

• [SLOW TEST:10.169 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:51:31.044: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-lgv6
STEP: Creating a pod to test atomic-volume-subpath
Sep 23 09:51:31.087: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lgv6" in namespace "subpath-656" to be "success or failure"
Sep 23 09:51:31.093: INFO: Pod "pod-subpath-test-configmap-lgv6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.563894ms
Sep 23 09:51:33.097: INFO: Pod "pod-subpath-test-configmap-lgv6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009939885s
Sep 23 09:51:35.101: INFO: Pod "pod-subpath-test-configmap-lgv6": Phase="Running", Reason="", readiness=true. Elapsed: 4.013989667s
Sep 23 09:51:37.105: INFO: Pod "pod-subpath-test-configmap-lgv6": Phase="Running", Reason="", readiness=true. Elapsed: 6.018146271s
Sep 23 09:51:39.109: INFO: Pod "pod-subpath-test-configmap-lgv6": Phase="Running", Reason="", readiness=true. Elapsed: 8.022338005s
Sep 23 09:51:41.114: INFO: Pod "pod-subpath-test-configmap-lgv6": Phase="Running", Reason="", readiness=true. Elapsed: 10.026795519s
Sep 23 09:51:43.118: INFO: Pod "pod-subpath-test-configmap-lgv6": Phase="Running", Reason="", readiness=true. Elapsed: 12.030640202s
Sep 23 09:51:45.122: INFO: Pod "pod-subpath-test-configmap-lgv6": Phase="Running", Reason="", readiness=true. Elapsed: 14.035190839s
Sep 23 09:51:47.127: INFO: Pod "pod-subpath-test-configmap-lgv6": Phase="Running", Reason="", readiness=true. Elapsed: 16.039502079s
Sep 23 09:51:49.130: INFO: Pod "pod-subpath-test-configmap-lgv6": Phase="Running", Reason="", readiness=true. Elapsed: 18.043066118s
Sep 23 09:51:51.134: INFO: Pod "pod-subpath-test-configmap-lgv6": Phase="Running", Reason="", readiness=true. Elapsed: 20.046915865s
Sep 23 09:51:53.138: INFO: Pod "pod-subpath-test-configmap-lgv6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.05107755s
STEP: Saw pod success
Sep 23 09:51:53.138: INFO: Pod "pod-subpath-test-configmap-lgv6" satisfied condition "success or failure"
Sep 23 09:51:53.141: INFO: Trying to get logs from node kube-node3 pod pod-subpath-test-configmap-lgv6 container test-container-subpath-configmap-lgv6: <nil>
STEP: delete the pod
Sep 23 09:51:53.160: INFO: Waiting for pod pod-subpath-test-configmap-lgv6 to disappear
Sep 23 09:51:53.162: INFO: Pod pod-subpath-test-configmap-lgv6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lgv6
Sep 23 09:51:53.162: INFO: Deleting pod "pod-subpath-test-configmap-lgv6" in namespace "subpath-656"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:51:53.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-656" for this suite.
Sep 23 09:51:59.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:51:59.263: INFO: namespace subpath-656 deletion completed in 6.094269699s

• [SLOW TEST:28.219 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:51:59.264: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 09:51:59.320: INFO: Create a RollingUpdate DaemonSet
Sep 23 09:51:59.324: INFO: Check that daemon pods launch on every node of the cluster
Sep 23 09:51:59.327: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:51:59.327: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:51:59.327: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:51:59.330: INFO: Number of nodes with available pods: 0
Sep 23 09:51:59.330: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 09:52:00.335: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:00.335: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:00.335: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:00.339: INFO: Number of nodes with available pods: 0
Sep 23 09:52:00.339: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 09:52:01.335: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:01.335: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:01.335: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:01.338: INFO: Number of nodes with available pods: 2
Sep 23 09:52:01.338: INFO: Node kube-node3 is running more than one daemon pod
Sep 23 09:52:02.335: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:02.335: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:02.335: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:02.338: INFO: Number of nodes with available pods: 3
Sep 23 09:52:02.338: INFO: Number of running nodes: 3, number of available pods: 3
Sep 23 09:52:02.338: INFO: Update the DaemonSet to trigger a rollout
Sep 23 09:52:02.344: INFO: Updating DaemonSet daemon-set
Sep 23 09:52:12.356: INFO: Roll back the DaemonSet before rollout is complete
Sep 23 09:52:12.362: INFO: Updating DaemonSet daemon-set
Sep 23 09:52:12.362: INFO: Make sure DaemonSet rollback is complete
Sep 23 09:52:12.365: INFO: Wrong image for pod: daemon-set-bznls. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep 23 09:52:12.365: INFO: Pod daemon-set-bznls is not available
Sep 23 09:52:12.370: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:12.370: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:12.370: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:13.375: INFO: Wrong image for pod: daemon-set-bznls. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep 23 09:52:13.375: INFO: Pod daemon-set-bznls is not available
Sep 23 09:52:13.379: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:13.379: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:13.379: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:14.436: INFO: Wrong image for pod: daemon-set-bznls. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep 23 09:52:14.436: INFO: Pod daemon-set-bznls is not available
Sep 23 09:52:14.650: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:14.650: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:14.650: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:15.374: INFO: Pod daemon-set-dkqq5 is not available
Sep 23 09:52:15.379: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:15.379: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 09:52:15.379: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2319, will wait for the garbage collector to delete the pods
Sep 23 09:52:15.444: INFO: Deleting DaemonSet.extensions daemon-set took: 6.715862ms
Sep 23 09:52:15.744: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.247101ms
Sep 23 09:52:18.447: INFO: Number of nodes with available pods: 0
Sep 23 09:52:18.447: INFO: Number of running nodes: 0, number of available pods: 0
Sep 23 09:52:18.450: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2319/daemonsets","resourceVersion":"349321"},"items":null}

Sep 23 09:52:18.452: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2319/pods","resourceVersion":"349321"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:52:18.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2319" for this suite.
Sep 23 09:52:24.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:52:24.555: INFO: namespace daemonsets-2319 deletion completed in 6.087533867s

• [SLOW TEST:25.291 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:52:24.555: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 23 09:52:24.598: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d8334126-dde7-11e9-93ab-0610dba1f5f1" in namespace "projected-8309" to be "success or failure"
Sep 23 09:52:24.601: INFO: Pod "downwardapi-volume-d8334126-dde7-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.681149ms
Sep 23 09:52:26.605: INFO: Pod "downwardapi-volume-d8334126-dde7-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006559648s
Sep 23 09:52:28.609: INFO: Pod "downwardapi-volume-d8334126-dde7-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010424021s
STEP: Saw pod success
Sep 23 09:52:28.609: INFO: Pod "downwardapi-volume-d8334126-dde7-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 09:52:28.612: INFO: Trying to get logs from node kube-node3 pod downwardapi-volume-d8334126-dde7-11e9-93ab-0610dba1f5f1 container client-container: <nil>
STEP: delete the pod
Sep 23 09:52:28.632: INFO: Waiting for pod downwardapi-volume-d8334126-dde7-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 09:52:28.634: INFO: Pod downwardapi-volume-d8334126-dde7-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:52:28.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8309" for this suite.
Sep 23 09:52:34.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:52:34.730: INFO: namespace projected-8309 deletion completed in 6.091395052s

• [SLOW TEST:10.175 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:52:34.730: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:52:34.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7692" for this suite.
Sep 23 09:52:56.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:52:56.886: INFO: namespace pods-7692 deletion completed in 22.101223607s

• [SLOW TEST:22.156 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:52:56.886: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep 23 09:53:01.453: INFO: Successfully updated pod "annotationupdateeb78069c-dde7-11e9-93ab-0610dba1f5f1"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:53:03.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6048" for this suite.
Sep 23 09:53:25.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:53:25.567: INFO: namespace downward-api-6048 deletion completed in 22.091129956s

• [SLOW TEST:28.681 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:53:25.568: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep 23 09:53:28.140: INFO: Successfully updated pod "labelsupdatefc90ed65-dde7-11e9-93ab-0610dba1f5f1"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:53:30.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8186" for this suite.
Sep 23 09:53:52.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:53:52.250: INFO: namespace downward-api-8186 deletion completed in 22.091748604s

• [SLOW TEST:26.683 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:53:52.250: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Sep 23 09:53:52.300: INFO: Waiting up to 5m0s for pod "client-containers-0c77f66c-dde8-11e9-93ab-0610dba1f5f1" in namespace "containers-9652" to be "success or failure"
Sep 23 09:53:52.303: INFO: Pod "client-containers-0c77f66c-dde8-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.472227ms
Sep 23 09:53:54.307: INFO: Pod "client-containers-0c77f66c-dde8-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007387082s
STEP: Saw pod success
Sep 23 09:53:54.307: INFO: Pod "client-containers-0c77f66c-dde8-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 09:53:54.310: INFO: Trying to get logs from node kube-node3 pod client-containers-0c77f66c-dde8-11e9-93ab-0610dba1f5f1 container test-container: <nil>
STEP: delete the pod
Sep 23 09:53:54.326: INFO: Waiting for pod client-containers-0c77f66c-dde8-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 09:53:54.333: INFO: Pod client-containers-0c77f66c-dde8-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:53:54.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9652" for this suite.
Sep 23 09:54:00.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:54:00.425: INFO: namespace containers-9652 deletion completed in 6.088940284s

• [SLOW TEST:8.175 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:54:00.426: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 09:54:00.499: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"1158bf85-dde8-11e9-a6ed-0cda411df060", Controller:(*bool)(0xc0023b4b02), BlockOwnerDeletion:(*bool)(0xc0023b4b03)}}
Sep 23 09:54:00.505: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"11578270-dde8-11e9-a6ed-0cda411df060", Controller:(*bool)(0xc002c294a2), BlockOwnerDeletion:(*bool)(0xc002c294a3)}}
Sep 23 09:54:00.511: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"115813e9-dde8-11e9-a6ed-0cda411df060", Controller:(*bool)(0xc002c296ba), BlockOwnerDeletion:(*bool)(0xc002c296bb)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:54:05.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2225" for this suite.
Sep 23 09:54:11.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:54:11.622: INFO: namespace gc-2225 deletion completed in 6.098035399s

• [SLOW TEST:11.197 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:54:11.623: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 23 09:54:11.660: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1803868d-dde8-11e9-93ab-0610dba1f5f1" in namespace "projected-6172" to be "success or failure"
Sep 23 09:54:11.663: INFO: Pod "downwardapi-volume-1803868d-dde8-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.421925ms
Sep 23 09:54:13.666: INFO: Pod "downwardapi-volume-1803868d-dde8-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005972402s
Sep 23 09:54:15.670: INFO: Pod "downwardapi-volume-1803868d-dde8-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009560567s
STEP: Saw pod success
Sep 23 09:54:15.670: INFO: Pod "downwardapi-volume-1803868d-dde8-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 09:54:15.672: INFO: Trying to get logs from node kube-node3 pod downwardapi-volume-1803868d-dde8-11e9-93ab-0610dba1f5f1 container client-container: <nil>
STEP: delete the pod
Sep 23 09:54:15.688: INFO: Waiting for pod downwardapi-volume-1803868d-dde8-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 09:54:15.690: INFO: Pod downwardapi-volume-1803868d-dde8-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:54:15.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6172" for this suite.
Sep 23 09:54:21.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:54:21.782: INFO: namespace projected-6172 deletion completed in 6.087975999s

• [SLOW TEST:10.160 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:54:21.782: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Sep 23 09:54:22.352: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep 23 09:54:32.737: INFO: Waited 8.319957829s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:54:33.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7911" for this suite.
Sep 23 09:54:39.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:54:39.643: INFO: namespace aggregator-7911 deletion completed in 6.185056415s

• [SLOW TEST:17.861 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:54:39.644: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 23 09:54:39.682: INFO: Waiting up to 5m0s for pod "downwardapi-volume-28b7439b-dde8-11e9-93ab-0610dba1f5f1" in namespace "projected-9431" to be "success or failure"
Sep 23 09:54:39.686: INFO: Pod "downwardapi-volume-28b7439b-dde8-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.801449ms
Sep 23 09:54:41.689: INFO: Pod "downwardapi-volume-28b7439b-dde8-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007126697s
STEP: Saw pod success
Sep 23 09:54:41.689: INFO: Pod "downwardapi-volume-28b7439b-dde8-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 09:54:41.692: INFO: Trying to get logs from node kube-node3 pod downwardapi-volume-28b7439b-dde8-11e9-93ab-0610dba1f5f1 container client-container: <nil>
STEP: delete the pod
Sep 23 09:54:41.709: INFO: Waiting for pod downwardapi-volume-28b7439b-dde8-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 09:54:41.714: INFO: Pod downwardapi-volume-28b7439b-dde8-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:54:41.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9431" for this suite.
Sep 23 09:54:47.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:54:47.816: INFO: namespace projected-9431 deletion completed in 6.098599087s

• [SLOW TEST:8.172 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:54:47.816: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 23 09:54:47.869: INFO: Waiting up to 5m0s for pod "pod-2d97a297-dde8-11e9-93ab-0610dba1f5f1" in namespace "emptydir-4700" to be "success or failure"
Sep 23 09:54:47.882: INFO: Pod "pod-2d97a297-dde8-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.535432ms
Sep 23 09:54:49.886: INFO: Pod "pod-2d97a297-dde8-11e9-93ab-0610dba1f5f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.016743534s
Sep 23 09:54:51.890: INFO: Pod "pod-2d97a297-dde8-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020427147s
STEP: Saw pod success
Sep 23 09:54:51.890: INFO: Pod "pod-2d97a297-dde8-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 09:54:51.892: INFO: Trying to get logs from node kube-node3 pod pod-2d97a297-dde8-11e9-93ab-0610dba1f5f1 container test-container: <nil>
STEP: delete the pod
Sep 23 09:54:51.911: INFO: Waiting for pod pod-2d97a297-dde8-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 09:54:51.914: INFO: Pod pod-2d97a297-dde8-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:54:51.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4700" for this suite.
Sep 23 09:54:57.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:54:58.009: INFO: namespace emptydir-4700 deletion completed in 6.091519512s

• [SLOW TEST:10.193 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:54:58.009: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Sep 23 09:54:58.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 create -f - --namespace=kubectl-551'
Sep 23 09:54:58.301: INFO: stderr: ""
Sep 23 09:54:58.301: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 23 09:54:58.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-551'
Sep 23 09:54:58.382: INFO: stderr: ""
Sep 23 09:54:58.382: INFO: stdout: "update-demo-nautilus-2w79k update-demo-nautilus-9lgfr "
Sep 23 09:54:58.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-2w79k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-551'
Sep 23 09:54:58.454: INFO: stderr: ""
Sep 23 09:54:58.455: INFO: stdout: ""
Sep 23 09:54:58.455: INFO: update-demo-nautilus-2w79k is created but not running
Sep 23 09:55:03.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-551'
Sep 23 09:55:03.532: INFO: stderr: ""
Sep 23 09:55:03.532: INFO: stdout: "update-demo-nautilus-2w79k update-demo-nautilus-9lgfr "
Sep 23 09:55:03.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-2w79k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-551'
Sep 23 09:55:03.608: INFO: stderr: ""
Sep 23 09:55:03.608: INFO: stdout: "true"
Sep 23 09:55:03.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-2w79k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-551'
Sep 23 09:55:03.681: INFO: stderr: ""
Sep 23 09:55:03.681: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 23 09:55:03.681: INFO: validating pod update-demo-nautilus-2w79k
Sep 23 09:55:03.690: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 23 09:55:03.690: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 23 09:55:03.690: INFO: update-demo-nautilus-2w79k is verified up and running
Sep 23 09:55:03.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-9lgfr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-551'
Sep 23 09:55:03.762: INFO: stderr: ""
Sep 23 09:55:03.762: INFO: stdout: "true"
Sep 23 09:55:03.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-9lgfr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-551'
Sep 23 09:55:03.836: INFO: stderr: ""
Sep 23 09:55:03.836: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 23 09:55:03.836: INFO: validating pod update-demo-nautilus-9lgfr
Sep 23 09:55:03.847: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 23 09:55:03.847: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 23 09:55:03.847: INFO: update-demo-nautilus-9lgfr is verified up and running
STEP: rolling-update to new replication controller
Sep 23 09:55:03.849: INFO: scanned /root for discovery docs: <nil>
Sep 23 09:55:03.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-551'
Sep 23 09:55:26.146: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep 23 09:55:26.146: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 23 09:55:26.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-551'
Sep 23 09:55:26.222: INFO: stderr: ""
Sep 23 09:55:26.222: INFO: stdout: "update-demo-kitten-9t6r7 update-demo-kitten-kkdk9 "
Sep 23 09:55:26.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-kitten-9t6r7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-551'
Sep 23 09:55:26.293: INFO: stderr: ""
Sep 23 09:55:26.293: INFO: stdout: "true"
Sep 23 09:55:26.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-kitten-9t6r7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-551'
Sep 23 09:55:26.362: INFO: stderr: ""
Sep 23 09:55:26.362: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep 23 09:55:26.362: INFO: validating pod update-demo-kitten-9t6r7
Sep 23 09:55:26.367: INFO: got data: {
  "image": "kitten.jpg"
}

Sep 23 09:55:26.367: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep 23 09:55:26.367: INFO: update-demo-kitten-9t6r7 is verified up and running
Sep 23 09:55:26.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-kitten-kkdk9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-551'
Sep 23 09:55:26.438: INFO: stderr: ""
Sep 23 09:55:26.438: INFO: stdout: "true"
Sep 23 09:55:26.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-kitten-kkdk9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-551'
Sep 23 09:55:26.516: INFO: stderr: ""
Sep 23 09:55:26.516: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep 23 09:55:26.516: INFO: validating pod update-demo-kitten-kkdk9
Sep 23 09:55:26.521: INFO: got data: {
  "image": "kitten.jpg"
}

Sep 23 09:55:26.521: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep 23 09:55:26.521: INFO: update-demo-kitten-kkdk9 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:55:26.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-551" for this suite.
Sep 23 09:55:48.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:55:48.617: INFO: namespace kubectl-551 deletion completed in 22.091597507s

• [SLOW TEST:50.608 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:55:48.617: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 09:55:48.648: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:55:50.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2251" for this suite.
Sep 23 09:56:34.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:56:34.768: INFO: namespace pods-2251 deletion completed in 44.088021117s

• [SLOW TEST:46.151 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:56:34.769: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-180
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-180 to expose endpoints map[]
Sep 23 09:56:34.819: INFO: Get endpoints failed (2.996487ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Sep 23 09:56:35.823: INFO: successfully validated that service endpoint-test2 in namespace services-180 exposes endpoints map[] (1.006713849s elapsed)
STEP: Creating pod pod1 in namespace services-180
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-180 to expose endpoints map[pod1:[80]]
Sep 23 09:56:38.856: INFO: successfully validated that service endpoint-test2 in namespace services-180 exposes endpoints map[pod1:[80]] (3.026459776s elapsed)
STEP: Creating pod pod2 in namespace services-180
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-180 to expose endpoints map[pod1:[80] pod2:[80]]
Sep 23 09:56:41.902: INFO: successfully validated that service endpoint-test2 in namespace services-180 exposes endpoints map[pod1:[80] pod2:[80]] (3.041476439s elapsed)
STEP: Deleting pod pod1 in namespace services-180
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-180 to expose endpoints map[pod2:[80]]
Sep 23 09:56:42.924: INFO: successfully validated that service endpoint-test2 in namespace services-180 exposes endpoints map[pod2:[80]] (1.016768304s elapsed)
STEP: Deleting pod pod2 in namespace services-180
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-180 to expose endpoints map[]
Sep 23 09:56:43.940: INFO: successfully validated that service endpoint-test2 in namespace services-180 exposes endpoints map[] (1.009919406s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:56:43.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-180" for this suite.
Sep 23 09:56:49.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:56:50.066: INFO: namespace services-180 deletion completed in 6.093731211s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:15.297 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:56:50.066: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-7674506b-dde8-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume secrets
Sep 23 09:56:50.108: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7674d358-dde8-11e9-93ab-0610dba1f5f1" in namespace "projected-5885" to be "success or failure"
Sep 23 09:56:50.111: INFO: Pod "pod-projected-secrets-7674d358-dde8-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.809791ms
Sep 23 09:56:52.115: INFO: Pod "pod-projected-secrets-7674d358-dde8-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006789643s
Sep 23 09:56:54.119: INFO: Pod "pod-projected-secrets-7674d358-dde8-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010687493s
STEP: Saw pod success
Sep 23 09:56:54.119: INFO: Pod "pod-projected-secrets-7674d358-dde8-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 09:56:54.121: INFO: Trying to get logs from node kube-node3 pod pod-projected-secrets-7674d358-dde8-11e9-93ab-0610dba1f5f1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 23 09:56:54.138: INFO: Waiting for pod pod-projected-secrets-7674d358-dde8-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 09:56:54.141: INFO: Pod pod-projected-secrets-7674d358-dde8-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:56:54.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5885" for this suite.
Sep 23 09:57:00.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:57:00.234: INFO: namespace projected-5885 deletion completed in 6.089296918s

• [SLOW TEST:10.168 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:57:00.235: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 23 09:57:00.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1812'
Sep 23 09:57:00.364: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 23 09:57:00.364: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Sep 23 09:57:00.375: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-tz6zq]
Sep 23 09:57:00.375: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-tz6zq" in namespace "kubectl-1812" to be "running and ready"
Sep 23 09:57:00.379: INFO: Pod "e2e-test-nginx-rc-tz6zq": Phase="Pending", Reason="", readiness=false. Elapsed: 3.255792ms
Sep 23 09:57:02.383: INFO: Pod "e2e-test-nginx-rc-tz6zq": Phase="Running", Reason="", readiness=true. Elapsed: 2.007538311s
Sep 23 09:57:02.383: INFO: Pod "e2e-test-nginx-rc-tz6zq" satisfied condition "running and ready"
Sep 23 09:57:02.383: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-tz6zq]
Sep 23 09:57:02.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 logs rc/e2e-test-nginx-rc --namespace=kubectl-1812'
Sep 23 09:57:02.488: INFO: stderr: ""
Sep 23 09:57:02.488: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Sep 23 09:57:02.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 delete rc e2e-test-nginx-rc --namespace=kubectl-1812'
Sep 23 09:57:02.567: INFO: stderr: ""
Sep 23 09:57:02.567: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:57:02.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1812" for this suite.
Sep 23 09:57:24.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:57:24.673: INFO: namespace kubectl-1812 deletion completed in 22.101352936s

• [SLOW TEST:24.439 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:57:24.674: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 23 09:57:24.712: INFO: Waiting up to 5m0s for pod "pod-8b14cfd3-dde8-11e9-93ab-0610dba1f5f1" in namespace "emptydir-2794" to be "success or failure"
Sep 23 09:57:24.715: INFO: Pod "pod-8b14cfd3-dde8-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.09403ms
Sep 23 09:57:26.719: INFO: Pod "pod-8b14cfd3-dde8-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007149038s
STEP: Saw pod success
Sep 23 09:57:26.719: INFO: Pod "pod-8b14cfd3-dde8-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 09:57:26.722: INFO: Trying to get logs from node kube-node3 pod pod-8b14cfd3-dde8-11e9-93ab-0610dba1f5f1 container test-container: <nil>
STEP: delete the pod
Sep 23 09:57:26.741: INFO: Waiting for pod pod-8b14cfd3-dde8-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 09:57:26.745: INFO: Pod pod-8b14cfd3-dde8-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:57:26.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2794" for this suite.
Sep 23 09:57:32.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:57:32.839: INFO: namespace emptydir-2794 deletion completed in 6.090054961s

• [SLOW TEST:8.165 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:57:32.839: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 23 09:57:32.882: INFO: Waiting up to 5m0s for pod "pod-8ff38d75-dde8-11e9-93ab-0610dba1f5f1" in namespace "emptydir-5913" to be "success or failure"
Sep 23 09:57:32.885: INFO: Pod "pod-8ff38d75-dde8-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.568771ms
Sep 23 09:57:34.889: INFO: Pod "pod-8ff38d75-dde8-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006673613s
STEP: Saw pod success
Sep 23 09:57:34.889: INFO: Pod "pod-8ff38d75-dde8-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 09:57:34.892: INFO: Trying to get logs from node kube-node3 pod pod-8ff38d75-dde8-11e9-93ab-0610dba1f5f1 container test-container: <nil>
STEP: delete the pod
Sep 23 09:57:34.912: INFO: Waiting for pod pod-8ff38d75-dde8-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 09:57:34.915: INFO: Pod pod-8ff38d75-dde8-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:57:34.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5913" for this suite.
Sep 23 09:57:40.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:57:41.018: INFO: namespace emptydir-5913 deletion completed in 6.096619441s

• [SLOW TEST:8.179 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:57:41.019: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Sep 23 09:57:41.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 create -f - --namespace=kubectl-9015'
Sep 23 09:57:41.258: INFO: stderr: ""
Sep 23 09:57:41.258: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 23 09:57:41.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9015'
Sep 23 09:57:41.346: INFO: stderr: ""
Sep 23 09:57:41.346: INFO: stdout: "update-demo-nautilus-p6f69 update-demo-nautilus-tx499 "
Sep 23 09:57:41.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-p6f69 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9015'
Sep 23 09:57:41.416: INFO: stderr: ""
Sep 23 09:57:41.416: INFO: stdout: ""
Sep 23 09:57:41.416: INFO: update-demo-nautilus-p6f69 is created but not running
Sep 23 09:57:46.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9015'
Sep 23 09:57:46.497: INFO: stderr: ""
Sep 23 09:57:46.498: INFO: stdout: "update-demo-nautilus-p6f69 update-demo-nautilus-tx499 "
Sep 23 09:57:46.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-p6f69 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9015'
Sep 23 09:57:46.580: INFO: stderr: ""
Sep 23 09:57:46.580: INFO: stdout: "true"
Sep 23 09:57:46.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-p6f69 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9015'
Sep 23 09:57:46.656: INFO: stderr: ""
Sep 23 09:57:46.656: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 23 09:57:46.656: INFO: validating pod update-demo-nautilus-p6f69
Sep 23 09:57:46.661: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 23 09:57:46.661: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 23 09:57:46.661: INFO: update-demo-nautilus-p6f69 is verified up and running
Sep 23 09:57:46.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-tx499 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9015'
Sep 23 09:57:46.731: INFO: stderr: ""
Sep 23 09:57:46.731: INFO: stdout: "true"
Sep 23 09:57:46.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-tx499 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9015'
Sep 23 09:57:46.800: INFO: stderr: ""
Sep 23 09:57:46.800: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 23 09:57:46.800: INFO: validating pod update-demo-nautilus-tx499
Sep 23 09:57:46.809: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 23 09:57:46.809: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 23 09:57:46.809: INFO: update-demo-nautilus-tx499 is verified up and running
STEP: scaling down the replication controller
Sep 23 09:57:46.811: INFO: scanned /root for discovery docs: <nil>
Sep 23 09:57:46.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9015'
Sep 23 09:57:47.903: INFO: stderr: ""
Sep 23 09:57:47.903: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 23 09:57:47.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9015'
Sep 23 09:57:47.981: INFO: stderr: ""
Sep 23 09:57:47.981: INFO: stdout: "update-demo-nautilus-p6f69 update-demo-nautilus-tx499 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 23 09:57:52.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9015'
Sep 23 09:57:53.057: INFO: stderr: ""
Sep 23 09:57:53.058: INFO: stdout: "update-demo-nautilus-p6f69 "
Sep 23 09:57:53.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-p6f69 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9015'
Sep 23 09:57:53.132: INFO: stderr: ""
Sep 23 09:57:53.132: INFO: stdout: "true"
Sep 23 09:57:53.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-p6f69 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9015'
Sep 23 09:57:53.210: INFO: stderr: ""
Sep 23 09:57:53.210: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 23 09:57:53.210: INFO: validating pod update-demo-nautilus-p6f69
Sep 23 09:57:53.213: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 23 09:57:53.213: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 23 09:57:53.213: INFO: update-demo-nautilus-p6f69 is verified up and running
STEP: scaling up the replication controller
Sep 23 09:57:53.215: INFO: scanned /root for discovery docs: <nil>
Sep 23 09:57:53.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9015'
Sep 23 09:57:54.321: INFO: stderr: ""
Sep 23 09:57:54.321: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 23 09:57:54.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9015'
Sep 23 09:57:54.397: INFO: stderr: ""
Sep 23 09:57:54.397: INFO: stdout: "update-demo-nautilus-7sr5l update-demo-nautilus-p6f69 "
Sep 23 09:57:54.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-7sr5l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9015'
Sep 23 09:57:54.470: INFO: stderr: ""
Sep 23 09:57:54.470: INFO: stdout: ""
Sep 23 09:57:54.470: INFO: update-demo-nautilus-7sr5l is created but not running
Sep 23 09:57:59.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9015'
Sep 23 09:57:59.549: INFO: stderr: ""
Sep 23 09:57:59.549: INFO: stdout: "update-demo-nautilus-7sr5l update-demo-nautilus-p6f69 "
Sep 23 09:57:59.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-7sr5l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9015'
Sep 23 09:57:59.627: INFO: stderr: ""
Sep 23 09:57:59.627: INFO: stdout: "true"
Sep 23 09:57:59.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-7sr5l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9015'
Sep 23 09:57:59.702: INFO: stderr: ""
Sep 23 09:57:59.702: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 23 09:57:59.702: INFO: validating pod update-demo-nautilus-7sr5l
Sep 23 09:57:59.707: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 23 09:57:59.707: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 23 09:57:59.707: INFO: update-demo-nautilus-7sr5l is verified up and running
Sep 23 09:57:59.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-p6f69 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9015'
Sep 23 09:57:59.780: INFO: stderr: ""
Sep 23 09:57:59.781: INFO: stdout: "true"
Sep 23 09:57:59.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-p6f69 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9015'
Sep 23 09:57:59.855: INFO: stderr: ""
Sep 23 09:57:59.855: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 23 09:57:59.855: INFO: validating pod update-demo-nautilus-p6f69
Sep 23 09:57:59.860: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 23 09:57:59.860: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 23 09:57:59.860: INFO: update-demo-nautilus-p6f69 is verified up and running
STEP: using delete to clean up resources
Sep 23 09:57:59.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 delete --grace-period=0 --force -f - --namespace=kubectl-9015'
Sep 23 09:57:59.934: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 09:57:59.934: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 23 09:57:59.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9015'
Sep 23 09:58:00.015: INFO: stderr: "No resources found.\n"
Sep 23 09:58:00.015: INFO: stdout: ""
Sep 23 09:58:00.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods -l name=update-demo --namespace=kubectl-9015 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 23 09:58:00.089: INFO: stderr: ""
Sep 23 09:58:00.089: INFO: stdout: "update-demo-nautilus-7sr5l\nupdate-demo-nautilus-p6f69\n"
Sep 23 09:58:00.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9015'
Sep 23 09:58:00.673: INFO: stderr: "No resources found.\n"
Sep 23 09:58:00.673: INFO: stdout: ""
Sep 23 09:58:00.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods -l name=update-demo --namespace=kubectl-9015 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 23 09:58:00.761: INFO: stderr: ""
Sep 23 09:58:00.762: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:58:00.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9015" for this suite.
Sep 23 09:58:22.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:58:22.863: INFO: namespace kubectl-9015 deletion completed in 22.094997838s

• [SLOW TEST:41.844 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:58:22.863: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 09:58:22.895: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:58:25.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5635" for this suite.
Sep 23 09:59:03.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:59:03.181: INFO: namespace pods-5635 deletion completed in 38.098257037s

• [SLOW TEST:40.318 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:59:03.181: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep 23 09:59:03.219: INFO: Pod name pod-release: Found 0 pods out of 1
Sep 23 09:59:08.223: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:59:09.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6538" for this suite.
Sep 23 09:59:15.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:59:15.334: INFO: namespace replication-controller-6538 deletion completed in 6.093393198s

• [SLOW TEST:12.153 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:59:15.334: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-cd0b3529-dde8-11e9-93ab-0610dba1f5f1
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-cd0b3529-dde8-11e9-93ab-0610dba1f5f1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:59:21.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9701" for this suite.
Sep 23 09:59:43.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 09:59:43.518: INFO: namespace configmap-9701 deletion completed in 22.087196183s

• [SLOW TEST:28.184 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 09:59:43.519: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep 23 09:59:46.086: INFO: Successfully updated pod "annotationupdateddd6c1bc-dde8-11e9-93ab-0610dba1f5f1"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 09:59:50.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6955" for this suite.
Sep 23 10:00:12.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:00:12.207: INFO: namespace projected-6955 deletion completed in 22.091779997s

• [SLOW TEST:28.688 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:00:12.208: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep 23 10:00:12.252: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3239,SelfLink:/api/v1/namespaces/watch-3239/configmaps/e2e-watch-test-configmap-a,UID:eef1e549-dde8-11e9-a6ed-0cda411df060,ResourceVersion:351323,Generation:0,CreationTimestamp:2019-09-23 10:00:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 23 10:00:12.252: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3239,SelfLink:/api/v1/namespaces/watch-3239/configmaps/e2e-watch-test-configmap-a,UID:eef1e549-dde8-11e9-a6ed-0cda411df060,ResourceVersion:351323,Generation:0,CreationTimestamp:2019-09-23 10:00:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep 23 10:00:22.259: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3239,SelfLink:/api/v1/namespaces/watch-3239/configmaps/e2e-watch-test-configmap-a,UID:eef1e549-dde8-11e9-a6ed-0cda411df060,ResourceVersion:351343,Generation:0,CreationTimestamp:2019-09-23 10:00:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep 23 10:00:22.259: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3239,SelfLink:/api/v1/namespaces/watch-3239/configmaps/e2e-watch-test-configmap-a,UID:eef1e549-dde8-11e9-a6ed-0cda411df060,ResourceVersion:351343,Generation:0,CreationTimestamp:2019-09-23 10:00:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep 23 10:00:32.266: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3239,SelfLink:/api/v1/namespaces/watch-3239/configmaps/e2e-watch-test-configmap-a,UID:eef1e549-dde8-11e9-a6ed-0cda411df060,ResourceVersion:351363,Generation:0,CreationTimestamp:2019-09-23 10:00:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 23 10:00:32.267: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3239,SelfLink:/api/v1/namespaces/watch-3239/configmaps/e2e-watch-test-configmap-a,UID:eef1e549-dde8-11e9-a6ed-0cda411df060,ResourceVersion:351363,Generation:0,CreationTimestamp:2019-09-23 10:00:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep 23 10:00:42.274: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3239,SelfLink:/api/v1/namespaces/watch-3239/configmaps/e2e-watch-test-configmap-a,UID:eef1e549-dde8-11e9-a6ed-0cda411df060,ResourceVersion:351383,Generation:0,CreationTimestamp:2019-09-23 10:00:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 23 10:00:42.274: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3239,SelfLink:/api/v1/namespaces/watch-3239/configmaps/e2e-watch-test-configmap-a,UID:eef1e549-dde8-11e9-a6ed-0cda411df060,ResourceVersion:351383,Generation:0,CreationTimestamp:2019-09-23 10:00:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep 23 10:00:52.282: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3239,SelfLink:/api/v1/namespaces/watch-3239/configmaps/e2e-watch-test-configmap-b,UID:06cd91c2-dde9-11e9-a6ed-0cda411df060,ResourceVersion:351405,Generation:0,CreationTimestamp:2019-09-23 10:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 23 10:00:52.282: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3239,SelfLink:/api/v1/namespaces/watch-3239/configmaps/e2e-watch-test-configmap-b,UID:06cd91c2-dde9-11e9-a6ed-0cda411df060,ResourceVersion:351405,Generation:0,CreationTimestamp:2019-09-23 10:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep 23 10:01:02.289: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3239,SelfLink:/api/v1/namespaces/watch-3239/configmaps/e2e-watch-test-configmap-b,UID:06cd91c2-dde9-11e9-a6ed-0cda411df060,ResourceVersion:351427,Generation:0,CreationTimestamp:2019-09-23 10:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 23 10:01:02.289: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3239,SelfLink:/api/v1/namespaces/watch-3239/configmaps/e2e-watch-test-configmap-b,UID:06cd91c2-dde9-11e9-a6ed-0cda411df060,ResourceVersion:351427,Generation:0,CreationTimestamp:2019-09-23 10:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:01:12.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3239" for this suite.
Sep 23 10:01:18.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:01:18.389: INFO: namespace watch-3239 deletion completed in 6.093622972s

• [SLOW TEST:66.182 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:01:18.390: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Sep 23 10:01:18.436: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-6649" to be "success or failure"
Sep 23 10:01:18.438: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.147089ms
Sep 23 10:01:20.442: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006216943s
STEP: Saw pod success
Sep 23 10:01:20.442: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Sep 23 10:01:20.445: INFO: Trying to get logs from node kube-node3 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Sep 23 10:01:20.462: INFO: Waiting for pod pod-host-path-test to disappear
Sep 23 10:01:20.465: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:01:20.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-6649" for this suite.
Sep 23 10:01:26.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:01:26.566: INFO: namespace hostpath-6649 deletion completed in 6.096097226s

• [SLOW TEST:8.176 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:01:26.566: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 23 10:01:32.646: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 23 10:01:32.649: INFO: Pod pod-with-prestop-http-hook still exists
Sep 23 10:01:34.649: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 23 10:01:34.653: INFO: Pod pod-with-prestop-http-hook still exists
Sep 23 10:01:36.649: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 23 10:01:36.653: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:01:36.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4839" for this suite.
Sep 23 10:01:58.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:01:58.753: INFO: namespace container-lifecycle-hook-4839 deletion completed in 22.088382988s

• [SLOW TEST:32.187 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:01:58.753: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-2e725e5c-dde9-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume configMaps
Sep 23 10:01:58.796: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2e72dabb-dde9-11e9-93ab-0610dba1f5f1" in namespace "projected-7394" to be "success or failure"
Sep 23 10:01:58.801: INFO: Pod "pod-projected-configmaps-2e72dabb-dde9-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.834432ms
Sep 23 10:02:00.804: INFO: Pod "pod-projected-configmaps-2e72dabb-dde9-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008087281s
STEP: Saw pod success
Sep 23 10:02:00.804: INFO: Pod "pod-projected-configmaps-2e72dabb-dde9-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:02:00.807: INFO: Trying to get logs from node kube-node3 pod pod-projected-configmaps-2e72dabb-dde9-11e9-93ab-0610dba1f5f1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 23 10:02:00.824: INFO: Waiting for pod pod-projected-configmaps-2e72dabb-dde9-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:02:00.830: INFO: Pod pod-projected-configmaps-2e72dabb-dde9-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:02:00.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7394" for this suite.
Sep 23 10:02:06.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:02:06.929: INFO: namespace projected-7394 deletion completed in 6.094598461s

• [SLOW TEST:8.176 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:02:06.929: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-3351b3c8-dde9-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume configMaps
Sep 23 10:02:06.970: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3352299d-dde9-11e9-93ab-0610dba1f5f1" in namespace "projected-1663" to be "success or failure"
Sep 23 10:02:06.973: INFO: Pod "pod-projected-configmaps-3352299d-dde9-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.493521ms
Sep 23 10:02:08.977: INFO: Pod "pod-projected-configmaps-3352299d-dde9-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006612963s
STEP: Saw pod success
Sep 23 10:02:08.977: INFO: Pod "pod-projected-configmaps-3352299d-dde9-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:02:08.979: INFO: Trying to get logs from node kube-node3 pod pod-projected-configmaps-3352299d-dde9-11e9-93ab-0610dba1f5f1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 23 10:02:08.999: INFO: Waiting for pod pod-projected-configmaps-3352299d-dde9-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:02:09.002: INFO: Pod pod-projected-configmaps-3352299d-dde9-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:02:09.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1663" for this suite.
Sep 23 10:02:15.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:02:15.099: INFO: namespace projected-1663 deletion completed in 6.093087363s

• [SLOW TEST:8.170 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:02:15.099: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-3830dc91-dde9-11e9-93ab-0610dba1f5f1
Sep 23 10:02:15.141: INFO: Pod name my-hostname-basic-3830dc91-dde9-11e9-93ab-0610dba1f5f1: Found 0 pods out of 1
Sep 23 10:02:20.144: INFO: Pod name my-hostname-basic-3830dc91-dde9-11e9-93ab-0610dba1f5f1: Found 1 pods out of 1
Sep 23 10:02:20.144: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-3830dc91-dde9-11e9-93ab-0610dba1f5f1" are running
Sep 23 10:02:20.147: INFO: Pod "my-hostname-basic-3830dc91-dde9-11e9-93ab-0610dba1f5f1-gx954" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-23 10:02:15 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-23 10:02:16 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-23 10:02:16 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-23 10:02:15 +0000 UTC Reason: Message:}])
Sep 23 10:02:20.147: INFO: Trying to dial the pod
Sep 23 10:02:25.157: INFO: Controller my-hostname-basic-3830dc91-dde9-11e9-93ab-0610dba1f5f1: Got expected result from replica 1 [my-hostname-basic-3830dc91-dde9-11e9-93ab-0610dba1f5f1-gx954]: "my-hostname-basic-3830dc91-dde9-11e9-93ab-0610dba1f5f1-gx954", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:02:25.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5084" for this suite.
Sep 23 10:02:31.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:02:31.253: INFO: namespace replication-controller-5084 deletion completed in 6.091129185s

• [SLOW TEST:16.154 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:02:31.253: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-6509
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 23 10:02:31.288: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 23 10:02:53.369: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.233.204 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6509 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 10:02:53.369: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 10:02:54.533: INFO: Found all expected endpoints: [netserver-0]
Sep 23 10:02:54.536: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.119.186 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6509 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 10:02:54.536: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 10:02:55.709: INFO: Found all expected endpoints: [netserver-1]
Sep 23 10:02:55.712: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.9.81 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6509 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 10:02:55.712: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 10:02:56.883: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:02:56.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6509" for this suite.
Sep 23 10:03:18.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:03:18.981: INFO: namespace pod-network-test-6509 deletion completed in 22.093037184s

• [SLOW TEST:47.728 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:03:18.981: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-5e4453e1-dde9-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume configMaps
Sep 23 10:03:19.026: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5e44d758-dde9-11e9-93ab-0610dba1f5f1" in namespace "projected-6824" to be "success or failure"
Sep 23 10:03:19.029: INFO: Pod "pod-projected-configmaps-5e44d758-dde9-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.19851ms
Sep 23 10:03:21.033: INFO: Pod "pod-projected-configmaps-5e44d758-dde9-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006719404s
STEP: Saw pod success
Sep 23 10:03:21.033: INFO: Pod "pod-projected-configmaps-5e44d758-dde9-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:03:21.035: INFO: Trying to get logs from node kube-node3 pod pod-projected-configmaps-5e44d758-dde9-11e9-93ab-0610dba1f5f1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 23 10:03:21.063: INFO: Waiting for pod pod-projected-configmaps-5e44d758-dde9-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:03:21.066: INFO: Pod pod-projected-configmaps-5e44d758-dde9-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:03:21.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6824" for this suite.
Sep 23 10:03:27.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:03:27.161: INFO: namespace projected-6824 deletion completed in 6.090779873s

• [SLOW TEST:8.180 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:03:27.161: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 10:03:27.212: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep 23 10:03:27.219: INFO: Number of nodes with available pods: 0
Sep 23 10:03:27.219: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep 23 10:03:27.238: INFO: Number of nodes with available pods: 0
Sep 23 10:03:27.238: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 10:03:28.242: INFO: Number of nodes with available pods: 0
Sep 23 10:03:28.242: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 10:03:29.242: INFO: Number of nodes with available pods: 1
Sep 23 10:03:29.242: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep 23 10:03:29.262: INFO: Number of nodes with available pods: 1
Sep 23 10:03:29.262: INFO: Number of running nodes: 0, number of available pods: 1
Sep 23 10:03:30.266: INFO: Number of nodes with available pods: 0
Sep 23 10:03:30.266: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep 23 10:03:30.276: INFO: Number of nodes with available pods: 0
Sep 23 10:03:30.276: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 10:03:31.280: INFO: Number of nodes with available pods: 0
Sep 23 10:03:31.280: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 10:03:32.280: INFO: Number of nodes with available pods: 0
Sep 23 10:03:32.280: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 10:03:33.279: INFO: Number of nodes with available pods: 0
Sep 23 10:03:33.279: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 10:03:34.280: INFO: Number of nodes with available pods: 0
Sep 23 10:03:34.280: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 10:03:35.280: INFO: Number of nodes with available pods: 1
Sep 23 10:03:35.280: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8011, will wait for the garbage collector to delete the pods
Sep 23 10:03:35.345: INFO: Deleting DaemonSet.extensions daemon-set took: 6.780455ms
Sep 23 10:03:35.645: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.317615ms
Sep 23 10:03:38.648: INFO: Number of nodes with available pods: 0
Sep 23 10:03:38.649: INFO: Number of running nodes: 0, number of available pods: 0
Sep 23 10:03:38.651: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8011/daemonsets","resourceVersion":"352130"},"items":null}

Sep 23 10:03:38.653: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8011/pods","resourceVersion":"352130"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:03:38.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8011" for this suite.
Sep 23 10:03:44.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:03:44.772: INFO: namespace daemonsets-8011 deletion completed in 6.090434962s

• [SLOW TEST:17.610 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:03:44.772: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 23 10:03:44.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8188'
Sep 23 10:03:44.895: INFO: stderr: ""
Sep 23 10:03:44.895: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Sep 23 10:03:44.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 delete pods e2e-test-nginx-pod --namespace=kubectl-8188'
Sep 23 10:03:51.425: INFO: stderr: ""
Sep 23 10:03:51.425: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:03:51.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8188" for this suite.
Sep 23 10:03:57.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:03:57.517: INFO: namespace kubectl-8188 deletion completed in 6.088097225s

• [SLOW TEST:12.745 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:03:57.517: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep 23 10:03:57.546: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:04:01.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5767" for this suite.
Sep 23 10:04:23.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:04:23.649: INFO: namespace init-container-5767 deletion completed in 22.089764085s

• [SLOW TEST:26.132 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:04:23.650: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 23 10:04:23.708: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:23.708: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:23.708: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:23.713: INFO: Number of nodes with available pods: 0
Sep 23 10:04:23.713: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 10:04:24.718: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:24.718: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:24.718: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:24.722: INFO: Number of nodes with available pods: 0
Sep 23 10:04:24.722: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 10:04:25.718: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:25.718: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:25.718: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:25.721: INFO: Number of nodes with available pods: 1
Sep 23 10:04:25.721: INFO: Node kube-node2 is running more than one daemon pod
Sep 23 10:04:26.718: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:26.718: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:26.718: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:26.722: INFO: Number of nodes with available pods: 3
Sep 23 10:04:26.722: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep 23 10:04:26.737: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:26.737: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:26.737: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:26.744: INFO: Number of nodes with available pods: 2
Sep 23 10:04:26.745: INFO: Node kube-node3 is running more than one daemon pod
Sep 23 10:04:27.750: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:27.750: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:27.750: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:27.752: INFO: Number of nodes with available pods: 2
Sep 23 10:04:27.753: INFO: Node kube-node3 is running more than one daemon pod
Sep 23 10:04:28.749: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:28.749: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:28.749: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:28.752: INFO: Number of nodes with available pods: 2
Sep 23 10:04:28.752: INFO: Node kube-node3 is running more than one daemon pod
Sep 23 10:04:29.750: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:29.750: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:29.750: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:29.753: INFO: Number of nodes with available pods: 2
Sep 23 10:04:29.753: INFO: Node kube-node3 is running more than one daemon pod
Sep 23 10:04:30.749: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:30.750: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:30.750: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:30.753: INFO: Number of nodes with available pods: 2
Sep 23 10:04:30.753: INFO: Node kube-node3 is running more than one daemon pod
Sep 23 10:04:31.749: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:31.749: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:31.749: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:31.752: INFO: Number of nodes with available pods: 2
Sep 23 10:04:31.752: INFO: Node kube-node3 is running more than one daemon pod
Sep 23 10:04:32.750: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:32.750: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:32.750: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:04:32.753: INFO: Number of nodes with available pods: 3
Sep 23 10:04:32.753: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3020, will wait for the garbage collector to delete the pods
Sep 23 10:04:32.815: INFO: Deleting DaemonSet.extensions daemon-set took: 6.839848ms
Sep 23 10:04:33.115: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.211999ms
Sep 23 10:04:42.019: INFO: Number of nodes with available pods: 0
Sep 23 10:04:42.019: INFO: Number of running nodes: 0, number of available pods: 0
Sep 23 10:04:42.021: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3020/daemonsets","resourceVersion":"352438"},"items":null}

Sep 23 10:04:42.024: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3020/pods","resourceVersion":"352438"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:04:42.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3020" for this suite.
Sep 23 10:04:48.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:04:48.128: INFO: namespace daemonsets-3020 deletion completed in 6.088795754s

• [SLOW TEST:24.478 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:04:48.128: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Sep 23 10:04:48.170: INFO: Waiting up to 5m0s for pod "client-containers-9366c53b-dde9-11e9-93ab-0610dba1f5f1" in namespace "containers-545" to be "success or failure"
Sep 23 10:04:48.173: INFO: Pod "client-containers-9366c53b-dde9-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.953697ms
Sep 23 10:04:50.177: INFO: Pod "client-containers-9366c53b-dde9-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006363615s
Sep 23 10:04:52.181: INFO: Pod "client-containers-9366c53b-dde9-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01097403s
STEP: Saw pod success
Sep 23 10:04:52.181: INFO: Pod "client-containers-9366c53b-dde9-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:04:52.185: INFO: Trying to get logs from node kube-node3 pod client-containers-9366c53b-dde9-11e9-93ab-0610dba1f5f1 container test-container: <nil>
STEP: delete the pod
Sep 23 10:04:52.209: INFO: Waiting for pod client-containers-9366c53b-dde9-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:04:52.211: INFO: Pod client-containers-9366c53b-dde9-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:04:52.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-545" for this suite.
Sep 23 10:04:58.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:04:58.307: INFO: namespace containers-545 deletion completed in 6.087039159s

• [SLOW TEST:10.179 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:04:58.308: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-9977916a-dde9-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume configMaps
Sep 23 10:04:58.347: INFO: Waiting up to 5m0s for pod "pod-configmaps-9977fd50-dde9-11e9-93ab-0610dba1f5f1" in namespace "configmap-8479" to be "success or failure"
Sep 23 10:04:58.349: INFO: Pod "pod-configmaps-9977fd50-dde9-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.400913ms
Sep 23 10:05:00.353: INFO: Pod "pod-configmaps-9977fd50-dde9-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006109201s
Sep 23 10:05:02.356: INFO: Pod "pod-configmaps-9977fd50-dde9-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009251532s
STEP: Saw pod success
Sep 23 10:05:02.356: INFO: Pod "pod-configmaps-9977fd50-dde9-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:05:02.358: INFO: Trying to get logs from node kube-node3 pod pod-configmaps-9977fd50-dde9-11e9-93ab-0610dba1f5f1 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 23 10:05:02.375: INFO: Waiting for pod pod-configmaps-9977fd50-dde9-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:05:02.378: INFO: Pod pod-configmaps-9977fd50-dde9-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:05:02.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8479" for this suite.
Sep 23 10:05:08.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:05:08.471: INFO: namespace configmap-8479 deletion completed in 6.090084366s

• [SLOW TEST:10.164 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:05:08.472: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep 23 10:05:11.030: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1738 pod-service-account-9fd54956-dde9-11e9-93ab-0610dba1f5f1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep 23 10:05:11.251: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1738 pod-service-account-9fd54956-dde9-11e9-93ab-0610dba1f5f1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep 23 10:05:11.473: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1738 pod-service-account-9fd54956-dde9-11e9-93ab-0610dba1f5f1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:05:11.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1738" for this suite.
Sep 23 10:05:17.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:05:17.785: INFO: namespace svcaccounts-1738 deletion completed in 6.091120434s

• [SLOW TEST:9.314 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:05:17.786: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 23 10:05:17.838: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a515c8e6-dde9-11e9-93ab-0610dba1f5f1" in namespace "downward-api-4421" to be "success or failure"
Sep 23 10:05:17.847: INFO: Pod "downwardapi-volume-a515c8e6-dde9-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.565394ms
Sep 23 10:05:19.852: INFO: Pod "downwardapi-volume-a515c8e6-dde9-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013760203s
Sep 23 10:05:21.855: INFO: Pod "downwardapi-volume-a515c8e6-dde9-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017285568s
STEP: Saw pod success
Sep 23 10:05:21.855: INFO: Pod "downwardapi-volume-a515c8e6-dde9-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:05:21.858: INFO: Trying to get logs from node kube-node3 pod downwardapi-volume-a515c8e6-dde9-11e9-93ab-0610dba1f5f1 container client-container: <nil>
STEP: delete the pod
Sep 23 10:05:21.875: INFO: Waiting for pod downwardapi-volume-a515c8e6-dde9-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:05:21.878: INFO: Pod downwardapi-volume-a515c8e6-dde9-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:05:21.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4421" for this suite.
Sep 23 10:05:27.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:05:27.987: INFO: namespace downward-api-4421 deletion completed in 6.104005172s

• [SLOW TEST:10.201 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:05:27.987: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 10:05:28.034: INFO: Creating deployment "test-recreate-deployment"
Sep 23 10:05:28.038: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep 23 10:05:28.043: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep 23 10:05:30.050: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep 23 10:05:30.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704829928, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704829928, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704829928, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704829928, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 10:05:32.056: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep 23 10:05:32.063: INFO: Updating deployment test-recreate-deployment
Sep 23 10:05:32.063: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep 23 10:05:32.124: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-7366,SelfLink:/apis/apps/v1/namespaces/deployment-7366/deployments/test-recreate-deployment,UID:ab2b1481-dde9-11e9-a6ed-0cda411df060,ResourceVersion:352740,Generation:2,CreationTimestamp:2019-09-23 10:05:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-09-23 10:05:32 +0000 UTC 2019-09-23 10:05:32 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-09-23 10:05:32 +0000 UTC 2019-09-23 10:05:28 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Sep 23 10:05:32.127: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-7366,SelfLink:/apis/apps/v1/namespaces/deployment-7366/replicasets/test-recreate-deployment-c9cbd8684,UID:ad95e144-dde9-11e9-a0c7-0cda411d3e75,ResourceVersion:352737,Generation:1,CreationTimestamp:2019-09-23 10:05:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment ab2b1481-dde9-11e9-a6ed-0cda411df060 0xc002c88ad0 0xc002c88ad1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 23 10:05:32.127: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep 23 10:05:32.127: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-7366,SelfLink:/apis/apps/v1/namespaces/deployment-7366/replicasets/test-recreate-deployment-7d57d5ff7c,UID:ab2bc3e1-dde9-11e9-a0c7-0cda411d3e75,ResourceVersion:352728,Generation:2,CreationTimestamp:2019-09-23 10:05:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment ab2b1481-dde9-11e9-a6ed-0cda411df060 0xc002c88a07 0xc002c88a08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 23 10:05:32.130: INFO: Pod "test-recreate-deployment-c9cbd8684-tk89w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-tk89w,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-7366,SelfLink:/api/v1/namespaces/deployment-7366/pods/test-recreate-deployment-c9cbd8684-tk89w,UID:ad9718df-dde9-11e9-a0c7-0cda411d3e75,ResourceVersion:352741,Generation:0,CreationTimestamp:2019-09-23 10:05:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 ad95e144-dde9-11e9-a0c7-0cda411d3e75 0xc002c89310 0xc002c89311}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b4c7t {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b4c7t,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b4c7t true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c89380} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c893a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:05:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:05:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:05:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:05:32 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.106,PodIP:,StartTime:2019-09-23 10:05:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:05:32.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7366" for this suite.
Sep 23 10:05:38.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:05:38.227: INFO: namespace deployment-7366 deletion completed in 6.09320975s

• [SLOW TEST:10.240 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:05:38.227: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 10:05:38.287: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep 23 10:05:38.296: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:38.296: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:38.296: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:38.299: INFO: Number of nodes with available pods: 0
Sep 23 10:05:38.299: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 10:05:39.304: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:39.304: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:39.304: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:39.308: INFO: Number of nodes with available pods: 0
Sep 23 10:05:39.308: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 10:05:40.304: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:40.304: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:40.304: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:40.307: INFO: Number of nodes with available pods: 0
Sep 23 10:05:40.307: INFO: Node kube-node1 is running more than one daemon pod
Sep 23 10:05:41.303: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:41.303: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:41.303: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:41.306: INFO: Number of nodes with available pods: 3
Sep 23 10:05:41.306: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep 23 10:05:41.326: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:41.326: INFO: Wrong image for pod: daemon-set-jgfml. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:41.326: INFO: Wrong image for pod: daemon-set-ztt6d. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:41.331: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:41.331: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:41.331: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:42.335: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:42.335: INFO: Wrong image for pod: daemon-set-jgfml. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:42.335: INFO: Wrong image for pod: daemon-set-ztt6d. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:42.339: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:42.339: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:42.339: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:43.335: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:43.335: INFO: Wrong image for pod: daemon-set-jgfml. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:43.335: INFO: Wrong image for pod: daemon-set-ztt6d. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:43.344: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:43.345: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:43.345: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:44.334: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:44.334: INFO: Wrong image for pod: daemon-set-jgfml. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:44.334: INFO: Wrong image for pod: daemon-set-ztt6d. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:44.334: INFO: Pod daemon-set-ztt6d is not available
Sep 23 10:05:44.338: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:44.338: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:44.338: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:45.334: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:45.334: INFO: Wrong image for pod: daemon-set-jgfml. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:45.334: INFO: Pod daemon-set-pwf9v is not available
Sep 23 10:05:45.338: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:45.338: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:45.338: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:46.335: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:46.335: INFO: Wrong image for pod: daemon-set-jgfml. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:46.335: INFO: Pod daemon-set-pwf9v is not available
Sep 23 10:05:46.339: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:46.339: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:46.339: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:47.334: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:47.334: INFO: Wrong image for pod: daemon-set-jgfml. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:47.338: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:47.338: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:47.338: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:48.334: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:48.334: INFO: Wrong image for pod: daemon-set-jgfml. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:48.334: INFO: Pod daemon-set-jgfml is not available
Sep 23 10:05:48.338: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:48.338: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:48.338: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:49.334: INFO: Pod daemon-set-bh4sl is not available
Sep 23 10:05:49.334: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:49.339: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:49.339: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:49.339: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:50.335: INFO: Pod daemon-set-bh4sl is not available
Sep 23 10:05:50.335: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:50.339: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:50.339: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:50.339: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:51.335: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:51.339: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:51.339: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:51.339: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:52.335: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:52.335: INFO: Pod daemon-set-gqrnw is not available
Sep 23 10:05:52.339: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:52.339: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:52.339: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:53.335: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:53.335: INFO: Pod daemon-set-gqrnw is not available
Sep 23 10:05:53.339: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:53.339: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:53.340: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:54.334: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:54.334: INFO: Pod daemon-set-gqrnw is not available
Sep 23 10:05:54.338: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:54.338: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:54.338: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:55.335: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:55.335: INFO: Pod daemon-set-gqrnw is not available
Sep 23 10:05:55.339: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:55.339: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:55.339: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:56.335: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:56.335: INFO: Pod daemon-set-gqrnw is not available
Sep 23 10:05:56.339: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:56.339: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:56.339: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:57.335: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:57.335: INFO: Pod daemon-set-gqrnw is not available
Sep 23 10:05:57.339: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:57.339: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:57.339: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:58.334: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:58.334: INFO: Pod daemon-set-gqrnw is not available
Sep 23 10:05:58.341: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:58.341: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:58.341: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:59.335: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:05:59.335: INFO: Pod daemon-set-gqrnw is not available
Sep 23 10:05:59.339: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:59.339: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:05:59.339: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:06:00.335: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:06:00.335: INFO: Pod daemon-set-gqrnw is not available
Sep 23 10:06:00.339: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:06:00.340: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:06:00.340: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:06:01.335: INFO: Wrong image for pod: daemon-set-gqrnw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 23 10:06:01.335: INFO: Pod daemon-set-gqrnw is not available
Sep 23 10:06:01.339: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:06:01.339: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:06:01.339: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:06:02.335: INFO: Pod daemon-set-kg6hj is not available
Sep 23 10:06:02.339: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:06:02.339: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:06:02.339: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Sep 23 10:06:02.343: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:06:02.343: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:06:02.343: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:06:02.346: INFO: Number of nodes with available pods: 2
Sep 23 10:06:02.346: INFO: Node kube-node3 is running more than one daemon pod
Sep 23 10:06:03.351: INFO: DaemonSet pods can't tolerate node kube-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:06:03.351: INFO: DaemonSet pods can't tolerate node kube-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:06:03.351: INFO: DaemonSet pods can't tolerate node kube-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:06:03.353: INFO: Number of nodes with available pods: 3
Sep 23 10:06:03.353: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9707, will wait for the garbage collector to delete the pods
Sep 23 10:06:03.426: INFO: Deleting DaemonSet.extensions daemon-set took: 7.620193ms
Sep 23 10:06:03.726: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.211938ms
Sep 23 10:06:12.229: INFO: Number of nodes with available pods: 0
Sep 23 10:06:12.229: INFO: Number of running nodes: 0, number of available pods: 0
Sep 23 10:06:12.232: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9707/daemonsets","resourceVersion":"353010"},"items":null}

Sep 23 10:06:12.235: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9707/pods","resourceVersion":"353010"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:06:12.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9707" for this suite.
Sep 23 10:06:18.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:06:18.340: INFO: namespace daemonsets-9707 deletion completed in 6.089471396s

• [SLOW TEST:40.113 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:06:18.341: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:06:18.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3155" for this suite.
Sep 23 10:06:24.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:06:24.469: INFO: namespace services-3155 deletion completed in 6.087833591s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.128 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:06:24.469: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Sep 23 10:06:24.501: INFO: namespace kubectl-124
Sep 23 10:06:24.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 create -f - --namespace=kubectl-124'
Sep 23 10:06:24.752: INFO: stderr: ""
Sep 23 10:06:24.752: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 23 10:06:25.756: INFO: Selector matched 1 pods for map[app:redis]
Sep 23 10:06:25.756: INFO: Found 0 / 1
Sep 23 10:06:26.755: INFO: Selector matched 1 pods for map[app:redis]
Sep 23 10:06:26.755: INFO: Found 0 / 1
Sep 23 10:06:27.756: INFO: Selector matched 1 pods for map[app:redis]
Sep 23 10:06:27.756: INFO: Found 1 / 1
Sep 23 10:06:27.756: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 23 10:06:27.759: INFO: Selector matched 1 pods for map[app:redis]
Sep 23 10:06:27.759: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 23 10:06:27.759: INFO: wait on redis-master startup in kubectl-124 
Sep 23 10:06:27.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 logs redis-master-xz2mr redis-master --namespace=kubectl-124'
Sep 23 10:06:27.861: INFO: stderr: ""
Sep 23 10:06:27.861: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 23 Sep 10:06:25.744 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 23 Sep 10:06:25.744 # Server started, Redis version 3.2.12\n1:M 23 Sep 10:06:25.744 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 23 Sep 10:06:25.744 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Sep 23 10:06:27.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-124'
Sep 23 10:06:27.960: INFO: stderr: ""
Sep 23 10:06:27.960: INFO: stdout: "service/rm2 exposed\n"
Sep 23 10:06:27.969: INFO: Service rm2 in namespace kubectl-124 found.
STEP: exposing service
Sep 23 10:06:29.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-124'
Sep 23 10:06:30.070: INFO: stderr: ""
Sep 23 10:06:30.070: INFO: stdout: "service/rm3 exposed\n"
Sep 23 10:06:30.075: INFO: Service rm3 in namespace kubectl-124 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:06:32.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-124" for this suite.
Sep 23 10:06:54.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:06:54.177: INFO: namespace kubectl-124 deletion completed in 22.092481484s

• [SLOW TEST:29.707 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:06:54.177: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-de87f964-dde9-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume secrets
Sep 23 10:06:54.216: INFO: Waiting up to 5m0s for pod "pod-secrets-de886774-dde9-11e9-93ab-0610dba1f5f1" in namespace "secrets-5721" to be "success or failure"
Sep 23 10:06:54.218: INFO: Pod "pod-secrets-de886774-dde9-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.392374ms
Sep 23 10:06:56.223: INFO: Pod "pod-secrets-de886774-dde9-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006601213s
STEP: Saw pod success
Sep 23 10:06:56.223: INFO: Pod "pod-secrets-de886774-dde9-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:06:56.225: INFO: Trying to get logs from node kube-node3 pod pod-secrets-de886774-dde9-11e9-93ab-0610dba1f5f1 container secret-volume-test: <nil>
STEP: delete the pod
Sep 23 10:06:56.243: INFO: Waiting for pod pod-secrets-de886774-dde9-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:06:56.252: INFO: Pod pod-secrets-de886774-dde9-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:06:56.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5721" for this suite.
Sep 23 10:07:02.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:07:02.349: INFO: namespace secrets-5721 deletion completed in 6.091860454s

• [SLOW TEST:8.172 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:07:02.350: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-e3675680-dde9-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume configMaps
Sep 23 10:07:02.393: INFO: Waiting up to 5m0s for pod "pod-configmaps-e367caa8-dde9-11e9-93ab-0610dba1f5f1" in namespace "configmap-1573" to be "success or failure"
Sep 23 10:07:02.397: INFO: Pod "pod-configmaps-e367caa8-dde9-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.941351ms
Sep 23 10:07:04.401: INFO: Pod "pod-configmaps-e367caa8-dde9-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007813694s
STEP: Saw pod success
Sep 23 10:07:04.401: INFO: Pod "pod-configmaps-e367caa8-dde9-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:07:04.404: INFO: Trying to get logs from node kube-node3 pod pod-configmaps-e367caa8-dde9-11e9-93ab-0610dba1f5f1 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 23 10:07:04.423: INFO: Waiting for pod pod-configmaps-e367caa8-dde9-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:07:04.427: INFO: Pod pod-configmaps-e367caa8-dde9-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:07:04.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1573" for this suite.
Sep 23 10:07:10.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:07:10.528: INFO: namespace configmap-1573 deletion completed in 6.097283428s

• [SLOW TEST:8.179 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:07:10.529: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-e847c062-dde9-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume configMaps
Sep 23 10:07:10.573: INFO: Waiting up to 5m0s for pod "pod-configmaps-e848379f-dde9-11e9-93ab-0610dba1f5f1" in namespace "configmap-3479" to be "success or failure"
Sep 23 10:07:10.579: INFO: Pod "pod-configmaps-e848379f-dde9-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.623236ms
Sep 23 10:07:12.583: INFO: Pod "pod-configmaps-e848379f-dde9-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010747417s
STEP: Saw pod success
Sep 23 10:07:12.583: INFO: Pod "pod-configmaps-e848379f-dde9-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:07:12.587: INFO: Trying to get logs from node kube-node3 pod pod-configmaps-e848379f-dde9-11e9-93ab-0610dba1f5f1 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 23 10:07:12.605: INFO: Waiting for pod pod-configmaps-e848379f-dde9-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:07:12.607: INFO: Pod pod-configmaps-e848379f-dde9-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:07:12.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3479" for this suite.
Sep 23 10:07:18.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:07:18.699: INFO: namespace configmap-3479 deletion completed in 6.088296913s

• [SLOW TEST:8.171 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:07:18.700: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep 23 10:07:18.749: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2834,SelfLink:/api/v1/namespaces/watch-2834/configmaps/e2e-watch-test-label-changed,UID:ed262ba4-dde9-11e9-a6ed-0cda411df060,ResourceVersion:353347,Generation:0,CreationTimestamp:2019-09-23 10:07:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 23 10:07:18.749: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2834,SelfLink:/api/v1/namespaces/watch-2834/configmaps/e2e-watch-test-label-changed,UID:ed262ba4-dde9-11e9-a6ed-0cda411df060,ResourceVersion:353348,Generation:0,CreationTimestamp:2019-09-23 10:07:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep 23 10:07:18.750: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2834,SelfLink:/api/v1/namespaces/watch-2834/configmaps/e2e-watch-test-label-changed,UID:ed262ba4-dde9-11e9-a6ed-0cda411df060,ResourceVersion:353349,Generation:0,CreationTimestamp:2019-09-23 10:07:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep 23 10:07:28.775: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2834,SelfLink:/api/v1/namespaces/watch-2834/configmaps/e2e-watch-test-label-changed,UID:ed262ba4-dde9-11e9-a6ed-0cda411df060,ResourceVersion:353370,Generation:0,CreationTimestamp:2019-09-23 10:07:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 23 10:07:28.776: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2834,SelfLink:/api/v1/namespaces/watch-2834/configmaps/e2e-watch-test-label-changed,UID:ed262ba4-dde9-11e9-a6ed-0cda411df060,ResourceVersion:353371,Generation:0,CreationTimestamp:2019-09-23 10:07:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Sep 23 10:07:28.776: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2834,SelfLink:/api/v1/namespaces/watch-2834/configmaps/e2e-watch-test-label-changed,UID:ed262ba4-dde9-11e9-a6ed-0cda411df060,ResourceVersion:353372,Generation:0,CreationTimestamp:2019-09-23 10:07:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:07:28.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2834" for this suite.
Sep 23 10:07:34.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:07:34.879: INFO: namespace watch-2834 deletion completed in 6.099377096s

• [SLOW TEST:16.180 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:07:34.880: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
W0923 10:07:35.952865      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 23 10:07:35.952: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:07:35.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-14" for this suite.
Sep 23 10:07:41.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:07:42.051: INFO: namespace gc-14 deletion completed in 6.093985474s

• [SLOW TEST:7.171 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:07:42.051: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep 23 10:07:45.120: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:07:46.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6753" for this suite.
Sep 23 10:08:08.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:08:08.227: INFO: namespace replicaset-6753 deletion completed in 22.089404233s

• [SLOW TEST:26.176 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:08:08.228: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-1294
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1294 to expose endpoints map[]
Sep 23 10:08:08.287: INFO: Get endpoints failed (11.964341ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Sep 23 10:08:09.291: INFO: successfully validated that service multi-endpoint-test in namespace services-1294 exposes endpoints map[] (1.015794488s elapsed)
STEP: Creating pod pod1 in namespace services-1294
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1294 to expose endpoints map[pod1:[100]]
Sep 23 10:08:11.315: INFO: successfully validated that service multi-endpoint-test in namespace services-1294 exposes endpoints map[pod1:[100]] (2.017115153s elapsed)
STEP: Creating pod pod2 in namespace services-1294
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1294 to expose endpoints map[pod1:[100] pod2:[101]]
Sep 23 10:08:13.377: INFO: successfully validated that service multi-endpoint-test in namespace services-1294 exposes endpoints map[pod1:[100] pod2:[101]] (2.05944037s elapsed)
STEP: Deleting pod pod1 in namespace services-1294
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1294 to expose endpoints map[pod2:[101]]
Sep 23 10:08:14.411: INFO: successfully validated that service multi-endpoint-test in namespace services-1294 exposes endpoints map[pod2:[101]] (1.026950203s elapsed)
STEP: Deleting pod pod2 in namespace services-1294
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1294 to expose endpoints map[]
Sep 23 10:08:15.425: INFO: successfully validated that service multi-endpoint-test in namespace services-1294 exposes endpoints map[] (1.008727001s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:08:15.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1294" for this suite.
Sep 23 10:08:37.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:08:37.577: INFO: namespace services-1294 deletion completed in 22.100364074s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:29.350 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:08:37.578: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-qk2t
STEP: Creating a pod to test atomic-volume-subpath
Sep 23 10:08:37.636: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-qk2t" in namespace "subpath-86" to be "success or failure"
Sep 23 10:08:37.639: INFO: Pod "pod-subpath-test-projected-qk2t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.890542ms
Sep 23 10:08:39.643: INFO: Pod "pod-subpath-test-projected-qk2t": Phase="Running", Reason="", readiness=true. Elapsed: 2.006997446s
Sep 23 10:08:41.647: INFO: Pod "pod-subpath-test-projected-qk2t": Phase="Running", Reason="", readiness=true. Elapsed: 4.010977297s
Sep 23 10:08:43.651: INFO: Pod "pod-subpath-test-projected-qk2t": Phase="Running", Reason="", readiness=true. Elapsed: 6.014847606s
Sep 23 10:08:45.654: INFO: Pod "pod-subpath-test-projected-qk2t": Phase="Running", Reason="", readiness=true. Elapsed: 8.018286152s
Sep 23 10:08:47.658: INFO: Pod "pod-subpath-test-projected-qk2t": Phase="Running", Reason="", readiness=true. Elapsed: 10.022092067s
Sep 23 10:08:49.661: INFO: Pod "pod-subpath-test-projected-qk2t": Phase="Running", Reason="", readiness=true. Elapsed: 12.025025695s
Sep 23 10:08:51.665: INFO: Pod "pod-subpath-test-projected-qk2t": Phase="Running", Reason="", readiness=true. Elapsed: 14.028723089s
Sep 23 10:08:53.668: INFO: Pod "pod-subpath-test-projected-qk2t": Phase="Running", Reason="", readiness=true. Elapsed: 16.032315909s
Sep 23 10:08:55.672: INFO: Pod "pod-subpath-test-projected-qk2t": Phase="Running", Reason="", readiness=true. Elapsed: 18.03618463s
Sep 23 10:08:57.676: INFO: Pod "pod-subpath-test-projected-qk2t": Phase="Running", Reason="", readiness=true. Elapsed: 20.039931441s
Sep 23 10:08:59.680: INFO: Pod "pod-subpath-test-projected-qk2t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.044145551s
STEP: Saw pod success
Sep 23 10:08:59.680: INFO: Pod "pod-subpath-test-projected-qk2t" satisfied condition "success or failure"
Sep 23 10:08:59.683: INFO: Trying to get logs from node kube-node3 pod pod-subpath-test-projected-qk2t container test-container-subpath-projected-qk2t: <nil>
STEP: delete the pod
Sep 23 10:08:59.704: INFO: Waiting for pod pod-subpath-test-projected-qk2t to disappear
Sep 23 10:08:59.707: INFO: Pod pod-subpath-test-projected-qk2t no longer exists
STEP: Deleting pod pod-subpath-test-projected-qk2t
Sep 23 10:08:59.707: INFO: Deleting pod "pod-subpath-test-projected-qk2t" in namespace "subpath-86"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:08:59.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-86" for this suite.
Sep 23 10:09:05.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:09:05.806: INFO: namespace subpath-86 deletion completed in 6.091929869s

• [SLOW TEST:28.228 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:09:05.806: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-6316
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 23 10:09:05.837: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 23 10:09:21.916: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.9.89:8080/dial?request=hostName&protocol=http&host=192.168.233.209&port=8080&tries=1'] Namespace:pod-network-test-6316 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 10:09:21.916: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 10:09:22.094: INFO: Waiting for endpoints: map[]
Sep 23 10:09:22.098: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.9.89:8080/dial?request=hostName&protocol=http&host=192.168.9.88&port=8080&tries=1'] Namespace:pod-network-test-6316 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 10:09:22.098: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 10:09:22.287: INFO: Waiting for endpoints: map[]
Sep 23 10:09:22.290: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.9.89:8080/dial?request=hostName&protocol=http&host=192.168.119.147&port=8080&tries=1'] Namespace:pod-network-test-6316 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 10:09:22.290: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 10:09:22.484: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:09:22.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6316" for this suite.
Sep 23 10:09:44.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:09:44.578: INFO: namespace pod-network-test-6316 deletion completed in 22.088861373s

• [SLOW TEST:38.772 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:09:44.578: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-4649
Sep 23 10:09:46.630: INFO: Started pod liveness-exec in namespace container-probe-4649
STEP: checking the pod's current state and verifying that restartCount is present
Sep 23 10:09:46.633: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:13:47.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4649" for this suite.
Sep 23 10:13:53.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:13:53.241: INFO: namespace container-probe-4649 deletion completed in 6.113903696s

• [SLOW TEST:248.662 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:13:53.241: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-834
I0923 10:13:53.283694      19 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-834, replica count: 1
I0923 10:13:54.334171      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 10:13:55.334386      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 23 10:13:55.447: INFO: Created: latency-svc-2w668
Sep 23 10:13:55.455: INFO: Got endpoints: latency-svc-2w668 [20.377036ms]
Sep 23 10:13:55.475: INFO: Created: latency-svc-pz2xf
Sep 23 10:13:55.480: INFO: Got endpoints: latency-svc-pz2xf [25.684537ms]
Sep 23 10:13:55.489: INFO: Created: latency-svc-p72pm
Sep 23 10:13:55.495: INFO: Got endpoints: latency-svc-p72pm [40.159803ms]
Sep 23 10:13:55.503: INFO: Created: latency-svc-dzzd4
Sep 23 10:13:55.517: INFO: Got endpoints: latency-svc-dzzd4 [62.065296ms]
Sep 23 10:13:55.532: INFO: Created: latency-svc-swftz
Sep 23 10:13:55.544: INFO: Got endpoints: latency-svc-swftz [88.649848ms]
Sep 23 10:13:55.553: INFO: Created: latency-svc-2qc7q
Sep 23 10:13:55.563: INFO: Got endpoints: latency-svc-2qc7q [107.876942ms]
Sep 23 10:13:55.583: INFO: Created: latency-svc-5wn7b
Sep 23 10:13:55.583: INFO: Got endpoints: latency-svc-5wn7b [128.170758ms]
Sep 23 10:13:55.589: INFO: Created: latency-svc-fjvhw
Sep 23 10:13:55.597: INFO: Got endpoints: latency-svc-fjvhw [142.000259ms]
Sep 23 10:13:55.604: INFO: Created: latency-svc-5qtqt
Sep 23 10:13:55.611: INFO: Got endpoints: latency-svc-5qtqt [156.522293ms]
Sep 23 10:13:55.620: INFO: Created: latency-svc-q72rn
Sep 23 10:13:55.626: INFO: Got endpoints: latency-svc-q72rn [170.830653ms]
Sep 23 10:13:55.636: INFO: Created: latency-svc-85q6l
Sep 23 10:13:55.648: INFO: Got endpoints: latency-svc-85q6l [193.312505ms]
Sep 23 10:13:55.661: INFO: Created: latency-svc-c467q
Sep 23 10:13:55.667: INFO: Got endpoints: latency-svc-c467q [212.312841ms]
Sep 23 10:13:55.683: INFO: Created: latency-svc-w86lk
Sep 23 10:13:55.695: INFO: Got endpoints: latency-svc-w86lk [240.46368ms]
Sep 23 10:13:55.701: INFO: Created: latency-svc-g7ps6
Sep 23 10:13:55.707: INFO: Got endpoints: latency-svc-g7ps6 [251.664814ms]
Sep 23 10:13:55.717: INFO: Created: latency-svc-28h5b
Sep 23 10:13:55.727: INFO: Got endpoints: latency-svc-28h5b [272.275183ms]
Sep 23 10:13:55.737: INFO: Created: latency-svc-hwt6r
Sep 23 10:13:55.750: INFO: Got endpoints: latency-svc-hwt6r [294.839803ms]
Sep 23 10:13:55.754: INFO: Created: latency-svc-zgxrb
Sep 23 10:13:55.763: INFO: Got endpoints: latency-svc-zgxrb [282.138356ms]
Sep 23 10:13:55.771: INFO: Created: latency-svc-5hw5f
Sep 23 10:13:55.774: INFO: Got endpoints: latency-svc-5hw5f [279.13251ms]
Sep 23 10:13:55.789: INFO: Created: latency-svc-hktbk
Sep 23 10:13:55.795: INFO: Got endpoints: latency-svc-hktbk [278.372984ms]
Sep 23 10:13:55.806: INFO: Created: latency-svc-qlp4f
Sep 23 10:13:55.813: INFO: Got endpoints: latency-svc-qlp4f [269.617043ms]
Sep 23 10:13:55.828: INFO: Created: latency-svc-vjpph
Sep 23 10:13:55.840: INFO: Got endpoints: latency-svc-vjpph [276.906851ms]
Sep 23 10:13:55.845: INFO: Created: latency-svc-rh9vj
Sep 23 10:13:55.856: INFO: Got endpoints: latency-svc-rh9vj [272.962527ms]
Sep 23 10:13:55.865: INFO: Created: latency-svc-49lvl
Sep 23 10:13:55.882: INFO: Got endpoints: latency-svc-49lvl [285.338628ms]
Sep 23 10:13:55.907: INFO: Created: latency-svc-7br5w
Sep 23 10:13:55.915: INFO: Got endpoints: latency-svc-7br5w [303.303302ms]
Sep 23 10:13:55.927: INFO: Created: latency-svc-mvjln
Sep 23 10:13:55.932: INFO: Got endpoints: latency-svc-mvjln [305.911135ms]
Sep 23 10:13:55.947: INFO: Created: latency-svc-qhtkq
Sep 23 10:13:55.947: INFO: Got endpoints: latency-svc-qhtkq [298.880709ms]
Sep 23 10:13:55.962: INFO: Created: latency-svc-qw8p2
Sep 23 10:13:55.981: INFO: Got endpoints: latency-svc-qw8p2 [313.192671ms]
Sep 23 10:13:55.981: INFO: Created: latency-svc-m6kfj
Sep 23 10:13:55.986: INFO: Got endpoints: latency-svc-m6kfj [290.286775ms]
Sep 23 10:13:55.994: INFO: Created: latency-svc-5ktj9
Sep 23 10:13:56.000: INFO: Got endpoints: latency-svc-5ktj9 [293.104595ms]
Sep 23 10:13:56.009: INFO: Created: latency-svc-8rfrj
Sep 23 10:13:56.020: INFO: Got endpoints: latency-svc-8rfrj [292.864916ms]
Sep 23 10:13:56.027: INFO: Created: latency-svc-qznx7
Sep 23 10:13:56.036: INFO: Got endpoints: latency-svc-qznx7 [285.696046ms]
Sep 23 10:13:56.045: INFO: Created: latency-svc-5rxjd
Sep 23 10:13:56.052: INFO: Got endpoints: latency-svc-5rxjd [289.129489ms]
Sep 23 10:13:56.063: INFO: Created: latency-svc-7v9zr
Sep 23 10:13:56.067: INFO: Got endpoints: latency-svc-7v9zr [292.214332ms]
Sep 23 10:13:56.076: INFO: Created: latency-svc-88b9m
Sep 23 10:13:56.082: INFO: Got endpoints: latency-svc-88b9m [286.926284ms]
Sep 23 10:13:56.092: INFO: Created: latency-svc-qfxgc
Sep 23 10:13:56.102: INFO: Got endpoints: latency-svc-qfxgc [289.129585ms]
Sep 23 10:13:56.106: INFO: Created: latency-svc-wwjsb
Sep 23 10:13:56.111: INFO: Got endpoints: latency-svc-wwjsb [271.420209ms]
Sep 23 10:13:56.121: INFO: Created: latency-svc-kbflr
Sep 23 10:13:56.127: INFO: Got endpoints: latency-svc-kbflr [271.250334ms]
Sep 23 10:13:56.134: INFO: Created: latency-svc-sf8rv
Sep 23 10:13:56.139: INFO: Got endpoints: latency-svc-sf8rv [257.238546ms]
Sep 23 10:13:56.149: INFO: Created: latency-svc-7n6c9
Sep 23 10:13:56.158: INFO: Got endpoints: latency-svc-7n6c9 [242.917989ms]
Sep 23 10:13:56.170: INFO: Created: latency-svc-mzx64
Sep 23 10:13:56.183: INFO: Got endpoints: latency-svc-mzx64 [251.446768ms]
Sep 23 10:13:56.193: INFO: Created: latency-svc-25mlf
Sep 23 10:13:56.199: INFO: Got endpoints: latency-svc-25mlf [251.371871ms]
Sep 23 10:13:56.208: INFO: Created: latency-svc-2hvql
Sep 23 10:13:56.231: INFO: Got endpoints: latency-svc-2hvql [250.618403ms]
Sep 23 10:13:56.252: INFO: Created: latency-svc-hb5dw
Sep 23 10:13:56.252: INFO: Created: latency-svc-dbscl
Sep 23 10:13:56.252: INFO: Got endpoints: latency-svc-dbscl [265.854417ms]
Sep 23 10:13:56.260: INFO: Got endpoints: latency-svc-hb5dw [260.368153ms]
Sep 23 10:13:56.264: INFO: Created: latency-svc-6czqg
Sep 23 10:13:56.278: INFO: Got endpoints: latency-svc-6czqg [258.333665ms]
Sep 23 10:13:56.284: INFO: Created: latency-svc-57c9f
Sep 23 10:13:56.296: INFO: Got endpoints: latency-svc-57c9f [259.853619ms]
Sep 23 10:13:56.298: INFO: Created: latency-svc-kb5c7
Sep 23 10:13:56.304: INFO: Got endpoints: latency-svc-kb5c7 [251.871178ms]
Sep 23 10:13:56.314: INFO: Created: latency-svc-pqbwm
Sep 23 10:13:56.337: INFO: Created: latency-svc-chbbl
Sep 23 10:13:56.340: INFO: Created: latency-svc-zgdn7
Sep 23 10:13:56.354: INFO: Got endpoints: latency-svc-pqbwm [287.12664ms]
Sep 23 10:13:56.359: INFO: Created: latency-svc-vkcvc
Sep 23 10:13:56.388: INFO: Created: latency-svc-b8ljl
Sep 23 10:13:56.400: INFO: Created: latency-svc-7w7r6
Sep 23 10:13:56.403: INFO: Got endpoints: latency-svc-chbbl [321.102157ms]
Sep 23 10:13:56.416: INFO: Created: latency-svc-7qp54
Sep 23 10:13:56.435: INFO: Created: latency-svc-ks4tk
Sep 23 10:13:56.451: INFO: Created: latency-svc-9mq2q
Sep 23 10:13:56.455: INFO: Got endpoints: latency-svc-zgdn7 [352.566403ms]
Sep 23 10:13:56.488: INFO: Created: latency-svc-knnt7
Sep 23 10:13:56.517: INFO: Created: latency-svc-lltjw
Sep 23 10:13:56.517: INFO: Got endpoints: latency-svc-vkcvc [405.51586ms]
Sep 23 10:13:56.520: INFO: Created: latency-svc-cqm2v
Sep 23 10:13:56.540: INFO: Created: latency-svc-2xwm7
Sep 23 10:13:56.560: INFO: Got endpoints: latency-svc-b8ljl [432.268436ms]
Sep 23 10:13:56.571: INFO: Created: latency-svc-kdsgp
Sep 23 10:13:56.597: INFO: Created: latency-svc-dn6x5
Sep 23 10:13:56.603: INFO: Got endpoints: latency-svc-7w7r6 [463.279035ms]
Sep 23 10:13:56.616: INFO: Created: latency-svc-h45zv
Sep 23 10:13:56.636: INFO: Created: latency-svc-4rj66
Sep 23 10:13:56.653: INFO: Created: latency-svc-qhz8z
Sep 23 10:13:56.657: INFO: Got endpoints: latency-svc-7qp54 [499.510537ms]
Sep 23 10:13:56.671: INFO: Created: latency-svc-d2lcl
Sep 23 10:13:56.705: INFO: Created: latency-svc-v4h9f
Sep 23 10:13:56.705: INFO: Got endpoints: latency-svc-ks4tk [521.35362ms]
Sep 23 10:13:56.721: INFO: Created: latency-svc-9qtvz
Sep 23 10:13:56.754: INFO: Created: latency-svc-4z79p
Sep 23 10:13:56.754: INFO: Got endpoints: latency-svc-9mq2q [555.452855ms]
Sep 23 10:13:56.771: INFO: Created: latency-svc-9b24w
Sep 23 10:13:56.788: INFO: Created: latency-svc-2n7dl
Sep 23 10:13:56.803: INFO: Got endpoints: latency-svc-knnt7 [571.196375ms]
Sep 23 10:13:56.821: INFO: Created: latency-svc-bgmkx
Sep 23 10:13:56.853: INFO: Got endpoints: latency-svc-lltjw [601.4157ms]
Sep 23 10:13:56.871: INFO: Created: latency-svc-ncwk8
Sep 23 10:13:56.903: INFO: Got endpoints: latency-svc-cqm2v [642.30787ms]
Sep 23 10:13:56.920: INFO: Created: latency-svc-cz74z
Sep 23 10:13:56.953: INFO: Got endpoints: latency-svc-2xwm7 [674.701ms]
Sep 23 10:13:56.979: INFO: Created: latency-svc-dcbx4
Sep 23 10:13:57.003: INFO: Got endpoints: latency-svc-kdsgp [707.483176ms]
Sep 23 10:13:57.027: INFO: Created: latency-svc-9rctd
Sep 23 10:13:57.056: INFO: Got endpoints: latency-svc-dn6x5 [751.95159ms]
Sep 23 10:13:57.074: INFO: Created: latency-svc-5wvsx
Sep 23 10:13:57.103: INFO: Got endpoints: latency-svc-h45zv [749.324593ms]
Sep 23 10:13:57.127: INFO: Created: latency-svc-gng4k
Sep 23 10:13:57.165: INFO: Got endpoints: latency-svc-4rj66 [761.984742ms]
Sep 23 10:13:57.190: INFO: Created: latency-svc-pj8td
Sep 23 10:13:57.202: INFO: Got endpoints: latency-svc-qhz8z [747.197483ms]
Sep 23 10:13:57.224: INFO: Created: latency-svc-shnlf
Sep 23 10:13:57.254: INFO: Got endpoints: latency-svc-d2lcl [737.341362ms]
Sep 23 10:13:57.290: INFO: Created: latency-svc-gxbgs
Sep 23 10:13:57.303: INFO: Got endpoints: latency-svc-v4h9f [743.552146ms]
Sep 23 10:13:57.320: INFO: Created: latency-svc-wx8q4
Sep 23 10:13:57.353: INFO: Got endpoints: latency-svc-9qtvz [750.56637ms]
Sep 23 10:13:57.373: INFO: Created: latency-svc-nqjc4
Sep 23 10:13:57.403: INFO: Got endpoints: latency-svc-4z79p [745.570959ms]
Sep 23 10:13:57.421: INFO: Created: latency-svc-hq56v
Sep 23 10:13:57.454: INFO: Got endpoints: latency-svc-9b24w [740.920136ms]
Sep 23 10:13:57.477: INFO: Created: latency-svc-wx8lf
Sep 23 10:13:57.503: INFO: Got endpoints: latency-svc-2n7dl [748.650482ms]
Sep 23 10:13:57.524: INFO: Created: latency-svc-d6pvk
Sep 23 10:13:57.553: INFO: Got endpoints: latency-svc-bgmkx [750.807424ms]
Sep 23 10:13:57.575: INFO: Created: latency-svc-2pcrn
Sep 23 10:13:57.603: INFO: Got endpoints: latency-svc-ncwk8 [750.207126ms]
Sep 23 10:13:57.622: INFO: Created: latency-svc-t77hq
Sep 23 10:13:57.655: INFO: Got endpoints: latency-svc-cz74z [752.480219ms]
Sep 23 10:13:57.674: INFO: Created: latency-svc-wmxlq
Sep 23 10:13:57.703: INFO: Got endpoints: latency-svc-dcbx4 [749.963003ms]
Sep 23 10:13:57.725: INFO: Created: latency-svc-fk58p
Sep 23 10:13:57.762: INFO: Got endpoints: latency-svc-9rctd [759.029966ms]
Sep 23 10:13:57.792: INFO: Created: latency-svc-nqctm
Sep 23 10:13:57.808: INFO: Got endpoints: latency-svc-5wvsx [751.928439ms]
Sep 23 10:13:57.835: INFO: Created: latency-svc-ssgx8
Sep 23 10:13:57.854: INFO: Got endpoints: latency-svc-gng4k [750.953607ms]
Sep 23 10:13:57.893: INFO: Created: latency-svc-kwxqf
Sep 23 10:13:57.905: INFO: Got endpoints: latency-svc-pj8td [739.432995ms]
Sep 23 10:13:57.936: INFO: Created: latency-svc-tzjnz
Sep 23 10:13:57.954: INFO: Got endpoints: latency-svc-shnlf [751.399441ms]
Sep 23 10:13:57.973: INFO: Created: latency-svc-nv2vm
Sep 23 10:13:58.005: INFO: Got endpoints: latency-svc-gxbgs [750.541015ms]
Sep 23 10:13:58.027: INFO: Created: latency-svc-hvk77
Sep 23 10:13:58.053: INFO: Got endpoints: latency-svc-wx8q4 [749.589925ms]
Sep 23 10:13:58.071: INFO: Created: latency-svc-cdmzp
Sep 23 10:13:58.103: INFO: Got endpoints: latency-svc-nqjc4 [749.483635ms]
Sep 23 10:13:58.124: INFO: Created: latency-svc-bl98d
Sep 23 10:13:58.153: INFO: Got endpoints: latency-svc-hq56v [749.928627ms]
Sep 23 10:13:58.181: INFO: Created: latency-svc-6tsfd
Sep 23 10:13:58.206: INFO: Got endpoints: latency-svc-wx8lf [751.933649ms]
Sep 23 10:13:58.224: INFO: Created: latency-svc-jgdb8
Sep 23 10:13:58.254: INFO: Got endpoints: latency-svc-d6pvk [750.735888ms]
Sep 23 10:13:58.279: INFO: Created: latency-svc-b2lkx
Sep 23 10:13:58.303: INFO: Got endpoints: latency-svc-2pcrn [750.003904ms]
Sep 23 10:13:58.322: INFO: Created: latency-svc-bzgr2
Sep 23 10:13:58.352: INFO: Got endpoints: latency-svc-t77hq [748.971821ms]
Sep 23 10:13:58.370: INFO: Created: latency-svc-5z6d4
Sep 23 10:13:58.404: INFO: Got endpoints: latency-svc-wmxlq [748.40945ms]
Sep 23 10:13:58.422: INFO: Created: latency-svc-msdb2
Sep 23 10:13:58.453: INFO: Got endpoints: latency-svc-fk58p [750.126139ms]
Sep 23 10:13:58.472: INFO: Created: latency-svc-vsmhh
Sep 23 10:13:58.503: INFO: Got endpoints: latency-svc-nqctm [740.870579ms]
Sep 23 10:13:58.525: INFO: Created: latency-svc-jfdg4
Sep 23 10:13:58.554: INFO: Got endpoints: latency-svc-ssgx8 [746.681371ms]
Sep 23 10:13:58.584: INFO: Created: latency-svc-v29tm
Sep 23 10:13:58.604: INFO: Got endpoints: latency-svc-kwxqf [749.325717ms]
Sep 23 10:13:58.622: INFO: Created: latency-svc-mkcfb
Sep 23 10:13:58.654: INFO: Got endpoints: latency-svc-tzjnz [749.452387ms]
Sep 23 10:13:58.674: INFO: Created: latency-svc-gv7pd
Sep 23 10:13:58.703: INFO: Got endpoints: latency-svc-nv2vm [749.145902ms]
Sep 23 10:13:58.725: INFO: Created: latency-svc-6zhms
Sep 23 10:13:58.754: INFO: Got endpoints: latency-svc-hvk77 [749.004683ms]
Sep 23 10:13:58.775: INFO: Created: latency-svc-2j52v
Sep 23 10:13:58.812: INFO: Got endpoints: latency-svc-cdmzp [758.716477ms]
Sep 23 10:13:58.832: INFO: Created: latency-svc-qfh74
Sep 23 10:13:58.853: INFO: Got endpoints: latency-svc-bl98d [749.629061ms]
Sep 23 10:13:58.869: INFO: Created: latency-svc-ph9jt
Sep 23 10:13:58.903: INFO: Got endpoints: latency-svc-6tsfd [749.825912ms]
Sep 23 10:13:58.920: INFO: Created: latency-svc-8g9h2
Sep 23 10:13:58.953: INFO: Got endpoints: latency-svc-jgdb8 [747.422695ms]
Sep 23 10:13:58.969: INFO: Created: latency-svc-tqxwv
Sep 23 10:13:59.003: INFO: Got endpoints: latency-svc-b2lkx [749.266341ms]
Sep 23 10:13:59.022: INFO: Created: latency-svc-hfvn9
Sep 23 10:13:59.057: INFO: Got endpoints: latency-svc-bzgr2 [753.617986ms]
Sep 23 10:13:59.082: INFO: Created: latency-svc-stwp6
Sep 23 10:13:59.103: INFO: Got endpoints: latency-svc-5z6d4 [750.440232ms]
Sep 23 10:13:59.122: INFO: Created: latency-svc-xc2kz
Sep 23 10:13:59.153: INFO: Got endpoints: latency-svc-msdb2 [749.715984ms]
Sep 23 10:13:59.170: INFO: Created: latency-svc-jqv9k
Sep 23 10:13:59.208: INFO: Got endpoints: latency-svc-vsmhh [755.149884ms]
Sep 23 10:13:59.228: INFO: Created: latency-svc-8n87b
Sep 23 10:13:59.254: INFO: Got endpoints: latency-svc-jfdg4 [750.414145ms]
Sep 23 10:13:59.272: INFO: Created: latency-svc-t479q
Sep 23 10:13:59.303: INFO: Got endpoints: latency-svc-v29tm [748.623378ms]
Sep 23 10:13:59.328: INFO: Created: latency-svc-rl999
Sep 23 10:13:59.353: INFO: Got endpoints: latency-svc-mkcfb [749.406908ms]
Sep 23 10:13:59.376: INFO: Created: latency-svc-26r5z
Sep 23 10:13:59.403: INFO: Got endpoints: latency-svc-gv7pd [748.468177ms]
Sep 23 10:13:59.431: INFO: Created: latency-svc-r7mzc
Sep 23 10:13:59.456: INFO: Got endpoints: latency-svc-6zhms [746.581165ms]
Sep 23 10:13:59.474: INFO: Created: latency-svc-5fsqq
Sep 23 10:13:59.503: INFO: Got endpoints: latency-svc-2j52v [749.613177ms]
Sep 23 10:13:59.521: INFO: Created: latency-svc-cg86r
Sep 23 10:13:59.553: INFO: Got endpoints: latency-svc-qfh74 [741.144254ms]
Sep 23 10:13:59.571: INFO: Created: latency-svc-v2qnz
Sep 23 10:13:59.603: INFO: Got endpoints: latency-svc-ph9jt [750.870975ms]
Sep 23 10:13:59.628: INFO: Created: latency-svc-vp6dd
Sep 23 10:13:59.653: INFO: Got endpoints: latency-svc-8g9h2 [749.752475ms]
Sep 23 10:13:59.672: INFO: Created: latency-svc-mgpbk
Sep 23 10:13:59.705: INFO: Got endpoints: latency-svc-tqxwv [751.261707ms]
Sep 23 10:13:59.727: INFO: Created: latency-svc-7gjv8
Sep 23 10:13:59.756: INFO: Got endpoints: latency-svc-hfvn9 [752.871983ms]
Sep 23 10:13:59.776: INFO: Created: latency-svc-7g2tq
Sep 23 10:13:59.807: INFO: Got endpoints: latency-svc-stwp6 [749.313648ms]
Sep 23 10:13:59.837: INFO: Created: latency-svc-vfwzx
Sep 23 10:13:59.853: INFO: Got endpoints: latency-svc-xc2kz [749.862339ms]
Sep 23 10:13:59.879: INFO: Created: latency-svc-krs45
Sep 23 10:13:59.907: INFO: Got endpoints: latency-svc-jqv9k [753.123946ms]
Sep 23 10:13:59.926: INFO: Created: latency-svc-9sfps
Sep 23 10:13:59.953: INFO: Got endpoints: latency-svc-8n87b [744.977717ms]
Sep 23 10:13:59.981: INFO: Created: latency-svc-jtq7p
Sep 23 10:14:00.003: INFO: Got endpoints: latency-svc-t479q [749.559235ms]
Sep 23 10:14:00.029: INFO: Created: latency-svc-wjqlr
Sep 23 10:14:00.055: INFO: Got endpoints: latency-svc-rl999 [751.483543ms]
Sep 23 10:14:00.086: INFO: Created: latency-svc-w76ss
Sep 23 10:14:00.109: INFO: Got endpoints: latency-svc-26r5z [755.587337ms]
Sep 23 10:14:00.131: INFO: Created: latency-svc-vx4db
Sep 23 10:14:00.153: INFO: Got endpoints: latency-svc-r7mzc [749.89455ms]
Sep 23 10:14:00.173: INFO: Created: latency-svc-vc9pb
Sep 23 10:14:00.203: INFO: Got endpoints: latency-svc-5fsqq [747.323089ms]
Sep 23 10:14:00.224: INFO: Created: latency-svc-b4k7j
Sep 23 10:14:00.258: INFO: Got endpoints: latency-svc-cg86r [754.195809ms]
Sep 23 10:14:00.277: INFO: Created: latency-svc-vm9v9
Sep 23 10:14:00.303: INFO: Got endpoints: latency-svc-v2qnz [750.227047ms]
Sep 23 10:14:00.332: INFO: Created: latency-svc-zqlkc
Sep 23 10:14:00.353: INFO: Got endpoints: latency-svc-vp6dd [749.396937ms]
Sep 23 10:14:00.379: INFO: Created: latency-svc-gjflz
Sep 23 10:14:00.403: INFO: Got endpoints: latency-svc-mgpbk [749.973906ms]
Sep 23 10:14:00.421: INFO: Created: latency-svc-mlndf
Sep 23 10:14:00.453: INFO: Got endpoints: latency-svc-7gjv8 [748.116555ms]
Sep 23 10:14:00.469: INFO: Created: latency-svc-qswcd
Sep 23 10:14:00.503: INFO: Got endpoints: latency-svc-7g2tq [746.861946ms]
Sep 23 10:14:00.525: INFO: Created: latency-svc-qx54r
Sep 23 10:14:00.553: INFO: Got endpoints: latency-svc-vfwzx [746.610433ms]
Sep 23 10:14:00.570: INFO: Created: latency-svc-ssxrl
Sep 23 10:14:00.604: INFO: Got endpoints: latency-svc-krs45 [751.038907ms]
Sep 23 10:14:00.620: INFO: Created: latency-svc-j6q4c
Sep 23 10:14:00.653: INFO: Got endpoints: latency-svc-9sfps [746.881065ms]
Sep 23 10:14:00.671: INFO: Created: latency-svc-kr4sm
Sep 23 10:14:00.708: INFO: Got endpoints: latency-svc-jtq7p [754.131734ms]
Sep 23 10:14:00.726: INFO: Created: latency-svc-f6b8m
Sep 23 10:14:00.754: INFO: Got endpoints: latency-svc-wjqlr [750.507292ms]
Sep 23 10:14:00.773: INFO: Created: latency-svc-l29np
Sep 23 10:14:00.803: INFO: Got endpoints: latency-svc-w76ss [748.604141ms]
Sep 23 10:14:00.824: INFO: Created: latency-svc-42pp7
Sep 23 10:14:00.855: INFO: Got endpoints: latency-svc-vx4db [746.355504ms]
Sep 23 10:14:00.873: INFO: Created: latency-svc-pncnl
Sep 23 10:14:00.912: INFO: Got endpoints: latency-svc-vc9pb [759.198496ms]
Sep 23 10:14:00.931: INFO: Created: latency-svc-zhvtw
Sep 23 10:14:00.953: INFO: Got endpoints: latency-svc-b4k7j [749.894177ms]
Sep 23 10:14:00.975: INFO: Created: latency-svc-n2dkm
Sep 23 10:14:01.003: INFO: Got endpoints: latency-svc-vm9v9 [745.649151ms]
Sep 23 10:14:01.022: INFO: Created: latency-svc-k2kzl
Sep 23 10:14:01.054: INFO: Got endpoints: latency-svc-zqlkc [750.419647ms]
Sep 23 10:14:01.074: INFO: Created: latency-svc-45kdr
Sep 23 10:14:01.105: INFO: Got endpoints: latency-svc-gjflz [751.643524ms]
Sep 23 10:14:01.131: INFO: Created: latency-svc-dx6t7
Sep 23 10:14:01.153: INFO: Got endpoints: latency-svc-mlndf [750.393014ms]
Sep 23 10:14:01.174: INFO: Created: latency-svc-9s95s
Sep 23 10:14:01.202: INFO: Got endpoints: latency-svc-qswcd [749.475641ms]
Sep 23 10:14:01.222: INFO: Created: latency-svc-xg5zs
Sep 23 10:14:01.264: INFO: Got endpoints: latency-svc-qx54r [760.682123ms]
Sep 23 10:14:01.291: INFO: Created: latency-svc-9hfmt
Sep 23 10:14:01.304: INFO: Got endpoints: latency-svc-ssxrl [750.375465ms]
Sep 23 10:14:01.337: INFO: Created: latency-svc-ckrrr
Sep 23 10:14:01.353: INFO: Got endpoints: latency-svc-j6q4c [749.2774ms]
Sep 23 10:14:01.372: INFO: Created: latency-svc-4828n
Sep 23 10:14:01.403: INFO: Got endpoints: latency-svc-kr4sm [749.608682ms]
Sep 23 10:14:01.442: INFO: Created: latency-svc-dnddp
Sep 23 10:14:01.453: INFO: Got endpoints: latency-svc-f6b8m [745.472549ms]
Sep 23 10:14:01.472: INFO: Created: latency-svc-7t84v
Sep 23 10:14:01.503: INFO: Got endpoints: latency-svc-l29np [749.203602ms]
Sep 23 10:14:01.524: INFO: Created: latency-svc-qmvpm
Sep 23 10:14:01.553: INFO: Got endpoints: latency-svc-42pp7 [749.947664ms]
Sep 23 10:14:01.580: INFO: Created: latency-svc-bwn52
Sep 23 10:14:01.604: INFO: Got endpoints: latency-svc-pncnl [748.440487ms]
Sep 23 10:14:01.629: INFO: Created: latency-svc-qbrb6
Sep 23 10:14:01.653: INFO: Got endpoints: latency-svc-zhvtw [741.205769ms]
Sep 23 10:14:01.672: INFO: Created: latency-svc-tsmvk
Sep 23 10:14:01.703: INFO: Got endpoints: latency-svc-n2dkm [749.92593ms]
Sep 23 10:14:01.732: INFO: Created: latency-svc-hdnlz
Sep 23 10:14:01.756: INFO: Got endpoints: latency-svc-k2kzl [753.046447ms]
Sep 23 10:14:01.774: INFO: Created: latency-svc-q4pq5
Sep 23 10:14:01.803: INFO: Got endpoints: latency-svc-45kdr [749.268895ms]
Sep 23 10:14:01.822: INFO: Created: latency-svc-v9xdl
Sep 23 10:14:01.853: INFO: Got endpoints: latency-svc-dx6t7 [748.552284ms]
Sep 23 10:14:01.872: INFO: Created: latency-svc-xwsrv
Sep 23 10:14:01.903: INFO: Got endpoints: latency-svc-9s95s [749.829242ms]
Sep 23 10:14:01.926: INFO: Created: latency-svc-lwv5g
Sep 23 10:14:01.953: INFO: Got endpoints: latency-svc-xg5zs [750.456053ms]
Sep 23 10:14:01.971: INFO: Created: latency-svc-bvmn4
Sep 23 10:14:02.003: INFO: Got endpoints: latency-svc-9hfmt [739.2999ms]
Sep 23 10:14:02.025: INFO: Created: latency-svc-jd55f
Sep 23 10:14:02.068: INFO: Got endpoints: latency-svc-ckrrr [764.632083ms]
Sep 23 10:14:02.093: INFO: Created: latency-svc-mpx68
Sep 23 10:14:02.104: INFO: Got endpoints: latency-svc-4828n [750.913684ms]
Sep 23 10:14:02.121: INFO: Created: latency-svc-6hq6v
Sep 23 10:14:02.153: INFO: Got endpoints: latency-svc-dnddp [749.666459ms]
Sep 23 10:14:02.172: INFO: Created: latency-svc-2v57x
Sep 23 10:14:02.217: INFO: Got endpoints: latency-svc-7t84v [763.565196ms]
Sep 23 10:14:02.237: INFO: Created: latency-svc-rdmjc
Sep 23 10:14:02.253: INFO: Got endpoints: latency-svc-qmvpm [750.345615ms]
Sep 23 10:14:02.281: INFO: Created: latency-svc-q28vz
Sep 23 10:14:02.303: INFO: Got endpoints: latency-svc-bwn52 [749.196252ms]
Sep 23 10:14:02.331: INFO: Created: latency-svc-g7sn7
Sep 23 10:14:02.354: INFO: Got endpoints: latency-svc-qbrb6 [749.925215ms]
Sep 23 10:14:02.371: INFO: Created: latency-svc-hmzs9
Sep 23 10:14:02.405: INFO: Got endpoints: latency-svc-tsmvk [751.34182ms]
Sep 23 10:14:02.430: INFO: Created: latency-svc-2tfjj
Sep 23 10:14:02.453: INFO: Got endpoints: latency-svc-hdnlz [740.158754ms]
Sep 23 10:14:02.472: INFO: Created: latency-svc-kbqf6
Sep 23 10:14:02.508: INFO: Got endpoints: latency-svc-q4pq5 [751.754016ms]
Sep 23 10:14:02.527: INFO: Created: latency-svc-ghq5q
Sep 23 10:14:02.554: INFO: Got endpoints: latency-svc-v9xdl [750.560553ms]
Sep 23 10:14:02.573: INFO: Created: latency-svc-dm7lv
Sep 23 10:14:02.603: INFO: Got endpoints: latency-svc-xwsrv [749.848816ms]
Sep 23 10:14:02.626: INFO: Created: latency-svc-m7246
Sep 23 10:14:02.656: INFO: Got endpoints: latency-svc-lwv5g [753.218427ms]
Sep 23 10:14:02.679: INFO: Created: latency-svc-5mxjp
Sep 23 10:14:02.703: INFO: Got endpoints: latency-svc-bvmn4 [750.487867ms]
Sep 23 10:14:02.726: INFO: Created: latency-svc-nq26n
Sep 23 10:14:02.753: INFO: Got endpoints: latency-svc-jd55f [750.274968ms]
Sep 23 10:14:02.770: INFO: Created: latency-svc-rw4k9
Sep 23 10:14:02.803: INFO: Got endpoints: latency-svc-mpx68 [734.608871ms]
Sep 23 10:14:02.852: INFO: Created: latency-svc-b6727
Sep 23 10:14:02.853: INFO: Got endpoints: latency-svc-6hq6v [748.728255ms]
Sep 23 10:14:02.870: INFO: Created: latency-svc-k7qzd
Sep 23 10:14:02.903: INFO: Got endpoints: latency-svc-2v57x [749.689143ms]
Sep 23 10:14:02.933: INFO: Created: latency-svc-8glpc
Sep 23 10:14:02.953: INFO: Got endpoints: latency-svc-rdmjc [736.296588ms]
Sep 23 10:14:02.969: INFO: Created: latency-svc-7dnks
Sep 23 10:14:03.004: INFO: Got endpoints: latency-svc-q28vz [750.53048ms]
Sep 23 10:14:03.032: INFO: Created: latency-svc-mqjzz
Sep 23 10:14:03.055: INFO: Got endpoints: latency-svc-g7sn7 [752.222088ms]
Sep 23 10:14:03.076: INFO: Created: latency-svc-tmsq6
Sep 23 10:14:03.103: INFO: Got endpoints: latency-svc-hmzs9 [749.266965ms]
Sep 23 10:14:03.129: INFO: Created: latency-svc-k6fz8
Sep 23 10:14:03.154: INFO: Got endpoints: latency-svc-2tfjj [748.967128ms]
Sep 23 10:14:03.182: INFO: Created: latency-svc-c8fmr
Sep 23 10:14:03.204: INFO: Got endpoints: latency-svc-kbqf6 [750.686554ms]
Sep 23 10:14:03.225: INFO: Created: latency-svc-zb68z
Sep 23 10:14:03.259: INFO: Got endpoints: latency-svc-ghq5q [750.465759ms]
Sep 23 10:14:03.282: INFO: Created: latency-svc-vqfqz
Sep 23 10:14:03.303: INFO: Got endpoints: latency-svc-dm7lv [749.288293ms]
Sep 23 10:14:03.353: INFO: Got endpoints: latency-svc-m7246 [749.683256ms]
Sep 23 10:14:03.403: INFO: Got endpoints: latency-svc-5mxjp [747.172947ms]
Sep 23 10:14:03.453: INFO: Got endpoints: latency-svc-nq26n [749.369005ms]
Sep 23 10:14:03.503: INFO: Got endpoints: latency-svc-rw4k9 [749.448548ms]
Sep 23 10:14:03.553: INFO: Got endpoints: latency-svc-b6727 [750.133794ms]
Sep 23 10:14:03.605: INFO: Got endpoints: latency-svc-k7qzd [751.706435ms]
Sep 23 10:14:03.654: INFO: Got endpoints: latency-svc-8glpc [750.817251ms]
Sep 23 10:14:03.703: INFO: Got endpoints: latency-svc-7dnks [749.835505ms]
Sep 23 10:14:03.754: INFO: Got endpoints: latency-svc-mqjzz [750.302884ms]
Sep 23 10:14:03.803: INFO: Got endpoints: latency-svc-tmsq6 [747.826702ms]
Sep 23 10:14:03.853: INFO: Got endpoints: latency-svc-k6fz8 [749.452054ms]
Sep 23 10:14:03.903: INFO: Got endpoints: latency-svc-c8fmr [748.948647ms]
Sep 23 10:14:03.953: INFO: Got endpoints: latency-svc-zb68z [749.14149ms]
Sep 23 10:14:04.003: INFO: Got endpoints: latency-svc-vqfqz [744.24629ms]
Sep 23 10:14:04.003: INFO: Latencies: [25.684537ms 40.159803ms 62.065296ms 88.649848ms 107.876942ms 128.170758ms 142.000259ms 156.522293ms 170.830653ms 193.312505ms 212.312841ms 240.46368ms 242.917989ms 250.618403ms 251.371871ms 251.446768ms 251.664814ms 251.871178ms 257.238546ms 258.333665ms 259.853619ms 260.368153ms 265.854417ms 269.617043ms 271.250334ms 271.420209ms 272.275183ms 272.962527ms 276.906851ms 278.372984ms 279.13251ms 282.138356ms 285.338628ms 285.696046ms 286.926284ms 287.12664ms 289.129489ms 289.129585ms 290.286775ms 292.214332ms 292.864916ms 293.104595ms 294.839803ms 298.880709ms 303.303302ms 305.911135ms 313.192671ms 321.102157ms 352.566403ms 405.51586ms 432.268436ms 463.279035ms 499.510537ms 521.35362ms 555.452855ms 571.196375ms 601.4157ms 642.30787ms 674.701ms 707.483176ms 734.608871ms 736.296588ms 737.341362ms 739.2999ms 739.432995ms 740.158754ms 740.870579ms 740.920136ms 741.144254ms 741.205769ms 743.552146ms 744.24629ms 744.977717ms 745.472549ms 745.570959ms 745.649151ms 746.355504ms 746.581165ms 746.610433ms 746.681371ms 746.861946ms 746.881065ms 747.172947ms 747.197483ms 747.323089ms 747.422695ms 747.826702ms 748.116555ms 748.40945ms 748.440487ms 748.468177ms 748.552284ms 748.604141ms 748.623378ms 748.650482ms 748.728255ms 748.948647ms 748.967128ms 748.971821ms 749.004683ms 749.14149ms 749.145902ms 749.196252ms 749.203602ms 749.266341ms 749.266965ms 749.268895ms 749.2774ms 749.288293ms 749.313648ms 749.324593ms 749.325717ms 749.369005ms 749.396937ms 749.406908ms 749.448548ms 749.452054ms 749.452387ms 749.475641ms 749.483635ms 749.559235ms 749.589925ms 749.608682ms 749.613177ms 749.629061ms 749.666459ms 749.683256ms 749.689143ms 749.715984ms 749.752475ms 749.825912ms 749.829242ms 749.835505ms 749.848816ms 749.862339ms 749.894177ms 749.89455ms 749.925215ms 749.92593ms 749.928627ms 749.947664ms 749.963003ms 749.973906ms 750.003904ms 750.126139ms 750.133794ms 750.207126ms 750.227047ms 750.274968ms 750.302884ms 750.345615ms 750.375465ms 750.393014ms 750.414145ms 750.419647ms 750.440232ms 750.456053ms 750.465759ms 750.487867ms 750.507292ms 750.53048ms 750.541015ms 750.560553ms 750.56637ms 750.686554ms 750.735888ms 750.807424ms 750.817251ms 750.870975ms 750.913684ms 750.953607ms 751.038907ms 751.261707ms 751.34182ms 751.399441ms 751.483543ms 751.643524ms 751.706435ms 751.754016ms 751.928439ms 751.933649ms 751.95159ms 752.222088ms 752.480219ms 752.871983ms 753.046447ms 753.123946ms 753.218427ms 753.617986ms 754.131734ms 754.195809ms 755.149884ms 755.587337ms 758.716477ms 759.029966ms 759.198496ms 760.682123ms 761.984742ms 763.565196ms 764.632083ms]
Sep 23 10:14:04.003: INFO: 50 %ile: 749.14149ms
Sep 23 10:14:04.003: INFO: 90 %ile: 751.933649ms
Sep 23 10:14:04.003: INFO: 99 %ile: 763.565196ms
Sep 23 10:14:04.003: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:14:04.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-834" for this suite.
Sep 23 10:14:24.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:14:24.107: INFO: namespace svc-latency-834 deletion completed in 20.094322779s

• [SLOW TEST:30.866 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:14:24.108: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 23 10:14:24.145: INFO: Waiting up to 5m0s for pod "pod-eab62e8d-ddea-11e9-93ab-0610dba1f5f1" in namespace "emptydir-9928" to be "success or failure"
Sep 23 10:14:24.147: INFO: Pod "pod-eab62e8d-ddea-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.314404ms
Sep 23 10:14:26.151: INFO: Pod "pod-eab62e8d-ddea-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006257343s
STEP: Saw pod success
Sep 23 10:14:26.151: INFO: Pod "pod-eab62e8d-ddea-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:14:26.154: INFO: Trying to get logs from node kube-node3 pod pod-eab62e8d-ddea-11e9-93ab-0610dba1f5f1 container test-container: <nil>
STEP: delete the pod
Sep 23 10:14:26.172: INFO: Waiting for pod pod-eab62e8d-ddea-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:14:26.178: INFO: Pod pod-eab62e8d-ddea-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:14:26.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9928" for this suite.
Sep 23 10:14:32.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:14:32.274: INFO: namespace emptydir-9928 deletion completed in 6.092820586s

• [SLOW TEST:8.167 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:14:32.274: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-ef9486d9-ddea-11e9-93ab-0610dba1f5f1
STEP: Creating configMap with name cm-test-opt-upd-ef948711-ddea-11e9-93ab-0610dba1f5f1
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-ef9486d9-ddea-11e9-93ab-0610dba1f5f1
STEP: Updating configmap cm-test-opt-upd-ef948711-ddea-11e9-93ab-0610dba1f5f1
STEP: Creating configMap with name cm-test-opt-create-ef948723-ddea-11e9-93ab-0610dba1f5f1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:15:54.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4599" for this suite.
Sep 23 10:16:16.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:16:16.860: INFO: namespace configmap-4599 deletion completed in 22.093814522s

• [SLOW TEST:104.586 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:16:16.860: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Sep 23 10:16:16.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 api-versions'
Sep 23 10:16:16.958: INFO: stderr: ""
Sep 23 10:16:16.958: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:16:16.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4494" for this suite.
Sep 23 10:16:22.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:16:23.051: INFO: namespace kubectl-4494 deletion completed in 6.08886209s

• [SLOW TEST:6.191 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:16:23.051: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-7419
Sep 23 10:16:25.104: INFO: Started pod liveness-http in namespace container-probe-7419
STEP: checking the pod's current state and verifying that restartCount is present
Sep 23 10:16:25.106: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:20:25.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7419" for this suite.
Sep 23 10:20:31.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:20:31.690: INFO: namespace container-probe-7419 deletion completed in 6.096550614s

• [SLOW TEST:248.639 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:20:31.691: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-c5cf81ff-ddeb-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume secrets
Sep 23 10:20:31.737: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c5d00850-ddeb-11e9-93ab-0610dba1f5f1" in namespace "projected-1551" to be "success or failure"
Sep 23 10:20:31.739: INFO: Pod "pod-projected-secrets-c5d00850-ddeb-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.527188ms
Sep 23 10:20:33.743: INFO: Pod "pod-projected-secrets-c5d00850-ddeb-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006077603s
Sep 23 10:20:35.746: INFO: Pod "pod-projected-secrets-c5d00850-ddeb-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009755149s
STEP: Saw pod success
Sep 23 10:20:35.746: INFO: Pod "pod-projected-secrets-c5d00850-ddeb-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:20:35.749: INFO: Trying to get logs from node kube-node3 pod pod-projected-secrets-c5d00850-ddeb-11e9-93ab-0610dba1f5f1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 23 10:20:35.768: INFO: Waiting for pod pod-projected-secrets-c5d00850-ddeb-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:20:35.770: INFO: Pod pod-projected-secrets-c5d00850-ddeb-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:20:35.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1551" for this suite.
Sep 23 10:20:41.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:20:41.862: INFO: namespace projected-1551 deletion completed in 6.088014456s

• [SLOW TEST:10.172 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:20:41.862: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-155
Sep 23 10:20:45.913: INFO: Started pod liveness-http in namespace container-probe-155
STEP: checking the pod's current state and verifying that restartCount is present
Sep 23 10:20:45.915: INFO: Initial restart count of pod liveness-http is 0
Sep 23 10:21:03.954: INFO: Restart count of pod container-probe-155/liveness-http is now 1 (18.038916421s elapsed)
Sep 23 10:21:23.991: INFO: Restart count of pod container-probe-155/liveness-http is now 2 (38.075666523s elapsed)
Sep 23 10:21:44.031: INFO: Restart count of pod container-probe-155/liveness-http is now 3 (58.116192284s elapsed)
Sep 23 10:22:04.069: INFO: Restart count of pod container-probe-155/liveness-http is now 4 (1m18.15416291s elapsed)
Sep 23 10:23:04.187: INFO: Restart count of pod container-probe-155/liveness-http is now 5 (2m18.272223002s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:23:04.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-155" for this suite.
Sep 23 10:23:10.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:23:10.302: INFO: namespace container-probe-155 deletion completed in 6.092744156s

• [SLOW TEST:148.440 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:23:10.303: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:23:12.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3668" for this suite.
Sep 23 10:23:52.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:23:52.460: INFO: namespace kubelet-test-3668 deletion completed in 40.090218819s

• [SLOW TEST:42.158 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:23:52.461: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Sep 23 10:23:52.494: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Sep 23 10:23:52.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 create -f - --namespace=kubectl-4339'
Sep 23 10:23:52.745: INFO: stderr: ""
Sep 23 10:23:52.745: INFO: stdout: "service/redis-slave created\n"
Sep 23 10:23:52.745: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Sep 23 10:23:52.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 create -f - --namespace=kubectl-4339'
Sep 23 10:23:52.899: INFO: stderr: ""
Sep 23 10:23:52.899: INFO: stdout: "service/redis-master created\n"
Sep 23 10:23:52.899: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep 23 10:23:52.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 create -f - --namespace=kubectl-4339'
Sep 23 10:23:53.116: INFO: stderr: ""
Sep 23 10:23:53.116: INFO: stdout: "service/frontend created\n"
Sep 23 10:23:53.116: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Sep 23 10:23:53.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 create -f - --namespace=kubectl-4339'
Sep 23 10:23:53.360: INFO: stderr: ""
Sep 23 10:23:53.360: INFO: stdout: "deployment.apps/frontend created\n"
Sep 23 10:23:53.360: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 23 10:23:53.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 create -f - --namespace=kubectl-4339'
Sep 23 10:23:53.575: INFO: stderr: ""
Sep 23 10:23:53.575: INFO: stdout: "deployment.apps/redis-master created\n"
Sep 23 10:23:53.576: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Sep 23 10:23:53.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 create -f - --namespace=kubectl-4339'
Sep 23 10:23:53.732: INFO: stderr: ""
Sep 23 10:23:53.732: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Sep 23 10:23:53.732: INFO: Waiting for all frontend pods to be Running.
Sep 23 10:23:58.783: INFO: Waiting for frontend to serve content.
Sep 23 10:23:58.798: INFO: Trying to add a new entry to the guestbook.
Sep 23 10:23:58.811: INFO: Verifying that added entry can be retrieved.
Sep 23 10:23:58.823: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep 23 10:24:03.838: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep 23 10:24:08.853: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep 23 10:24:13.867: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep 23 10:24:18.882: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep 23 10:24:23.896: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep 23 10:24:28.911: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep 23 10:24:33.928: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep 23 10:24:38.941: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep 23 10:24:43.956: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep 23 10:24:48.972: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Sep 23 10:24:53.986: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Sep 23 10:24:59.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 delete --grace-period=0 --force -f - --namespace=kubectl-4339'
Sep 23 10:24:59.115: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 10:24:59.115: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Sep 23 10:24:59.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 delete --grace-period=0 --force -f - --namespace=kubectl-4339'
Sep 23 10:24:59.210: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 10:24:59.210: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 23 10:24:59.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 delete --grace-period=0 --force -f - --namespace=kubectl-4339'
Sep 23 10:24:59.315: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 10:24:59.315: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 23 10:24:59.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 delete --grace-period=0 --force -f - --namespace=kubectl-4339'
Sep 23 10:24:59.410: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 10:24:59.410: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 23 10:24:59.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 delete --grace-period=0 --force -f - --namespace=kubectl-4339'
Sep 23 10:24:59.485: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 10:24:59.485: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 23 10:24:59.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 delete --grace-period=0 --force -f - --namespace=kubectl-4339'
Sep 23 10:24:59.569: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 10:24:59.569: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:24:59.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4339" for this suite.
Sep 23 10:25:45.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:25:45.676: INFO: namespace kubectl-4339 deletion completed in 46.102106889s

• [SLOW TEST:113.215 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:25:45.676: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 23 10:25:49.742: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 23 10:25:49.745: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 23 10:25:51.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 23 10:25:51.748: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 23 10:25:53.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 23 10:25:53.749: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 23 10:25:55.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 23 10:25:55.748: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 23 10:25:57.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 23 10:25:57.749: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 23 10:25:59.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 23 10:25:59.749: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 23 10:26:01.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 23 10:26:01.749: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 23 10:26:03.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 23 10:26:03.749: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 23 10:26:05.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 23 10:26:05.749: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 23 10:26:07.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 23 10:26:07.749: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 23 10:26:09.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 23 10:26:09.749: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 23 10:26:11.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 23 10:26:11.749: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:26:11.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3839" for this suite.
Sep 23 10:26:33.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:26:33.852: INFO: namespace container-lifecycle-hook-3839 deletion completed in 22.090199549s

• [SLOW TEST:48.177 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:26:33.853: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Sep 23 10:26:33.885: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-533696960 proxy --unix-socket=/tmp/kubectl-proxy-unix720384963/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:26:33.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4520" for this suite.
Sep 23 10:26:39.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:26:40.048: INFO: namespace kubectl-4520 deletion completed in 6.097249105s

• [SLOW TEST:6.195 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:26:40.048: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 10:26:40.079: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep 23 10:26:40.086: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 23 10:26:45.090: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 23 10:26:45.090: INFO: Creating deployment "test-rolling-update-deployment"
Sep 23 10:26:45.095: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep 23 10:26:45.100: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep 23 10:26:47.107: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep 23 10:26:47.110: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704831205, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704831205, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704831205, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704831205, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 10:26:49.114: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep 23 10:26:49.121: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-9145,SelfLink:/apis/apps/v1/namespaces/deployment-9145/deployments/test-rolling-update-deployment,UID:a45a5481-ddec-11e9-a6ed-0cda411df060,ResourceVersion:357975,Generation:1,CreationTimestamp:2019-09-23 10:26:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-23 10:26:45 +0000 UTC 2019-09-23 10:26:45 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-23 10:26:47 +0000 UTC 2019-09-23 10:26:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep 23 10:26:49.125: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-9145,SelfLink:/apis/apps/v1/namespaces/deployment-9145/replicasets/test-rolling-update-deployment-67599b4d9,UID:a45ca91a-ddec-11e9-a0c7-0cda411d3e75,ResourceVersion:357964,Generation:1,CreationTimestamp:2019-09-23 10:26:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment a45a5481-ddec-11e9-a6ed-0cda411df060 0xc002db2990 0xc002db2991}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep 23 10:26:49.125: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep 23 10:26:49.125: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-9145,SelfLink:/apis/apps/v1/namespaces/deployment-9145/replicasets/test-rolling-update-controller,UID:a15dc096-ddec-11e9-a6ed-0cda411df060,ResourceVersion:357973,Generation:2,CreationTimestamp:2019-09-23 10:26:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment a45a5481-ddec-11e9-a6ed-0cda411df060 0xc002db28c7 0xc002db28c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 23 10:26:49.128: INFO: Pod "test-rolling-update-deployment-67599b4d9-pqz5j" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-pqz5j,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-9145,SelfLink:/api/v1/namespaces/deployment-9145/pods/test-rolling-update-deployment-67599b4d9-pqz5j,UID:a45d23c7-ddec-11e9-a0c7-0cda411d3e75,ResourceVersion:357963,Generation:0,CreationTimestamp:2019-09-23 10:26:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.233.213/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 a45ca91a-ddec-11e9-a0c7-0cda411d3e75 0xc002db3470 0xc002db3471}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b8ngk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b8ngk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-b8ngk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db34e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db3500}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:26:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:26:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:26:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:26:45 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.105,PodIP:192.168.233.213,StartTime:2019-09-23 10:26:45 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-23 10:26:46 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker://sha256:e4423e943a205fe1d81768e60603c8f2c5821576bad0801c1e91b8ba586124a0 docker://93a29745f398563151778eb4f036d02ab23e22441ee5518e16d9e118ce8cc647}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:26:49.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9145" for this suite.
Sep 23 10:26:55.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:26:55.223: INFO: namespace deployment-9145 deletion completed in 6.090452192s

• [SLOW TEST:15.174 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:26:55.223: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep 23 10:26:55.265: INFO: Waiting up to 5m0s for pod "downward-api-aa69d86e-ddec-11e9-93ab-0610dba1f5f1" in namespace "downward-api-5377" to be "success or failure"
Sep 23 10:26:55.268: INFO: Pod "downward-api-aa69d86e-ddec-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.144646ms
Sep 23 10:26:57.272: INFO: Pod "downward-api-aa69d86e-ddec-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006994696s
STEP: Saw pod success
Sep 23 10:26:57.272: INFO: Pod "downward-api-aa69d86e-ddec-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:26:57.275: INFO: Trying to get logs from node kube-node3 pod downward-api-aa69d86e-ddec-11e9-93ab-0610dba1f5f1 container dapi-container: <nil>
STEP: delete the pod
Sep 23 10:26:57.293: INFO: Waiting for pod downward-api-aa69d86e-ddec-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:26:57.296: INFO: Pod downward-api-aa69d86e-ddec-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:26:57.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5377" for this suite.
Sep 23 10:27:03.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:27:03.393: INFO: namespace downward-api-5377 deletion completed in 6.092883725s

• [SLOW TEST:8.170 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:27:03.393: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:27:09.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1315" for this suite.
Sep 23 10:27:15.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:27:15.605: INFO: namespace namespaces-1315 deletion completed in 6.09068483s
STEP: Destroying namespace "nsdeletetest-5052" for this suite.
Sep 23 10:27:15.607: INFO: Namespace nsdeletetest-5052 was already deleted
STEP: Destroying namespace "nsdeletetest-7716" for this suite.
Sep 23 10:27:21.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:27:21.702: INFO: namespace nsdeletetest-7716 deletion completed in 6.095125162s

• [SLOW TEST:18.310 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:27:21.703: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 23 10:27:21.740: INFO: Waiting up to 5m0s for pod "pod-ba3197a9-ddec-11e9-93ab-0610dba1f5f1" in namespace "emptydir-6781" to be "success or failure"
Sep 23 10:27:21.742: INFO: Pod "pod-ba3197a9-ddec-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.769384ms
Sep 23 10:27:23.746: INFO: Pod "pod-ba3197a9-ddec-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006615675s
STEP: Saw pod success
Sep 23 10:27:23.746: INFO: Pod "pod-ba3197a9-ddec-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:27:23.749: INFO: Trying to get logs from node kube-node3 pod pod-ba3197a9-ddec-11e9-93ab-0610dba1f5f1 container test-container: <nil>
STEP: delete the pod
Sep 23 10:27:23.765: INFO: Waiting for pod pod-ba3197a9-ddec-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:27:23.773: INFO: Pod pod-ba3197a9-ddec-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:27:23.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6781" for this suite.
Sep 23 10:27:29.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:27:29.874: INFO: namespace emptydir-6781 deletion completed in 6.096169417s

• [SLOW TEST:8.171 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:27:29.874: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-8826/configmap-test-bf105a1e-ddec-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume configMaps
Sep 23 10:27:29.913: INFO: Waiting up to 5m0s for pod "pod-configmaps-bf10d69c-ddec-11e9-93ab-0610dba1f5f1" in namespace "configmap-8826" to be "success or failure"
Sep 23 10:27:29.917: INFO: Pod "pod-configmaps-bf10d69c-ddec-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.316448ms
Sep 23 10:27:31.925: INFO: Pod "pod-configmaps-bf10d69c-ddec-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011146537s
STEP: Saw pod success
Sep 23 10:27:31.925: INFO: Pod "pod-configmaps-bf10d69c-ddec-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:27:31.927: INFO: Trying to get logs from node kube-node3 pod pod-configmaps-bf10d69c-ddec-11e9-93ab-0610dba1f5f1 container env-test: <nil>
STEP: delete the pod
Sep 23 10:27:31.943: INFO: Waiting for pod pod-configmaps-bf10d69c-ddec-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:27:31.945: INFO: Pod pod-configmaps-bf10d69c-ddec-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:27:31.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8826" for this suite.
Sep 23 10:27:37.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:27:38.040: INFO: namespace configmap-8826 deletion completed in 6.091531935s

• [SLOW TEST:8.166 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:27:38.041: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 23 10:27:42.118: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 23 10:27:42.121: INFO: Pod pod-with-poststart-http-hook still exists
Sep 23 10:27:44.122: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 23 10:27:44.125: INFO: Pod pod-with-poststart-http-hook still exists
Sep 23 10:27:46.122: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 23 10:27:46.126: INFO: Pod pod-with-poststart-http-hook still exists
Sep 23 10:27:48.122: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 23 10:27:48.126: INFO: Pod pod-with-poststart-http-hook still exists
Sep 23 10:27:50.122: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 23 10:27:50.125: INFO: Pod pod-with-poststart-http-hook still exists
Sep 23 10:27:52.122: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 23 10:27:52.125: INFO: Pod pod-with-poststart-http-hook still exists
Sep 23 10:27:54.122: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 23 10:27:54.125: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:27:54.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2316" for this suite.
Sep 23 10:28:16.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:28:16.218: INFO: namespace container-lifecycle-hook-2316 deletion completed in 22.089041037s

• [SLOW TEST:38.178 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:28:16.219: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6661
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Sep 23 10:28:16.266: INFO: Found 0 stateful pods, waiting for 3
Sep 23 10:28:26.271: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 10:28:26.271: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 10:28:26.271: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep 23 10:28:26.296: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep 23 10:28:36.325: INFO: Updating stateful set ss2
Sep 23 10:28:36.334: INFO: Waiting for Pod statefulset-6661/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Sep 23 10:28:46.408: INFO: Found 2 stateful pods, waiting for 3
Sep 23 10:28:56.412: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 10:28:56.412: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 10:28:56.412: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep 23 10:28:56.434: INFO: Updating stateful set ss2
Sep 23 10:28:56.440: INFO: Waiting for Pod statefulset-6661/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Sep 23 10:29:06.464: INFO: Updating stateful set ss2
Sep 23 10:29:06.470: INFO: Waiting for StatefulSet statefulset-6661/ss2 to complete update
Sep 23 10:29:06.470: INFO: Waiting for Pod statefulset-6661/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep 23 10:29:16.476: INFO: Deleting all statefulset in ns statefulset-6661
Sep 23 10:29:16.479: INFO: Scaling statefulset ss2 to 0
Sep 23 10:29:36.492: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 10:29:36.495: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:29:36.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6661" for this suite.
Sep 23 10:29:42.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:29:42.602: INFO: namespace statefulset-6661 deletion completed in 6.09249949s

• [SLOW TEST:86.384 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:29:42.603: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 10:29:42.634: INFO: Creating ReplicaSet my-hostname-basic-0e2d4f0b-dded-11e9-93ab-0610dba1f5f1
Sep 23 10:29:42.641: INFO: Pod name my-hostname-basic-0e2d4f0b-dded-11e9-93ab-0610dba1f5f1: Found 0 pods out of 1
Sep 23 10:29:47.645: INFO: Pod name my-hostname-basic-0e2d4f0b-dded-11e9-93ab-0610dba1f5f1: Found 1 pods out of 1
Sep 23 10:29:47.645: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0e2d4f0b-dded-11e9-93ab-0610dba1f5f1" is running
Sep 23 10:29:47.648: INFO: Pod "my-hostname-basic-0e2d4f0b-dded-11e9-93ab-0610dba1f5f1-2fmm9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-23 10:29:42 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-23 10:29:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-23 10:29:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-23 10:29:42 +0000 UTC Reason: Message:}])
Sep 23 10:29:47.648: INFO: Trying to dial the pod
Sep 23 10:29:52.662: INFO: Controller my-hostname-basic-0e2d4f0b-dded-11e9-93ab-0610dba1f5f1: Got expected result from replica 1 [my-hostname-basic-0e2d4f0b-dded-11e9-93ab-0610dba1f5f1-2fmm9]: "my-hostname-basic-0e2d4f0b-dded-11e9-93ab-0610dba1f5f1-2fmm9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:29:52.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8568" for this suite.
Sep 23 10:29:58.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:29:58.752: INFO: namespace replicaset-8568 deletion completed in 6.085670591s

• [SLOW TEST:16.149 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:29:58.752: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Sep 23 10:29:58.783: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-533696960 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:29:58.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3135" for this suite.
Sep 23 10:30:04.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:30:04.943: INFO: namespace kubectl-3135 deletion completed in 6.089023913s

• [SLOW TEST:6.191 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:30:04.943: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep 23 10:30:06.994: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-1b7e7968-dded-11e9-93ab-0610dba1f5f1,GenerateName:,Namespace:events-7858,SelfLink:/api/v1/namespaces/events-7858/pods/send-events-1b7e7968-dded-11e9-93ab-0610dba1f5f1,UID:1b7ee96f-dded-11e9-a6ed-0cda411df060,ResourceVersion:358951,Generation:0,CreationTimestamp:2019-09-23 10:30:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 977192459,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.119.169/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nt4sw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nt4sw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-nt4sw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030852a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030852c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:30:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:30:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:30:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:30:04 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.106,PodIP:192.168.119.169,StartTime:2019-09-23 10:30:04 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-09-23 10:30:05 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker://sha256:c6b8a28d5611cf83d297661ddd7f40672d286eafd7eb3852267e634e8eee0948 docker://fb7f70a20c6f5a8ff39c32ea7580d44a02c4b1891e378f0eabc4e1f011c8119f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Sep 23 10:30:08.998: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep 23 10:30:11.001: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:30:11.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7858" for this suite.
Sep 23 10:30:53.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:30:53.108: INFO: namespace events-7858 deletion completed in 42.097191969s

• [SLOW TEST:48.165 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:30:53.108: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Sep 23 10:30:53.144: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 23 10:30:53.152: INFO: Waiting for terminating namespaces to be deleted...
Sep 23 10:30:53.154: INFO: 
Logging pods the kubelet thinks is on node kube-node1 before test
Sep 23 10:30:53.161: INFO: coredns-fb8b8dccf-8hk76 from kube-system started at 2019-09-21 10:06:33 +0000 UTC (1 container statuses recorded)
Sep 23 10:30:53.162: INFO: 	Container coredns ready: true, restart count 1
Sep 23 10:30:53.162: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-23 09:39:57 +0000 UTC (1 container statuses recorded)
Sep 23 10:30:53.162: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 23 10:30:53.162: INFO: sonobuoy-systemd-logs-daemon-set-cd4257a81b874ce2-czgzw from heptio-sonobuoy started at 2019-09-23 09:40:00 +0000 UTC (2 container statuses recorded)
Sep 23 10:30:53.162: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Sep 23 10:30:53.162: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 10:30:53.162: INFO: kube-proxy-6ffbt from kube-system started at 2019-09-21 09:49:32 +0000 UTC (1 container statuses recorded)
Sep 23 10:30:53.162: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 23 10:30:53.162: INFO: calico-node-6w4gz from kube-system started at 2019-09-21 10:06:27 +0000 UTC (1 container statuses recorded)
Sep 23 10:30:53.162: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 10:30:53.162: INFO: 
Logging pods the kubelet thinks is on node kube-node2 before test
Sep 23 10:30:53.169: INFO: sonobuoy-systemd-logs-daemon-set-cd4257a81b874ce2-d2bcl from heptio-sonobuoy started at 2019-09-23 09:40:00 +0000 UTC (2 container statuses recorded)
Sep 23 10:30:53.169: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Sep 23 10:30:53.169: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 10:30:53.169: INFO: calico-node-6xs6x from kube-system started at 2019-09-21 10:06:27 +0000 UTC (1 container statuses recorded)
Sep 23 10:30:53.169: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 10:30:53.169: INFO: coredns-fb8b8dccf-zjxw7 from kube-system started at 2019-09-21 10:06:33 +0000 UTC (1 container statuses recorded)
Sep 23 10:30:53.169: INFO: 	Container coredns ready: true, restart count 1
Sep 23 10:30:53.169: INFO: sonobuoy-e2e-job-8f4e803f5fd84ff6 from heptio-sonobuoy started at 2019-09-23 09:40:00 +0000 UTC (2 container statuses recorded)
Sep 23 10:30:53.169: INFO: 	Container e2e ready: true, restart count 0
Sep 23 10:30:53.169: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 10:30:53.169: INFO: kube-proxy-fztxb from kube-system started at 2019-09-21 09:49:42 +0000 UTC (1 container statuses recorded)
Sep 23 10:30:53.169: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 23 10:30:53.169: INFO: 
Logging pods the kubelet thinks is on node kube-node3 before test
Sep 23 10:30:53.174: INFO: kube-proxy-4cj6r from kube-system started at 2019-09-21 09:49:51 +0000 UTC (1 container statuses recorded)
Sep 23 10:30:53.174: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 23 10:30:53.174: INFO: calico-node-bmhnd from kube-system started at 2019-09-21 10:06:27 +0000 UTC (1 container statuses recorded)
Sep 23 10:30:53.174: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 10:30:53.174: INFO: calico-kube-controllers-f6ff9cbbb-m8s47 from kube-system started at 2019-09-21 10:06:36 +0000 UTC (1 container statuses recorded)
Sep 23 10:30:53.174: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep 23 10:30:53.174: INFO: sonobuoy-systemd-logs-daemon-set-cd4257a81b874ce2-kmzgk from heptio-sonobuoy started at 2019-09-23 09:40:00 +0000 UTC (2 container statuses recorded)
Sep 23 10:30:53.174: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Sep 23 10:30:53.174: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node kube-node1
STEP: verifying the node has the label node kube-node2
STEP: verifying the node has the label node kube-node3
Sep 23 10:30:53.240: INFO: Pod sonobuoy requesting resource cpu=0m on Node kube-node1
Sep 23 10:30:53.240: INFO: Pod sonobuoy-e2e-job-8f4e803f5fd84ff6 requesting resource cpu=0m on Node kube-node2
Sep 23 10:30:53.240: INFO: Pod sonobuoy-systemd-logs-daemon-set-cd4257a81b874ce2-czgzw requesting resource cpu=0m on Node kube-node1
Sep 23 10:30:53.240: INFO: Pod sonobuoy-systemd-logs-daemon-set-cd4257a81b874ce2-d2bcl requesting resource cpu=0m on Node kube-node2
Sep 23 10:30:53.240: INFO: Pod sonobuoy-systemd-logs-daemon-set-cd4257a81b874ce2-kmzgk requesting resource cpu=0m on Node kube-node3
Sep 23 10:30:53.240: INFO: Pod calico-kube-controllers-f6ff9cbbb-m8s47 requesting resource cpu=0m on Node kube-node3
Sep 23 10:30:53.240: INFO: Pod calico-node-6w4gz requesting resource cpu=250m on Node kube-node1
Sep 23 10:30:53.240: INFO: Pod calico-node-6xs6x requesting resource cpu=250m on Node kube-node2
Sep 23 10:30:53.240: INFO: Pod calico-node-bmhnd requesting resource cpu=250m on Node kube-node3
Sep 23 10:30:53.240: INFO: Pod coredns-fb8b8dccf-8hk76 requesting resource cpu=100m on Node kube-node1
Sep 23 10:30:53.240: INFO: Pod coredns-fb8b8dccf-zjxw7 requesting resource cpu=100m on Node kube-node2
Sep 23 10:30:53.240: INFO: Pod kube-proxy-4cj6r requesting resource cpu=0m on Node kube-node3
Sep 23 10:30:53.240: INFO: Pod kube-proxy-6ffbt requesting resource cpu=0m on Node kube-node1
Sep 23 10:30:53.240: INFO: Pod kube-proxy-fztxb requesting resource cpu=0m on Node kube-node2
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3842ef8b-dded-11e9-93ab-0610dba1f5f1.15c70a9a5c43a5ca], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1984/filler-pod-3842ef8b-dded-11e9-93ab-0610dba1f5f1 to kube-node1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3842ef8b-dded-11e9-93ab-0610dba1f5f1.15c70a9a8e935e62], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3842ef8b-dded-11e9-93ab-0610dba1f5f1.15c70a9a903de2b5], Reason = [Created], Message = [Created container filler-pod-3842ef8b-dded-11e9-93ab-0610dba1f5f1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3842ef8b-dded-11e9-93ab-0610dba1f5f1.15c70a9a9b1ef6c7], Reason = [Started], Message = [Started container filler-pod-3842ef8b-dded-11e9-93ab-0610dba1f5f1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3843d2d8-dded-11e9-93ab-0610dba1f5f1.15c70a9a5ca72923], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1984/filler-pod-3843d2d8-dded-11e9-93ab-0610dba1f5f1 to kube-node2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3843d2d8-dded-11e9-93ab-0610dba1f5f1.15c70a9a8f0507af], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3843d2d8-dded-11e9-93ab-0610dba1f5f1.15c70a9a9080c4c2], Reason = [Created], Message = [Created container filler-pod-3843d2d8-dded-11e9-93ab-0610dba1f5f1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3843d2d8-dded-11e9-93ab-0610dba1f5f1.15c70a9a9cb3c137], Reason = [Started], Message = [Started container filler-pod-3843d2d8-dded-11e9-93ab-0610dba1f5f1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3844dc90-dded-11e9-93ab-0610dba1f5f1.15c70a9a5d5f07da], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1984/filler-pod-3844dc90-dded-11e9-93ab-0610dba1f5f1 to kube-node3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3844dc90-dded-11e9-93ab-0610dba1f5f1.15c70a9a8eee8632], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3844dc90-dded-11e9-93ab-0610dba1f5f1.15c70a9a90af00c2], Reason = [Created], Message = [Created container filler-pod-3844dc90-dded-11e9-93ab-0610dba1f5f1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3844dc90-dded-11e9-93ab-0610dba1f5f1.15c70a9a9c07af02], Reason = [Started], Message = [Started container filler-pod-3844dc90-dded-11e9-93ab-0610dba1f5f1]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15c70a9b4cb8f8c0], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node kube-node1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node kube-node2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node kube-node3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:30:58.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1984" for this suite.
Sep 23 10:31:04.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:31:04.438: INFO: namespace sched-pred-1984 deletion completed in 6.094609619s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:11.330 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:31:04.438: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep 23 10:31:04.481: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9800,SelfLink:/api/v1/namespaces/watch-9800/configmaps/e2e-watch-test-watch-closed,UID:3ef4e522-dded-11e9-a6ed-0cda411df060,ResourceVersion:359176,Generation:0,CreationTimestamp:2019-09-23 10:31:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 23 10:31:04.481: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9800,SelfLink:/api/v1/namespaces/watch-9800/configmaps/e2e-watch-test-watch-closed,UID:3ef4e522-dded-11e9-a6ed-0cda411df060,ResourceVersion:359177,Generation:0,CreationTimestamp:2019-09-23 10:31:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep 23 10:31:04.493: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9800,SelfLink:/api/v1/namespaces/watch-9800/configmaps/e2e-watch-test-watch-closed,UID:3ef4e522-dded-11e9-a6ed-0cda411df060,ResourceVersion:359178,Generation:0,CreationTimestamp:2019-09-23 10:31:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 23 10:31:04.493: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9800,SelfLink:/api/v1/namespaces/watch-9800/configmaps/e2e-watch-test-watch-closed,UID:3ef4e522-dded-11e9-a6ed-0cda411df060,ResourceVersion:359179,Generation:0,CreationTimestamp:2019-09-23 10:31:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:31:04.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9800" for this suite.
Sep 23 10:31:10.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:31:10.593: INFO: namespace watch-9800 deletion completed in 6.095338327s

• [SLOW TEST:6.155 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:31:10.593: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 23 10:31:13.161: INFO: Successfully updated pod "pod-update-activedeadlineseconds-42a093e7-dded-11e9-93ab-0610dba1f5f1"
Sep 23 10:31:13.161: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-42a093e7-dded-11e9-93ab-0610dba1f5f1" in namespace "pods-2857" to be "terminated due to deadline exceeded"
Sep 23 10:31:13.164: INFO: Pod "pod-update-activedeadlineseconds-42a093e7-dded-11e9-93ab-0610dba1f5f1": Phase="Running", Reason="", readiness=true. Elapsed: 3.197385ms
Sep 23 10:31:15.168: INFO: Pod "pod-update-activedeadlineseconds-42a093e7-dded-11e9-93ab-0610dba1f5f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007233201s
Sep 23 10:31:17.172: INFO: Pod "pod-update-activedeadlineseconds-42a093e7-dded-11e9-93ab-0610dba1f5f1": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.011242222s
Sep 23 10:31:17.172: INFO: Pod "pod-update-activedeadlineseconds-42a093e7-dded-11e9-93ab-0610dba1f5f1" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:31:17.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2857" for this suite.
Sep 23 10:31:23.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:31:23.278: INFO: namespace pods-2857 deletion completed in 6.101521067s

• [SLOW TEST:12.685 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:31:23.279: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 23 10:31:23.327: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a301cd3-dded-11e9-93ab-0610dba1f5f1" in namespace "downward-api-4341" to be "success or failure"
Sep 23 10:31:23.330: INFO: Pod "downwardapi-volume-4a301cd3-dded-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.336097ms
Sep 23 10:31:25.334: INFO: Pod "downwardapi-volume-4a301cd3-dded-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007153726s
STEP: Saw pod success
Sep 23 10:31:25.334: INFO: Pod "downwardapi-volume-4a301cd3-dded-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:31:25.337: INFO: Trying to get logs from node kube-node3 pod downwardapi-volume-4a301cd3-dded-11e9-93ab-0610dba1f5f1 container client-container: <nil>
STEP: delete the pod
Sep 23 10:31:25.356: INFO: Waiting for pod downwardapi-volume-4a301cd3-dded-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:31:25.359: INFO: Pod downwardapi-volume-4a301cd3-dded-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:31:25.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4341" for this suite.
Sep 23 10:31:31.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:31:31.462: INFO: namespace downward-api-4341 deletion completed in 6.099260299s

• [SLOW TEST:8.184 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:31:31.462: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 10:31:31.504: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep 23 10:31:36.509: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 23 10:31:36.509: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep 23 10:31:36.526: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-3102,SelfLink:/apis/apps/v1/namespaces/deployment-3102/deployments/test-cleanup-deployment,UID:520de501-dded-11e9-a6ed-0cda411df060,ResourceVersion:359335,Generation:1,CreationTimestamp:2019-09-23 10:31:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Sep 23 10:31:36.530: INFO: New ReplicaSet "test-cleanup-deployment-55cbfbc8f5" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5,GenerateName:,Namespace:deployment-3102,SelfLink:/apis/apps/v1/namespaces/deployment-3102/replicasets/test-cleanup-deployment-55cbfbc8f5,UID:520faed7-dded-11e9-a0c7-0cda411d3e75,ResourceVersion:359337,Generation:1,CreationTimestamp:2019-09-23 10:31:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 520de501-dded-11e9-a6ed-0cda411df060 0xc002884df7 0xc002884df8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 23 10:31:36.530: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep 23 10:31:36.530: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-3102,SelfLink:/apis/apps/v1/namespaces/deployment-3102/replicasets/test-cleanup-controller,UID:4f1049cd-dded-11e9-a6ed-0cda411df060,ResourceVersion:359336,Generation:1,CreationTimestamp:2019-09-23 10:31:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 520de501-dded-11e9-a6ed-0cda411df060 0xc002884d27 0xc002884d28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep 23 10:31:36.534: INFO: Pod "test-cleanup-controller-mr28p" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-mr28p,GenerateName:test-cleanup-controller-,Namespace:deployment-3102,SelfLink:/api/v1/namespaces/deployment-3102/pods/test-cleanup-controller-mr28p,UID:4f11ac1c-dded-11e9-a0c7-0cda411d3e75,ResourceVersion:359325,Generation:0,CreationTimestamp:2019-09-23 10:31:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.119.173/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 4f1049cd-dded-11e9-a6ed-0cda411df060 0xc002885787 0xc002885788}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cxbjm {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cxbjm,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cxbjm true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002885800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002885820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:31:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:31:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:31:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:31:31 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.106,PodIP:192.168.119.173,StartTime:2019-09-23 10:31:31 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-23 10:31:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:8a2fb25a19f5dc1528b7a3fabe8b3145ff57fe10e4f1edac6c718a3cf4aa4b73 docker://a345fefbb77488248200808f5c7e93c181b1c8596a203d20f21cc9b5abd1172e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 10:31:36.534: INFO: Pod "test-cleanup-deployment-55cbfbc8f5-xhlgz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5-xhlgz,GenerateName:test-cleanup-deployment-55cbfbc8f5-,Namespace:deployment-3102,SelfLink:/api/v1/namespaces/deployment-3102/pods/test-cleanup-deployment-55cbfbc8f5-xhlgz,UID:52102e8d-dded-11e9-a0c7-0cda411d3e75,ResourceVersion:359338,Generation:0,CreationTimestamp:2019-09-23 10:31:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55cbfbc8f5 520faed7-dded-11e9-a0c7-0cda411d3e75 0xc002885907 0xc002885908}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cxbjm {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cxbjm,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-cxbjm true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002885980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028859a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:31:36.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3102" for this suite.
Sep 23 10:31:42.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:31:42.639: INFO: namespace deployment-3102 deletion completed in 6.09886046s

• [SLOW TEST:11.177 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:31:42.639: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-55ba5688-dded-11e9-93ab-0610dba1f5f1
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:31:42.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8842" for this suite.
Sep 23 10:31:48.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:31:48.775: INFO: namespace configmap-8842 deletion completed in 6.092373713s

• [SLOW TEST:6.136 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:31:48.775: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Sep 23 10:31:48.815: INFO: Waiting up to 5m0s for pod "client-containers-5961ef6d-dded-11e9-93ab-0610dba1f5f1" in namespace "containers-8422" to be "success or failure"
Sep 23 10:31:48.817: INFO: Pod "client-containers-5961ef6d-dded-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.781121ms
Sep 23 10:31:50.821: INFO: Pod "client-containers-5961ef6d-dded-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006812301s
Sep 23 10:31:52.825: INFO: Pod "client-containers-5961ef6d-dded-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010500027s
STEP: Saw pod success
Sep 23 10:31:52.825: INFO: Pod "client-containers-5961ef6d-dded-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:31:52.828: INFO: Trying to get logs from node kube-node3 pod client-containers-5961ef6d-dded-11e9-93ab-0610dba1f5f1 container test-container: <nil>
STEP: delete the pod
Sep 23 10:31:52.844: INFO: Waiting for pod client-containers-5961ef6d-dded-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:31:52.848: INFO: Pod client-containers-5961ef6d-dded-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:31:52.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8422" for this suite.
Sep 23 10:31:58.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:31:58.943: INFO: namespace containers-8422 deletion completed in 6.090392047s

• [SLOW TEST:10.167 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:31:58.943: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-5f7127a9-dded-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume configMaps
Sep 23 10:31:58.984: INFO: Waiting up to 5m0s for pod "pod-configmaps-5f71ad9c-dded-11e9-93ab-0610dba1f5f1" in namespace "configmap-7169" to be "success or failure"
Sep 23 10:31:58.988: INFO: Pod "pod-configmaps-5f71ad9c-dded-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.127365ms
Sep 23 10:32:00.991: INFO: Pod "pod-configmaps-5f71ad9c-dded-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007820646s
Sep 23 10:32:02.995: INFO: Pod "pod-configmaps-5f71ad9c-dded-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011656508s
STEP: Saw pod success
Sep 23 10:32:02.995: INFO: Pod "pod-configmaps-5f71ad9c-dded-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:32:02.998: INFO: Trying to get logs from node kube-node3 pod pod-configmaps-5f71ad9c-dded-11e9-93ab-0610dba1f5f1 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 23 10:32:03.018: INFO: Waiting for pod pod-configmaps-5f71ad9c-dded-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:32:03.021: INFO: Pod pod-configmaps-5f71ad9c-dded-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:32:03.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7169" for this suite.
Sep 23 10:32:09.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:32:09.114: INFO: namespace configmap-7169 deletion completed in 6.089200617s

• [SLOW TEST:10.171 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:32:09.115: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-65815a8d-dded-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume secrets
Sep 23 10:32:09.158: INFO: Waiting up to 5m0s for pod "pod-secrets-6581d50d-dded-11e9-93ab-0610dba1f5f1" in namespace "secrets-8610" to be "success or failure"
Sep 23 10:32:09.164: INFO: Pod "pod-secrets-6581d50d-dded-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.609553ms
Sep 23 10:32:11.168: INFO: Pod "pod-secrets-6581d50d-dded-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009427775s
Sep 23 10:32:13.172: INFO: Pod "pod-secrets-6581d50d-dded-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013530923s
STEP: Saw pod success
Sep 23 10:32:13.172: INFO: Pod "pod-secrets-6581d50d-dded-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:32:13.175: INFO: Trying to get logs from node kube-node3 pod pod-secrets-6581d50d-dded-11e9-93ab-0610dba1f5f1 container secret-volume-test: <nil>
STEP: delete the pod
Sep 23 10:32:13.213: INFO: Waiting for pod pod-secrets-6581d50d-dded-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:32:13.216: INFO: Pod pod-secrets-6581d50d-dded-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:32:13.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8610" for this suite.
Sep 23 10:32:19.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:32:19.319: INFO: namespace secrets-8610 deletion completed in 6.098640453s

• [SLOW TEST:10.204 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:32:19.319: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-4618
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 23 10:32:19.350: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 23 10:32:37.422: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.119.177:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4618 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 10:32:37.422: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 10:32:37.615: INFO: Found all expected endpoints: [netserver-0]
Sep 23 10:32:37.618: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.233.217:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4618 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 10:32:37.618: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 10:32:37.824: INFO: Found all expected endpoints: [netserver-1]
Sep 23 10:32:37.828: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.9.96:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4618 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 10:32:37.828: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 10:32:38.025: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:32:38.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4618" for this suite.
Sep 23 10:33:00.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:33:00.121: INFO: namespace pod-network-test-4618 deletion completed in 22.090978782s

• [SLOW TEST:40.802 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:33:00.121: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-wdpq
STEP: Creating a pod to test atomic-volume-subpath
Sep 23 10:33:00.169: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-wdpq" in namespace "subpath-6010" to be "success or failure"
Sep 23 10:33:00.171: INFO: Pod "pod-subpath-test-secret-wdpq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.406205ms
Sep 23 10:33:02.175: INFO: Pod "pod-subpath-test-secret-wdpq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006333374s
Sep 23 10:33:04.179: INFO: Pod "pod-subpath-test-secret-wdpq": Phase="Running", Reason="", readiness=true. Elapsed: 4.010143647s
Sep 23 10:33:06.183: INFO: Pod "pod-subpath-test-secret-wdpq": Phase="Running", Reason="", readiness=true. Elapsed: 6.014318942s
Sep 23 10:33:08.187: INFO: Pod "pod-subpath-test-secret-wdpq": Phase="Running", Reason="", readiness=true. Elapsed: 8.018178781s
Sep 23 10:33:10.191: INFO: Pod "pod-subpath-test-secret-wdpq": Phase="Running", Reason="", readiness=true. Elapsed: 10.022137152s
Sep 23 10:33:12.194: INFO: Pod "pod-subpath-test-secret-wdpq": Phase="Running", Reason="", readiness=true. Elapsed: 12.025813737s
Sep 23 10:33:14.199: INFO: Pod "pod-subpath-test-secret-wdpq": Phase="Running", Reason="", readiness=true. Elapsed: 14.030052905s
Sep 23 10:33:16.203: INFO: Pod "pod-subpath-test-secret-wdpq": Phase="Running", Reason="", readiness=true. Elapsed: 16.034683438s
Sep 23 10:33:18.208: INFO: Pod "pod-subpath-test-secret-wdpq": Phase="Running", Reason="", readiness=true. Elapsed: 18.039297062s
Sep 23 10:33:20.212: INFO: Pod "pod-subpath-test-secret-wdpq": Phase="Running", Reason="", readiness=true. Elapsed: 20.042887837s
Sep 23 10:33:22.216: INFO: Pod "pod-subpath-test-secret-wdpq": Phase="Running", Reason="", readiness=true. Elapsed: 22.046993214s
Sep 23 10:33:24.219: INFO: Pod "pod-subpath-test-secret-wdpq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.050430825s
STEP: Saw pod success
Sep 23 10:33:24.219: INFO: Pod "pod-subpath-test-secret-wdpq" satisfied condition "success or failure"
Sep 23 10:33:24.222: INFO: Trying to get logs from node kube-node3 pod pod-subpath-test-secret-wdpq container test-container-subpath-secret-wdpq: <nil>
STEP: delete the pod
Sep 23 10:33:24.241: INFO: Waiting for pod pod-subpath-test-secret-wdpq to disappear
Sep 23 10:33:24.247: INFO: Pod pod-subpath-test-secret-wdpq no longer exists
STEP: Deleting pod pod-subpath-test-secret-wdpq
Sep 23 10:33:24.247: INFO: Deleting pod "pod-subpath-test-secret-wdpq" in namespace "subpath-6010"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:33:24.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6010" for this suite.
Sep 23 10:33:30.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:33:30.350: INFO: namespace subpath-6010 deletion completed in 6.096985096s

• [SLOW TEST:30.229 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:33:30.350: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 10:33:30.433: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:33:31.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5364" for this suite.
Sep 23 10:33:37.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:33:37.570: INFO: namespace custom-resource-definition-5364 deletion completed in 6.09402277s

• [SLOW TEST:7.220 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:33:37.571: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-3004/secret-test-9a3b8bab-dded-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume secrets
Sep 23 10:33:37.621: INFO: Waiting up to 5m0s for pod "pod-configmaps-9a3c041a-dded-11e9-93ab-0610dba1f5f1" in namespace "secrets-3004" to be "success or failure"
Sep 23 10:33:37.623: INFO: Pod "pod-configmaps-9a3c041a-dded-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.725592ms
Sep 23 10:33:39.627: INFO: Pod "pod-configmaps-9a3c041a-dded-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006677624s
Sep 23 10:33:41.631: INFO: Pod "pod-configmaps-9a3c041a-dded-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010081509s
STEP: Saw pod success
Sep 23 10:33:41.631: INFO: Pod "pod-configmaps-9a3c041a-dded-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:33:41.633: INFO: Trying to get logs from node kube-node3 pod pod-configmaps-9a3c041a-dded-11e9-93ab-0610dba1f5f1 container env-test: <nil>
STEP: delete the pod
Sep 23 10:33:41.650: INFO: Waiting for pod pod-configmaps-9a3c041a-dded-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:33:41.653: INFO: Pod pod-configmaps-9a3c041a-dded-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:33:41.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3004" for this suite.
Sep 23 10:33:47.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:33:47.761: INFO: namespace secrets-3004 deletion completed in 6.103939764s

• [SLOW TEST:10.190 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:33:47.761: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 23 10:33:47.821: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a04f3b30-dded-11e9-93ab-0610dba1f5f1" in namespace "projected-5988" to be "success or failure"
Sep 23 10:33:47.826: INFO: Pod "downwardapi-volume-a04f3b30-dded-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.957756ms
Sep 23 10:33:49.830: INFO: Pod "downwardapi-volume-a04f3b30-dded-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008959279s
STEP: Saw pod success
Sep 23 10:33:49.830: INFO: Pod "downwardapi-volume-a04f3b30-dded-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:33:49.833: INFO: Trying to get logs from node kube-node2 pod downwardapi-volume-a04f3b30-dded-11e9-93ab-0610dba1f5f1 container client-container: <nil>
STEP: delete the pod
Sep 23 10:33:49.850: INFO: Waiting for pod downwardapi-volume-a04f3b30-dded-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:33:49.853: INFO: Pod downwardapi-volume-a04f3b30-dded-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:33:49.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5988" for this suite.
Sep 23 10:33:55.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:33:55.959: INFO: namespace projected-5988 deletion completed in 6.101901285s

• [SLOW TEST:8.198 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:33:55.959: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-a53121df-dded-11e9-93ab-0610dba1f5f1
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-a53121df-dded-11e9-93ab-0610dba1f5f1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:34:00.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2693" for this suite.
Sep 23 10:34:22.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:34:22.140: INFO: namespace projected-2693 deletion completed in 22.095281958s

• [SLOW TEST:26.181 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:34:22.140: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 23 10:34:22.185: INFO: Waiting up to 5m0s for pod "pod-b4cc5aa8-dded-11e9-93ab-0610dba1f5f1" in namespace "emptydir-3187" to be "success or failure"
Sep 23 10:34:22.193: INFO: Pod "pod-b4cc5aa8-dded-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.973388ms
Sep 23 10:34:24.197: INFO: Pod "pod-b4cc5aa8-dded-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011990763s
STEP: Saw pod success
Sep 23 10:34:24.197: INFO: Pod "pod-b4cc5aa8-dded-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:34:24.200: INFO: Trying to get logs from node kube-node3 pod pod-b4cc5aa8-dded-11e9-93ab-0610dba1f5f1 container test-container: <nil>
STEP: delete the pod
Sep 23 10:34:24.216: INFO: Waiting for pod pod-b4cc5aa8-dded-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:34:24.221: INFO: Pod pod-b4cc5aa8-dded-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:34:24.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3187" for this suite.
Sep 23 10:34:30.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:34:30.314: INFO: namespace emptydir-3187 deletion completed in 6.089268293s

• [SLOW TEST:8.174 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:34:30.315: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 23 10:34:30.355: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9ab230a-dded-11e9-93ab-0610dba1f5f1" in namespace "downward-api-4088" to be "success or failure"
Sep 23 10:34:30.362: INFO: Pod "downwardapi-volume-b9ab230a-dded-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.485808ms
Sep 23 10:34:32.365: INFO: Pod "downwardapi-volume-b9ab230a-dded-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010151447s
STEP: Saw pod success
Sep 23 10:34:32.366: INFO: Pod "downwardapi-volume-b9ab230a-dded-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:34:32.368: INFO: Trying to get logs from node kube-node1 pod downwardapi-volume-b9ab230a-dded-11e9-93ab-0610dba1f5f1 container client-container: <nil>
STEP: delete the pod
Sep 23 10:34:32.385: INFO: Waiting for pod downwardapi-volume-b9ab230a-dded-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:34:32.387: INFO: Pod downwardapi-volume-b9ab230a-dded-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:34:32.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4088" for this suite.
Sep 23 10:34:38.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:34:38.489: INFO: namespace downward-api-4088 deletion completed in 6.097907792s

• [SLOW TEST:8.174 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:34:38.489: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Sep 23 10:34:38.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 create -f - --namespace=kubectl-9841'
Sep 23 10:34:38.767: INFO: stderr: ""
Sep 23 10:34:38.767: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 23 10:34:39.771: INFO: Selector matched 1 pods for map[app:redis]
Sep 23 10:34:39.772: INFO: Found 0 / 1
Sep 23 10:34:40.771: INFO: Selector matched 1 pods for map[app:redis]
Sep 23 10:34:40.771: INFO: Found 1 / 1
Sep 23 10:34:40.771: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep 23 10:34:40.774: INFO: Selector matched 1 pods for map[app:redis]
Sep 23 10:34:40.774: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 23 10:34:40.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 patch pod redis-master-22l6s --namespace=kubectl-9841 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep 23 10:34:40.865: INFO: stderr: ""
Sep 23 10:34:40.865: INFO: stdout: "pod/redis-master-22l6s patched\n"
STEP: checking annotations
Sep 23 10:34:40.868: INFO: Selector matched 1 pods for map[app:redis]
Sep 23 10:34:40.868: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:34:40.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9841" for this suite.
Sep 23 10:35:02.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:35:02.982: INFO: namespace kubectl-9841 deletion completed in 22.107358203s

• [SLOW TEST:24.493 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:35:02.982: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 10:35:29.037: INFO: Container started at 2019-09-23 10:35:04 +0000 UTC, pod became ready at 2019-09-23 10:35:27 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:35:29.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9143" for this suite.
Sep 23 10:35:51.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:35:51.139: INFO: namespace container-probe-9143 deletion completed in 22.098402457s

• [SLOW TEST:48.157 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:35:51.139: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 10:35:53.222: INFO: Waiting up to 5m0s for pod "client-envvars-eb0d3d0a-dded-11e9-93ab-0610dba1f5f1" in namespace "pods-9558" to be "success or failure"
Sep 23 10:35:53.227: INFO: Pod "client-envvars-eb0d3d0a-dded-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.622004ms
Sep 23 10:35:55.231: INFO: Pod "client-envvars-eb0d3d0a-dded-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00969917s
STEP: Saw pod success
Sep 23 10:35:55.231: INFO: Pod "client-envvars-eb0d3d0a-dded-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:35:55.234: INFO: Trying to get logs from node kube-node3 pod client-envvars-eb0d3d0a-dded-11e9-93ab-0610dba1f5f1 container env3cont: <nil>
STEP: delete the pod
Sep 23 10:35:55.252: INFO: Waiting for pod client-envvars-eb0d3d0a-dded-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:35:55.260: INFO: Pod client-envvars-eb0d3d0a-dded-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:35:55.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9558" for this suite.
Sep 23 10:36:45.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:36:45.357: INFO: namespace pods-9558 deletion completed in 50.092407932s

• [SLOW TEST:54.217 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:36:45.357: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Sep 23 10:36:45.396: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 23 10:36:45.403: INFO: Waiting for terminating namespaces to be deleted...
Sep 23 10:36:45.406: INFO: 
Logging pods the kubelet thinks is on node kube-node1 before test
Sep 23 10:36:45.412: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-23 09:39:57 +0000 UTC (1 container statuses recorded)
Sep 23 10:36:45.413: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 23 10:36:45.413: INFO: sonobuoy-systemd-logs-daemon-set-cd4257a81b874ce2-czgzw from heptio-sonobuoy started at 2019-09-23 09:40:00 +0000 UTC (2 container statuses recorded)
Sep 23 10:36:45.413: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Sep 23 10:36:45.413: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 10:36:45.413: INFO: kube-proxy-6ffbt from kube-system started at 2019-09-21 09:49:32 +0000 UTC (1 container statuses recorded)
Sep 23 10:36:45.413: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 23 10:36:45.413: INFO: calico-node-6w4gz from kube-system started at 2019-09-21 10:06:27 +0000 UTC (1 container statuses recorded)
Sep 23 10:36:45.413: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 10:36:45.413: INFO: coredns-fb8b8dccf-8hk76 from kube-system started at 2019-09-21 10:06:33 +0000 UTC (1 container statuses recorded)
Sep 23 10:36:45.413: INFO: 	Container coredns ready: true, restart count 1
Sep 23 10:36:45.413: INFO: 
Logging pods the kubelet thinks is on node kube-node2 before test
Sep 23 10:36:45.421: INFO: kube-proxy-fztxb from kube-system started at 2019-09-21 09:49:42 +0000 UTC (1 container statuses recorded)
Sep 23 10:36:45.421: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 23 10:36:45.421: INFO: calico-node-6xs6x from kube-system started at 2019-09-21 10:06:27 +0000 UTC (1 container statuses recorded)
Sep 23 10:36:45.421: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 10:36:45.421: INFO: coredns-fb8b8dccf-zjxw7 from kube-system started at 2019-09-21 10:06:33 +0000 UTC (1 container statuses recorded)
Sep 23 10:36:45.421: INFO: 	Container coredns ready: true, restart count 1
Sep 23 10:36:45.421: INFO: sonobuoy-e2e-job-8f4e803f5fd84ff6 from heptio-sonobuoy started at 2019-09-23 09:40:00 +0000 UTC (2 container statuses recorded)
Sep 23 10:36:45.421: INFO: 	Container e2e ready: true, restart count 0
Sep 23 10:36:45.421: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 10:36:45.421: INFO: sonobuoy-systemd-logs-daemon-set-cd4257a81b874ce2-d2bcl from heptio-sonobuoy started at 2019-09-23 09:40:00 +0000 UTC (2 container statuses recorded)
Sep 23 10:36:45.421: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Sep 23 10:36:45.421: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 10:36:45.421: INFO: 
Logging pods the kubelet thinks is on node kube-node3 before test
Sep 23 10:36:45.428: INFO: calico-node-bmhnd from kube-system started at 2019-09-21 10:06:27 +0000 UTC (1 container statuses recorded)
Sep 23 10:36:45.428: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 10:36:45.428: INFO: calico-kube-controllers-f6ff9cbbb-m8s47 from kube-system started at 2019-09-21 10:06:36 +0000 UTC (1 container statuses recorded)
Sep 23 10:36:45.428: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep 23 10:36:45.428: INFO: sonobuoy-systemd-logs-daemon-set-cd4257a81b874ce2-kmzgk from heptio-sonobuoy started at 2019-09-23 09:40:00 +0000 UTC (2 container statuses recorded)
Sep 23 10:36:45.428: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Sep 23 10:36:45.428: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 10:36:45.428: INFO: kube-proxy-4cj6r from kube-system started at 2019-09-21 09:49:51 +0000 UTC (1 container statuses recorded)
Sep 23 10:36:45.428: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15c70aec5cf1f846], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:36:46.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1026" for this suite.
Sep 23 10:36:52.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:36:52.546: INFO: namespace sched-pred-1026 deletion completed in 6.091016977s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.189 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:36:52.546: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:36:54.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2587" for this suite.
Sep 23 10:37:44.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:37:44.694: INFO: namespace kubelet-test-2587 deletion completed in 50.086733643s

• [SLOW TEST:52.148 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:37:44.694: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-2d87eaa4-ddee-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume secrets
Sep 23 10:37:44.745: INFO: Waiting up to 5m0s for pod "pod-secrets-2d8877f8-ddee-11e9-93ab-0610dba1f5f1" in namespace "secrets-6888" to be "success or failure"
Sep 23 10:37:44.749: INFO: Pod "pod-secrets-2d8877f8-ddee-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.718719ms
Sep 23 10:37:46.753: INFO: Pod "pod-secrets-2d8877f8-ddee-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008480301s
STEP: Saw pod success
Sep 23 10:37:46.753: INFO: Pod "pod-secrets-2d8877f8-ddee-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:37:46.756: INFO: Trying to get logs from node kube-node3 pod pod-secrets-2d8877f8-ddee-11e9-93ab-0610dba1f5f1 container secret-volume-test: <nil>
STEP: delete the pod
Sep 23 10:37:46.774: INFO: Waiting for pod pod-secrets-2d8877f8-ddee-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:37:46.776: INFO: Pod pod-secrets-2d8877f8-ddee-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:37:46.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6888" for this suite.
Sep 23 10:37:52.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:37:52.869: INFO: namespace secrets-6888 deletion completed in 6.08857154s

• [SLOW TEST:8.175 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:37:52.869: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:38:14.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2094" for this suite.
Sep 23 10:38:20.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:38:20.168: INFO: namespace container-runtime-2094 deletion completed in 6.089116929s

• [SLOW TEST:27.299 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:38:20.169: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 23 10:38:20.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5020'
Sep 23 10:38:20.297: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 23 10:38:20.297: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Sep 23 10:38:20.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 delete jobs e2e-test-nginx-job --namespace=kubectl-5020'
Sep 23 10:38:20.401: INFO: stderr: ""
Sep 23 10:38:20.401: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:38:20.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5020" for this suite.
Sep 23 10:38:26.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:38:26.509: INFO: namespace kubectl-5020 deletion completed in 6.10358034s

• [SLOW TEST:6.341 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:38:26.509: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-4673dc0e-ddee-11e9-93ab-0610dba1f5f1
STEP: Creating secret with name secret-projected-all-test-volume-4673dbf4-ddee-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep 23 10:38:26.558: INFO: Waiting up to 5m0s for pod "projected-volume-4673dba9-ddee-11e9-93ab-0610dba1f5f1" in namespace "projected-4887" to be "success or failure"
Sep 23 10:38:26.560: INFO: Pod "projected-volume-4673dba9-ddee-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.421207ms
Sep 23 10:38:28.563: INFO: Pod "projected-volume-4673dba9-ddee-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005202065s
STEP: Saw pod success
Sep 23 10:38:28.563: INFO: Pod "projected-volume-4673dba9-ddee-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:38:28.565: INFO: Trying to get logs from node kube-node3 pod projected-volume-4673dba9-ddee-11e9-93ab-0610dba1f5f1 container projected-all-volume-test: <nil>
STEP: delete the pod
Sep 23 10:38:28.582: INFO: Waiting for pod projected-volume-4673dba9-ddee-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:38:28.584: INFO: Pod projected-volume-4673dba9-ddee-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:38:28.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4887" for this suite.
Sep 23 10:38:34.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:38:34.679: INFO: namespace projected-4887 deletion completed in 6.090843577s

• [SLOW TEST:8.169 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:38:34.679: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep 23 10:38:34.949: INFO: Pod name wrapped-volume-race-4b6c5d5e-ddee-11e9-93ab-0610dba1f5f1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-4b6c5d5e-ddee-11e9-93ab-0610dba1f5f1 in namespace emptydir-wrapper-4440, will wait for the garbage collector to delete the pods
Sep 23 10:38:49.070: INFO: Deleting ReplicationController wrapped-volume-race-4b6c5d5e-ddee-11e9-93ab-0610dba1f5f1 took: 8.768397ms
Sep 23 10:38:49.371: INFO: Terminating ReplicationController wrapped-volume-race-4b6c5d5e-ddee-11e9-93ab-0610dba1f5f1 pods took: 300.379751ms
STEP: Creating RC which spawns configmap-volume pods
Sep 23 10:39:32.286: INFO: Pod name wrapped-volume-race-6da0b94a-ddee-11e9-93ab-0610dba1f5f1: Found 0 pods out of 5
Sep 23 10:39:37.293: INFO: Pod name wrapped-volume-race-6da0b94a-ddee-11e9-93ab-0610dba1f5f1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6da0b94a-ddee-11e9-93ab-0610dba1f5f1 in namespace emptydir-wrapper-4440, will wait for the garbage collector to delete the pods
Sep 23 10:39:47.378: INFO: Deleting ReplicationController wrapped-volume-race-6da0b94a-ddee-11e9-93ab-0610dba1f5f1 took: 7.109782ms
Sep 23 10:39:47.678: INFO: Terminating ReplicationController wrapped-volume-race-6da0b94a-ddee-11e9-93ab-0610dba1f5f1 pods took: 300.30829ms
STEP: Creating RC which spawns configmap-volume pods
Sep 23 10:40:23.597: INFO: Pod name wrapped-volume-race-8c35a673-ddee-11e9-93ab-0610dba1f5f1: Found 0 pods out of 5
Sep 23 10:40:28.603: INFO: Pod name wrapped-volume-race-8c35a673-ddee-11e9-93ab-0610dba1f5f1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8c35a673-ddee-11e9-93ab-0610dba1f5f1 in namespace emptydir-wrapper-4440, will wait for the garbage collector to delete the pods
Sep 23 10:40:38.688: INFO: Deleting ReplicationController wrapped-volume-race-8c35a673-ddee-11e9-93ab-0610dba1f5f1 took: 7.492823ms
Sep 23 10:40:38.989: INFO: Terminating ReplicationController wrapped-volume-race-8c35a673-ddee-11e9-93ab-0610dba1f5f1 pods took: 300.252069ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:41:23.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4440" for this suite.
Sep 23 10:41:31.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:41:31.568: INFO: namespace emptydir-wrapper-4440 deletion completed in 8.0872852s

• [SLOW TEST:176.889 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:41:31.568: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 23 10:41:31.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1958'
Sep 23 10:41:31.680: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 23 10:41:31.680: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Sep 23 10:41:31.685: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Sep 23 10:41:31.685: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Sep 23 10:41:31.703: INFO: scanned /root for discovery docs: <nil>
Sep 23 10:41:31.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-1958'
Sep 23 10:41:47.444: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep 23 10:41:47.444: INFO: stdout: "Created e2e-test-nginx-rc-b13575f7f457fe423c9b1332446e768b\nScaling up e2e-test-nginx-rc-b13575f7f457fe423c9b1332446e768b from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-b13575f7f457fe423c9b1332446e768b up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-b13575f7f457fe423c9b1332446e768b to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Sep 23 10:41:47.444: INFO: stdout: "Created e2e-test-nginx-rc-b13575f7f457fe423c9b1332446e768b\nScaling up e2e-test-nginx-rc-b13575f7f457fe423c9b1332446e768b from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-b13575f7f457fe423c9b1332446e768b up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-b13575f7f457fe423c9b1332446e768b to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Sep 23 10:41:47.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-1958'
Sep 23 10:41:47.520: INFO: stderr: ""
Sep 23 10:41:47.520: INFO: stdout: "e2e-test-nginx-rc-b13575f7f457fe423c9b1332446e768b-q8rsl "
Sep 23 10:41:47.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods e2e-test-nginx-rc-b13575f7f457fe423c9b1332446e768b-q8rsl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1958'
Sep 23 10:41:47.591: INFO: stderr: ""
Sep 23 10:41:47.591: INFO: stdout: "true"
Sep 23 10:41:47.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods e2e-test-nginx-rc-b13575f7f457fe423c9b1332446e768b-q8rsl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1958'
Sep 23 10:41:47.663: INFO: stderr: ""
Sep 23 10:41:47.663: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Sep 23 10:41:47.663: INFO: e2e-test-nginx-rc-b13575f7f457fe423c9b1332446e768b-q8rsl is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Sep 23 10:41:47.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 delete rc e2e-test-nginx-rc --namespace=kubectl-1958'
Sep 23 10:41:47.742: INFO: stderr: ""
Sep 23 10:41:47.742: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:41:47.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1958" for this suite.
Sep 23 10:41:53.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:41:53.843: INFO: namespace kubectl-1958 deletion completed in 6.096783477s

• [SLOW TEST:22.275 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:41:53.843: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 23 10:41:53.879: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c20792fb-ddee-11e9-93ab-0610dba1f5f1" in namespace "downward-api-4056" to be "success or failure"
Sep 23 10:41:53.881: INFO: Pod "downwardapi-volume-c20792fb-ddee-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.604392ms
Sep 23 10:41:55.885: INFO: Pod "downwardapi-volume-c20792fb-ddee-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006469597s
Sep 23 10:41:57.890: INFO: Pod "downwardapi-volume-c20792fb-ddee-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010999773s
STEP: Saw pod success
Sep 23 10:41:57.890: INFO: Pod "downwardapi-volume-c20792fb-ddee-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:41:57.893: INFO: Trying to get logs from node kube-node1 pod downwardapi-volume-c20792fb-ddee-11e9-93ab-0610dba1f5f1 container client-container: <nil>
STEP: delete the pod
Sep 23 10:41:57.919: INFO: Waiting for pod downwardapi-volume-c20792fb-ddee-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:41:57.927: INFO: Pod downwardapi-volume-c20792fb-ddee-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:41:57.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4056" for this suite.
Sep 23 10:42:03.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:42:04.029: INFO: namespace downward-api-4056 deletion completed in 6.098283771s

• [SLOW TEST:10.186 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:42:04.030: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 23 10:42:04.069: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c81a3f83-ddee-11e9-93ab-0610dba1f5f1" in namespace "downward-api-1911" to be "success or failure"
Sep 23 10:42:04.075: INFO: Pod "downwardapi-volume-c81a3f83-ddee-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.559075ms
Sep 23 10:42:06.079: INFO: Pod "downwardapi-volume-c81a3f83-ddee-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010385664s
STEP: Saw pod success
Sep 23 10:42:06.079: INFO: Pod "downwardapi-volume-c81a3f83-ddee-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:42:06.082: INFO: Trying to get logs from node kube-node3 pod downwardapi-volume-c81a3f83-ddee-11e9-93ab-0610dba1f5f1 container client-container: <nil>
STEP: delete the pod
Sep 23 10:42:06.100: INFO: Waiting for pod downwardapi-volume-c81a3f83-ddee-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:42:06.102: INFO: Pod downwardapi-volume-c81a3f83-ddee-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:42:06.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1911" for this suite.
Sep 23 10:42:12.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:42:12.198: INFO: namespace downward-api-1911 deletion completed in 6.092020049s

• [SLOW TEST:8.169 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:42:12.198: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 23 10:42:12.241: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ccf92153-ddee-11e9-93ab-0610dba1f5f1" in namespace "projected-856" to be "success or failure"
Sep 23 10:42:12.244: INFO: Pod "downwardapi-volume-ccf92153-ddee-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.021733ms
Sep 23 10:42:14.248: INFO: Pod "downwardapi-volume-ccf92153-ddee-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006910115s
STEP: Saw pod success
Sep 23 10:42:14.248: INFO: Pod "downwardapi-volume-ccf92153-ddee-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:42:14.250: INFO: Trying to get logs from node kube-node3 pod downwardapi-volume-ccf92153-ddee-11e9-93ab-0610dba1f5f1 container client-container: <nil>
STEP: delete the pod
Sep 23 10:42:14.276: INFO: Waiting for pod downwardapi-volume-ccf92153-ddee-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:42:14.278: INFO: Pod downwardapi-volume-ccf92153-ddee-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:42:14.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-856" for this suite.
Sep 23 10:42:20.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:42:20.377: INFO: namespace projected-856 deletion completed in 6.094915254s

• [SLOW TEST:8.179 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:42:20.377: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0923 10:42:30.431368      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 23 10:42:30.431: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:42:30.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6667" for this suite.
Sep 23 10:42:36.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:42:36.529: INFO: namespace gc-6667 deletion completed in 6.094061123s

• [SLOW TEST:16.151 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:42:36.529: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 10:42:36.574: INFO: (0) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.769472ms)
Sep 23 10:42:36.577: INFO: (1) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.993258ms)
Sep 23 10:42:36.580: INFO: (2) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.001489ms)
Sep 23 10:42:36.583: INFO: (3) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.096874ms)
Sep 23 10:42:36.587: INFO: (4) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.083238ms)
Sep 23 10:42:36.590: INFO: (5) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.159339ms)
Sep 23 10:42:36.593: INFO: (6) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.122564ms)
Sep 23 10:42:36.596: INFO: (7) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.001289ms)
Sep 23 10:42:36.599: INFO: (8) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.794258ms)
Sep 23 10:42:36.602: INFO: (9) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.250135ms)
Sep 23 10:42:36.605: INFO: (10) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.129591ms)
Sep 23 10:42:36.608: INFO: (11) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.055405ms)
Sep 23 10:42:36.611: INFO: (12) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.915811ms)
Sep 23 10:42:36.614: INFO: (13) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.986328ms)
Sep 23 10:42:36.619: INFO: (14) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.366605ms)
Sep 23 10:42:36.622: INFO: (15) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.835815ms)
Sep 23 10:42:36.626: INFO: (16) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.381334ms)
Sep 23 10:42:36.629: INFO: (17) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.277151ms)
Sep 23 10:42:36.632: INFO: (18) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.249722ms)
Sep 23 10:42:36.635: INFO: (19) /api/v1/nodes/kube-node1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.877444ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:42:36.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5349" for this suite.
Sep 23 10:42:42.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:42:42.733: INFO: namespace proxy-5349 deletion completed in 6.093304458s

• [SLOW TEST:6.204 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:42:42.733: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8068.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8068.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8068.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8068.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8068.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8068.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 23 10:42:44.807: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-8068/dns-test-df2c0c31-ddee-11e9-93ab-0610dba1f5f1: the server could not find the requested resource (get pods dns-test-df2c0c31-ddee-11e9-93ab-0610dba1f5f1)
Sep 23 10:42:44.811: INFO: Unable to read jessie_udp@PodARecord from pod dns-8068/dns-test-df2c0c31-ddee-11e9-93ab-0610dba1f5f1: the server could not find the requested resource (get pods dns-test-df2c0c31-ddee-11e9-93ab-0610dba1f5f1)
Sep 23 10:42:44.814: INFO: Unable to read jessie_tcp@PodARecord from pod dns-8068/dns-test-df2c0c31-ddee-11e9-93ab-0610dba1f5f1: the server could not find the requested resource (get pods dns-test-df2c0c31-ddee-11e9-93ab-0610dba1f5f1)
Sep 23 10:42:44.814: INFO: Lookups using dns-8068/dns-test-df2c0c31-ddee-11e9-93ab-0610dba1f5f1 failed for: [jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep 23 10:42:49.839: INFO: DNS probes using dns-8068/dns-test-df2c0c31-ddee-11e9-93ab-0610dba1f5f1 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:42:49.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8068" for this suite.
Sep 23 10:42:55.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:42:55.972: INFO: namespace dns-8068 deletion completed in 6.105550531s

• [SLOW TEST:13.239 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:42:55.972: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-e7104096-ddee-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume secrets
Sep 23 10:42:56.015: INFO: Waiting up to 5m0s for pod "pod-secrets-e710baf7-ddee-11e9-93ab-0610dba1f5f1" in namespace "secrets-3048" to be "success or failure"
Sep 23 10:42:56.017: INFO: Pod "pod-secrets-e710baf7-ddee-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.488133ms
Sep 23 10:42:58.021: INFO: Pod "pod-secrets-e710baf7-ddee-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00627056s
STEP: Saw pod success
Sep 23 10:42:58.021: INFO: Pod "pod-secrets-e710baf7-ddee-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:42:58.024: INFO: Trying to get logs from node kube-node3 pod pod-secrets-e710baf7-ddee-11e9-93ab-0610dba1f5f1 container secret-volume-test: <nil>
STEP: delete the pod
Sep 23 10:42:58.040: INFO: Waiting for pod pod-secrets-e710baf7-ddee-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:42:58.042: INFO: Pod pod-secrets-e710baf7-ddee-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:42:58.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3048" for this suite.
Sep 23 10:43:04.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:43:04.171: INFO: namespace secrets-3048 deletion completed in 6.1250281s

• [SLOW TEST:8.199 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:43:04.171: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Sep 23 10:43:04.720: INFO: created pod pod-service-account-defaultsa
Sep 23 10:43:04.720: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep 23 10:43:04.724: INFO: created pod pod-service-account-mountsa
Sep 23 10:43:04.724: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep 23 10:43:04.746: INFO: created pod pod-service-account-nomountsa
Sep 23 10:43:04.746: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep 23 10:43:04.751: INFO: created pod pod-service-account-defaultsa-mountspec
Sep 23 10:43:04.751: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep 23 10:43:04.765: INFO: created pod pod-service-account-mountsa-mountspec
Sep 23 10:43:04.765: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep 23 10:43:04.784: INFO: created pod pod-service-account-nomountsa-mountspec
Sep 23 10:43:04.784: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep 23 10:43:04.789: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep 23 10:43:04.789: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep 23 10:43:04.795: INFO: created pod pod-service-account-mountsa-nomountspec
Sep 23 10:43:04.795: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep 23 10:43:04.809: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep 23 10:43:04.810: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:43:04.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7432" for this suite.
Sep 23 10:43:10.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:43:10.921: INFO: namespace svcaccounts-7432 deletion completed in 6.103939083s

• [SLOW TEST:6.750 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:43:10.921: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-eff917ef-ddee-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume secrets
Sep 23 10:43:10.964: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eff9a553-ddee-11e9-93ab-0610dba1f5f1" in namespace "projected-5049" to be "success or failure"
Sep 23 10:43:10.967: INFO: Pod "pod-projected-secrets-eff9a553-ddee-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.974001ms
Sep 23 10:43:12.971: INFO: Pod "pod-projected-secrets-eff9a553-ddee-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007065392s
Sep 23 10:43:14.974: INFO: Pod "pod-projected-secrets-eff9a553-ddee-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010864645s
STEP: Saw pod success
Sep 23 10:43:14.975: INFO: Pod "pod-projected-secrets-eff9a553-ddee-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:43:14.977: INFO: Trying to get logs from node kube-node3 pod pod-projected-secrets-eff9a553-ddee-11e9-93ab-0610dba1f5f1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 23 10:43:14.997: INFO: Waiting for pod pod-projected-secrets-eff9a553-ddee-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:43:15.000: INFO: Pod pod-projected-secrets-eff9a553-ddee-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:43:15.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5049" for this suite.
Sep 23 10:43:21.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:43:21.091: INFO: namespace projected-5049 deletion completed in 6.086914115s

• [SLOW TEST:10.170 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:43:21.092: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep 23 10:43:21.127: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:43:25.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8001" for this suite.
Sep 23 10:43:31.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:43:31.307: INFO: namespace init-container-8001 deletion completed in 6.090536531s

• [SLOW TEST:10.215 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:43:31.307: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep 23 10:43:35.888: INFO: Successfully updated pod "labelsupdatefc20c33a-ddee-11e9-93ab-0610dba1f5f1"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:43:37.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9648" for this suite.
Sep 23 10:43:59.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:44:00.012: INFO: namespace projected-9648 deletion completed in 22.100834172s

• [SLOW TEST:28.705 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:44:00.012: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 23 10:44:00.049: INFO: Waiting up to 5m0s for pod "pod-0d3b8a02-ddef-11e9-93ab-0610dba1f5f1" in namespace "emptydir-6331" to be "success or failure"
Sep 23 10:44:00.052: INFO: Pod "pod-0d3b8a02-ddef-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.886786ms
Sep 23 10:44:02.056: INFO: Pod "pod-0d3b8a02-ddef-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006133496s
STEP: Saw pod success
Sep 23 10:44:02.056: INFO: Pod "pod-0d3b8a02-ddef-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:44:02.058: INFO: Trying to get logs from node kube-node3 pod pod-0d3b8a02-ddef-11e9-93ab-0610dba1f5f1 container test-container: <nil>
STEP: delete the pod
Sep 23 10:44:02.077: INFO: Waiting for pod pod-0d3b8a02-ddef-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:44:02.082: INFO: Pod pod-0d3b8a02-ddef-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:44:02.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6331" for this suite.
Sep 23 10:44:08.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:44:08.181: INFO: namespace emptydir-6331 deletion completed in 6.095626878s

• [SLOW TEST:8.169 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:44:08.182: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Sep 23 10:44:08.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 cluster-info'
Sep 23 10:44:08.289: INFO: stderr: ""
Sep 23 10:44:08.289: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:44:08.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6195" for this suite.
Sep 23 10:44:14.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:44:14.390: INFO: namespace kubectl-6195 deletion completed in 6.096730813s

• [SLOW TEST:6.208 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:44:14.391: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-8369
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Sep 23 10:44:14.441: INFO: Found 0 stateful pods, waiting for 3
Sep 23 10:44:24.446: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 10:44:24.446: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 10:44:24.446: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 10:44:24.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-8369 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 23 10:44:24.708: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 23 10:44:24.708: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 23 10:44:24.708: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep 23 10:44:34.739: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep 23 10:44:44.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-8369 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 23 10:44:44.999: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 23 10:44:44.999: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 23 10:44:44.999: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 23 10:45:05.019: INFO: Waiting for StatefulSet statefulset-8369/ss2 to complete update
Sep 23 10:45:05.019: INFO: Waiting for Pod statefulset-8369/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Sep 23 10:45:15.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-8369 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 23 10:45:15.275: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 23 10:45:15.275: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 23 10:45:15.275: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 23 10:45:25.309: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep 23 10:45:35.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-8369 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 23 10:45:35.567: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 23 10:45:35.567: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 23 10:45:35.567: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 23 10:45:55.587: INFO: Waiting for StatefulSet statefulset-8369/ss2 to complete update
Sep 23 10:45:55.587: INFO: Waiting for Pod statefulset-8369/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep 23 10:46:05.593: INFO: Deleting all statefulset in ns statefulset-8369
Sep 23 10:46:05.596: INFO: Scaling statefulset ss2 to 0
Sep 23 10:46:25.609: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 10:46:25.611: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:46:25.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8369" for this suite.
Sep 23 10:46:31.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:46:31.723: INFO: namespace statefulset-8369 deletion completed in 6.091906234s

• [SLOW TEST:137.331 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:46:31.723: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1497
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-1497
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1497
Sep 23 10:46:31.788: INFO: Found 0 stateful pods, waiting for 1
Sep 23 10:46:41.792: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep 23 10:46:41.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-1497 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 23 10:46:42.047: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 23 10:46:42.047: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 23 10:46:42.047: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 23 10:46:42.051: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 23 10:46:52.055: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 23 10:46:52.055: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 10:46:52.065: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Sep 23 10:46:52.065: INFO: ss-0  kube-node3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:42 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:31 +0000 UTC  }]
Sep 23 10:46:52.065: INFO: 
Sep 23 10:46:52.065: INFO: StatefulSet ss has not reached scale 3, at 1
Sep 23 10:46:53.070: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997409115s
Sep 23 10:46:54.074: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992624592s
Sep 23 10:46:55.078: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988363076s
Sep 23 10:46:56.083: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984241488s
Sep 23 10:46:57.093: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973393701s
Sep 23 10:46:58.098: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.96930643s
Sep 23 10:46:59.102: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.964841433s
Sep 23 10:47:00.106: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.959975915s
Sep 23 10:47:01.111: INFO: Verifying statefulset ss doesn't scale past 3 for another 956.118511ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1497
Sep 23 10:47:02.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-1497 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 23 10:47:02.331: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 23 10:47:02.331: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 23 10:47:02.331: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 23 10:47:02.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-1497 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 23 10:47:02.559: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 23 10:47:02.559: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 23 10:47:02.559: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 23 10:47:02.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-1497 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 23 10:47:02.775: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 23 10:47:02.775: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 23 10:47:02.775: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 23 10:47:02.779: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Sep 23 10:47:12.783: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 10:47:12.783: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 10:47:12.783: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep 23 10:47:12.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-1497 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 23 10:47:13.010: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 23 10:47:13.010: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 23 10:47:13.010: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 23 10:47:13.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-1497 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 23 10:47:13.263: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 23 10:47:13.263: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 23 10:47:13.263: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 23 10:47:13.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 exec --namespace=statefulset-1497 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 23 10:47:13.521: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 23 10:47:13.521: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 23 10:47:13.521: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 23 10:47:13.521: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 10:47:13.524: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep 23 10:47:23.531: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 23 10:47:23.531: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 23 10:47:23.531: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 23 10:47:23.541: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Sep 23 10:47:23.541: INFO: ss-0  kube-node3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:31 +0000 UTC  }]
Sep 23 10:47:23.541: INFO: ss-1  kube-node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  }]
Sep 23 10:47:23.541: INFO: ss-2  kube-node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  }]
Sep 23 10:47:23.541: INFO: 
Sep 23 10:47:23.541: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 23 10:47:24.545: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Sep 23 10:47:24.546: INFO: ss-0  kube-node3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:31 +0000 UTC  }]
Sep 23 10:47:24.546: INFO: ss-1  kube-node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  }]
Sep 23 10:47:24.546: INFO: ss-2  kube-node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  }]
Sep 23 10:47:24.546: INFO: 
Sep 23 10:47:24.546: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 23 10:47:25.550: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Sep 23 10:47:25.550: INFO: ss-0  kube-node3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:31 +0000 UTC  }]
Sep 23 10:47:25.550: INFO: ss-1  kube-node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  }]
Sep 23 10:47:25.550: INFO: ss-2  kube-node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  }]
Sep 23 10:47:25.550: INFO: 
Sep 23 10:47:25.550: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 23 10:47:26.554: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Sep 23 10:47:26.554: INFO: ss-0  kube-node3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:31 +0000 UTC  }]
Sep 23 10:47:26.554: INFO: ss-1  kube-node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  }]
Sep 23 10:47:26.554: INFO: ss-2  kube-node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  }]
Sep 23 10:47:26.555: INFO: 
Sep 23 10:47:26.555: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 23 10:47:27.558: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Sep 23 10:47:27.558: INFO: ss-0  kube-node3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:31 +0000 UTC  }]
Sep 23 10:47:27.558: INFO: ss-1  kube-node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  }]
Sep 23 10:47:27.558: INFO: ss-2  kube-node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  }]
Sep 23 10:47:27.558: INFO: 
Sep 23 10:47:27.558: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 23 10:47:28.563: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Sep 23 10:47:28.563: INFO: ss-0  kube-node3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:31 +0000 UTC  }]
Sep 23 10:47:28.563: INFO: ss-1  kube-node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  }]
Sep 23 10:47:28.563: INFO: ss-2  kube-node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  }]
Sep 23 10:47:28.563: INFO: 
Sep 23 10:47:28.563: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 23 10:47:29.568: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Sep 23 10:47:29.568: INFO: ss-0  kube-node3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:31 +0000 UTC  }]
Sep 23 10:47:29.568: INFO: ss-1  kube-node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  }]
Sep 23 10:47:29.568: INFO: ss-2  kube-node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  }]
Sep 23 10:47:29.568: INFO: 
Sep 23 10:47:29.568: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 23 10:47:30.572: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Sep 23 10:47:30.572: INFO: ss-0  kube-node3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:31 +0000 UTC  }]
Sep 23 10:47:30.572: INFO: ss-1  kube-node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  }]
Sep 23 10:47:30.572: INFO: ss-2  kube-node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  }]
Sep 23 10:47:30.572: INFO: 
Sep 23 10:47:30.572: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 23 10:47:31.576: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Sep 23 10:47:31.576: INFO: ss-1  kube-node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  }]
Sep 23 10:47:31.577: INFO: ss-2  kube-node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:47:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:46:52 +0000 UTC  }]
Sep 23 10:47:31.577: INFO: 
Sep 23 10:47:31.577: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 23 10:47:32.580: INFO: Verifying statefulset ss doesn't scale past 0 for another 961.399528ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1497
Sep 23 10:47:33.583: INFO: Scaling statefulset ss to 0
Sep 23 10:47:33.592: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep 23 10:47:33.596: INFO: Deleting all statefulset in ns statefulset-1497
Sep 23 10:47:33.600: INFO: Scaling statefulset ss to 0
Sep 23 10:47:33.608: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 10:47:33.611: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:47:33.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1497" for this suite.
Sep 23 10:47:39.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:47:39.727: INFO: namespace statefulset-1497 deletion completed in 6.10151457s

• [SLOW TEST:68.004 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:47:39.727: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 10:47:39.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 version'
Sep 23 10:47:39.839: INFO: stderr: ""
Sep 23 10:47:39.839: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:53:57Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:45:25Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:47:39.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3412" for this suite.
Sep 23 10:47:45.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:47:45.935: INFO: namespace kubectl-3412 deletion completed in 6.09120424s

• [SLOW TEST:6.208 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:47:45.935: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 23 10:47:45.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-1329'
Sep 23 10:47:46.114: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 23 10:47:46.114: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Sep 23 10:47:48.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 delete deployment e2e-test-nginx-deployment --namespace=kubectl-1329'
Sep 23 10:47:48.202: INFO: stderr: ""
Sep 23 10:47:48.202: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:47:48.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1329" for this suite.
Sep 23 10:47:54.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:47:54.298: INFO: namespace kubectl-1329 deletion completed in 6.091733743s

• [SLOW TEST:8.363 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:47:54.298: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-546
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-546
STEP: Creating statefulset with conflicting port in namespace statefulset-546
STEP: Waiting until pod test-pod will start running in namespace statefulset-546
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-546
Sep 23 10:47:56.388: INFO: Observed stateful pod in namespace: statefulset-546, name: ss-0, uid: 999ee776-ddef-11e9-a0c7-0cda411d3e75, status phase: Pending. Waiting for statefulset controller to delete.
Sep 23 10:47:56.564: INFO: Observed stateful pod in namespace: statefulset-546, name: ss-0, uid: 999ee776-ddef-11e9-a0c7-0cda411d3e75, status phase: Failed. Waiting for statefulset controller to delete.
Sep 23 10:47:56.571: INFO: Observed stateful pod in namespace: statefulset-546, name: ss-0, uid: 999ee776-ddef-11e9-a0c7-0cda411d3e75, status phase: Failed. Waiting for statefulset controller to delete.
Sep 23 10:47:56.573: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-546
STEP: Removing pod with conflicting port in namespace statefulset-546
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-546 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep 23 10:47:58.590: INFO: Deleting all statefulset in ns statefulset-546
Sep 23 10:47:58.593: INFO: Scaling statefulset ss to 0
Sep 23 10:48:08.609: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 10:48:08.612: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:48:08.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-546" for this suite.
Sep 23 10:48:14.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:48:14.720: INFO: namespace statefulset-546 deletion completed in 6.093955877s

• [SLOW TEST:20.422 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:48:14.721: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-a50d23c4-ddef-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume secrets
Sep 23 10:48:14.762: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a50da54f-ddef-11e9-93ab-0610dba1f5f1" in namespace "projected-894" to be "success or failure"
Sep 23 10:48:14.768: INFO: Pod "pod-projected-secrets-a50da54f-ddef-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.352456ms
Sep 23 10:48:16.772: INFO: Pod "pod-projected-secrets-a50da54f-ddef-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009227462s
Sep 23 10:48:18.780: INFO: Pod "pod-projected-secrets-a50da54f-ddef-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017423807s
STEP: Saw pod success
Sep 23 10:48:18.780: INFO: Pod "pod-projected-secrets-a50da54f-ddef-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:48:18.787: INFO: Trying to get logs from node kube-node3 pod pod-projected-secrets-a50da54f-ddef-11e9-93ab-0610dba1f5f1 container secret-volume-test: <nil>
STEP: delete the pod
Sep 23 10:48:18.823: INFO: Waiting for pod pod-projected-secrets-a50da54f-ddef-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:48:18.825: INFO: Pod pod-projected-secrets-a50da54f-ddef-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:48:18.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-894" for this suite.
Sep 23 10:48:24.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:48:24.936: INFO: namespace projected-894 deletion completed in 6.107350314s

• [SLOW TEST:10.215 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:48:24.936: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-7664/configmap-test-ab244788-ddef-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume configMaps
Sep 23 10:48:24.980: INFO: Waiting up to 5m0s for pod "pod-configmaps-ab24c3bb-ddef-11e9-93ab-0610dba1f5f1" in namespace "configmap-7664" to be "success or failure"
Sep 23 10:48:24.983: INFO: Pod "pod-configmaps-ab24c3bb-ddef-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.65748ms
Sep 23 10:48:26.987: INFO: Pod "pod-configmaps-ab24c3bb-ddef-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006512133s
Sep 23 10:48:28.991: INFO: Pod "pod-configmaps-ab24c3bb-ddef-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010531451s
STEP: Saw pod success
Sep 23 10:48:28.991: INFO: Pod "pod-configmaps-ab24c3bb-ddef-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:48:28.993: INFO: Trying to get logs from node kube-node3 pod pod-configmaps-ab24c3bb-ddef-11e9-93ab-0610dba1f5f1 container env-test: <nil>
STEP: delete the pod
Sep 23 10:48:29.011: INFO: Waiting for pod pod-configmaps-ab24c3bb-ddef-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:48:29.014: INFO: Pod pod-configmaps-ab24c3bb-ddef-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:48:29.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7664" for this suite.
Sep 23 10:48:35.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:48:35.108: INFO: namespace configmap-7664 deletion completed in 6.090864044s

• [SLOW TEST:10.172 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:48:35.108: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 23 10:48:43.183: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 23 10:48:43.185: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 23 10:48:45.186: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 23 10:48:45.192: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 23 10:48:47.186: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 23 10:48:47.189: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 23 10:48:49.186: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 23 10:48:49.192: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 23 10:48:51.186: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 23 10:48:51.189: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 23 10:48:53.186: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 23 10:48:53.191: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 23 10:48:55.186: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 23 10:48:55.190: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 23 10:48:57.186: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 23 10:48:57.191: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 23 10:48:59.186: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 23 10:48:59.189: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 23 10:49:01.186: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 23 10:49:01.191: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:49:01.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-866" for this suite.
Sep 23 10:49:23.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:49:23.284: INFO: namespace container-lifecycle-hook-866 deletion completed in 22.089349752s

• [SLOW TEST:48.176 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:49:23.285: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-4845
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 23 10:49:23.316: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 23 10:49:45.392: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.233.228:8080/dial?request=hostName&protocol=udp&host=192.168.9.124&port=8081&tries=1'] Namespace:pod-network-test-4845 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 10:49:45.392: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 10:49:45.567: INFO: Waiting for endpoints: map[]
Sep 23 10:49:45.570: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.233.228:8080/dial?request=hostName&protocol=udp&host=192.168.233.227&port=8081&tries=1'] Namespace:pod-network-test-4845 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 10:49:45.570: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 10:49:45.756: INFO: Waiting for endpoints: map[]
Sep 23 10:49:45.760: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.233.228:8080/dial?request=hostName&protocol=udp&host=192.168.119.155&port=8081&tries=1'] Namespace:pod-network-test-4845 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 23 10:49:45.760: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
Sep 23 10:49:45.950: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:49:45.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4845" for this suite.
Sep 23 10:50:07.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:50:08.048: INFO: namespace pod-network-test-4845 deletion completed in 22.092749762s

• [SLOW TEST:44.763 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:50:08.048: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep 23 10:50:08.086: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Sep 23 10:50:17.120: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:50:17.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2359" for this suite.
Sep 23 10:50:23.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:50:23.221: INFO: namespace pods-2359 deletion completed in 6.093172153s

• [SLOW TEST:15.172 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:50:23.221: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-f1a5c92a-ddef-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume configMaps
Sep 23 10:50:23.270: INFO: Waiting up to 5m0s for pod "pod-configmaps-f1a653b3-ddef-11e9-93ab-0610dba1f5f1" in namespace "configmap-3654" to be "success or failure"
Sep 23 10:50:23.281: INFO: Pod "pod-configmaps-f1a653b3-ddef-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.41281ms
Sep 23 10:50:25.285: INFO: Pod "pod-configmaps-f1a653b3-ddef-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014550028s
Sep 23 10:50:27.288: INFO: Pod "pod-configmaps-f1a653b3-ddef-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018197536s
STEP: Saw pod success
Sep 23 10:50:27.288: INFO: Pod "pod-configmaps-f1a653b3-ddef-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:50:27.291: INFO: Trying to get logs from node kube-node3 pod pod-configmaps-f1a653b3-ddef-11e9-93ab-0610dba1f5f1 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 23 10:50:27.308: INFO: Waiting for pod pod-configmaps-f1a653b3-ddef-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:50:27.310: INFO: Pod pod-configmaps-f1a653b3-ddef-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:50:27.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3654" for this suite.
Sep 23 10:50:33.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:50:33.412: INFO: namespace configmap-3654 deletion completed in 6.09822663s

• [SLOW TEST:10.191 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:50:33.413: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
Sep 23 10:50:33.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 create -f - --namespace=kubectl-5656'
Sep 23 10:50:33.655: INFO: stderr: ""
Sep 23 10:50:33.655: INFO: stdout: "pod/pause created\n"
Sep 23 10:50:33.655: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep 23 10:50:33.655: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5656" to be "running and ready"
Sep 23 10:50:33.658: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.824371ms
Sep 23 10:50:35.662: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007045381s
Sep 23 10:50:35.662: INFO: Pod "pause" satisfied condition "running and ready"
Sep 23 10:50:35.663: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Sep 23 10:50:35.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 label pods pause testing-label=testing-label-value --namespace=kubectl-5656'
Sep 23 10:50:35.741: INFO: stderr: ""
Sep 23 10:50:35.741: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep 23 10:50:35.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pod pause -L testing-label --namespace=kubectl-5656'
Sep 23 10:50:35.811: INFO: stderr: ""
Sep 23 10:50:35.811: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep 23 10:50:35.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 label pods pause testing-label- --namespace=kubectl-5656'
Sep 23 10:50:35.886: INFO: stderr: ""
Sep 23 10:50:35.887: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep 23 10:50:35.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pod pause -L testing-label --namespace=kubectl-5656'
Sep 23 10:50:35.959: INFO: stderr: ""
Sep 23 10:50:35.959: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
Sep 23 10:50:35.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 delete --grace-period=0 --force -f - --namespace=kubectl-5656'
Sep 23 10:50:36.041: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 10:50:36.041: INFO: stdout: "pod \"pause\" force deleted\n"
Sep 23 10:50:36.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get rc,svc -l name=pause --no-headers --namespace=kubectl-5656'
Sep 23 10:50:36.123: INFO: stderr: "No resources found.\n"
Sep 23 10:50:36.123: INFO: stdout: ""
Sep 23 10:50:36.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods -l name=pause --namespace=kubectl-5656 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 23 10:50:36.201: INFO: stderr: ""
Sep 23 10:50:36.201: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:50:36.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5656" for this suite.
Sep 23 10:50:42.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:50:42.297: INFO: namespace kubectl-5656 deletion completed in 6.090295641s

• [SLOW TEST:8.885 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:50:42.297: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-9076
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9076
STEP: Deleting pre-stop pod
Sep 23 10:50:51.372: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:50:51.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9076" for this suite.
Sep 23 10:51:35.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:51:35.490: INFO: namespace prestop-9076 deletion completed in 44.108480336s

• [SLOW TEST:53.193 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:51:35.491: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 10:51:35.533: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep 23 10:51:40.536: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 23 10:51:40.536: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep 23 10:51:42.540: INFO: Creating deployment "test-rollover-deployment"
Sep 23 10:51:42.546: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep 23 10:51:44.552: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep 23 10:51:44.558: INFO: Ensure that both replica sets have 1 created replica
Sep 23 10:51:44.562: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep 23 10:51:44.569: INFO: Updating deployment test-rollover-deployment
Sep 23 10:51:44.569: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep 23 10:51:46.576: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep 23 10:51:46.582: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep 23 10:51:46.588: INFO: all replica sets need to contain the pod-template-hash label
Sep 23 10:51:46.588: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832702, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832702, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832706, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832702, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 10:51:48.595: INFO: all replica sets need to contain the pod-template-hash label
Sep 23 10:51:48.595: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832702, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832702, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832706, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832702, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 10:51:50.595: INFO: all replica sets need to contain the pod-template-hash label
Sep 23 10:51:50.595: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832702, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832702, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832706, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832702, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 10:51:52.595: INFO: all replica sets need to contain the pod-template-hash label
Sep 23 10:51:52.595: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832702, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832702, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832706, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832702, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 10:51:54.595: INFO: all replica sets need to contain the pod-template-hash label
Sep 23 10:51:54.595: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832702, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832702, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832706, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63704832702, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 10:51:56.595: INFO: 
Sep 23 10:51:56.595: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep 23 10:51:56.603: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-4466,SelfLink:/apis/apps/v1/namespaces/deployment-4466/deployments/test-rollover-deployment,UID:20e701bb-ddf0-11e9-a6ed-0cda411df060,ResourceVersion:365493,Generation:2,CreationTimestamp:2019-09-23 10:51:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-23 10:51:42 +0000 UTC 2019-09-23 10:51:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-23 10:51:56 +0000 UTC 2019-09-23 10:51:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep 23 10:51:56.607: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-4466,SelfLink:/apis/apps/v1/namespaces/deployment-4466/replicasets/test-rollover-deployment-766b4d6c9d,UID:221cab0d-ddf0-11e9-a0c7-0cda411d3e75,ResourceVersion:365482,Generation:2,CreationTimestamp:2019-09-23 10:51:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 20e701bb-ddf0-11e9-a6ed-0cda411df060 0xc000d92317 0xc000d92318}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep 23 10:51:56.607: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep 23 10:51:56.607: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-4466,SelfLink:/apis/apps/v1/namespaces/deployment-4466/replicasets/test-rollover-controller,UID:1cb8bced-ddf0-11e9-a6ed-0cda411df060,ResourceVersion:365492,Generation:2,CreationTimestamp:2019-09-23 10:51:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 20e701bb-ddf0-11e9-a6ed-0cda411df060 0xc000d920f7 0xc000d920f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 23 10:51:56.607: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-4466,SelfLink:/apis/apps/v1/namespaces/deployment-4466/replicasets/test-rollover-deployment-6455657675,UID:20e8a49c-ddf0-11e9-a0c7-0cda411d3e75,ResourceVersion:365441,Generation:2,CreationTimestamp:2019-09-23 10:51:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 20e701bb-ddf0-11e9-a6ed-0cda411df060 0xc000d921c7 0xc000d921c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 23 10:51:56.610: INFO: Pod "test-rollover-deployment-766b4d6c9d-b29wz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-b29wz,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-4466,SelfLink:/api/v1/namespaces/deployment-4466/pods/test-rollover-deployment-766b4d6c9d-b29wz,UID:22210249-ddf0-11e9-a0c7-0cda411d3e75,ResourceVersion:365458,Generation:0,CreationTimestamp:2019-09-23 10:51:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.233.229/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d 221cab0d-ddf0-11e9-a0c7-0cda411d3e75 0xc000d92ef7 0xc000d92ef8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xxtm9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xxtm9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-xxtm9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d92f70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d92f90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:51:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:51:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:51:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 10:51:44 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.105,PodIP:192.168.233.229,StartTime:2019-09-23 10:51:44 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-23 10:51:45 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker://sha256:e4423e943a205fe1d81768e60603c8f2c5821576bad0801c1e91b8ba586124a0 docker://7c743ad3cc3573a31578ede049815bbefccc1887461f3785fdc965e1d80c8ea2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:51:56.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4466" for this suite.
Sep 23 10:52:02.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:52:02.705: INFO: namespace deployment-4466 deletion completed in 6.091265135s

• [SLOW TEST:27.214 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:52:02.705: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Sep 23 10:52:02.736: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 23 10:52:02.743: INFO: Waiting for terminating namespaces to be deleted...
Sep 23 10:52:02.746: INFO: 
Logging pods the kubelet thinks is on node kube-node1 before test
Sep 23 10:52:02.753: INFO: kube-proxy-6ffbt from kube-system started at 2019-09-21 09:49:32 +0000 UTC (1 container statuses recorded)
Sep 23 10:52:02.753: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 23 10:52:02.753: INFO: coredns-fb8b8dccf-8hk76 from kube-system started at 2019-09-21 10:06:33 +0000 UTC (1 container statuses recorded)
Sep 23 10:52:02.753: INFO: 	Container coredns ready: true, restart count 1
Sep 23 10:52:02.753: INFO: sonobuoy-systemd-logs-daemon-set-cd4257a81b874ce2-czgzw from heptio-sonobuoy started at 2019-09-23 09:40:00 +0000 UTC (2 container statuses recorded)
Sep 23 10:52:02.753: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Sep 23 10:52:02.753: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 23 10:52:02.753: INFO: calico-node-6w4gz from kube-system started at 2019-09-21 10:06:27 +0000 UTC (1 container statuses recorded)
Sep 23 10:52:02.753: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 10:52:02.753: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-23 09:39:57 +0000 UTC (1 container statuses recorded)
Sep 23 10:52:02.753: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 23 10:52:02.753: INFO: 
Logging pods the kubelet thinks is on node kube-node2 before test
Sep 23 10:52:02.759: INFO: kube-proxy-fztxb from kube-system started at 2019-09-21 09:49:42 +0000 UTC (1 container statuses recorded)
Sep 23 10:52:02.759: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 23 10:52:02.759: INFO: sonobuoy-e2e-job-8f4e803f5fd84ff6 from heptio-sonobuoy started at 2019-09-23 09:40:00 +0000 UTC (2 container statuses recorded)
Sep 23 10:52:02.759: INFO: 	Container e2e ready: true, restart count 0
Sep 23 10:52:02.759: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 10:52:02.759: INFO: sonobuoy-systemd-logs-daemon-set-cd4257a81b874ce2-d2bcl from heptio-sonobuoy started at 2019-09-23 09:40:00 +0000 UTC (2 container statuses recorded)
Sep 23 10:52:02.759: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Sep 23 10:52:02.759: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 23 10:52:02.759: INFO: calico-node-6xs6x from kube-system started at 2019-09-21 10:06:27 +0000 UTC (1 container statuses recorded)
Sep 23 10:52:02.759: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 10:52:02.759: INFO: coredns-fb8b8dccf-zjxw7 from kube-system started at 2019-09-21 10:06:33 +0000 UTC (1 container statuses recorded)
Sep 23 10:52:02.759: INFO: 	Container coredns ready: true, restart count 1
Sep 23 10:52:02.759: INFO: 
Logging pods the kubelet thinks is on node kube-node3 before test
Sep 23 10:52:02.764: INFO: kube-proxy-4cj6r from kube-system started at 2019-09-21 09:49:51 +0000 UTC (1 container statuses recorded)
Sep 23 10:52:02.764: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 23 10:52:02.764: INFO: calico-node-bmhnd from kube-system started at 2019-09-21 10:06:27 +0000 UTC (1 container statuses recorded)
Sep 23 10:52:02.764: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 10:52:02.764: INFO: calico-kube-controllers-f6ff9cbbb-m8s47 from kube-system started at 2019-09-21 10:06:36 +0000 UTC (1 container statuses recorded)
Sep 23 10:52:02.764: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep 23 10:52:02.764: INFO: sonobuoy-systemd-logs-daemon-set-cd4257a81b874ce2-kmzgk from heptio-sonobuoy started at 2019-09-23 09:40:00 +0000 UTC (2 container statuses recorded)
Sep 23 10:52:02.764: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Sep 23 10:52:02.764: INFO: 	Container sonobuoy-worker ready: true, restart count 1
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-2e2a9370-ddf0-11e9-93ab-0610dba1f5f1 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-2e2a9370-ddf0-11e9-93ab-0610dba1f5f1 off the node kube-node3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2e2a9370-ddf0-11e9-93ab-0610dba1f5f1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:52:06.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-855" for this suite.
Sep 23 10:52:14.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:52:14.962: INFO: namespace sched-pred-855 deletion completed in 8.11759329s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:12.257 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:52:14.964: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-343f1d7d-ddf0-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume secrets
Sep 23 10:52:15.004: INFO: Waiting up to 5m0s for pod "pod-secrets-343f9caa-ddf0-11e9-93ab-0610dba1f5f1" in namespace "secrets-8222" to be "success or failure"
Sep 23 10:52:15.008: INFO: Pod "pod-secrets-343f9caa-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.488295ms
Sep 23 10:52:17.012: INFO: Pod "pod-secrets-343f9caa-ddf0-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007609235s
STEP: Saw pod success
Sep 23 10:52:17.012: INFO: Pod "pod-secrets-343f9caa-ddf0-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:52:17.014: INFO: Trying to get logs from node kube-node3 pod pod-secrets-343f9caa-ddf0-11e9-93ab-0610dba1f5f1 container secret-volume-test: <nil>
STEP: delete the pod
Sep 23 10:52:17.033: INFO: Waiting for pod pod-secrets-343f9caa-ddf0-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:52:17.035: INFO: Pod pod-secrets-343f9caa-ddf0-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:52:17.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8222" for this suite.
Sep 23 10:52:23.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:52:23.133: INFO: namespace secrets-8222 deletion completed in 6.094406744s

• [SLOW TEST:8.170 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:52:23.134: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Sep 23 10:52:23.181: INFO: Waiting up to 5m0s for pod "var-expansion-391e8299-ddf0-11e9-93ab-0610dba1f5f1" in namespace "var-expansion-1832" to be "success or failure"
Sep 23 10:52:23.184: INFO: Pod "var-expansion-391e8299-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.607025ms
Sep 23 10:52:25.190: INFO: Pod "var-expansion-391e8299-ddf0-11e9-93ab-0610dba1f5f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008867388s
Sep 23 10:52:27.194: INFO: Pod "var-expansion-391e8299-ddf0-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012650378s
STEP: Saw pod success
Sep 23 10:52:27.194: INFO: Pod "var-expansion-391e8299-ddf0-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:52:27.197: INFO: Trying to get logs from node kube-node3 pod var-expansion-391e8299-ddf0-11e9-93ab-0610dba1f5f1 container dapi-container: <nil>
STEP: delete the pod
Sep 23 10:52:27.217: INFO: Waiting for pod var-expansion-391e8299-ddf0-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:52:27.220: INFO: Pod var-expansion-391e8299-ddf0-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:52:27.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1832" for this suite.
Sep 23 10:52:33.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:52:33.330: INFO: namespace var-expansion-1832 deletion completed in 6.105536992s

• [SLOW TEST:10.196 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:52:33.330: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Sep 23 10:52:33.375: INFO: Waiting up to 5m0s for pod "pod-3f32ec0c-ddf0-11e9-93ab-0610dba1f5f1" in namespace "emptydir-6352" to be "success or failure"
Sep 23 10:52:33.378: INFO: Pod "pod-3f32ec0c-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.297069ms
Sep 23 10:52:35.382: INFO: Pod "pod-3f32ec0c-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006254546s
Sep 23 10:52:37.386: INFO: Pod "pod-3f32ec0c-ddf0-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01024554s
STEP: Saw pod success
Sep 23 10:52:37.386: INFO: Pod "pod-3f32ec0c-ddf0-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:52:37.388: INFO: Trying to get logs from node kube-node3 pod pod-3f32ec0c-ddf0-11e9-93ab-0610dba1f5f1 container test-container: <nil>
STEP: delete the pod
Sep 23 10:52:37.406: INFO: Waiting for pod pod-3f32ec0c-ddf0-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:52:37.409: INFO: Pod pod-3f32ec0c-ddf0-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:52:37.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6352" for this suite.
Sep 23 10:52:43.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:52:43.513: INFO: namespace emptydir-6352 deletion completed in 6.099839983s

• [SLOW TEST:10.182 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:52:43.513: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 23 10:52:43.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-3187'
Sep 23 10:52:43.634: INFO: stderr: ""
Sep 23 10:52:43.634: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Sep 23 10:52:48.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pod e2e-test-nginx-pod --namespace=kubectl-3187 -o json'
Sep 23 10:52:48.756: INFO: stderr: ""
Sep 23 10:52:48.757: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.119.166/32\"\n        },\n        \"creationTimestamp\": \"2019-09-23T10:52:43Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-3187\",\n        \"resourceVersion\": \"365796\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-3187/pods/e2e-test-nginx-pod\",\n        \"uid\": \"454f8a1b-ddf0-11e9-a0c7-0cda411d3e75\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-jt9t2\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"kube-node3\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-jt9t2\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-jt9t2\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-23T10:52:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-23T10:52:45Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-23T10:52:45Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-23T10:52:43Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://06684fd51ffbbf587427d55ec22f79bc686cafde7b661637dddafa0cc375cdbd\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker://sha256:8a2fb25a19f5dc1528b7a3fabe8b3145ff57fe10e4f1edac6c718a3cf4aa4b73\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-09-23T10:52:44Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.16.151.106\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.119.166\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-09-23T10:52:43Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep 23 10:52:48.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 replace -f - --namespace=kubectl-3187'
Sep 23 10:52:48.967: INFO: stderr: ""
Sep 23 10:52:48.967: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Sep 23 10:52:48.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 delete pods e2e-test-nginx-pod --namespace=kubectl-3187'
Sep 23 10:53:01.435: INFO: stderr: ""
Sep 23 10:53:01.435: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:53:01.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3187" for this suite.
Sep 23 10:53:07.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:53:07.533: INFO: namespace kubectl-3187 deletion completed in 6.092430117s

• [SLOW TEST:24.020 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:53:07.533: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 23 10:53:07.573: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5394e38d-ddf0-11e9-93ab-0610dba1f5f1" in namespace "downward-api-9269" to be "success or failure"
Sep 23 10:53:07.576: INFO: Pod "downwardapi-volume-5394e38d-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.674172ms
Sep 23 10:53:09.581: INFO: Pod "downwardapi-volume-5394e38d-ddf0-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008021617s
STEP: Saw pod success
Sep 23 10:53:09.581: INFO: Pod "downwardapi-volume-5394e38d-ddf0-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:53:09.583: INFO: Trying to get logs from node kube-node3 pod downwardapi-volume-5394e38d-ddf0-11e9-93ab-0610dba1f5f1 container client-container: <nil>
STEP: delete the pod
Sep 23 10:53:09.601: INFO: Waiting for pod downwardapi-volume-5394e38d-ddf0-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:53:09.603: INFO: Pod downwardapi-volume-5394e38d-ddf0-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:53:09.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9269" for this suite.
Sep 23 10:53:15.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:53:15.713: INFO: namespace downward-api-9269 deletion completed in 6.10099686s

• [SLOW TEST:8.180 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:53:15.713: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 23 10:53:18.276: INFO: Successfully updated pod "pod-update-5874d7cf-ddf0-11e9-93ab-0610dba1f5f1"
STEP: verifying the updated pod is in kubernetes
Sep 23 10:53:18.284: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:53:18.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3761" for this suite.
Sep 23 10:53:40.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:53:40.390: INFO: namespace pods-3761 deletion completed in 22.102427167s

• [SLOW TEST:24.676 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:53:40.390: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 23 10:53:40.428: INFO: Waiting up to 5m0s for pod "downwardapi-volume-672a57ce-ddf0-11e9-93ab-0610dba1f5f1" in namespace "projected-6412" to be "success or failure"
Sep 23 10:53:40.431: INFO: Pod "downwardapi-volume-672a57ce-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.833621ms
Sep 23 10:53:42.435: INFO: Pod "downwardapi-volume-672a57ce-ddf0-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006961508s
STEP: Saw pod success
Sep 23 10:53:42.435: INFO: Pod "downwardapi-volume-672a57ce-ddf0-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:53:42.438: INFO: Trying to get logs from node kube-node3 pod downwardapi-volume-672a57ce-ddf0-11e9-93ab-0610dba1f5f1 container client-container: <nil>
STEP: delete the pod
Sep 23 10:53:42.458: INFO: Waiting for pod downwardapi-volume-672a57ce-ddf0-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:53:42.461: INFO: Pod downwardapi-volume-672a57ce-ddf0-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:53:42.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6412" for this suite.
Sep 23 10:53:48.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:53:48.565: INFO: namespace projected-6412 deletion completed in 6.100151624s

• [SLOW TEST:8.175 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:53:48.565: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Sep 23 10:53:48.606: INFO: Waiting up to 5m0s for pod "client-containers-6c09e425-ddf0-11e9-93ab-0610dba1f5f1" in namespace "containers-1583" to be "success or failure"
Sep 23 10:53:48.609: INFO: Pod "client-containers-6c09e425-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.01033ms
Sep 23 10:53:50.613: INFO: Pod "client-containers-6c09e425-ddf0-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007199976s
STEP: Saw pod success
Sep 23 10:53:50.613: INFO: Pod "client-containers-6c09e425-ddf0-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:53:50.616: INFO: Trying to get logs from node kube-node3 pod client-containers-6c09e425-ddf0-11e9-93ab-0610dba1f5f1 container test-container: <nil>
STEP: delete the pod
Sep 23 10:53:50.640: INFO: Waiting for pod client-containers-6c09e425-ddf0-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:53:50.647: INFO: Pod client-containers-6c09e425-ddf0-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:53:50.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1583" for this suite.
Sep 23 10:53:56.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:53:56.749: INFO: namespace containers-1583 deletion completed in 6.098632039s

• [SLOW TEST:8.184 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:53:56.749: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep 23 10:53:56.806: INFO: Waiting up to 5m0s for pod "pod-70ebd8c3-ddf0-11e9-93ab-0610dba1f5f1" in namespace "emptydir-2089" to be "success or failure"
Sep 23 10:53:56.809: INFO: Pod "pod-70ebd8c3-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.054747ms
Sep 23 10:53:58.813: INFO: Pod "pod-70ebd8c3-ddf0-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007455495s
STEP: Saw pod success
Sep 23 10:53:58.813: INFO: Pod "pod-70ebd8c3-ddf0-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:53:58.816: INFO: Trying to get logs from node kube-node3 pod pod-70ebd8c3-ddf0-11e9-93ab-0610dba1f5f1 container test-container: <nil>
STEP: delete the pod
Sep 23 10:53:58.840: INFO: Waiting for pod pod-70ebd8c3-ddf0-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:53:58.842: INFO: Pod pod-70ebd8c3-ddf0-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:53:58.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2089" for this suite.
Sep 23 10:54:04.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:54:04.939: INFO: namespace emptydir-2089 deletion completed in 6.092383924s

• [SLOW TEST:8.189 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:54:04.939: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 23 10:54:04.978: INFO: Waiting up to 5m0s for pod "pod-75cc2a38-ddf0-11e9-93ab-0610dba1f5f1" in namespace "emptydir-5530" to be "success or failure"
Sep 23 10:54:04.985: INFO: Pod "pod-75cc2a38-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.887958ms
Sep 23 10:54:06.989: INFO: Pod "pod-75cc2a38-ddf0-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011564625s
STEP: Saw pod success
Sep 23 10:54:06.989: INFO: Pod "pod-75cc2a38-ddf0-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:54:06.992: INFO: Trying to get logs from node kube-node3 pod pod-75cc2a38-ddf0-11e9-93ab-0610dba1f5f1 container test-container: <nil>
STEP: delete the pod
Sep 23 10:54:07.010: INFO: Waiting for pod pod-75cc2a38-ddf0-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:54:07.013: INFO: Pod pod-75cc2a38-ddf0-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:54:07.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5530" for this suite.
Sep 23 10:54:13.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:54:13.128: INFO: namespace emptydir-5530 deletion completed in 6.106331556s

• [SLOW TEST:8.189 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:54:13.129: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0923 10:54:19.234040      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 23 10:54:19.234: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:54:19.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3728" for this suite.
Sep 23 10:54:25.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:54:25.383: INFO: namespace gc-3728 deletion completed in 6.139342574s

• [SLOW TEST:12.255 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:54:25.383: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 23 10:54:25.424: INFO: Waiting up to 5m0s for pod "pod-81fc0df0-ddf0-11e9-93ab-0610dba1f5f1" in namespace "emptydir-3956" to be "success or failure"
Sep 23 10:54:25.426: INFO: Pod "pod-81fc0df0-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.478219ms
Sep 23 10:54:27.430: INFO: Pod "pod-81fc0df0-ddf0-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006205149s
STEP: Saw pod success
Sep 23 10:54:27.430: INFO: Pod "pod-81fc0df0-ddf0-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:54:27.433: INFO: Trying to get logs from node kube-node3 pod pod-81fc0df0-ddf0-11e9-93ab-0610dba1f5f1 container test-container: <nil>
STEP: delete the pod
Sep 23 10:54:27.450: INFO: Waiting for pod pod-81fc0df0-ddf0-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:54:27.453: INFO: Pod pod-81fc0df0-ddf0-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:54:27.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3956" for this suite.
Sep 23 10:54:35.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:54:35.562: INFO: namespace emptydir-3956 deletion completed in 8.104504207s

• [SLOW TEST:10.178 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:54:35.562: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7223.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7223.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 23 10:54:39.683: INFO: DNS probes using dns-7223/dns-test-881385e8-ddf0-11e9-93ab-0610dba1f5f1 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:54:39.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7223" for this suite.
Sep 23 10:54:45.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:54:45.812: INFO: namespace dns-7223 deletion completed in 6.102733818s

• [SLOW TEST:10.250 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:54:45.812: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 23 10:54:45.852: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8e2944f4-ddf0-11e9-93ab-0610dba1f5f1" in namespace "projected-7752" to be "success or failure"
Sep 23 10:54:45.855: INFO: Pod "downwardapi-volume-8e2944f4-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.595055ms
Sep 23 10:54:47.859: INFO: Pod "downwardapi-volume-8e2944f4-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006738691s
Sep 23 10:54:49.863: INFO: Pod "downwardapi-volume-8e2944f4-ddf0-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010799001s
STEP: Saw pod success
Sep 23 10:54:49.863: INFO: Pod "downwardapi-volume-8e2944f4-ddf0-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:54:49.866: INFO: Trying to get logs from node kube-node3 pod downwardapi-volume-8e2944f4-ddf0-11e9-93ab-0610dba1f5f1 container client-container: <nil>
STEP: delete the pod
Sep 23 10:54:49.886: INFO: Waiting for pod downwardapi-volume-8e2944f4-ddf0-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:54:49.898: INFO: Pod downwardapi-volume-8e2944f4-ddf0-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:54:49.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7752" for this suite.
Sep 23 10:54:55.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:54:55.996: INFO: namespace projected-7752 deletion completed in 6.093963099s

• [SLOW TEST:10.184 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:54:55.996: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-943b79c4-ddf0-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume secrets
Sep 23 10:54:56.078: INFO: Waiting up to 5m0s for pod "pod-secrets-94418f1b-ddf0-11e9-93ab-0610dba1f5f1" in namespace "secrets-9501" to be "success or failure"
Sep 23 10:54:56.080: INFO: Pod "pod-secrets-94418f1b-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.641169ms
Sep 23 10:54:58.084: INFO: Pod "pod-secrets-94418f1b-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006089249s
Sep 23 10:55:00.088: INFO: Pod "pod-secrets-94418f1b-ddf0-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010409139s
STEP: Saw pod success
Sep 23 10:55:00.088: INFO: Pod "pod-secrets-94418f1b-ddf0-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:55:00.091: INFO: Trying to get logs from node kube-node3 pod pod-secrets-94418f1b-ddf0-11e9-93ab-0610dba1f5f1 container secret-volume-test: <nil>
STEP: delete the pod
Sep 23 10:55:00.108: INFO: Waiting for pod pod-secrets-94418f1b-ddf0-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:55:00.111: INFO: Pod pod-secrets-94418f1b-ddf0-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:55:00.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9501" for this suite.
Sep 23 10:55:06.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:55:06.202: INFO: namespace secrets-9501 deletion completed in 6.086999462s
STEP: Destroying namespace "secret-namespace-8665" for this suite.
Sep 23 10:55:12.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:55:12.290: INFO: namespace secret-namespace-8665 deletion completed in 6.087976613s

• [SLOW TEST:16.294 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:55:12.290: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 23 10:55:12.333: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9df1d989-ddf0-11e9-93ab-0610dba1f5f1" in namespace "downward-api-5430" to be "success or failure"
Sep 23 10:55:12.335: INFO: Pod "downwardapi-volume-9df1d989-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.456171ms
Sep 23 10:55:14.339: INFO: Pod "downwardapi-volume-9df1d989-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006493609s
Sep 23 10:55:16.343: INFO: Pod "downwardapi-volume-9df1d989-ddf0-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010093848s
STEP: Saw pod success
Sep 23 10:55:16.343: INFO: Pod "downwardapi-volume-9df1d989-ddf0-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:55:16.345: INFO: Trying to get logs from node kube-node3 pod downwardapi-volume-9df1d989-ddf0-11e9-93ab-0610dba1f5f1 container client-container: <nil>
STEP: delete the pod
Sep 23 10:55:16.367: INFO: Waiting for pod downwardapi-volume-9df1d989-ddf0-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:55:16.372: INFO: Pod downwardapi-volume-9df1d989-ddf0-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:55:16.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5430" for this suite.
Sep 23 10:55:22.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:55:22.470: INFO: namespace downward-api-5430 deletion completed in 6.094334858s

• [SLOW TEST:10.180 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:55:22.470: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep 23 10:55:22.511: INFO: Waiting up to 5m0s for pod "downward-api-a402e01e-ddf0-11e9-93ab-0610dba1f5f1" in namespace "downward-api-6370" to be "success or failure"
Sep 23 10:55:22.513: INFO: Pod "downward-api-a402e01e-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.359053ms
Sep 23 10:55:24.518: INFO: Pod "downward-api-a402e01e-ddf0-11e9-93ab-0610dba1f5f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00667601s
Sep 23 10:55:26.522: INFO: Pod "downward-api-a402e01e-ddf0-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011100148s
STEP: Saw pod success
Sep 23 10:55:26.522: INFO: Pod "downward-api-a402e01e-ddf0-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:55:26.525: INFO: Trying to get logs from node kube-node1 pod downward-api-a402e01e-ddf0-11e9-93ab-0610dba1f5f1 container dapi-container: <nil>
STEP: delete the pod
Sep 23 10:55:26.544: INFO: Waiting for pod downward-api-a402e01e-ddf0-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:55:26.554: INFO: Pod downward-api-a402e01e-ddf0-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:55:26.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6370" for this suite.
Sep 23 10:55:32.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:55:32.652: INFO: namespace downward-api-6370 deletion completed in 6.093422437s

• [SLOW TEST:10.182 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:55:32.652: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 23 10:55:32.692: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa147482-ddf0-11e9-93ab-0610dba1f5f1" in namespace "downward-api-8035" to be "success or failure"
Sep 23 10:55:32.694: INFO: Pod "downwardapi-volume-aa147482-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.547868ms
Sep 23 10:55:34.699: INFO: Pod "downwardapi-volume-aa147482-ddf0-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006944766s
STEP: Saw pod success
Sep 23 10:55:34.699: INFO: Pod "downwardapi-volume-aa147482-ddf0-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:55:34.702: INFO: Trying to get logs from node kube-node2 pod downwardapi-volume-aa147482-ddf0-11e9-93ab-0610dba1f5f1 container client-container: <nil>
STEP: delete the pod
Sep 23 10:55:34.720: INFO: Waiting for pod downwardapi-volume-aa147482-ddf0-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:55:34.725: INFO: Pod downwardapi-volume-aa147482-ddf0-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:55:34.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8035" for this suite.
Sep 23 10:55:40.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:55:40.836: INFO: namespace downward-api-8035 deletion completed in 6.107322287s

• [SLOW TEST:8.184 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:55:40.836: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-aef58732-ddf0-11e9-93ab-0610dba1f5f1
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:55:42.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3653" for this suite.
Sep 23 10:56:04.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:56:04.992: INFO: namespace configmap-3653 deletion completed in 22.084648247s

• [SLOW TEST:24.156 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:56:04.993: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-bd5b20f3-ddf0-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume configMaps
Sep 23 10:56:05.036: INFO: Waiting up to 5m0s for pod "pod-configmaps-bd5ba043-ddf0-11e9-93ab-0610dba1f5f1" in namespace "configmap-4184" to be "success or failure"
Sep 23 10:56:05.040: INFO: Pod "pod-configmaps-bd5ba043-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.645769ms
Sep 23 10:56:07.043: INFO: Pod "pod-configmaps-bd5ba043-ddf0-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007352217s
STEP: Saw pod success
Sep 23 10:56:07.043: INFO: Pod "pod-configmaps-bd5ba043-ddf0-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:56:07.046: INFO: Trying to get logs from node kube-node3 pod pod-configmaps-bd5ba043-ddf0-11e9-93ab-0610dba1f5f1 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 23 10:56:07.064: INFO: Waiting for pod pod-configmaps-bd5ba043-ddf0-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:56:07.067: INFO: Pod pod-configmaps-bd5ba043-ddf0-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:56:07.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4184" for this suite.
Sep 23 10:56:13.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:56:13.160: INFO: namespace configmap-4184 deletion completed in 6.088573135s

• [SLOW TEST:8.167 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:56:13.160: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:56:17.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5839" for this suite.
Sep 23 10:56:23.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:56:23.313: INFO: namespace kubelet-test-5839 deletion completed in 6.095906899s

• [SLOW TEST:10.153 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:56:23.314: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep 23 10:56:23.347: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:56:26.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5968" for this suite.
Sep 23 10:56:32.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:56:32.346: INFO: namespace init-container-5968 deletion completed in 6.097152076s

• [SLOW TEST:9.032 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:56:32.346: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep 23 10:56:32.382: INFO: Waiting up to 5m0s for pod "downward-api-cda8606e-ddf0-11e9-93ab-0610dba1f5f1" in namespace "downward-api-347" to be "success or failure"
Sep 23 10:56:32.384: INFO: Pod "downward-api-cda8606e-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.524433ms
Sep 23 10:56:34.388: INFO: Pod "downward-api-cda8606e-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005780372s
Sep 23 10:56:36.391: INFO: Pod "downward-api-cda8606e-ddf0-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009716862s
STEP: Saw pod success
Sep 23 10:56:36.392: INFO: Pod "downward-api-cda8606e-ddf0-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:56:36.394: INFO: Trying to get logs from node kube-node3 pod downward-api-cda8606e-ddf0-11e9-93ab-0610dba1f5f1 container dapi-container: <nil>
STEP: delete the pod
Sep 23 10:56:36.412: INFO: Waiting for pod downward-api-cda8606e-ddf0-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:56:36.414: INFO: Pod downward-api-cda8606e-ddf0-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:56:36.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-347" for this suite.
Sep 23 10:56:42.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:56:42.513: INFO: namespace downward-api-347 deletion completed in 6.09503674s

• [SLOW TEST:10.167 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:56:42.513: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep 23 10:56:42.564: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-3174,SelfLink:/api/v1/namespaces/watch-3174/configmaps/e2e-watch-test-resource-version,UID:d3b80f8a-ddf0-11e9-a6ed-0cda411df060,ResourceVersion:367169,Generation:0,CreationTimestamp:2019-09-23 10:56:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 23 10:56:42.564: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-3174,SelfLink:/api/v1/namespaces/watch-3174/configmaps/e2e-watch-test-resource-version,UID:d3b80f8a-ddf0-11e9-a6ed-0cda411df060,ResourceVersion:367170,Generation:0,CreationTimestamp:2019-09-23 10:56:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:56:42.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3174" for this suite.
Sep 23 10:56:48.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:56:48.667: INFO: namespace watch-3174 deletion completed in 6.099302728s

• [SLOW TEST:6.154 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:56:48.667: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-d763028b-ddf0-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume configMaps
Sep 23 10:56:48.707: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d76375c6-ddf0-11e9-93ab-0610dba1f5f1" in namespace "projected-8781" to be "success or failure"
Sep 23 10:56:48.712: INFO: Pod "pod-projected-configmaps-d76375c6-ddf0-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.164768ms
Sep 23 10:56:50.716: INFO: Pod "pod-projected-configmaps-d76375c6-ddf0-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008093304s
STEP: Saw pod success
Sep 23 10:56:50.716: INFO: Pod "pod-projected-configmaps-d76375c6-ddf0-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:56:50.719: INFO: Trying to get logs from node kube-node3 pod pod-projected-configmaps-d76375c6-ddf0-11e9-93ab-0610dba1f5f1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 23 10:56:50.738: INFO: Waiting for pod pod-projected-configmaps-d76375c6-ddf0-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:56:50.741: INFO: Pod pod-projected-configmaps-d76375c6-ddf0-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:56:50.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8781" for this suite.
Sep 23 10:56:56.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:56:56.849: INFO: namespace projected-8781 deletion completed in 6.104068713s

• [SLOW TEST:8.182 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:56:56.850: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-dc44476d-ddf0-11e9-93ab-0610dba1f5f1
STEP: Creating configMap with name cm-test-opt-upd-dc4447ab-ddf0-11e9-93ab-0610dba1f5f1
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-dc44476d-ddf0-11e9-93ab-0610dba1f5f1
STEP: Updating configmap cm-test-opt-upd-dc4447ab-ddf0-11e9-93ab-0610dba1f5f1
STEP: Creating configMap with name cm-test-opt-create-dc4447bc-ddf0-11e9-93ab-0610dba1f5f1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:58:15.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6235" for this suite.
Sep 23 10:58:37.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:58:37.419: INFO: namespace projected-6235 deletion completed in 22.091344353s

• [SLOW TEST:100.569 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:58:37.419: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0923 10:59:07.984569      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 23 10:59:07.984: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:59:07.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2169" for this suite.
Sep 23 10:59:14.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:59:14.081: INFO: namespace gc-2169 deletion completed in 6.093689069s

• [SLOW TEST:36.663 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:59:14.082: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:59:16.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4745" for this suite.
Sep 23 10:59:54.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 10:59:54.233: INFO: namespace kubelet-test-4745 deletion completed in 38.092593236s

• [SLOW TEST:40.152 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 10:59:54.234: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep 23 10:59:54.274: INFO: Waiting up to 5m0s for pod "downward-api-45fe860f-ddf1-11e9-93ab-0610dba1f5f1" in namespace "downward-api-3630" to be "success or failure"
Sep 23 10:59:54.280: INFO: Pod "downward-api-45fe860f-ddf1-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.660706ms
Sep 23 10:59:56.284: INFO: Pod "downward-api-45fe860f-ddf1-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009985578s
STEP: Saw pod success
Sep 23 10:59:56.284: INFO: Pod "downward-api-45fe860f-ddf1-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 10:59:56.287: INFO: Trying to get logs from node kube-node2 pod downward-api-45fe860f-ddf1-11e9-93ab-0610dba1f5f1 container dapi-container: <nil>
STEP: delete the pod
Sep 23 10:59:56.306: INFO: Waiting for pod downward-api-45fe860f-ddf1-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 10:59:56.309: INFO: Pod downward-api-45fe860f-ddf1-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 10:59:56.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3630" for this suite.
Sep 23 11:00:02.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 11:00:02.408: INFO: namespace downward-api-3630 deletion completed in 6.094726578s

• [SLOW TEST:8.174 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 11:00:02.408: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 11:00:02.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 version --client'
Sep 23 11:00:02.497: INFO: stderr: ""
Sep 23 11:00:02.497: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:53:57Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Sep 23 11:00:02.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 create -f - --namespace=kubectl-105'
Sep 23 11:00:02.738: INFO: stderr: ""
Sep 23 11:00:02.739: INFO: stdout: "replicationcontroller/redis-master created\n"
Sep 23 11:00:02.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 create -f - --namespace=kubectl-105'
Sep 23 11:00:02.881: INFO: stderr: ""
Sep 23 11:00:02.881: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 23 11:00:03.885: INFO: Selector matched 1 pods for map[app:redis]
Sep 23 11:00:03.885: INFO: Found 0 / 1
Sep 23 11:00:04.885: INFO: Selector matched 1 pods for map[app:redis]
Sep 23 11:00:04.885: INFO: Found 1 / 1
Sep 23 11:00:04.885: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 23 11:00:04.889: INFO: Selector matched 1 pods for map[app:redis]
Sep 23 11:00:04.889: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 23 11:00:04.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 describe pod redis-master-8rblp --namespace=kubectl-105'
Sep 23 11:00:04.973: INFO: stderr: ""
Sep 23 11:00:04.973: INFO: stdout: "Name:               redis-master-8rblp\nNamespace:          kubectl-105\nPriority:           0\nPriorityClassName:  <none>\nNode:               kube-node3/172.16.151.106\nStart Time:         Mon, 23 Sep 2019 11:00:02 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 192.168.119.189/32\nStatus:             Running\nIP:                 192.168.119.189\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://10387a973e59911461fd3c0e93432c1274dae4ee7e53bb20f7994c3dd28ad7b4\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker://sha256:e4423e943a205fe1d81768e60603c8f2c5821576bad0801c1e91b8ba586124a0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 23 Sep 2019 11:00:03 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-cwbq9 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-cwbq9:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-cwbq9\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                 Message\n  ----    ------     ----  ----                 -------\n  Normal  Scheduled  2s    default-scheduler    Successfully assigned kubectl-105/redis-master-8rblp to kube-node3\n  Normal  Pulled     1s    kubelet, kube-node3  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, kube-node3  Created container redis-master\n  Normal  Started    1s    kubelet, kube-node3  Started container redis-master\n"
Sep 23 11:00:04.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 describe rc redis-master --namespace=kubectl-105'
Sep 23 11:00:05.064: INFO: stderr: ""
Sep 23 11:00:05.065: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-105\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-8rblp\n"
Sep 23 11:00:05.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 describe service redis-master --namespace=kubectl-105'
Sep 23 11:00:05.149: INFO: stderr: ""
Sep 23 11:00:05.149: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-105\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.107.174.70\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         192.168.119.189:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep 23 11:00:05.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 describe node kube-master1'
Sep 23 11:00:05.247: INFO: stderr: ""
Sep 23 11:00:05.247: INFO: stdout: "Name:               kube-master1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=kube-master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.16.151.101/16\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.102.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 21 Sep 2019 09:39:27 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sat, 21 Sep 2019 10:06:30 +0000   Sat, 21 Sep 2019 10:06:30 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 23 Sep 2019 10:59:48 +0000   Sat, 21 Sep 2019 09:39:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 23 Sep 2019 10:59:48 +0000   Sat, 21 Sep 2019 09:39:25 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 23 Sep 2019 10:59:48 +0000   Sat, 21 Sep 2019 09:39:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 23 Sep 2019 10:59:48 +0000   Sat, 21 Sep 2019 10:06:38 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.16.151.101\n  Hostname:    kube-master1\nCapacity:\n cpu:                8\n ephemeral-storage:  51175Mi\n hugepages-2Mi:      0\n memory:             16245084Ki\n pods:               110\nAllocatable:\n cpu:                8\n ephemeral-storage:  48294789041\n hugepages-2Mi:      0\n memory:             16142684Ki\n pods:               110\nSystem Info:\n Machine ID:                 53c33f7b152e472699aec599c2898aed\n System UUID:                53C33F7B-152E-4726-99AE-C599C2898AED\n Boot ID:                    4922b84c-7976-40fc-94f2-324d363cb877\n Kernel Version:             3.10.0-862.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.14.0\n Kube-Proxy Version:         v1.14.0\nPodCIDR:                     192.168.0.0/24\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-cd4257a81b874ce2-g5kmv    0 (0%)        0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                calico-node-fxjpk                                          250m (3%)     0 (0%)      0 (0%)           0 (0%)         2d\n  kube-system                etcd-kube-master1                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d1h\n  kube-system                kube-apiserver-kube-master1                                250m (3%)     0 (0%)      0 (0%)           0 (0%)         2d1h\n  kube-system                kube-controller-manager-kube-master1                       200m (2%)     0 (0%)      0 (0%)           0 (0%)         2d1h\n  kube-system                kube-proxy-87456                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d1h\n  kube-system                kube-scheduler-kube-master1                                100m (1%)     0 (0%)      0 (0%)           0 (0%)         2d1h\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                800m (10%)  0 (0%)\n  memory             0 (0%)      0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Sep 23 11:00:05.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 describe namespace kubectl-105'
Sep 23 11:00:05.331: INFO: stderr: ""
Sep 23 11:00:05.331: INFO: stdout: "Name:         kubectl-105\nLabels:       e2e-framework=kubectl\n              e2e-run=1e553bc0-dde6-11e9-93ab-0610dba1f5f1\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 11:00:05.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-105" for this suite.
Sep 23 11:00:27.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 11:00:27.431: INFO: namespace kubectl-105 deletion completed in 22.095145316s

• [SLOW TEST:25.023 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 11:00:27.431: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 11:00:29.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6581" for this suite.
Sep 23 11:00:35.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 11:00:35.610: INFO: namespace emptydir-wrapper-6581 deletion completed in 6.094528763s

• [SLOW TEST:8.179 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 11:00:35.610: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 11:00:38.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6821" for this suite.
Sep 23 11:01:00.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 11:01:00.768: INFO: namespace replication-controller-6821 deletion completed in 22.092308641s

• [SLOW TEST:25.158 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 11:01:00.769: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep 23 11:01:00.815: INFO: Waiting up to 5m0s for pod "downward-api-6da7ed46-ddf1-11e9-93ab-0610dba1f5f1" in namespace "downward-api-9488" to be "success or failure"
Sep 23 11:01:00.821: INFO: Pod "downward-api-6da7ed46-ddf1-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.302555ms
Sep 23 11:01:02.825: INFO: Pod "downward-api-6da7ed46-ddf1-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009972685s
STEP: Saw pod success
Sep 23 11:01:02.825: INFO: Pod "downward-api-6da7ed46-ddf1-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 11:01:02.827: INFO: Trying to get logs from node kube-node2 pod downward-api-6da7ed46-ddf1-11e9-93ab-0610dba1f5f1 container dapi-container: <nil>
STEP: delete the pod
Sep 23 11:01:02.844: INFO: Waiting for pod downward-api-6da7ed46-ddf1-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 11:01:02.847: INFO: Pod downward-api-6da7ed46-ddf1-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 11:01:02.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9488" for this suite.
Sep 23 11:01:08.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 11:01:08.936: INFO: namespace downward-api-9488 deletion completed in 6.084824086s

• [SLOW TEST:8.168 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 11:01:08.937: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-72859fc8-ddf1-11e9-93ab-0610dba1f5f1
STEP: Creating secret with name s-test-opt-upd-7285a084-ddf1-11e9-93ab-0610dba1f5f1
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-72859fc8-ddf1-11e9-93ab-0610dba1f5f1
STEP: Updating secret s-test-opt-upd-7285a084-ddf1-11e9-93ab-0610dba1f5f1
STEP: Creating secret with name s-test-opt-create-7285a0cb-ddf1-11e9-93ab-0610dba1f5f1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 11:01:13.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5608" for this suite.
Sep 23 11:01:35.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 11:01:35.158: INFO: namespace secrets-5608 deletion completed in 22.098486723s

• [SLOW TEST:26.221 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 11:01:35.158: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 23 11:01:35.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-822787e3-ddf1-11e9-93ab-0610dba1f5f1" in namespace "downward-api-5212" to be "success or failure"
Sep 23 11:01:35.208: INFO: Pod "downwardapi-volume-822787e3-ddf1-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.923425ms
Sep 23 11:01:37.212: INFO: Pod "downwardapi-volume-822787e3-ddf1-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006881784s
STEP: Saw pod success
Sep 23 11:01:37.212: INFO: Pod "downwardapi-volume-822787e3-ddf1-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 11:01:37.215: INFO: Trying to get logs from node kube-node3 pod downwardapi-volume-822787e3-ddf1-11e9-93ab-0610dba1f5f1 container client-container: <nil>
STEP: delete the pod
Sep 23 11:01:37.232: INFO: Waiting for pod downwardapi-volume-822787e3-ddf1-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 11:01:37.235: INFO: Pod downwardapi-volume-822787e3-ddf1-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 11:01:37.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5212" for this suite.
Sep 23 11:01:43.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 11:01:43.333: INFO: namespace downward-api-5212 deletion completed in 6.09117097s

• [SLOW TEST:8.174 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 11:01:43.333: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 23 11:01:43.363: INFO: Creating deployment "nginx-deployment"
Sep 23 11:01:43.367: INFO: Waiting for observed generation 1
Sep 23 11:01:45.374: INFO: Waiting for all required pods to come up
Sep 23 11:01:45.378: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep 23 11:01:45.378: INFO: Waiting for deployment "nginx-deployment" to complete
Sep 23 11:01:45.383: INFO: Updating deployment "nginx-deployment" with a non-existent image
Sep 23 11:01:45.392: INFO: Updating deployment nginx-deployment
Sep 23 11:01:45.392: INFO: Waiting for observed generation 2
Sep 23 11:01:47.398: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep 23 11:01:47.401: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep 23 11:01:47.404: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep 23 11:01:47.413: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep 23 11:01:47.413: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep 23 11:01:47.416: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep 23 11:01:47.422: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Sep 23 11:01:47.422: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Sep 23 11:01:47.433: INFO: Updating deployment nginx-deployment
Sep 23 11:01:47.433: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Sep 23 11:01:47.445: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep 23 11:01:47.458: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep 23 11:01:47.481: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-3385,SelfLink:/apis/apps/v1/namespaces/deployment-3385/deployments/nginx-deployment,UID:87056405-ddf1-11e9-a6ed-0cda411df060,ResourceVersion:368502,Generation:3,CreationTimestamp:2019-09-23 11:01:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-09-23 11:01:45 +0000 UTC 2019-09-23 11:01:43 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.} {Available False 2019-09-23 11:01:47 +0000 UTC 2019-09-23 11:01:47 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Sep 23 11:01:47.493: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-3385,SelfLink:/apis/apps/v1/namespaces/deployment-3385/replicasets/nginx-deployment-5f9595f595,UID:883b0be9-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368499,Generation:3,CreationTimestamp:2019-09-23 11:01:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 87056405-ddf1-11e9-a6ed-0cda411df060 0xc002fb9837 0xc002fb9838}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 23 11:01:47.493: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Sep 23 11:01:47.493: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-3385,SelfLink:/apis/apps/v1/namespaces/deployment-3385/replicasets/nginx-deployment-6f478d8d8,UID:870610a7-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368495,Generation:3,CreationTimestamp:2019-09-23 11:01:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 87056405-ddf1-11e9-a6ed-0cda411df060 0xc002fb9907 0xc002fb9908}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Sep 23 11:01:47.520: INFO: Pod "nginx-deployment-5f9595f595-4tjkx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-4tjkx,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-5f9595f595-4tjkx,UID:883b97c3-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368482,Generation:0,CreationTimestamp:2019-09-23 11:01:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.9.74/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 883b0be9-ddf1-11e9-a0c7-0cda411d3e75 0xc0035b69e7 0xc0035b69e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035b6ac0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035b6b20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.104,PodIP:,StartTime:2019-09-23 11:01:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on [::1]:53: read udp [::1]:33764->[::1]:53: read: connection refused,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.520: INFO: Pod "nginx-deployment-5f9595f595-7mvtc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-7mvtc,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-5f9595f595-7mvtc,UID:88469b2d-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368474,Generation:0,CreationTimestamp:2019-09-23 11:01:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.233.242/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 883b0be9-ddf1-11e9-a0c7-0cda411d3e75 0xc0035b6ec0 0xc0035b6ec1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035b7060} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035b7080}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.105,PodIP:192.168.233.242,StartTime:2019-09-23 11:01:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on [::1]:53: read udp [::1]:48513->[::1]:53: read: connection refused,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.520: INFO: Pod "nginx-deployment-5f9595f595-b92gc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-b92gc,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-5f9595f595-b92gc,UID:8845e1c3-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368480,Generation:0,CreationTimestamp:2019-09-23 11:01:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.9.75/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 883b0be9-ddf1-11e9-a0c7-0cda411d3e75 0xc0035b7340 0xc0035b7341}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035b73e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035b7400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.104,PodIP:,StartTime:2019-09-23 11:01:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on [::1]:53: read udp [::1]:39860->[::1]:53: read: connection refused,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.520: INFO: Pod "nginx-deployment-5f9595f595-tcc6m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-tcc6m,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-5f9595f595-tcc6m,UID:883c814b-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368476,Generation:0,CreationTimestamp:2019-09-23 11:01:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.233.241/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 883b0be9-ddf1-11e9-a0c7-0cda411d3e75 0xc0035b7530 0xc0035b7531}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035b75b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035b75d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.105,PodIP:,StartTime:2019-09-23 11:01:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on [::1]:53: read udp [::1]:44029->[::1]:53: read: connection refused,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.521: INFO: Pod "nginx-deployment-5f9595f595-tgjvf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-tgjvf,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-5f9595f595-tgjvf,UID:883c877f-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368478,Generation:0,CreationTimestamp:2019-09-23 11:01:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.119.136/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 883b0be9-ddf1-11e9-a0c7-0cda411d3e75 0xc0035b76c0 0xc0035b76c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035b7740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035b7760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.106,PodIP:,StartTime:2019-09-23 11:01:45 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on [::1]:53: read udp [::1]:41507->[::1]:53: read: connection refused,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.521: INFO: Pod "nginx-deployment-5f9595f595-wzbvg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-wzbvg,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-5f9595f595-wzbvg,UID:8975b7d0-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368511,Generation:0,CreationTimestamp:2019-09-23 11:01:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 883b0be9-ddf1-11e9-a0c7-0cda411d3e75 0xc0035b7840 0xc0035b7841}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035b78c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035b78e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.521: INFO: Pod "nginx-deployment-6f478d8d8-4prp8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-4prp8,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-6f478d8d8-4prp8,UID:87083230-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368350,Generation:0,CreationTimestamp:2019-09-23 11:01:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.233.238/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 870610a7-ddf1-11e9-a0c7-0cda411d3e75 0xc0035b7960 0xc0035b7961}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035b79d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035b79f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:43 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.105,PodIP:192.168.233.238,StartTime:2019-09-23 11:01:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-23 11:01:44 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:8a2fb25a19f5dc1528b7a3fabe8b3145ff57fe10e4f1edac6c718a3cf4aa4b73 docker://ca90771195cead0e5b60393d91f9c04845fc12c9fdc96cb5c74e1830bf76ee9e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.521: INFO: Pod "nginx-deployment-6f478d8d8-58829" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-58829,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-6f478d8d8-58829,UID:897702ad-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368521,Generation:0,CreationTimestamp:2019-09-23 11:01:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 870610a7-ddf1-11e9-a0c7-0cda411d3e75 0xc0035b7ae7 0xc0035b7ae8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035b7b60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035b7b80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.521: INFO: Pod "nginx-deployment-6f478d8d8-5s9pl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-5s9pl,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-6f478d8d8-5s9pl,UID:897453a0-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368507,Generation:0,CreationTimestamp:2019-09-23 11:01:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 870610a7-ddf1-11e9-a0c7-0cda411d3e75 0xc0035b7c00 0xc0035b7c01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035b7c70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035b7c90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.521: INFO: Pod "nginx-deployment-6f478d8d8-99ljs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-99ljs,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-6f478d8d8-99ljs,UID:8707124b-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368361,Generation:0,CreationTimestamp:2019-09-23 11:01:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.119.132/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 870610a7-ddf1-11e9-a0c7-0cda411d3e75 0xc0035b7d10 0xc0035b7d11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035b7d90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035b7db0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:43 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.106,PodIP:192.168.119.132,StartTime:2019-09-23 11:01:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-23 11:01:44 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:8a2fb25a19f5dc1528b7a3fabe8b3145ff57fe10e4f1edac6c718a3cf4aa4b73 docker://da309d7cf15f6de619d0ae689ffbb3ceda0142a321c2cb2d34390328d607844c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.522: INFO: Pod "nginx-deployment-6f478d8d8-9dxfj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-9dxfj,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-6f478d8d8-9dxfj,UID:870f493b-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368363,Generation:0,CreationTimestamp:2019-09-23 11:01:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.119.134/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 870610a7-ddf1-11e9-a0c7-0cda411d3e75 0xc0035b7e87 0xc0035b7e88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035b7f00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035b7f20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:43 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.106,PodIP:192.168.119.134,StartTime:2019-09-23 11:01:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-23 11:01:44 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:8a2fb25a19f5dc1528b7a3fabe8b3145ff57fe10e4f1edac6c718a3cf4aa4b73 docker://aa1832ce6924ea0abe680c0efa3538d124fd4cad9fcd91f7e5f704fff1ffe23a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.522: INFO: Pod "nginx-deployment-6f478d8d8-cc9tz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-cc9tz,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-6f478d8d8-cc9tz,UID:8974a499-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368509,Generation:0,CreationTimestamp:2019-09-23 11:01:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 870610a7-ddf1-11e9-a0c7-0cda411d3e75 0xc0035b7ff7 0xc0035b7ff8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db2070} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db2090}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.523: INFO: Pod "nginx-deployment-6f478d8d8-d2fxx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-d2fxx,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-6f478d8d8-d2fxx,UID:870837fb-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368366,Generation:0,CreationTimestamp:2019-09-23 11:01:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.9.71/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 870610a7-ddf1-11e9-a0c7-0cda411d3e75 0xc002db2120 0xc002db2121}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db2190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db21b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:43 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.104,PodIP:192.168.9.71,StartTime:2019-09-23 11:01:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-23 11:01:44 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:8a2fb25a19f5dc1528b7a3fabe8b3145ff57fe10e4f1edac6c718a3cf4aa4b73 docker://50823e16b377a345b8505d278064c7b13ee45fd4fad69d3424a1239d6ecf8dbc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.523: INFO: Pod "nginx-deployment-6f478d8d8-dr6m5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-dr6m5,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-6f478d8d8-dr6m5,UID:89732741-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368523,Generation:0,CreationTimestamp:2019-09-23 11:01:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 870610a7-ddf1-11e9-a0c7-0cda411d3e75 0xc002db2287 0xc002db2288}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db2300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db2320}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:47 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.106,PodIP:,StartTime:2019-09-23 11:01:47 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.523: INFO: Pod "nginx-deployment-6f478d8d8-hrgb4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-hrgb4,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-6f478d8d8-hrgb4,UID:870bece1-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368374,Generation:0,CreationTimestamp:2019-09-23 11:01:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.9.72/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 870610a7-ddf1-11e9-a0c7-0cda411d3e75 0xc002db23f7 0xc002db23f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db2470} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db2490}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:43 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.104,PodIP:192.168.9.72,StartTime:2019-09-23 11:01:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-23 11:01:44 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:8a2fb25a19f5dc1528b7a3fabe8b3145ff57fe10e4f1edac6c718a3cf4aa4b73 docker://2227efeadae61f1c219a3e79c44bbe4b113e24b84588742218367b4364ec41de}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.523: INFO: Pod "nginx-deployment-6f478d8d8-jnszj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-jnszj,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-6f478d8d8-jnszj,UID:8976ca91-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368519,Generation:0,CreationTimestamp:2019-09-23 11:01:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 870610a7-ddf1-11e9-a0c7-0cda411d3e75 0xc002db2567 0xc002db2568}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db25e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db2600}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.523: INFO: Pod "nginx-deployment-6f478d8d8-kmsgd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-kmsgd,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-6f478d8d8-kmsgd,UID:8976d6f4-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368522,Generation:0,CreationTimestamp:2019-09-23 11:01:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 870610a7-ddf1-11e9-a0c7-0cda411d3e75 0xc002db2680 0xc002db2681}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db26f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db2710}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.524: INFO: Pod "nginx-deployment-6f478d8d8-mvvb6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-mvvb6,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-6f478d8d8-mvvb6,UID:870f3bc4-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368356,Generation:0,CreationTimestamp:2019-09-23 11:01:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.233.240/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 870610a7-ddf1-11e9-a0c7-0cda411d3e75 0xc002db2790 0xc002db2791}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db2800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db2820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:43 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.105,PodIP:192.168.233.240,StartTime:2019-09-23 11:01:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-23 11:01:44 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:8a2fb25a19f5dc1528b7a3fabe8b3145ff57fe10e4f1edac6c718a3cf4aa4b73 docker://b0b2ea1ff1d63ed53a9382fe6d5671ef0ea57c2be8dc715f7c1231e4a72f3a0f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.524: INFO: Pod "nginx-deployment-6f478d8d8-rxhbd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rxhbd,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-6f478d8d8-rxhbd,UID:870f211b-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368370,Generation:0,CreationTimestamp:2019-09-23 11:01:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.9.73/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 870610a7-ddf1-11e9-a0c7-0cda411d3e75 0xc002db2907 0xc002db2908}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db2980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db29a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:43 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.104,PodIP:192.168.9.73,StartTime:2019-09-23 11:01:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-23 11:01:44 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:8a2fb25a19f5dc1528b7a3fabe8b3145ff57fe10e4f1edac6c718a3cf4aa4b73 docker://e4b1c04bd0fbbf88c0709a961ec40bef670891f6d6d0feb47073dbcbb71174f7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.524: INFO: Pod "nginx-deployment-6f478d8d8-vr6wr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-vr6wr,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-6f478d8d8-vr6wr,UID:8976d687-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368520,Generation:0,CreationTimestamp:2019-09-23 11:01:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 870610a7-ddf1-11e9-a0c7-0cda411d3e75 0xc002db2a87 0xc002db2a88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db2b00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db2b20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:47 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 23 11:01:47.524: INFO: Pod "nginx-deployment-6f478d8d8-w4bwx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-w4bwx,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3385,SelfLink:/api/v1/namespaces/deployment-3385/pods/nginx-deployment-6f478d8d8-w4bwx,UID:870be370-ddf1-11e9-a0c7-0cda411d3e75,ResourceVersion:368352,Generation:0,CreationTimestamp:2019-09-23 11:01:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.233.239/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 870610a7-ddf1-11e9-a0c7-0cda411d3e75 0xc002db2bb0 0xc002db2bb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hngdz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hngdz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hngdz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db2c20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db2d10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-23 11:01:43 +0000 UTC  }],Message:,Reason:,HostIP:172.16.151.105,PodIP:192.168.233.239,StartTime:2019-09-23 11:01:43 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-23 11:01:44 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:8a2fb25a19f5dc1528b7a3fabe8b3145ff57fe10e4f1edac6c718a3cf4aa4b73 docker://8863900c6da84369de696b07d8cd7f277609c4d8b21dbf2c911dcc37de60c8f8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 11:01:47.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3385" for this suite.
Sep 23 11:01:55.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 11:01:55.794: INFO: namespace deployment-3385 deletion completed in 8.219124399s

• [SLOW TEST:12.461 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 11:01:55.794: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 23 11:01:55.842: INFO: Waiting up to 5m0s for pod "pod-8e747d72-ddf1-11e9-93ab-0610dba1f5f1" in namespace "emptydir-8141" to be "success or failure"
Sep 23 11:01:55.850: INFO: Pod "pod-8e747d72-ddf1-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.528351ms
Sep 23 11:01:57.854: INFO: Pod "pod-8e747d72-ddf1-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011681715s
STEP: Saw pod success
Sep 23 11:01:57.854: INFO: Pod "pod-8e747d72-ddf1-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 11:01:57.857: INFO: Trying to get logs from node kube-node3 pod pod-8e747d72-ddf1-11e9-93ab-0610dba1f5f1 container test-container: <nil>
STEP: delete the pod
Sep 23 11:01:57.878: INFO: Waiting for pod pod-8e747d72-ddf1-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 11:01:57.880: INFO: Pod pod-8e747d72-ddf1-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 11:01:57.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8141" for this suite.
Sep 23 11:02:03.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 11:02:03.988: INFO: namespace emptydir-8141 deletion completed in 6.104248684s

• [SLOW TEST:8.194 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 11:02:03.988: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-9g89
STEP: Creating a pod to test atomic-volume-subpath
Sep 23 11:02:04.044: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9g89" in namespace "subpath-6680" to be "success or failure"
Sep 23 11:02:04.047: INFO: Pod "pod-subpath-test-configmap-9g89": Phase="Pending", Reason="", readiness=false. Elapsed: 3.451076ms
Sep 23 11:02:06.051: INFO: Pod "pod-subpath-test-configmap-9g89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00749368s
Sep 23 11:02:08.055: INFO: Pod "pod-subpath-test-configmap-9g89": Phase="Running", Reason="", readiness=true. Elapsed: 4.010928757s
Sep 23 11:02:10.059: INFO: Pod "pod-subpath-test-configmap-9g89": Phase="Running", Reason="", readiness=true. Elapsed: 6.014653806s
Sep 23 11:02:12.062: INFO: Pod "pod-subpath-test-configmap-9g89": Phase="Running", Reason="", readiness=true. Elapsed: 8.018241063s
Sep 23 11:02:14.066: INFO: Pod "pod-subpath-test-configmap-9g89": Phase="Running", Reason="", readiness=true. Elapsed: 10.022157798s
Sep 23 11:02:16.070: INFO: Pod "pod-subpath-test-configmap-9g89": Phase="Running", Reason="", readiness=true. Elapsed: 12.026112152s
Sep 23 11:02:18.074: INFO: Pod "pod-subpath-test-configmap-9g89": Phase="Running", Reason="", readiness=true. Elapsed: 14.029998484s
Sep 23 11:02:20.077: INFO: Pod "pod-subpath-test-configmap-9g89": Phase="Running", Reason="", readiness=true. Elapsed: 16.033386044s
Sep 23 11:02:22.081: INFO: Pod "pod-subpath-test-configmap-9g89": Phase="Running", Reason="", readiness=true. Elapsed: 18.036632589s
Sep 23 11:02:24.085: INFO: Pod "pod-subpath-test-configmap-9g89": Phase="Running", Reason="", readiness=true. Elapsed: 20.040818896s
Sep 23 11:02:26.089: INFO: Pod "pod-subpath-test-configmap-9g89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.044605506s
STEP: Saw pod success
Sep 23 11:02:26.089: INFO: Pod "pod-subpath-test-configmap-9g89" satisfied condition "success or failure"
Sep 23 11:02:26.091: INFO: Trying to get logs from node kube-node3 pod pod-subpath-test-configmap-9g89 container test-container-subpath-configmap-9g89: <nil>
STEP: delete the pod
Sep 23 11:02:26.108: INFO: Waiting for pod pod-subpath-test-configmap-9g89 to disappear
Sep 23 11:02:26.110: INFO: Pod pod-subpath-test-configmap-9g89 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9g89
Sep 23 11:02:26.110: INFO: Deleting pod "pod-subpath-test-configmap-9g89" in namespace "subpath-6680"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 11:02:26.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6680" for this suite.
Sep 23 11:02:32.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 11:02:32.208: INFO: namespace subpath-6680 deletion completed in 6.091700078s

• [SLOW TEST:28.220 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 11:02:32.208: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-a4279717-ddf1-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume secrets
Sep 23 11:02:32.251: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a4280eca-ddf1-11e9-93ab-0610dba1f5f1" in namespace "projected-610" to be "success or failure"
Sep 23 11:02:32.253: INFO: Pod "pod-projected-secrets-a4280eca-ddf1-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.527261ms
Sep 23 11:02:34.260: INFO: Pod "pod-projected-secrets-a4280eca-ddf1-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008914122s
STEP: Saw pod success
Sep 23 11:02:34.260: INFO: Pod "pod-projected-secrets-a4280eca-ddf1-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 11:02:34.262: INFO: Trying to get logs from node kube-node3 pod pod-projected-secrets-a4280eca-ddf1-11e9-93ab-0610dba1f5f1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 23 11:02:34.279: INFO: Waiting for pod pod-projected-secrets-a4280eca-ddf1-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 11:02:34.281: INFO: Pod pod-projected-secrets-a4280eca-ddf1-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 11:02:34.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-610" for this suite.
Sep 23 11:02:40.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 11:02:40.385: INFO: namespace projected-610 deletion completed in 6.09614708s

• [SLOW TEST:8.177 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 11:02:40.386: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-a9079f22-ddf1-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume secrets
Sep 23 11:02:40.430: INFO: Waiting up to 5m0s for pod "pod-secrets-a9081e9b-ddf1-11e9-93ab-0610dba1f5f1" in namespace "secrets-155" to be "success or failure"
Sep 23 11:02:40.432: INFO: Pod "pod-secrets-a9081e9b-ddf1-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.903335ms
Sep 23 11:02:42.436: INFO: Pod "pod-secrets-a9081e9b-ddf1-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006801767s
STEP: Saw pod success
Sep 23 11:02:42.436: INFO: Pod "pod-secrets-a9081e9b-ddf1-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 11:02:42.443: INFO: Trying to get logs from node kube-node3 pod pod-secrets-a9081e9b-ddf1-11e9-93ab-0610dba1f5f1 container secret-env-test: <nil>
STEP: delete the pod
Sep 23 11:02:42.462: INFO: Waiting for pod pod-secrets-a9081e9b-ddf1-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 11:02:42.466: INFO: Pod pod-secrets-a9081e9b-ddf1-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 11:02:42.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-155" for this suite.
Sep 23 11:02:48.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 11:02:48.571: INFO: namespace secrets-155 deletion completed in 6.101506989s

• [SLOW TEST:8.185 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 11:02:48.571: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Sep 23 11:02:48.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 create -f - --namespace=kubectl-3670'
Sep 23 11:02:48.754: INFO: stderr: ""
Sep 23 11:02:48.754: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 23 11:02:48.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3670'
Sep 23 11:02:48.838: INFO: stderr: ""
Sep 23 11:02:48.838: INFO: stdout: "update-demo-nautilus-qh476 update-demo-nautilus-tthwt "
Sep 23 11:02:48.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-qh476 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Sep 23 11:02:48.913: INFO: stderr: ""
Sep 23 11:02:48.913: INFO: stdout: ""
Sep 23 11:02:48.913: INFO: update-demo-nautilus-qh476 is created but not running
Sep 23 11:02:53.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3670'
Sep 23 11:02:53.990: INFO: stderr: ""
Sep 23 11:02:53.990: INFO: stdout: "update-demo-nautilus-qh476 update-demo-nautilus-tthwt "
Sep 23 11:02:53.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-qh476 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Sep 23 11:02:54.061: INFO: stderr: ""
Sep 23 11:02:54.061: INFO: stdout: "true"
Sep 23 11:02:54.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-qh476 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Sep 23 11:02:54.132: INFO: stderr: ""
Sep 23 11:02:54.132: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 23 11:02:54.132: INFO: validating pod update-demo-nautilus-qh476
Sep 23 11:02:54.137: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 23 11:02:54.137: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 23 11:02:54.137: INFO: update-demo-nautilus-qh476 is verified up and running
Sep 23 11:02:54.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-tthwt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Sep 23 11:02:54.208: INFO: stderr: ""
Sep 23 11:02:54.208: INFO: stdout: "true"
Sep 23 11:02:54.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods update-demo-nautilus-tthwt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3670'
Sep 23 11:02:54.279: INFO: stderr: ""
Sep 23 11:02:54.279: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 23 11:02:54.279: INFO: validating pod update-demo-nautilus-tthwt
Sep 23 11:02:54.285: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 23 11:02:54.285: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 23 11:02:54.285: INFO: update-demo-nautilus-tthwt is verified up and running
STEP: using delete to clean up resources
Sep 23 11:02:54.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 delete --grace-period=0 --force -f - --namespace=kubectl-3670'
Sep 23 11:02:54.361: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 11:02:54.361: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 23 11:02:54.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3670'
Sep 23 11:02:54.441: INFO: stderr: "No resources found.\n"
Sep 23 11:02:54.441: INFO: stdout: ""
Sep 23 11:02:54.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods -l name=update-demo --namespace=kubectl-3670 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 23 11:02:54.514: INFO: stderr: ""
Sep 23 11:02:54.514: INFO: stdout: "update-demo-nautilus-qh476\nupdate-demo-nautilus-tthwt\n"
Sep 23 11:02:55.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3670'
Sep 23 11:02:55.099: INFO: stderr: "No resources found.\n"
Sep 23 11:02:55.099: INFO: stdout: ""
Sep 23 11:02:55.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods -l name=update-demo --namespace=kubectl-3670 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 23 11:02:55.179: INFO: stderr: ""
Sep 23 11:02:55.179: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 11:02:55.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3670" for this suite.
Sep 23 11:03:17.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 11:03:17.281: INFO: namespace kubectl-3670 deletion completed in 22.097501039s

• [SLOW TEST:28.710 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 11:03:17.282: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-bf04e65a-ddf1-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume configMaps
Sep 23 11:03:17.324: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bf057b48-ddf1-11e9-93ab-0610dba1f5f1" in namespace "projected-7335" to be "success or failure"
Sep 23 11:03:17.326: INFO: Pod "pod-projected-configmaps-bf057b48-ddf1-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.503581ms
Sep 23 11:03:19.330: INFO: Pod "pod-projected-configmaps-bf057b48-ddf1-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005948198s
Sep 23 11:03:21.333: INFO: Pod "pod-projected-configmaps-bf057b48-ddf1-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009786625s
STEP: Saw pod success
Sep 23 11:03:21.333: INFO: Pod "pod-projected-configmaps-bf057b48-ddf1-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 11:03:21.336: INFO: Trying to get logs from node kube-node3 pod pod-projected-configmaps-bf057b48-ddf1-11e9-93ab-0610dba1f5f1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 23 11:03:21.361: INFO: Waiting for pod pod-projected-configmaps-bf057b48-ddf1-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 11:03:21.364: INFO: Pod pod-projected-configmaps-bf057b48-ddf1-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 11:03:21.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7335" for this suite.
Sep 23 11:03:27.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 11:03:27.457: INFO: namespace projected-7335 deletion completed in 6.087981627s

• [SLOW TEST:10.175 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 11:03:27.457: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
Sep 23 11:03:27.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 create -f - --namespace=kubectl-42'
Sep 23 11:03:27.698: INFO: stderr: ""
Sep 23 11:03:27.698: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Sep 23 11:03:28.702: INFO: Selector matched 1 pods for map[app:redis]
Sep 23 11:03:28.702: INFO: Found 0 / 1
Sep 23 11:03:29.702: INFO: Selector matched 1 pods for map[app:redis]
Sep 23 11:03:29.702: INFO: Found 1 / 1
Sep 23 11:03:29.702: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 23 11:03:29.704: INFO: Selector matched 1 pods for map[app:redis]
Sep 23 11:03:29.704: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Sep 23 11:03:29.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 logs redis-master-q66tn redis-master --namespace=kubectl-42'
Sep 23 11:03:29.790: INFO: stderr: ""
Sep 23 11:03:29.790: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 23 Sep 11:03:28.694 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 23 Sep 11:03:28.694 # Server started, Redis version 3.2.12\n1:M 23 Sep 11:03:28.694 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 23 Sep 11:03:28.694 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Sep 23 11:03:29.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 log redis-master-q66tn redis-master --namespace=kubectl-42 --tail=1'
Sep 23 11:03:29.881: INFO: stderr: ""
Sep 23 11:03:29.881: INFO: stdout: "1:M 23 Sep 11:03:28.694 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Sep 23 11:03:29.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 log redis-master-q66tn redis-master --namespace=kubectl-42 --limit-bytes=1'
Sep 23 11:03:29.965: INFO: stderr: ""
Sep 23 11:03:29.965: INFO: stdout: " "
STEP: exposing timestamps
Sep 23 11:03:29.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 log redis-master-q66tn redis-master --namespace=kubectl-42 --tail=1 --timestamps'
Sep 23 11:03:30.055: INFO: stderr: ""
Sep 23 11:03:30.055: INFO: stdout: "2019-09-23T11:03:28.695125074Z 1:M 23 Sep 11:03:28.694 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Sep 23 11:03:32.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 log redis-master-q66tn redis-master --namespace=kubectl-42 --since=1s'
Sep 23 11:03:32.641: INFO: stderr: ""
Sep 23 11:03:32.641: INFO: stdout: ""
Sep 23 11:03:32.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 log redis-master-q66tn redis-master --namespace=kubectl-42 --since=24h'
Sep 23 11:03:32.727: INFO: stderr: ""
Sep 23 11:03:32.727: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 23 Sep 11:03:28.694 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 23 Sep 11:03:28.694 # Server started, Redis version 3.2.12\n1:M 23 Sep 11:03:28.694 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 23 Sep 11:03:28.694 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
Sep 23 11:03:32.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 delete --grace-period=0 --force -f - --namespace=kubectl-42'
Sep 23 11:03:32.802: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 11:03:32.802: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Sep 23 11:03:32.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get rc,svc -l name=nginx --no-headers --namespace=kubectl-42'
Sep 23 11:03:32.879: INFO: stderr: "No resources found.\n"
Sep 23 11:03:32.879: INFO: stdout: ""
Sep 23 11:03:32.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-533696960 get pods -l name=nginx --namespace=kubectl-42 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 23 11:03:32.950: INFO: stderr: ""
Sep 23 11:03:32.950: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 11:03:32.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-42" for this suite.
Sep 23 11:03:38.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 11:03:39.047: INFO: namespace kubectl-42 deletion completed in 6.092555866s

• [SLOW TEST:11.590 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 11:03:39.047: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3121.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3121.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3121.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3121.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3121.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3121.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3121.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3121.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3121.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 252.206.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.206.252_udp@PTR;check="$$(dig +tcp +noall +answer +search 252.206.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.206.252_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3121.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3121.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3121.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3121.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3121.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3121.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3121.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3121.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3121.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3121.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 252.206.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.206.252_udp@PTR;check="$$(dig +tcp +noall +answer +search 252.206.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.206.252_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 23 11:03:41.133: INFO: Unable to read wheezy_udp@dns-test-service.dns-3121.svc.cluster.local from pod dns-3121/dns-test-cc01d985-ddf1-11e9-93ab-0610dba1f5f1: the server could not find the requested resource (get pods dns-test-cc01d985-ddf1-11e9-93ab-0610dba1f5f1)
Sep 23 11:03:41.136: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3121.svc.cluster.local from pod dns-3121/dns-test-cc01d985-ddf1-11e9-93ab-0610dba1f5f1: the server could not find the requested resource (get pods dns-test-cc01d985-ddf1-11e9-93ab-0610dba1f5f1)
Sep 23 11:03:41.139: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3121.svc.cluster.local from pod dns-3121/dns-test-cc01d985-ddf1-11e9-93ab-0610dba1f5f1: the server could not find the requested resource (get pods dns-test-cc01d985-ddf1-11e9-93ab-0610dba1f5f1)
Sep 23 11:03:41.142: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3121.svc.cluster.local from pod dns-3121/dns-test-cc01d985-ddf1-11e9-93ab-0610dba1f5f1: the server could not find the requested resource (get pods dns-test-cc01d985-ddf1-11e9-93ab-0610dba1f5f1)
Sep 23 11:03:41.160: INFO: Unable to read jessie_udp@dns-test-service.dns-3121.svc.cluster.local from pod dns-3121/dns-test-cc01d985-ddf1-11e9-93ab-0610dba1f5f1: the server could not find the requested resource (get pods dns-test-cc01d985-ddf1-11e9-93ab-0610dba1f5f1)
Sep 23 11:03:41.163: INFO: Unable to read jessie_tcp@dns-test-service.dns-3121.svc.cluster.local from pod dns-3121/dns-test-cc01d985-ddf1-11e9-93ab-0610dba1f5f1: the server could not find the requested resource (get pods dns-test-cc01d985-ddf1-11e9-93ab-0610dba1f5f1)
Sep 23 11:03:41.166: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3121.svc.cluster.local from pod dns-3121/dns-test-cc01d985-ddf1-11e9-93ab-0610dba1f5f1: the server could not find the requested resource (get pods dns-test-cc01d985-ddf1-11e9-93ab-0610dba1f5f1)
Sep 23 11:03:41.168: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3121.svc.cluster.local from pod dns-3121/dns-test-cc01d985-ddf1-11e9-93ab-0610dba1f5f1: the server could not find the requested resource (get pods dns-test-cc01d985-ddf1-11e9-93ab-0610dba1f5f1)
Sep 23 11:03:41.189: INFO: Lookups using dns-3121/dns-test-cc01d985-ddf1-11e9-93ab-0610dba1f5f1 failed for: [wheezy_udp@dns-test-service.dns-3121.svc.cluster.local wheezy_tcp@dns-test-service.dns-3121.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3121.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3121.svc.cluster.local jessie_udp@dns-test-service.dns-3121.svc.cluster.local jessie_tcp@dns-test-service.dns-3121.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3121.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3121.svc.cluster.local]

Sep 23 11:03:46.249: INFO: DNS probes using dns-3121/dns-test-cc01d985-ddf1-11e9-93ab-0610dba1f5f1 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 11:03:46.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3121" for this suite.
Sep 23 11:03:52.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 11:03:52.441: INFO: namespace dns-3121 deletion completed in 6.097693506s

• [SLOW TEST:13.394 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 11:03:52.441: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0923 11:04:02.535230      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 23 11:04:02.535: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 11:04:02.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3816" for this suite.
Sep 23 11:04:08.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 11:04:08.634: INFO: namespace gc-3816 deletion completed in 6.095428707s

• [SLOW TEST:16.193 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 23 11:04:08.634: INFO: >>> kubeConfig: /tmp/kubeconfig-533696960
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-dda0a5c8-ddf1-11e9-93ab-0610dba1f5f1
STEP: Creating a pod to test consume configMaps
Sep 23 11:04:08.677: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dda176a8-ddf1-11e9-93ab-0610dba1f5f1" in namespace "projected-7918" to be "success or failure"
Sep 23 11:04:08.680: INFO: Pod "pod-projected-configmaps-dda176a8-ddf1-11e9-93ab-0610dba1f5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.800371ms
Sep 23 11:04:10.684: INFO: Pod "pod-projected-configmaps-dda176a8-ddf1-11e9-93ab-0610dba1f5f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007316558s
STEP: Saw pod success
Sep 23 11:04:10.684: INFO: Pod "pod-projected-configmaps-dda176a8-ddf1-11e9-93ab-0610dba1f5f1" satisfied condition "success or failure"
Sep 23 11:04:10.687: INFO: Trying to get logs from node kube-node3 pod pod-projected-configmaps-dda176a8-ddf1-11e9-93ab-0610dba1f5f1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 23 11:04:10.708: INFO: Waiting for pod pod-projected-configmaps-dda176a8-ddf1-11e9-93ab-0610dba1f5f1 to disappear
Sep 23 11:04:10.713: INFO: Pod pod-projected-configmaps-dda176a8-ddf1-11e9-93ab-0610dba1f5f1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 23 11:04:10.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7918" for this suite.
Sep 23 11:04:16.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 23 11:04:16.805: INFO: namespace projected-7918 deletion completed in 6.08775767s

• [SLOW TEST:8.171 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSep 23 11:04:16.805: INFO: Running AfterSuite actions on all nodes
Sep 23 11:04:16.805: INFO: Running AfterSuite actions on node 1
Sep 23 11:04:16.805: INFO: Skipping dumping logs from cluster

Ran 204 of 3584 Specs in 5052.280 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3380 Skipped PASS

Ginkgo ran 1 suite in 1h24m14.129624877s
Test Suite Passed
