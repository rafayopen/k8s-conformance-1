I0827 18:12:54.698714      18 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-561368167
I0827 18:12:54.698887      18 e2e.go:240] Starting e2e run "49a7a9d1-c8f6-11e9-a9da-1ec1181dd093" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1566929573 - Will randomize all specs
Will run 204 of 3586 specs

Aug 27 18:12:54.892: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 18:12:54.895: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 27 18:12:54.936: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 27 18:12:55.001: INFO: 23 / 23 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 27 18:12:55.001: INFO: expected 11 pod replicas in namespace 'kube-system', 11 are Running and Ready.
Aug 27 18:12:55.001: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 27 18:12:55.019: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Aug 27 18:12:55.019: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Aug 27 18:12:55.019: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-kube-fluentd' (0 seconds elapsed)
Aug 27 18:12:55.019: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'ibm-master-proxy' (0 seconds elapsed)
Aug 27 18:12:55.019: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
Aug 27 18:12:55.019: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Aug 27 18:12:55.019: INFO: e2e test version: v1.14.6
Aug 27 18:12:55.022: INFO: kube-apiserver version: v1.14.6+IKS
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:12:55.024: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename pods
Aug 27 18:12:55.103: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Aug 27 18:12:55.146: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6693
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 27 18:13:01.857: INFO: Successfully updated pod "pod-update-4aba6e8b-c8f6-11e9-a9da-1ec1181dd093"
STEP: verifying the updated pod is in kubernetes
Aug 27 18:13:01.897: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:13:01.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6693" for this suite.
Aug 27 18:13:23.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:13:24.273: INFO: namespace pods-6693 deletion completed in 22.366572793s

• [SLOW TEST:29.249 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:13:24.274: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1594
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:13:30.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1594" for this suite.
Aug 27 18:14:20.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:14:20.965: INFO: namespace kubelet-test-1594 deletion completed in 50.380485827s

• [SLOW TEST:56.691 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:14:20.967: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6067
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 18:14:21.163: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 27 18:14:21.183: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 27 18:14:26.193: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 27 18:14:26.193: INFO: Creating deployment "test-rolling-update-deployment"
Aug 27 18:14:26.204: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 27 18:14:26.220: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 27 18:14:28.238: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 27 18:14:28.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702526466, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702526466, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702526466, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702526466, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-57b6b5bb54\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 18:14:30.257: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702526466, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702526466, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702526466, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702526466, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-57b6b5bb54\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 18:14:32.258: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug 27 18:14:32.290: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-6067,SelfLink:/apis/apps/v1/namespaces/deployment-6067/deployments/test-rolling-update-deployment,UID:80e86486-c8f6-11e9-9af8-1e15abefec17,ResourceVersion:19544,Generation:1,CreationTimestamp:2019-08-27 18:14:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-27 18:14:26 +0000 UTC 2019-08-27 18:14:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-27 18:14:30 +0000 UTC 2019-08-27 18:14:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-57b6b5bb54" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 27 18:14:32.297: INFO: New ReplicaSet "test-rolling-update-deployment-57b6b5bb54" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-57b6b5bb54,GenerateName:,Namespace:deployment-6067,SelfLink:/apis/apps/v1/namespaces/deployment-6067/replicasets/test-rolling-update-deployment-57b6b5bb54,UID:80ec4286-c8f6-11e9-9af8-1e15abefec17,ResourceVersion:19534,Generation:1,CreationTimestamp:2019-08-27 18:14:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 80e86486-c8f6-11e9-9af8-1e15abefec17 0xc0022db217 0xc0022db218}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 27 18:14:32.297: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 27 18:14:32.297: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-6067,SelfLink:/apis/apps/v1/namespaces/deployment-6067/replicasets/test-rolling-update-controller,UID:7de8e674-c8f6-11e9-9af8-1e15abefec17,ResourceVersion:19543,Generation:2,CreationTimestamp:2019-08-27 18:14:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 80e86486-c8f6-11e9-9af8-1e15abefec17 0xc0022db147 0xc0022db148}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 27 18:14:32.308: INFO: Pod "test-rolling-update-deployment-57b6b5bb54-q52t2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-57b6b5bb54-q52t2,GenerateName:test-rolling-update-deployment-57b6b5bb54-,Namespace:deployment-6067,SelfLink:/api/v1/namespaces/deployment-6067/pods/test-rolling-update-deployment-57b6b5bb54-q52t2,UID:80ed3708-c8f6-11e9-9af8-1e15abefec17,ResourceVersion:19533,Generation:0,CreationTimestamp:2019-08-27 18:14:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-57b6b5bb54 80ec4286-c8f6-11e9-9af8-1e15abefec17 0xc000db8047 0xc000db8048}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jsmjt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jsmjt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-jsmjt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000db80c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000db80e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:14:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:14:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:14:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:14:26 +0000 UTC  }],Message:,Reason:,HostIP:10.135.206.222,PodIP:172.30.172.44,StartTime:2019-08-27 18:14:26 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-27 18:14:29 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://d10c92a582720ad3d003fb4956a7fdc2665105d33683a78c262d27369ba86a46}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:14:32.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6067" for this suite.
Aug 27 18:14:38.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:14:38.662: INFO: namespace deployment-6067 deletion completed in 6.345611183s

• [SLOW TEST:17.695 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:14:38.662: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5736
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Aug 27 18:14:38.862: INFO: Waiting up to 5m0s for pod "var-expansion-8875d6ec-c8f6-11e9-a9da-1ec1181dd093" in namespace "var-expansion-5736" to be "success or failure"
Aug 27 18:14:38.870: INFO: Pod "var-expansion-8875d6ec-c8f6-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 8.776737ms
Aug 27 18:14:40.881: INFO: Pod "var-expansion-8875d6ec-c8f6-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019034301s
STEP: Saw pod success
Aug 27 18:14:40.881: INFO: Pod "var-expansion-8875d6ec-c8f6-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:14:40.889: INFO: Trying to get logs from node 10.135.206.197 pod var-expansion-8875d6ec-c8f6-11e9-a9da-1ec1181dd093 container dapi-container: <nil>
STEP: delete the pod
Aug 27 18:14:40.939: INFO: Waiting for pod var-expansion-8875d6ec-c8f6-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:14:40.952: INFO: Pod var-expansion-8875d6ec-c8f6-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:14:40.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5736" for this suite.
Aug 27 18:14:46.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:14:47.311: INFO: namespace var-expansion-5736 deletion completed in 6.351264119s

• [SLOW TEST:8.649 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:14:47.313: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2053
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-7wj5
STEP: Creating a pod to test atomic-volume-subpath
Aug 27 18:14:47.545: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-7wj5" in namespace "subpath-2053" to be "success or failure"
Aug 27 18:14:47.557: INFO: Pod "pod-subpath-test-configmap-7wj5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.041203ms
Aug 27 18:14:49.568: INFO: Pod "pod-subpath-test-configmap-7wj5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022979399s
Aug 27 18:14:51.577: INFO: Pod "pod-subpath-test-configmap-7wj5": Phase="Running", Reason="", readiness=true. Elapsed: 4.032177815s
Aug 27 18:14:53.587: INFO: Pod "pod-subpath-test-configmap-7wj5": Phase="Running", Reason="", readiness=true. Elapsed: 6.041845912s
Aug 27 18:14:55.598: INFO: Pod "pod-subpath-test-configmap-7wj5": Phase="Running", Reason="", readiness=true. Elapsed: 8.052791951s
Aug 27 18:14:57.607: INFO: Pod "pod-subpath-test-configmap-7wj5": Phase="Running", Reason="", readiness=true. Elapsed: 10.062246235s
Aug 27 18:14:59.622: INFO: Pod "pod-subpath-test-configmap-7wj5": Phase="Running", Reason="", readiness=true. Elapsed: 12.076875277s
Aug 27 18:15:01.633: INFO: Pod "pod-subpath-test-configmap-7wj5": Phase="Running", Reason="", readiness=true. Elapsed: 14.088238003s
Aug 27 18:15:03.643: INFO: Pod "pod-subpath-test-configmap-7wj5": Phase="Running", Reason="", readiness=true. Elapsed: 16.097456905s
Aug 27 18:15:05.669: INFO: Pod "pod-subpath-test-configmap-7wj5": Phase="Running", Reason="", readiness=true. Elapsed: 18.124223207s
Aug 27 18:15:07.681: INFO: Pod "pod-subpath-test-configmap-7wj5": Phase="Running", Reason="", readiness=true. Elapsed: 20.135724453s
Aug 27 18:15:09.692: INFO: Pod "pod-subpath-test-configmap-7wj5": Phase="Running", Reason="", readiness=true. Elapsed: 22.147236898s
Aug 27 18:15:11.704: INFO: Pod "pod-subpath-test-configmap-7wj5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.158604577s
STEP: Saw pod success
Aug 27 18:15:11.704: INFO: Pod "pod-subpath-test-configmap-7wj5" satisfied condition "success or failure"
Aug 27 18:15:11.719: INFO: Trying to get logs from node 10.134.235.118 pod pod-subpath-test-configmap-7wj5 container test-container-subpath-configmap-7wj5: <nil>
STEP: delete the pod
Aug 27 18:15:11.773: INFO: Waiting for pod pod-subpath-test-configmap-7wj5 to disappear
Aug 27 18:15:11.782: INFO: Pod pod-subpath-test-configmap-7wj5 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-7wj5
Aug 27 18:15:11.782: INFO: Deleting pod "pod-subpath-test-configmap-7wj5" in namespace "subpath-2053"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:15:11.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2053" for this suite.
Aug 27 18:15:17.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:15:18.182: INFO: namespace subpath-2053 deletion completed in 6.378644764s

• [SLOW TEST:30.870 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:15:18.183: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Aug 27 18:15:18.383: INFO: Waiting up to 5m0s for pod "downward-api-a004557d-c8f6-11e9-a9da-1ec1181dd093" in namespace "downward-api-2961" to be "success or failure"
Aug 27 18:15:18.400: INFO: Pod "downward-api-a004557d-c8f6-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 16.819558ms
Aug 27 18:15:20.417: INFO: Pod "downward-api-a004557d-c8f6-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033186478s
Aug 27 18:15:22.470: INFO: Pod "downward-api-a004557d-c8f6-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.086173708s
STEP: Saw pod success
Aug 27 18:15:22.470: INFO: Pod "downward-api-a004557d-c8f6-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:15:22.479: INFO: Trying to get logs from node 10.134.235.118 pod downward-api-a004557d-c8f6-11e9-a9da-1ec1181dd093 container dapi-container: <nil>
STEP: delete the pod
Aug 27 18:15:22.526: INFO: Waiting for pod downward-api-a004557d-c8f6-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:15:22.539: INFO: Pod downward-api-a004557d-c8f6-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:15:22.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2961" for this suite.
Aug 27 18:15:28.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:15:28.895: INFO: namespace downward-api-2961 deletion completed in 6.347050626s

• [SLOW TEST:10.712 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:15:28.895: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7705
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-a665ff64-c8f6-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume configMaps
Aug 27 18:15:29.101: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a667c4b7-c8f6-11e9-a9da-1ec1181dd093" in namespace "projected-7705" to be "success or failure"
Aug 27 18:15:29.112: INFO: Pod "pod-projected-configmaps-a667c4b7-c8f6-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 10.635196ms
Aug 27 18:15:31.125: INFO: Pod "pod-projected-configmaps-a667c4b7-c8f6-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024428785s
Aug 27 18:15:33.145: INFO: Pod "pod-projected-configmaps-a667c4b7-c8f6-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043687783s
STEP: Saw pod success
Aug 27 18:15:33.145: INFO: Pod "pod-projected-configmaps-a667c4b7-c8f6-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:15:33.154: INFO: Trying to get logs from node 10.135.206.197 pod pod-projected-configmaps-a667c4b7-c8f6-11e9-a9da-1ec1181dd093 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 18:15:33.203: INFO: Waiting for pod pod-projected-configmaps-a667c4b7-c8f6-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:15:33.212: INFO: Pod pod-projected-configmaps-a667c4b7-c8f6-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:15:33.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7705" for this suite.
Aug 27 18:15:39.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:15:39.569: INFO: namespace projected-7705 deletion completed in 6.348972775s

• [SLOW TEST:10.674 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:15:39.573: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2000
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0827 18:15:49.906432      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 27 18:15:49.906: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:15:49.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2000" for this suite.
Aug 27 18:15:57.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:15:58.236: INFO: namespace gc-2000 deletion completed in 8.32217106s

• [SLOW TEST:18.662 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:15:58.236: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1083
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3187
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1494
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:16:04.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1083" for this suite.
Aug 27 18:16:10.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:16:11.224: INFO: namespace namespaces-1083 deletion completed in 6.397241151s
STEP: Destroying namespace "nsdeletetest-3187" for this suite.
Aug 27 18:16:11.236: INFO: Namespace nsdeletetest-3187 was already deleted
STEP: Destroying namespace "nsdeletetest-1494" for this suite.
Aug 27 18:16:17.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:16:17.629: INFO: namespace nsdeletetest-1494 deletion completed in 6.392379218s

• [SLOW TEST:19.393 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:16:17.629: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2491
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-c3738631-c8f6-11e9-a9da-1ec1181dd093
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:16:17.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2491" for this suite.
Aug 27 18:16:23.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:16:24.152: INFO: namespace configmap-2491 deletion completed in 6.321261726s

• [SLOW TEST:6.523 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:16:24.154: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2261
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-c756676d-c8f6-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume secrets
Aug 27 18:16:24.361: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c757e0db-c8f6-11e9-a9da-1ec1181dd093" in namespace "projected-2261" to be "success or failure"
Aug 27 18:16:24.373: INFO: Pod "pod-projected-secrets-c757e0db-c8f6-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 11.207535ms
Aug 27 18:16:26.382: INFO: Pod "pod-projected-secrets-c757e0db-c8f6-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020974966s
Aug 27 18:16:28.392: INFO: Pod "pod-projected-secrets-c757e0db-c8f6-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030745608s
STEP: Saw pod success
Aug 27 18:16:28.392: INFO: Pod "pod-projected-secrets-c757e0db-c8f6-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:16:28.402: INFO: Trying to get logs from node 10.135.206.222 pod pod-projected-secrets-c757e0db-c8f6-11e9-a9da-1ec1181dd093 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 27 18:16:28.482: INFO: Waiting for pod pod-projected-secrets-c757e0db-c8f6-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:16:28.489: INFO: Pod pod-projected-secrets-c757e0db-c8f6-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:16:28.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2261" for this suite.
Aug 27 18:16:34.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:16:34.818: INFO: namespace projected-2261 deletion completed in 6.318925497s

• [SLOW TEST:10.664 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:16:34.819: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7969
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:16:35.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7969" for this suite.
Aug 27 18:16:57.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:16:57.388: INFO: namespace pods-7969 deletion completed in 22.359870406s

• [SLOW TEST:22.569 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:16:57.388: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2390
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-db271227-c8f6-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume secrets
Aug 27 18:16:57.644: INFO: Waiting up to 5m0s for pod "pod-secrets-db2873fd-c8f6-11e9-a9da-1ec1181dd093" in namespace "secrets-2390" to be "success or failure"
Aug 27 18:16:57.653: INFO: Pod "pod-secrets-db2873fd-c8f6-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 8.760575ms
Aug 27 18:16:59.663: INFO: Pod "pod-secrets-db2873fd-c8f6-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01847396s
STEP: Saw pod success
Aug 27 18:16:59.663: INFO: Pod "pod-secrets-db2873fd-c8f6-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:16:59.672: INFO: Trying to get logs from node 10.134.235.118 pod pod-secrets-db2873fd-c8f6-11e9-a9da-1ec1181dd093 container secret-env-test: <nil>
STEP: delete the pod
Aug 27 18:16:59.722: INFO: Waiting for pod pod-secrets-db2873fd-c8f6-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:16:59.731: INFO: Pod pod-secrets-db2873fd-c8f6-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:16:59.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2390" for this suite.
Aug 27 18:17:05.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:17:06.127: INFO: namespace secrets-2390 deletion completed in 6.386534308s

• [SLOW TEST:8.740 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:17:06.128: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8958
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-e05c666e-c8f6-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume secrets
Aug 27 18:17:06.352: INFO: Waiting up to 5m0s for pod "pod-secrets-e05deb9c-c8f6-11e9-a9da-1ec1181dd093" in namespace "secrets-8958" to be "success or failure"
Aug 27 18:17:06.370: INFO: Pod "pod-secrets-e05deb9c-c8f6-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 18.742154ms
Aug 27 18:17:08.380: INFO: Pod "pod-secrets-e05deb9c-c8f6-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027948013s
Aug 27 18:17:10.389: INFO: Pod "pod-secrets-e05deb9c-c8f6-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037410545s
STEP: Saw pod success
Aug 27 18:17:10.389: INFO: Pod "pod-secrets-e05deb9c-c8f6-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:17:10.398: INFO: Trying to get logs from node 10.135.206.222 pod pod-secrets-e05deb9c-c8f6-11e9-a9da-1ec1181dd093 container secret-volume-test: <nil>
STEP: delete the pod
Aug 27 18:17:10.453: INFO: Waiting for pod pod-secrets-e05deb9c-c8f6-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:17:10.466: INFO: Pod pod-secrets-e05deb9c-c8f6-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:17:10.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8958" for this suite.
Aug 27 18:17:16.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:17:16.833: INFO: namespace secrets-8958 deletion completed in 6.358839911s

• [SLOW TEST:10.706 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:17:16.834: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1577
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 27 18:17:17.031: INFO: Waiting up to 5m0s for pod "pod-e6bc1f5c-c8f6-11e9-a9da-1ec1181dd093" in namespace "emptydir-1577" to be "success or failure"
Aug 27 18:17:17.049: INFO: Pod "pod-e6bc1f5c-c8f6-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 18.451492ms
Aug 27 18:17:19.059: INFO: Pod "pod-e6bc1f5c-c8f6-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028150172s
Aug 27 18:17:21.069: INFO: Pod "pod-e6bc1f5c-c8f6-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037979522s
STEP: Saw pod success
Aug 27 18:17:21.069: INFO: Pod "pod-e6bc1f5c-c8f6-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:17:21.079: INFO: Trying to get logs from node 10.135.206.222 pod pod-e6bc1f5c-c8f6-11e9-a9da-1ec1181dd093 container test-container: <nil>
STEP: delete the pod
Aug 27 18:17:21.141: INFO: Waiting for pod pod-e6bc1f5c-c8f6-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:17:21.149: INFO: Pod pod-e6bc1f5c-c8f6-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:17:21.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1577" for this suite.
Aug 27 18:17:27.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:17:27.514: INFO: namespace emptydir-1577 deletion completed in 6.356100013s

• [SLOW TEST:10.681 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:17:27.514: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-435
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-ed1d9be6-c8f6-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume configMaps
Aug 27 18:17:27.772: INFO: Waiting up to 5m0s for pod "pod-configmaps-ed1f1f97-c8f6-11e9-a9da-1ec1181dd093" in namespace "configmap-435" to be "success or failure"
Aug 27 18:17:27.795: INFO: Pod "pod-configmaps-ed1f1f97-c8f6-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 23.000323ms
Aug 27 18:17:29.806: INFO: Pod "pod-configmaps-ed1f1f97-c8f6-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034315793s
STEP: Saw pod success
Aug 27 18:17:29.806: INFO: Pod "pod-configmaps-ed1f1f97-c8f6-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:17:29.817: INFO: Trying to get logs from node 10.134.235.118 pod pod-configmaps-ed1f1f97-c8f6-11e9-a9da-1ec1181dd093 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 18:17:29.863: INFO: Waiting for pod pod-configmaps-ed1f1f97-c8f6-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:17:29.876: INFO: Pod pod-configmaps-ed1f1f97-c8f6-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:17:29.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-435" for this suite.
Aug 27 18:17:35.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:17:36.219: INFO: namespace configmap-435 deletion completed in 6.335451166s

• [SLOW TEST:8.705 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:17:36.220: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9670
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-f24bbf8e-c8f6-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume configMaps
Aug 27 18:17:36.438: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f24d53e8-c8f6-11e9-a9da-1ec1181dd093" in namespace "projected-9670" to be "success or failure"
Aug 27 18:17:36.455: INFO: Pod "pod-projected-configmaps-f24d53e8-c8f6-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 16.861028ms
Aug 27 18:17:38.466: INFO: Pod "pod-projected-configmaps-f24d53e8-c8f6-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027619744s
Aug 27 18:17:40.478: INFO: Pod "pod-projected-configmaps-f24d53e8-c8f6-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039609539s
STEP: Saw pod success
Aug 27 18:17:40.478: INFO: Pod "pod-projected-configmaps-f24d53e8-c8f6-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:17:40.491: INFO: Trying to get logs from node 10.135.206.222 pod pod-projected-configmaps-f24d53e8-c8f6-11e9-a9da-1ec1181dd093 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 18:17:40.545: INFO: Waiting for pod pod-projected-configmaps-f24d53e8-c8f6-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:17:40.553: INFO: Pod pod-projected-configmaps-f24d53e8-c8f6-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:17:40.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9670" for this suite.
Aug 27 18:17:46.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:17:46.912: INFO: namespace projected-9670 deletion completed in 6.351132414s

• [SLOW TEST:10.692 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:17:46.918: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5644
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Aug 27 18:17:47.123: INFO: Waiting up to 5m0s for pod "client-containers-f8aaa127-c8f6-11e9-a9da-1ec1181dd093" in namespace "containers-5644" to be "success or failure"
Aug 27 18:17:47.135: INFO: Pod "client-containers-f8aaa127-c8f6-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 11.087211ms
Aug 27 18:17:49.148: INFO: Pod "client-containers-f8aaa127-c8f6-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024829423s
Aug 27 18:17:51.157: INFO: Pod "client-containers-f8aaa127-c8f6-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033775886s
STEP: Saw pod success
Aug 27 18:17:51.157: INFO: Pod "client-containers-f8aaa127-c8f6-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:17:51.167: INFO: Trying to get logs from node 10.135.206.222 pod client-containers-f8aaa127-c8f6-11e9-a9da-1ec1181dd093 container test-container: <nil>
STEP: delete the pod
Aug 27 18:17:51.227: INFO: Waiting for pod client-containers-f8aaa127-c8f6-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:17:51.241: INFO: Pod client-containers-f8aaa127-c8f6-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:17:51.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5644" for this suite.
Aug 27 18:17:57.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:17:57.623: INFO: namespace containers-5644 deletion completed in 6.37444203s

• [SLOW TEST:10.706 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:17:57.624: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1923
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Aug 27 18:17:57.807: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:18:01.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1923" for this suite.
Aug 27 18:18:07.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:18:08.065: INFO: namespace init-container-1923 deletion completed in 6.409443289s

• [SLOW TEST:10.442 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:18:08.066: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1389
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 27 18:18:08.272: INFO: Waiting up to 5m0s for pod "pod-05471416-c8f7-11e9-a9da-1ec1181dd093" in namespace "emptydir-1389" to be "success or failure"
Aug 27 18:18:08.297: INFO: Pod "pod-05471416-c8f7-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 25.523157ms
Aug 27 18:18:10.319: INFO: Pod "pod-05471416-c8f7-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046882993s
STEP: Saw pod success
Aug 27 18:18:10.319: INFO: Pod "pod-05471416-c8f7-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:18:10.331: INFO: Trying to get logs from node 10.135.206.197 pod pod-05471416-c8f7-11e9-a9da-1ec1181dd093 container test-container: <nil>
STEP: delete the pod
Aug 27 18:18:10.392: INFO: Waiting for pod pod-05471416-c8f7-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:18:10.403: INFO: Pod pod-05471416-c8f7-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:18:10.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1389" for this suite.
Aug 27 18:18:16.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:18:16.735: INFO: namespace emptydir-1389 deletion completed in 6.318777839s

• [SLOW TEST:8.670 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:18:16.736: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5245
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 27 18:18:16.945: INFO: Waiting up to 5m0s for pod "pod-0a71edfb-c8f7-11e9-a9da-1ec1181dd093" in namespace "emptydir-5245" to be "success or failure"
Aug 27 18:18:16.960: INFO: Pod "pod-0a71edfb-c8f7-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 15.465381ms
Aug 27 18:18:18.973: INFO: Pod "pod-0a71edfb-c8f7-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028382122s
Aug 27 18:18:20.984: INFO: Pod "pod-0a71edfb-c8f7-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039211333s
STEP: Saw pod success
Aug 27 18:18:20.984: INFO: Pod "pod-0a71edfb-c8f7-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:18:20.993: INFO: Trying to get logs from node 10.135.206.197 pod pod-0a71edfb-c8f7-11e9-a9da-1ec1181dd093 container test-container: <nil>
STEP: delete the pod
Aug 27 18:18:21.053: INFO: Waiting for pod pod-0a71edfb-c8f7-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:18:21.065: INFO: Pod pod-0a71edfb-c8f7-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:18:21.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5245" for this suite.
Aug 27 18:18:27.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:18:27.449: INFO: namespace emptydir-5245 deletion completed in 6.374284035s

• [SLOW TEST:10.712 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:18:27.449: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1516
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Aug 27 18:18:27.640: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 27 18:18:27.657: INFO: Waiting for terminating namespaces to be deleted...
Aug 27 18:18:27.667: INFO: 
Logging pods the kubelet thinks is on node 10.134.235.118 before test
Aug 27 18:18:27.687: INFO: ibm-kube-fluentd-ddtlg from kube-system started at 2019-08-27 16:49:08 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.687: INFO: 	Container fluentd ready: true, restart count 0
Aug 27 18:18:27.687: INFO: sonobuoy from heptio-sonobuoy started at 2019-08-27 18:12:16 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.687: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 27 18:18:27.687: INFO: ibm-keepalived-watcher-29grz from kube-system started at 2019-08-27 16:49:08 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.687: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 27 18:18:27.687: INFO: test-k8s-e2e-pvg-master-verification from default started at 2019-08-27 16:51:22 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.687: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Aug 27 18:18:27.687: INFO: vpn-7754bb6d4-zks4g from kube-system started at 2019-08-27 16:59:00 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.687: INFO: 	Container vpn ready: true, restart count 0
Aug 27 18:18:27.687: INFO: sonobuoy-e2e-job-94e2d131d27f4c56 from heptio-sonobuoy started at 2019-08-27 18:12:22 +0000 UTC (2 container statuses recorded)
Aug 27 18:18:27.687: INFO: 	Container e2e ready: true, restart count 0
Aug 27 18:18:27.687: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 27 18:18:27.687: INFO: ibm-master-proxy-static-10.134.235.118 from kube-system started at <nil> (0 container statuses recorded)
Aug 27 18:18:27.687: INFO: calico-node-fbflt from kube-system started at 2019-08-27 16:49:08 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.687: INFO: 	Container calico-node ready: true, restart count 0
Aug 27 18:18:27.687: INFO: sonobuoy-systemd-logs-daemon-set-c96a900eeb9a46c8-qhnrg from heptio-sonobuoy started at 2019-08-27 18:12:22 +0000 UTC (2 container statuses recorded)
Aug 27 18:18:27.687: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 27 18:18:27.687: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 27 18:18:27.687: INFO: 
Logging pods the kubelet thinks is on node 10.135.206.197 before test
Aug 27 18:18:27.719: INFO: ibm-master-proxy-static-10.135.206.197 from kube-system started at <nil> (0 container statuses recorded)
Aug 27 18:18:27.719: INFO: calico-kube-controllers-64fcffdcf4-8nnzm from kube-system started at 2019-08-27 16:48:50 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.719: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 27 18:18:27.719: INFO: ibm-file-plugin-554c9f89b6-pmh82 from kube-system started at 2019-08-27 16:48:50 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.719: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 27 18:18:27.719: INFO: ibm-storage-watcher-55867f764f-vwpv2 from kube-system started at 2019-08-27 16:48:50 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.719: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 27 18:18:27.719: INFO: ibm-kube-fluentd-hzmm2 from kube-system started at 2019-08-27 16:49:00 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.719: INFO: 	Container fluentd ready: true, restart count 0
Aug 27 18:18:27.719: INFO: kubernetes-dashboard-5c8c9b7546-sl4zb from kube-system started at 2019-08-27 16:48:50 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.719: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Aug 27 18:18:27.719: INFO: metrics-server-5d78bd76f6-ktqhz from kube-system started at 2019-08-27 16:49:29 +0000 UTC (2 container statuses recorded)
Aug 27 18:18:27.719: INFO: 	Container metrics-server ready: true, restart count 0
Aug 27 18:18:27.719: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Aug 27 18:18:27.719: INFO: sonobuoy-systemd-logs-daemon-set-c96a900eeb9a46c8-zk44g from heptio-sonobuoy started at 2019-08-27 18:12:22 +0000 UTC (2 container statuses recorded)
Aug 27 18:18:27.719: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 27 18:18:27.719: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 27 18:18:27.719: INFO: coredns-autoscaler-5d4db8dd68-whxxk from kube-system started at 2019-08-27 16:48:50 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.719: INFO: 	Container autoscaler ready: true, restart count 0
Aug 27 18:18:27.719: INFO: ibm-keepalived-watcher-972mx from kube-system started at 2019-08-27 16:48:32 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.719: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 27 18:18:27.719: INFO: calico-node-2s8wd from kube-system started at 2019-08-27 16:48:32 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.719: INFO: 	Container calico-node ready: true, restart count 0
Aug 27 18:18:27.719: INFO: coredns-9dd7747c7-lxd4t from kube-system started at 2019-08-27 16:59:22 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.719: INFO: 	Container coredns ready: true, restart count 0
Aug 27 18:18:27.719: INFO: ibm-cloud-provider-ip-158-177-215-165-79887d87b5-svwm9 from ibm-system started at 2019-08-27 17:00:47 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.719: INFO: 	Container ibm-cloud-provider-ip-158-177-215-165 ready: true, restart count 0
Aug 27 18:18:27.719: INFO: public-crbliljj8f06leh3daoe40-alb1-545ccf9dcb-4sskw from kube-system started at 2019-08-27 17:02:44 +0000 UTC (4 container statuses recorded)
Aug 27 18:18:27.719: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Aug 27 18:18:27.719: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Aug 27 18:18:27.719: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Aug 27 18:18:27.719: INFO: 	Container nginx-ingress ready: true, restart count 0
Aug 27 18:18:27.719: INFO: 
Logging pods the kubelet thinks is on node 10.135.206.222 before test
Aug 27 18:18:27.757: INFO: coredns-9dd7747c7-tp5vn from kube-system started at 2019-08-27 16:59:22 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.757: INFO: 	Container coredns ready: true, restart count 0
Aug 27 18:18:27.757: INFO: sonobuoy-systemd-logs-daemon-set-c96a900eeb9a46c8-qq5nz from heptio-sonobuoy started at 2019-08-27 18:12:22 +0000 UTC (2 container statuses recorded)
Aug 27 18:18:27.757: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 27 18:18:27.757: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 27 18:18:27.757: INFO: ibm-keepalived-watcher-kg5cs from kube-system started at 2019-08-27 16:49:24 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.757: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 27 18:18:27.757: INFO: ibm-kube-fluentd-9rjpc from kube-system started at 2019-08-27 16:49:24 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.757: INFO: 	Container fluentd ready: true, restart count 0
Aug 27 18:18:27.757: INFO: ibm-cloud-provider-ip-158-177-215-165-79887d87b5-f4cv5 from ibm-system started at 2019-08-27 17:00:47 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.757: INFO: 	Container ibm-cloud-provider-ip-158-177-215-165 ready: true, restart count 0
Aug 27 18:18:27.757: INFO: ibm-master-proxy-static-10.135.206.222 from kube-system started at <nil> (0 container statuses recorded)
Aug 27 18:18:27.757: INFO: calico-node-k2dkh from kube-system started at 2019-08-27 16:49:24 +0000 UTC (1 container statuses recorded)
Aug 27 18:18:27.757: INFO: 	Container calico-node ready: true, restart count 0
Aug 27 18:18:27.757: INFO: public-crbliljj8f06leh3daoe40-alb1-545ccf9dcb-pbqzv from kube-system started at 2019-08-27 17:02:44 +0000 UTC (4 container statuses recorded)
Aug 27 18:18:27.757: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Aug 27 18:18:27.758: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Aug 27 18:18:27.758: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Aug 27 18:18:27.758: INFO: 	Container nginx-ingress ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15beda72fd9a291a], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:18:28.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1516" for this suite.
Aug 27 18:18:34.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:18:35.166: INFO: namespace sched-pred-1516 deletion completed in 6.335943781s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.717 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:18:35.167: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4871
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 18:18:35.367: INFO: Waiting up to 5m0s for pod "downwardapi-volume-156dc955-c8f7-11e9-a9da-1ec1181dd093" in namespace "projected-4871" to be "success or failure"
Aug 27 18:18:35.381: INFO: Pod "downwardapi-volume-156dc955-c8f7-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 13.109222ms
Aug 27 18:18:37.390: INFO: Pod "downwardapi-volume-156dc955-c8f7-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022579771s
Aug 27 18:18:39.400: INFO: Pod "downwardapi-volume-156dc955-c8f7-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032930846s
STEP: Saw pod success
Aug 27 18:18:39.400: INFO: Pod "downwardapi-volume-156dc955-c8f7-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:18:39.409: INFO: Trying to get logs from node 10.134.235.118 pod downwardapi-volume-156dc955-c8f7-11e9-a9da-1ec1181dd093 container client-container: <nil>
STEP: delete the pod
Aug 27 18:18:39.456: INFO: Waiting for pod downwardapi-volume-156dc955-c8f7-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:18:39.464: INFO: Pod downwardapi-volume-156dc955-c8f7-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:18:39.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4871" for this suite.
Aug 27 18:18:45.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:18:45.790: INFO: namespace projected-4871 deletion completed in 6.316521659s

• [SLOW TEST:10.622 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:18:45.790: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9644
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 27 18:18:45.988: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 27 18:18:50.998: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:18:52.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9644" for this suite.
Aug 27 18:18:58.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:18:58.394: INFO: namespace replication-controller-9644 deletion completed in 6.341075899s

• [SLOW TEST:12.604 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:18:58.400: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6411
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-2347d706-c8f7-11e9-a9da-1ec1181dd093
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-2347d706-c8f7-11e9-a9da-1ec1181dd093
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:20:09.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6411" for this suite.
Aug 27 18:20:33.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:20:33.709: INFO: namespace projected-6411 deletion completed in 24.33858454s

• [SLOW TEST:95.310 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:20:33.709: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7538
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7538
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7538
STEP: Creating statefulset with conflicting port in namespace statefulset-7538
STEP: Waiting until pod test-pod will start running in namespace statefulset-7538
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7538
Aug 27 18:20:35.988: INFO: Observed stateful pod in namespace: statefulset-7538, name: ss-0, uid: 5c56f3f2-c8f7-11e9-9af8-1e15abefec17, status phase: Pending. Waiting for statefulset controller to delete.
Aug 27 18:20:36.222: INFO: Observed stateful pod in namespace: statefulset-7538, name: ss-0, uid: 5c56f3f2-c8f7-11e9-9af8-1e15abefec17, status phase: Failed. Waiting for statefulset controller to delete.
Aug 27 18:20:36.242: INFO: Observed stateful pod in namespace: statefulset-7538, name: ss-0, uid: 5c56f3f2-c8f7-11e9-9af8-1e15abefec17, status phase: Failed. Waiting for statefulset controller to delete.
Aug 27 18:20:36.251: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7538
STEP: Removing pod with conflicting port in namespace statefulset-7538
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7538 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug 27 18:20:38.297: INFO: Deleting all statefulset in ns statefulset-7538
Aug 27 18:20:38.306: INFO: Scaling statefulset ss to 0
Aug 27 18:20:48.344: INFO: Waiting for statefulset status.replicas updated to 0
Aug 27 18:20:48.352: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:20:48.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7538" for this suite.
Aug 27 18:20:54.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:20:54.738: INFO: namespace statefulset-7538 deletion completed in 6.344136278s

• [SLOW TEST:21.029 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:20:54.740: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8528
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Aug 27 18:20:54.924: INFO: PodSpec: initContainers in spec.initContainers
Aug 27 18:21:39.634: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-689f24ae-c8f7-11e9-a9da-1ec1181dd093", GenerateName:"", Namespace:"init-container-8528", SelfLink:"/api/v1/namespaces/init-container-8528/pods/pod-init-689f24ae-c8f7-11e9-a9da-1ec1181dd093", UID:"689cf0cf-c8f7-11e9-9af8-1e15abefec17", ResourceVersion:"21595", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63702526854, loc:(*time.Location)(0x8825120)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"924824982"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-sgxtn", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002491700), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-sgxtn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-sgxtn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-sgxtn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00180d0e8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.135.206.197", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0027db8c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00180d200)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00180d270)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00180d278), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00180d27c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702526854, loc:(*time.Location)(0x8825120)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702526854, loc:(*time.Location)(0x8825120)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702526854, loc:(*time.Location)(0x8825120)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702526854, loc:(*time.Location)(0x8825120)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.135.206.197", PodIP:"172.30.183.124", StartTime:(*v1.Time)(0xc0018e37a0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000eef9d0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000eefa40)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://05133baf6f77a3bc0fc4b8eaff6429173ed5834c0f06eedaba7a509c912aa69b"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0018e3820), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0018e37e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:21:39.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8528" for this suite.
Aug 27 18:22:03.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:22:03.988: INFO: namespace init-container-8528 deletion completed in 24.342724895s

• [SLOW TEST:69.248 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:22:03.991: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1558
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Aug 27 18:22:04.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 --namespace=kubectl-1558 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Aug 27 18:22:06.911: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Aug 27 18:22:06.911: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:22:08.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1558" for this suite.
Aug 27 18:22:14.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:22:15.272: INFO: namespace kubectl-1558 deletion completed in 6.331677369s

• [SLOW TEST:11.282 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:22:15.273: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6341
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-989f9ab6-c8f7-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume configMaps
Aug 27 18:22:15.501: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-98a2cbdc-c8f7-11e9-a9da-1ec1181dd093" in namespace "projected-6341" to be "success or failure"
Aug 27 18:22:15.514: INFO: Pod "pod-projected-configmaps-98a2cbdc-c8f7-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 12.243125ms
Aug 27 18:22:17.526: INFO: Pod "pod-projected-configmaps-98a2cbdc-c8f7-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024051241s
STEP: Saw pod success
Aug 27 18:22:17.526: INFO: Pod "pod-projected-configmaps-98a2cbdc-c8f7-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:22:17.538: INFO: Trying to get logs from node 10.134.235.118 pod pod-projected-configmaps-98a2cbdc-c8f7-11e9-a9da-1ec1181dd093 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 18:22:17.596: INFO: Waiting for pod pod-projected-configmaps-98a2cbdc-c8f7-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:22:17.605: INFO: Pod pod-projected-configmaps-98a2cbdc-c8f7-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:22:17.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6341" for this suite.
Aug 27 18:22:23.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:22:23.978: INFO: namespace projected-6341 deletion completed in 6.359180859s

• [SLOW TEST:8.705 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:22:23.978: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1148
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1649
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 27 18:22:24.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1148'
Aug 27 18:22:24.304: INFO: stderr: ""
Aug 27 18:22:24.304: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1654
Aug 27 18:22:24.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 delete pods e2e-test-nginx-pod --namespace=kubectl-1148'
Aug 27 18:22:26.785: INFO: stderr: ""
Aug 27 18:22:26.785: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:22:26.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1148" for this suite.
Aug 27 18:22:32.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:22:33.142: INFO: namespace kubectl-1148 deletion completed in 6.33899118s

• [SLOW TEST:9.163 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:22:33.142: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9971
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-a344e15f-c8f7-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume secrets
Aug 27 18:22:33.345: INFO: Waiting up to 5m0s for pod "pod-secrets-a3461399-c8f7-11e9-a9da-1ec1181dd093" in namespace "secrets-9971" to be "success or failure"
Aug 27 18:22:33.358: INFO: Pod "pod-secrets-a3461399-c8f7-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 12.720949ms
Aug 27 18:22:35.368: INFO: Pod "pod-secrets-a3461399-c8f7-11e9-a9da-1ec1181dd093": Phase="Running", Reason="", readiness=true. Elapsed: 2.023098563s
Aug 27 18:22:37.380: INFO: Pod "pod-secrets-a3461399-c8f7-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034608481s
STEP: Saw pod success
Aug 27 18:22:37.380: INFO: Pod "pod-secrets-a3461399-c8f7-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:22:37.389: INFO: Trying to get logs from node 10.135.206.197 pod pod-secrets-a3461399-c8f7-11e9-a9da-1ec1181dd093 container secret-volume-test: <nil>
STEP: delete the pod
Aug 27 18:22:37.472: INFO: Waiting for pod pod-secrets-a3461399-c8f7-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:22:37.482: INFO: Pod pod-secrets-a3461399-c8f7-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:22:37.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9971" for this suite.
Aug 27 18:22:43.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:22:43.811: INFO: namespace secrets-9971 deletion completed in 6.317803515s

• [SLOW TEST:10.669 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:22:43.813: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4906
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Aug 27 18:22:43.999: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:22:49.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4906" for this suite.
Aug 27 18:23:13.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:23:13.641: INFO: namespace init-container-4906 deletion completed in 24.417594023s

• [SLOW TEST:29.828 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:23:13.643: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6793
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 27 18:23:13.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6793'
Aug 27 18:23:13.981: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 27 18:23:13.981: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Aug 27 18:23:13.994: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Aug 27 18:23:14.000: INFO: scanned /root for discovery docs: <nil>
Aug 27 18:23:14.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-6793'
Aug 27 18:23:29.905: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 27 18:23:29.905: INFO: stdout: "Created e2e-test-nginx-rc-e3f9d67a5630fb2983c40fb7a86e10c0\nScaling up e2e-test-nginx-rc-e3f9d67a5630fb2983c40fb7a86e10c0 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-e3f9d67a5630fb2983c40fb7a86e10c0 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-e3f9d67a5630fb2983c40fb7a86e10c0 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Aug 27 18:23:29.905: INFO: stdout: "Created e2e-test-nginx-rc-e3f9d67a5630fb2983c40fb7a86e10c0\nScaling up e2e-test-nginx-rc-e3f9d67a5630fb2983c40fb7a86e10c0 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-e3f9d67a5630fb2983c40fb7a86e10c0 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-e3f9d67a5630fb2983c40fb7a86e10c0 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Aug 27 18:23:29.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-6793'
Aug 27 18:23:30.022: INFO: stderr: ""
Aug 27 18:23:30.022: INFO: stdout: "e2e-test-nginx-rc-e3f9d67a5630fb2983c40fb7a86e10c0-8c4cf "
Aug 27 18:23:30.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods e2e-test-nginx-rc-e3f9d67a5630fb2983c40fb7a86e10c0-8c4cf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6793'
Aug 27 18:23:30.145: INFO: stderr: ""
Aug 27 18:23:30.145: INFO: stdout: "true"
Aug 27 18:23:30.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods e2e-test-nginx-rc-e3f9d67a5630fb2983c40fb7a86e10c0-8c4cf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6793'
Aug 27 18:23:30.267: INFO: stderr: ""
Aug 27 18:23:30.267: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Aug 27 18:23:30.267: INFO: e2e-test-nginx-rc-e3f9d67a5630fb2983c40fb7a86e10c0-8c4cf is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1486
Aug 27 18:23:30.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 delete rc e2e-test-nginx-rc --namespace=kubectl-6793'
Aug 27 18:23:30.427: INFO: stderr: ""
Aug 27 18:23:30.427: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:23:30.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6793" for this suite.
Aug 27 18:23:54.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:23:54.771: INFO: namespace kubectl-6793 deletion completed in 24.334654174s

• [SLOW TEST:41.128 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:23:54.772: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3556
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1576
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 27 18:23:54.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3556'
Aug 27 18:23:55.073: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 27 18:23:55.073: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
Aug 27 18:23:55.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 delete jobs e2e-test-nginx-job --namespace=kubectl-3556'
Aug 27 18:23:55.216: INFO: stderr: ""
Aug 27 18:23:55.216: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:23:55.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3556" for this suite.
Aug 27 18:24:01.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:24:01.559: INFO: namespace kubectl-3556 deletion completed in 6.332536686s

• [SLOW TEST:6.787 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:24:01.559: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9294
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-9294
Aug 27 18:24:07.810: INFO: Started pod liveness-http in namespace container-probe-9294
STEP: checking the pod's current state and verifying that restartCount is present
Aug 27 18:24:07.821: INFO: Initial restart count of pod liveness-http is 0
Aug 27 18:24:25.933: INFO: Restart count of pod container-probe-9294/liveness-http is now 1 (18.112448486s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:24:25.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9294" for this suite.
Aug 27 18:24:32.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:24:32.316: INFO: namespace container-probe-9294 deletion completed in 6.345414856s

• [SLOW TEST:30.757 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:24:32.316: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1750
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 18:24:36.593: INFO: Waiting up to 5m0s for pod "client-envvars-ecbd1278-c8f7-11e9-a9da-1ec1181dd093" in namespace "pods-1750" to be "success or failure"
Aug 27 18:24:36.604: INFO: Pod "client-envvars-ecbd1278-c8f7-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 10.634004ms
Aug 27 18:24:38.613: INFO: Pod "client-envvars-ecbd1278-c8f7-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0194734s
STEP: Saw pod success
Aug 27 18:24:38.613: INFO: Pod "client-envvars-ecbd1278-c8f7-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:24:38.621: INFO: Trying to get logs from node 10.134.235.118 pod client-envvars-ecbd1278-c8f7-11e9-a9da-1ec1181dd093 container env3cont: <nil>
STEP: delete the pod
Aug 27 18:24:38.672: INFO: Waiting for pod client-envvars-ecbd1278-c8f7-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:24:38.683: INFO: Pod client-envvars-ecbd1278-c8f7-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:24:38.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1750" for this suite.
Aug 27 18:25:28.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:25:29.054: INFO: namespace pods-1750 deletion completed in 50.361851196s

• [SLOW TEST:56.738 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:25:29.054: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7411
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7411
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-7411
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7411
Aug 27 18:25:29.285: INFO: Found 0 stateful pods, waiting for 1
Aug 27 18:25:39.294: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 27 18:25:39.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 27 18:25:39.784: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 27 18:25:39.784: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 27 18:25:39.784: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 27 18:25:39.794: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 27 18:25:49.811: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 27 18:25:49.811: INFO: Waiting for statefulset status.replicas updated to 0
Aug 27 18:25:49.848: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 27 18:25:49.848: INFO: ss-0  10.135.206.222  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:29 +0000 UTC  }]
Aug 27 18:25:49.848: INFO: 
Aug 27 18:25:49.848: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 27 18:25:50.858: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990301751s
Aug 27 18:25:51.878: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.978830806s
Aug 27 18:25:52.888: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.960958957s
Aug 27 18:25:53.898: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.951191629s
Aug 27 18:25:54.911: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.941391795s
Aug 27 18:25:55.923: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.928057234s
Aug 27 18:25:56.932: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.916354861s
Aug 27 18:25:57.943: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.906731084s
Aug 27 18:25:58.953: INFO: Verifying statefulset ss doesn't scale past 3 for another 895.60835ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7411
Aug 27 18:25:59.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:26:00.397: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 27 18:26:00.397: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 27 18:26:00.397: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 27 18:26:00.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:26:00.816: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 27 18:26:00.816: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 27 18:26:00.816: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 27 18:26:00.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:26:01.301: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 27 18:26:01.301: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 27 18:26:01.301: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 27 18:26:01.310: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 18:26:01.310: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 18:26:01.310: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 27 18:26:01.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 27 18:26:01.763: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 27 18:26:01.763: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 27 18:26:01.763: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 27 18:26:01.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 27 18:26:02.184: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 27 18:26:02.184: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 27 18:26:02.184: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 27 18:26:02.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 27 18:26:02.627: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 27 18:26:02.627: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 27 18:26:02.627: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 27 18:26:02.627: INFO: Waiting for statefulset status.replicas updated to 0
Aug 27 18:26:02.636: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 27 18:26:12.657: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 27 18:26:12.657: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 27 18:26:12.657: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 27 18:26:12.687: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 27 18:26:12.687: INFO: ss-0  10.135.206.222  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:29 +0000 UTC  }]
Aug 27 18:26:12.687: INFO: ss-1  10.134.235.118  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  }]
Aug 27 18:26:12.687: INFO: ss-2  10.135.206.197  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  }]
Aug 27 18:26:12.687: INFO: 
Aug 27 18:26:12.687: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 27 18:26:13.696: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 27 18:26:13.696: INFO: ss-0  10.135.206.222  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:29 +0000 UTC  }]
Aug 27 18:26:13.696: INFO: ss-1  10.134.235.118  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  }]
Aug 27 18:26:13.696: INFO: ss-2  10.135.206.197  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  }]
Aug 27 18:26:13.696: INFO: 
Aug 27 18:26:13.696: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 27 18:26:14.707: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 27 18:26:14.707: INFO: ss-2  10.135.206.197  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  }]
Aug 27 18:26:14.708: INFO: 
Aug 27 18:26:14.708: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 27 18:26:15.717: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 27 18:26:15.717: INFO: ss-2  10.135.206.197  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  }]
Aug 27 18:26:15.717: INFO: 
Aug 27 18:26:15.717: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 27 18:26:16.729: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 27 18:26:16.729: INFO: ss-2  10.135.206.197  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  }]
Aug 27 18:26:16.729: INFO: 
Aug 27 18:26:16.729: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 27 18:26:17.751: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 27 18:26:17.751: INFO: ss-2  10.135.206.197  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  }]
Aug 27 18:26:17.751: INFO: 
Aug 27 18:26:17.751: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 27 18:26:18.761: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 27 18:26:18.761: INFO: ss-2  10.135.206.197  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  }]
Aug 27 18:26:18.761: INFO: 
Aug 27 18:26:18.761: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 27 18:26:19.770: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 27 18:26:19.770: INFO: ss-2  10.135.206.197  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  }]
Aug 27 18:26:19.770: INFO: 
Aug 27 18:26:19.770: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 27 18:26:20.781: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 27 18:26:20.781: INFO: ss-2  10.135.206.197  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  }]
Aug 27 18:26:20.781: INFO: 
Aug 27 18:26:20.781: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 27 18:26:21.791: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 27 18:26:21.791: INFO: ss-2  10.135.206.197  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:25:49 +0000 UTC  }]
Aug 27 18:26:21.791: INFO: 
Aug 27 18:26:21.791: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7411
Aug 27 18:26:22.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:26:23.000: INFO: rc: 1
Aug 27 18:26:23.000: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc000952420 exit status 1 <nil> <nil> true [0xc0001ec978 0xc0001eca08 0xc0001eca70] [0xc0001ec978 0xc0001eca08 0xc0001eca70] [0xc0001ec9c0 0xc0001eca40] [0x9c0ae0 0x9c0ae0] 0xc002118300 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Aug 27 18:26:33.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:26:33.119: INFO: rc: 1
Aug 27 18:26:33.119: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000952780 exit status 1 <nil> <nil> true [0xc0001eca80 0xc0001ecac8 0xc0001ecb18] [0xc0001eca80 0xc0001ecac8 0xc0001ecb18] [0xc0001ecab8 0xc0001ecaf8] [0x9c0ae0 0x9c0ae0] 0xc002118780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:26:43.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:26:43.237: INFO: rc: 1
Aug 27 18:26:43.237: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000952ae0 exit status 1 <nil> <nil> true [0xc0001ecb28 0xc0001ecba0 0xc0001ecc10] [0xc0001ecb28 0xc0001ecba0 0xc0001ecc10] [0xc0001ecb58 0xc0001ecc00] [0x9c0ae0 0x9c0ae0] 0xc002118b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:26:53.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:26:53.346: INFO: rc: 1
Aug 27 18:26:53.346: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a4c330 exit status 1 <nil> <nil> true [0xc0019be010 0xc0019be050 0xc0019be078] [0xc0019be010 0xc0019be050 0xc0019be078] [0xc0019be048 0xc0019be060] [0x9c0ae0 0x9c0ae0] 0xc0023622a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:27:03.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:27:03.456: INFO: rc: 1
Aug 27 18:27:03.456: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a4c8d0 exit status 1 <nil> <nil> true [0xc0019be098 0xc0019be0d8 0xc0019be118] [0xc0019be098 0xc0019be0d8 0xc0019be118] [0xc0019be0c0 0xc0019be110] [0x9c0ae0 0x9c0ae0] 0xc002362660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:27:13.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:27:13.585: INFO: rc: 1
Aug 27 18:27:13.585: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a4cd50 exit status 1 <nil> <nil> true [0xc0019be120 0xc0019be158 0xc0019be170] [0xc0019be120 0xc0019be158 0xc0019be170] [0xc0019be140 0xc0019be168] [0x9c0ae0 0x9c0ae0] 0xc0023629c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:27:23.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:27:23.694: INFO: rc: 1
Aug 27 18:27:23.694: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0022fcdb0 exit status 1 <nil> <nil> true [0xc0022340c8 0xc0022340e0 0xc002234130] [0xc0022340c8 0xc0022340e0 0xc002234130] [0xc0022340d8 0xc002234118] [0x9c0ae0 0x9c0ae0] 0xc001a98de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:27:33.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:27:33.803: INFO: rc: 1
Aug 27 18:27:33.803: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0022fd0e0 exit status 1 <nil> <nil> true [0xc002234138 0xc002234150 0xc002234188] [0xc002234138 0xc002234150 0xc002234188] [0xc002234148 0xc002234180] [0x9c0ae0 0x9c0ae0] 0xc001a99320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:27:43.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:27:43.905: INFO: rc: 1
Aug 27 18:27:43.906: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0016aa300 exit status 1 <nil> <nil> true [0xc0014ade78 0xc0014adf60 0xc00220e010] [0xc0014ade78 0xc0014adf60 0xc00220e010] [0xc0014adf40 0xc0014adfb0] [0x9c0ae0 0x9c0ae0] 0xc001fcd680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:27:53.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:27:54.015: INFO: rc: 1
Aug 27 18:27:54.015: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0016aa630 exit status 1 <nil> <nil> true [0xc00220e028 0xc00220e058 0xc00220e0b8] [0xc00220e028 0xc00220e058 0xc00220e0b8] [0xc00220e048 0xc00220e0b0] [0x9c0ae0 0x9c0ae0] 0xc0027060c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:28:04.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:28:04.124: INFO: rc: 1
Aug 27 18:28:04.124: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00209c300 exit status 1 <nil> <nil> true [0xc000010478 0xc000010878 0xc000010bd0] [0xc000010478 0xc000010878 0xc000010bd0] [0xc000010718 0xc000010bb0] [0x9c0ae0 0x9c0ae0] 0xc001fcc300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:28:14.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:28:14.227: INFO: rc: 1
Aug 27 18:28:14.227: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00197c840 exit status 1 <nil> <nil> true [0xc0014ac030 0xc0014ac238 0xc0014ac558] [0xc0014ac030 0xc0014ac238 0xc0014ac558] [0xc0014ac160 0xc0014ac4e8] [0x9c0ae0 0x9c0ae0] 0xc002704480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:28:24.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:28:24.348: INFO: rc: 1
Aug 27 18:28:24.348: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00234c540 exit status 1 <nil> <nil> true [0xc0000cc358 0xc00032f7c0 0xc00220e010] [0xc0000cc358 0xc00032f7c0 0xc00220e010] [0xc00032f540 0xc00032ff60] [0x9c0ae0 0x9c0ae0] 0xc0025bd320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:28:34.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:28:34.456: INFO: rc: 1
Aug 27 18:28:34.456: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023de150 exit status 1 <nil> <nil> true [0xc002234018 0xc002234060 0xc0022340a0] [0xc002234018 0xc002234060 0xc0022340a0] [0xc002234040 0xc002234088] [0x9c0ae0 0x9c0ae0] 0xc0027d25a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:28:44.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:28:44.574: INFO: rc: 1
Aug 27 18:28:44.574: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023de4b0 exit status 1 <nil> <nil> true [0xc0022340a8 0xc0022340c0 0xc0022340d8] [0xc0022340a8 0xc0022340c0 0xc0022340d8] [0xc0022340b8 0xc0022340d0] [0x9c0ae0 0x9c0ae0] 0xc0027d2d80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:28:54.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:28:54.687: INFO: rc: 1
Aug 27 18:28:54.687: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023de810 exit status 1 <nil> <nil> true [0xc0022340e0 0xc002234130 0xc002234148] [0xc0022340e0 0xc002234130 0xc002234148] [0xc002234118 0xc002234140] [0x9c0ae0 0x9c0ae0] 0xc0027d3bc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:29:04.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:29:04.809: INFO: rc: 1
Aug 27 18:29:04.809: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023deba0 exit status 1 <nil> <nil> true [0xc002234150 0xc002234188 0xc0022341a0] [0xc002234150 0xc002234188 0xc0022341a0] [0xc002234180 0xc002234198] [0x9c0ae0 0x9c0ae0] 0xc002706180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:29:14.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:29:14.917: INFO: rc: 1
Aug 27 18:29:14.917: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00234c8a0 exit status 1 <nil> <nil> true [0xc00220e028 0xc00220e058 0xc00220e0b8] [0xc00220e028 0xc00220e058 0xc00220e0b8] [0xc00220e048 0xc00220e0b0] [0x9c0ae0 0x9c0ae0] 0xc0025bdf80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:29:24.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:29:25.028: INFO: rc: 1
Aug 27 18:29:25.028: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00234cc00 exit status 1 <nil> <nil> true [0xc00220e0c0 0xc00220e100 0xc00220e158] [0xc00220e0c0 0xc00220e100 0xc00220e158] [0xc00220e0e8 0xc00220e128] [0x9c0ae0 0x9c0ae0] 0xc001a98300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:29:35.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:29:35.131: INFO: rc: 1
Aug 27 18:29:35.131: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00234cf60 exit status 1 <nil> <nil> true [0xc00220e1a8 0xc00220e248 0xc00220e2d8] [0xc00220e1a8 0xc00220e248 0xc00220e2d8] [0xc00220e208 0xc00220e2d0] [0x9c0ae0 0x9c0ae0] 0xc001a98660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:29:45.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:29:45.247: INFO: rc: 1
Aug 27 18:29:45.247: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023def00 exit status 1 <nil> <nil> true [0xc0022341a8 0xc0022341f8 0xc002234228] [0xc0022341a8 0xc0022341f8 0xc002234228] [0xc0022341e0 0xc002234220] [0x9c0ae0 0x9c0ae0] 0xc002706540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:29:55.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:29:55.351: INFO: rc: 1
Aug 27 18:29:55.352: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00209c720 exit status 1 <nil> <nil> true [0xc000010bd8 0xc000010ff8 0xc000011310] [0xc000010bd8 0xc000010ff8 0xc000011310] [0xc000010ee8 0xc0000111e0] [0x9c0ae0 0x9c0ae0] 0xc001fcc660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:30:05.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:30:05.462: INFO: rc: 1
Aug 27 18:30:05.462: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023de180 exit status 1 <nil> <nil> true [0xc00032f540 0xc00032ff60 0xc002234030] [0xc00032f540 0xc00032ff60 0xc002234030] [0xc00032fde8 0xc002234018] [0x9c0ae0 0x9c0ae0] 0xc0027d25a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:30:15.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:30:15.577: INFO: rc: 1
Aug 27 18:30:15.577: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00209c330 exit status 1 <nil> <nil> true [0xc00220e010 0xc00220e048 0xc00220e0b0] [0xc00220e010 0xc00220e048 0xc00220e0b0] [0xc00220e038 0xc00220e090] [0x9c0ae0 0x9c0ae0] 0xc0025bd320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:30:25.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:30:25.683: INFO: rc: 1
Aug 27 18:30:25.683: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00234c570 exit status 1 <nil> <nil> true [0xc000010358 0xc000010718 0xc000010bb0] [0xc000010358 0xc000010718 0xc000010bb0] [0xc000010598 0xc000010a20] [0x9c0ae0 0x9c0ae0] 0xc002706180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:30:35.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:30:35.800: INFO: rc: 1
Aug 27 18:30:35.801: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023de4e0 exit status 1 <nil> <nil> true [0xc002234040 0xc002234088 0xc0022340b0] [0xc002234040 0xc002234088 0xc0022340b0] [0xc002234078 0xc0022340a8] [0x9c0ae0 0x9c0ae0] 0xc0027d2d80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:30:45.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:30:45.905: INFO: rc: 1
Aug 27 18:30:45.905: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00197c870 exit status 1 <nil> <nil> true [0xc0014ac030 0xc0014ac238 0xc0014ac558] [0xc0014ac030 0xc0014ac238 0xc0014ac558] [0xc0014ac160 0xc0014ac4e8] [0x9c0ae0 0x9c0ae0] 0xc001a982a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:30:55.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:30:56.012: INFO: rc: 1
Aug 27 18:30:56.012: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00234c900 exit status 1 <nil> <nil> true [0xc000010bd0 0xc000010ee8 0xc0000111e0] [0xc000010bd0 0xc000010ee8 0xc0000111e0] [0xc000010cc0 0xc000011198] [0x9c0ae0 0x9c0ae0] 0xc002706540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:31:06.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:31:06.132: INFO: rc: 1
Aug 27 18:31:06.132: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00234ccc0 exit status 1 <nil> <nil> true [0xc000011310 0xc0000115a0 0xc0000118c8] [0xc000011310 0xc0000115a0 0xc0000118c8] [0xc000011488 0xc0000117e8] [0x9c0ae0 0x9c0ae0] 0xc002706900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:31:16.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:31:16.243: INFO: rc: 1
Aug 27 18:31:16.243: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00209c690 exit status 1 <nil> <nil> true [0xc00220e0b8 0xc00220e0e8 0xc00220e128] [0xc00220e0b8 0xc00220e0e8 0xc00220e128] [0xc00220e0c8 0xc00220e118] [0x9c0ae0 0x9c0ae0] 0xc0025bdf80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Aug 27 18:31:26.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-7411 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:31:26.364: INFO: rc: 1
Aug 27 18:31:26.364: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Aug 27 18:31:26.364: INFO: Scaling statefulset ss to 0
Aug 27 18:31:26.397: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug 27 18:31:26.406: INFO: Deleting all statefulset in ns statefulset-7411
Aug 27 18:31:26.417: INFO: Scaling statefulset ss to 0
Aug 27 18:31:26.471: INFO: Waiting for statefulset status.replicas updated to 0
Aug 27 18:31:26.481: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:31:26.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7411" for this suite.
Aug 27 18:31:32.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:31:32.856: INFO: namespace statefulset-7411 deletion completed in 6.329917863s

• [SLOW TEST:363.802 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:31:32.860: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2401
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:32:33.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2401" for this suite.
Aug 27 18:32:57.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:32:57.445: INFO: namespace container-probe-2401 deletion completed in 24.369616999s

• [SLOW TEST:84.586 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:32:57.446: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3561
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-98z5
STEP: Creating a pod to test atomic-volume-subpath
Aug 27 18:32:57.671: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-98z5" in namespace "subpath-3561" to be "success or failure"
Aug 27 18:32:57.681: INFO: Pod "pod-subpath-test-projected-98z5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.530811ms
Aug 27 18:32:59.691: INFO: Pod "pod-subpath-test-projected-98z5": Phase="Running", Reason="", readiness=true. Elapsed: 2.01909717s
Aug 27 18:33:01.700: INFO: Pod "pod-subpath-test-projected-98z5": Phase="Running", Reason="", readiness=true. Elapsed: 4.028656578s
Aug 27 18:33:03.711: INFO: Pod "pod-subpath-test-projected-98z5": Phase="Running", Reason="", readiness=true. Elapsed: 6.039931669s
Aug 27 18:33:05.721: INFO: Pod "pod-subpath-test-projected-98z5": Phase="Running", Reason="", readiness=true. Elapsed: 8.049928446s
Aug 27 18:33:07.737: INFO: Pod "pod-subpath-test-projected-98z5": Phase="Running", Reason="", readiness=true. Elapsed: 10.065820221s
Aug 27 18:33:09.749: INFO: Pod "pod-subpath-test-projected-98z5": Phase="Running", Reason="", readiness=true. Elapsed: 12.077313154s
Aug 27 18:33:11.782: INFO: Pod "pod-subpath-test-projected-98z5": Phase="Running", Reason="", readiness=true. Elapsed: 14.11028846s
Aug 27 18:33:13.792: INFO: Pod "pod-subpath-test-projected-98z5": Phase="Running", Reason="", readiness=true. Elapsed: 16.120928617s
Aug 27 18:33:15.805: INFO: Pod "pod-subpath-test-projected-98z5": Phase="Running", Reason="", readiness=true. Elapsed: 18.13319708s
Aug 27 18:33:17.815: INFO: Pod "pod-subpath-test-projected-98z5": Phase="Running", Reason="", readiness=true. Elapsed: 20.143048377s
Aug 27 18:33:19.826: INFO: Pod "pod-subpath-test-projected-98z5": Phase="Running", Reason="", readiness=true. Elapsed: 22.154597112s
Aug 27 18:33:21.836: INFO: Pod "pod-subpath-test-projected-98z5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.164980639s
STEP: Saw pod success
Aug 27 18:33:21.837: INFO: Pod "pod-subpath-test-projected-98z5" satisfied condition "success or failure"
Aug 27 18:33:21.847: INFO: Trying to get logs from node 10.135.206.197 pod pod-subpath-test-projected-98z5 container test-container-subpath-projected-98z5: <nil>
STEP: delete the pod
Aug 27 18:33:21.906: INFO: Waiting for pod pod-subpath-test-projected-98z5 to disappear
Aug 27 18:33:21.918: INFO: Pod pod-subpath-test-projected-98z5 no longer exists
STEP: Deleting pod pod-subpath-test-projected-98z5
Aug 27 18:33:21.918: INFO: Deleting pod "pod-subpath-test-projected-98z5" in namespace "subpath-3561"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:33:21.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3561" for this suite.
Aug 27 18:33:27.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:33:28.264: INFO: namespace subpath-3561 deletion completed in 6.326912134s

• [SLOW TEST:30.818 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:33:28.266: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-2987
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Aug 27 18:33:28.909: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 27 18:33:30.995: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702527608, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702527608, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702527608, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702527608, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 18:33:33.005: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702527608, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702527608, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702527608, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702527608, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 18:33:35.004: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702527608, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702527608, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702527608, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702527608, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 18:33:37.005: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702527608, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702527608, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702527608, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702527608, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 18:33:39.774: INFO: Waited 757.707437ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:33:40.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2987" for this suite.
Aug 27 18:33:46.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:33:46.623: INFO: namespace aggregator-2987 deletion completed in 6.387453344s

• [SLOW TEST:18.357 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:33:46.623: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5830
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5830
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 27 18:33:46.801: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 27 18:34:13.135: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.172.61:8080/dial?request=hostName&protocol=http&host=172.30.183.76&port=8080&tries=1'] Namespace:pod-network-test-5830 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 18:34:13.135: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 18:34:13.482: INFO: Waiting for endpoints: map[]
Aug 27 18:34:13.491: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.172.61:8080/dial?request=hostName&protocol=http&host=172.30.79.72&port=8080&tries=1'] Namespace:pod-network-test-5830 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 18:34:13.492: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 18:34:13.814: INFO: Waiting for endpoints: map[]
Aug 27 18:34:13.823: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.172.61:8080/dial?request=hostName&protocol=http&host=172.30.172.60&port=8080&tries=1'] Namespace:pod-network-test-5830 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 18:34:13.823: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 18:34:14.147: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:34:14.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5830" for this suite.
Aug 27 18:34:38.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:34:38.470: INFO: namespace pod-network-test-5830 deletion completed in 24.312699174s

• [SLOW TEST:51.847 seconds]
[sig-network] Networking
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:34:38.473: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4785
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Aug 27 18:34:38.655: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 27 18:34:38.669: INFO: Waiting for terminating namespaces to be deleted...
Aug 27 18:34:38.678: INFO: 
Logging pods the kubelet thinks is on node 10.134.235.118 before test
Aug 27 18:34:38.704: INFO: ibm-master-proxy-static-10.134.235.118 from kube-system started at <nil> (0 container statuses recorded)
Aug 27 18:34:38.704: INFO: calico-node-fbflt from kube-system started at 2019-08-27 16:49:08 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.704: INFO: 	Container calico-node ready: true, restart count 0
Aug 27 18:34:38.704: INFO: sonobuoy-systemd-logs-daemon-set-c96a900eeb9a46c8-qhnrg from heptio-sonobuoy started at 2019-08-27 18:12:22 +0000 UTC (2 container statuses recorded)
Aug 27 18:34:38.704: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 27 18:34:38.704: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 27 18:34:38.704: INFO: ibm-kube-fluentd-ddtlg from kube-system started at 2019-08-27 16:49:08 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.704: INFO: 	Container fluentd ready: true, restart count 0
Aug 27 18:34:38.704: INFO: sonobuoy from heptio-sonobuoy started at 2019-08-27 18:12:16 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.704: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 27 18:34:38.704: INFO: ibm-keepalived-watcher-29grz from kube-system started at 2019-08-27 16:49:08 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.704: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 27 18:34:38.704: INFO: test-k8s-e2e-pvg-master-verification from default started at 2019-08-27 16:51:22 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.704: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Aug 27 18:34:38.704: INFO: vpn-7754bb6d4-zks4g from kube-system started at 2019-08-27 16:59:00 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.705: INFO: 	Container vpn ready: true, restart count 0
Aug 27 18:34:38.705: INFO: sonobuoy-e2e-job-94e2d131d27f4c56 from heptio-sonobuoy started at 2019-08-27 18:12:22 +0000 UTC (2 container statuses recorded)
Aug 27 18:34:38.705: INFO: 	Container e2e ready: true, restart count 0
Aug 27 18:34:38.705: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 27 18:34:38.705: INFO: 
Logging pods the kubelet thinks is on node 10.135.206.197 before test
Aug 27 18:34:38.728: INFO: coredns-autoscaler-5d4db8dd68-whxxk from kube-system started at 2019-08-27 16:48:50 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.728: INFO: 	Container autoscaler ready: true, restart count 0
Aug 27 18:34:38.728: INFO: calico-node-2s8wd from kube-system started at 2019-08-27 16:48:32 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.728: INFO: 	Container calico-node ready: true, restart count 0
Aug 27 18:34:38.728: INFO: coredns-9dd7747c7-lxd4t from kube-system started at 2019-08-27 16:59:22 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.728: INFO: 	Container coredns ready: true, restart count 0
Aug 27 18:34:38.728: INFO: ibm-cloud-provider-ip-158-177-215-165-79887d87b5-svwm9 from ibm-system started at 2019-08-27 17:00:47 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.728: INFO: 	Container ibm-cloud-provider-ip-158-177-215-165 ready: true, restart count 0
Aug 27 18:34:38.728: INFO: public-crbliljj8f06leh3daoe40-alb1-545ccf9dcb-4sskw from kube-system started at 2019-08-27 17:02:44 +0000 UTC (4 container statuses recorded)
Aug 27 18:34:38.728: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Aug 27 18:34:38.728: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Aug 27 18:34:38.728: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Aug 27 18:34:38.728: INFO: 	Container nginx-ingress ready: true, restart count 0
Aug 27 18:34:38.728: INFO: ibm-keepalived-watcher-972mx from kube-system started at 2019-08-27 16:48:32 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.728: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 27 18:34:38.728: INFO: calico-kube-controllers-64fcffdcf4-8nnzm from kube-system started at 2019-08-27 16:48:50 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.728: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 27 18:34:38.728: INFO: ibm-file-plugin-554c9f89b6-pmh82 from kube-system started at 2019-08-27 16:48:50 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.728: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 27 18:34:38.728: INFO: ibm-storage-watcher-55867f764f-vwpv2 from kube-system started at 2019-08-27 16:48:50 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.728: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 27 18:34:38.728: INFO: ibm-kube-fluentd-hzmm2 from kube-system started at 2019-08-27 16:49:00 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.728: INFO: 	Container fluentd ready: true, restart count 0
Aug 27 18:34:38.728: INFO: ibm-master-proxy-static-10.135.206.197 from kube-system started at <nil> (0 container statuses recorded)
Aug 27 18:34:38.728: INFO: metrics-server-5d78bd76f6-ktqhz from kube-system started at 2019-08-27 16:49:29 +0000 UTC (2 container statuses recorded)
Aug 27 18:34:38.728: INFO: 	Container metrics-server ready: true, restart count 0
Aug 27 18:34:38.728: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Aug 27 18:34:38.728: INFO: sonobuoy-systemd-logs-daemon-set-c96a900eeb9a46c8-zk44g from heptio-sonobuoy started at 2019-08-27 18:12:22 +0000 UTC (2 container statuses recorded)
Aug 27 18:34:38.728: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 27 18:34:38.728: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 27 18:34:38.728: INFO: kubernetes-dashboard-5c8c9b7546-sl4zb from kube-system started at 2019-08-27 16:48:50 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.728: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Aug 27 18:34:38.728: INFO: 
Logging pods the kubelet thinks is on node 10.135.206.222 before test
Aug 27 18:34:38.755: INFO: calico-node-k2dkh from kube-system started at 2019-08-27 16:49:24 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.755: INFO: 	Container calico-node ready: true, restart count 0
Aug 27 18:34:38.755: INFO: public-crbliljj8f06leh3daoe40-alb1-545ccf9dcb-pbqzv from kube-system started at 2019-08-27 17:02:44 +0000 UTC (4 container statuses recorded)
Aug 27 18:34:38.755: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Aug 27 18:34:38.755: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Aug 27 18:34:38.755: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Aug 27 18:34:38.755: INFO: 	Container nginx-ingress ready: true, restart count 0
Aug 27 18:34:38.755: INFO: coredns-9dd7747c7-tp5vn from kube-system started at 2019-08-27 16:59:22 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.755: INFO: 	Container coredns ready: true, restart count 0
Aug 27 18:34:38.755: INFO: sonobuoy-systemd-logs-daemon-set-c96a900eeb9a46c8-qq5nz from heptio-sonobuoy started at 2019-08-27 18:12:22 +0000 UTC (2 container statuses recorded)
Aug 27 18:34:38.755: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 27 18:34:38.755: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 27 18:34:38.755: INFO: ibm-keepalived-watcher-kg5cs from kube-system started at 2019-08-27 16:49:24 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.755: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 27 18:34:38.755: INFO: ibm-kube-fluentd-9rjpc from kube-system started at 2019-08-27 16:49:24 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.755: INFO: 	Container fluentd ready: true, restart count 0
Aug 27 18:34:38.755: INFO: ibm-cloud-provider-ip-158-177-215-165-79887d87b5-f4cv5 from ibm-system started at 2019-08-27 17:00:47 +0000 UTC (1 container statuses recorded)
Aug 27 18:34:38.755: INFO: 	Container ibm-cloud-provider-ip-158-177-215-165 ready: true, restart count 0
Aug 27 18:34:38.755: INFO: ibm-master-proxy-static-10.135.206.222 from kube-system started at <nil> (0 container statuses recorded)
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-54e6d442-c8f9-11e9-a9da-1ec1181dd093 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-54e6d442-c8f9-11e9-a9da-1ec1181dd093 off the node 10.134.235.118
STEP: verifying the node doesn't have the label kubernetes.io/e2e-54e6d442-c8f9-11e9-a9da-1ec1181dd093
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:34:42.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4785" for this suite.
Aug 27 18:34:58.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:34:59.294: INFO: namespace sched-pred-4785 deletion completed in 16.373648371s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:20.821 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:34:59.294: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2625
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-60035d87-c8f9-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume configMaps
Aug 27 18:34:59.506: INFO: Waiting up to 5m0s for pod "pod-configmaps-60053dbb-c8f9-11e9-a9da-1ec1181dd093" in namespace "configmap-2625" to be "success or failure"
Aug 27 18:34:59.517: INFO: Pod "pod-configmaps-60053dbb-c8f9-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 10.800246ms
Aug 27 18:35:01.528: INFO: Pod "pod-configmaps-60053dbb-c8f9-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022003274s
STEP: Saw pod success
Aug 27 18:35:01.528: INFO: Pod "pod-configmaps-60053dbb-c8f9-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:35:01.539: INFO: Trying to get logs from node 10.135.206.197 pod pod-configmaps-60053dbb-c8f9-11e9-a9da-1ec1181dd093 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 18:35:01.601: INFO: Waiting for pod pod-configmaps-60053dbb-c8f9-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:35:01.612: INFO: Pod pod-configmaps-60053dbb-c8f9-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:35:01.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2625" for this suite.
Aug 27 18:35:07.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:35:07.986: INFO: namespace configmap-2625 deletion completed in 6.364829858s

• [SLOW TEST:8.693 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:35:07.988: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1825
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1825
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Aug 27 18:35:08.208: INFO: Found 0 stateful pods, waiting for 3
Aug 27 18:35:18.218: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 18:35:18.218: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 18:35:18.218: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 18:35:18.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-1825 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 27 18:35:18.688: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 27 18:35:18.688: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 27 18:35:18.688: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug 27 18:35:28.779: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 27 18:35:38.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-1825 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:35:39.272: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 27 18:35:39.273: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 27 18:35:39.273: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 27 18:35:39.312: INFO: Waiting for StatefulSet statefulset-1825/ss2 to complete update
Aug 27 18:35:39.312: INFO: Waiting for Pod statefulset-1825/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 27 18:35:39.312: INFO: Waiting for Pod statefulset-1825/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 27 18:35:39.312: INFO: Waiting for Pod statefulset-1825/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 27 18:35:49.332: INFO: Waiting for StatefulSet statefulset-1825/ss2 to complete update
Aug 27 18:35:49.332: INFO: Waiting for Pod statefulset-1825/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 27 18:35:49.332: INFO: Waiting for Pod statefulset-1825/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 27 18:35:49.332: INFO: Waiting for Pod statefulset-1825/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 27 18:35:59.332: INFO: Waiting for StatefulSet statefulset-1825/ss2 to complete update
Aug 27 18:35:59.332: INFO: Waiting for Pod statefulset-1825/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 27 18:35:59.332: INFO: Waiting for Pod statefulset-1825/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 27 18:36:09.337: INFO: Waiting for StatefulSet statefulset-1825/ss2 to complete update
Aug 27 18:36:09.337: INFO: Waiting for Pod statefulset-1825/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 27 18:36:19.339: INFO: Waiting for StatefulSet statefulset-1825/ss2 to complete update
STEP: Rolling back to a previous revision
Aug 27 18:36:29.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-1825 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 27 18:36:29.776: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 27 18:36:29.776: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 27 18:36:29.776: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 27 18:36:39.863: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 27 18:36:49.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-1825 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 18:36:50.379: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 27 18:36:50.379: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 27 18:36:50.379: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 27 18:37:10.438: INFO: Waiting for StatefulSet statefulset-1825/ss2 to complete update
Aug 27 18:37:10.438: INFO: Waiting for Pod statefulset-1825/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug 27 18:37:20.457: INFO: Deleting all statefulset in ns statefulset-1825
Aug 27 18:37:20.466: INFO: Scaling statefulset ss2 to 0
Aug 27 18:37:40.507: INFO: Waiting for statefulset status.replicas updated to 0
Aug 27 18:37:40.516: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:37:40.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1825" for this suite.
Aug 27 18:37:46.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:37:46.880: INFO: namespace statefulset-1825 deletion completed in 6.315512515s

• [SLOW TEST:158.892 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:37:46.883: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6927
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0827 18:37:53.134733      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 27 18:37:53.134: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:37:53.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6927" for this suite.
Aug 27 18:38:01.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:38:01.499: INFO: namespace gc-6927 deletion completed in 8.354504797s

• [SLOW TEST:14.617 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:38:01.501: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6498
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 27 18:38:01.711: INFO: Waiting up to 5m0s for pod "pod-cc9e0792-c8f9-11e9-a9da-1ec1181dd093" in namespace "emptydir-6498" to be "success or failure"
Aug 27 18:38:01.722: INFO: Pod "pod-cc9e0792-c8f9-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 11.344467ms
Aug 27 18:38:03.734: INFO: Pod "pod-cc9e0792-c8f9-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023091439s
Aug 27 18:38:05.756: INFO: Pod "pod-cc9e0792-c8f9-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044788181s
STEP: Saw pod success
Aug 27 18:38:05.756: INFO: Pod "pod-cc9e0792-c8f9-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:38:05.766: INFO: Trying to get logs from node 10.134.235.118 pod pod-cc9e0792-c8f9-11e9-a9da-1ec1181dd093 container test-container: <nil>
STEP: delete the pod
Aug 27 18:38:05.817: INFO: Waiting for pod pod-cc9e0792-c8f9-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:38:05.828: INFO: Pod pod-cc9e0792-c8f9-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:38:05.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6498" for this suite.
Aug 27 18:38:11.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:38:12.193: INFO: namespace emptydir-6498 deletion completed in 6.357165327s

• [SLOW TEST:10.693 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:38:12.194: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6931
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-d3022c90-c8f9-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume secrets
Aug 27 18:38:12.431: INFO: Waiting up to 5m0s for pod "pod-secrets-d3033f44-c8f9-11e9-a9da-1ec1181dd093" in namespace "secrets-6931" to be "success or failure"
Aug 27 18:38:12.440: INFO: Pod "pod-secrets-d3033f44-c8f9-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 8.887081ms
Aug 27 18:38:14.450: INFO: Pod "pod-secrets-d3033f44-c8f9-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01907485s
STEP: Saw pod success
Aug 27 18:38:14.450: INFO: Pod "pod-secrets-d3033f44-c8f9-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:38:14.462: INFO: Trying to get logs from node 10.135.206.222 pod pod-secrets-d3033f44-c8f9-11e9-a9da-1ec1181dd093 container secret-volume-test: <nil>
STEP: delete the pod
Aug 27 18:38:14.511: INFO: Waiting for pod pod-secrets-d3033f44-c8f9-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:38:14.519: INFO: Pod pod-secrets-d3033f44-c8f9-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:38:14.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6931" for this suite.
Aug 27 18:38:20.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:38:20.866: INFO: namespace secrets-6931 deletion completed in 6.336928508s

• [SLOW TEST:8.673 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:38:20.867: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9623
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 18:38:21.074: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d829a741-c8f9-11e9-a9da-1ec1181dd093" in namespace "downward-api-9623" to be "success or failure"
Aug 27 18:38:21.083: INFO: Pod "downwardapi-volume-d829a741-c8f9-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 8.705383ms
Aug 27 18:38:23.092: INFO: Pod "downwardapi-volume-d829a741-c8f9-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018504369s
Aug 27 18:38:25.102: INFO: Pod "downwardapi-volume-d829a741-c8f9-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028159219s
STEP: Saw pod success
Aug 27 18:38:25.102: INFO: Pod "downwardapi-volume-d829a741-c8f9-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:38:25.111: INFO: Trying to get logs from node 10.135.206.222 pod downwardapi-volume-d829a741-c8f9-11e9-a9da-1ec1181dd093 container client-container: <nil>
STEP: delete the pod
Aug 27 18:38:25.166: INFO: Waiting for pod downwardapi-volume-d829a741-c8f9-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:38:25.177: INFO: Pod downwardapi-volume-d829a741-c8f9-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:38:25.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9623" for this suite.
Aug 27 18:38:31.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:38:31.517: INFO: namespace downward-api-9623 deletion completed in 6.327061336s

• [SLOW TEST:10.651 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:38:31.518: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 27 18:38:35.817: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 27 18:38:35.831: INFO: Pod pod-with-prestop-http-hook still exists
Aug 27 18:38:37.831: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 27 18:38:37.840: INFO: Pod pod-with-prestop-http-hook still exists
Aug 27 18:38:39.831: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 27 18:38:39.840: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:38:39.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7647" for this suite.
Aug 27 18:39:03.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:39:04.225: INFO: namespace container-lifecycle-hook-7647 deletion completed in 24.358043952s

• [SLOW TEST:32.707 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:39:04.226: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3415
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1174
STEP: creating the pod
Aug 27 18:39:04.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 create -f - --namespace=kubectl-3415'
Aug 27 18:39:04.810: INFO: stderr: ""
Aug 27 18:39:04.810: INFO: stdout: "pod/pause created\n"
Aug 27 18:39:04.810: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 27 18:39:04.810: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3415" to be "running and ready"
Aug 27 18:39:04.822: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 11.38301ms
Aug 27 18:39:06.846: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035512813s
Aug 27 18:39:08.857: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.046226185s
Aug 27 18:39:08.857: INFO: Pod "pause" satisfied condition "running and ready"
Aug 27 18:39:08.857: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 27 18:39:08.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 label pods pause testing-label=testing-label-value --namespace=kubectl-3415'
Aug 27 18:39:08.987: INFO: stderr: ""
Aug 27 18:39:08.987: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 27 18:39:08.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pod pause -L testing-label --namespace=kubectl-3415'
Aug 27 18:39:09.109: INFO: stderr: ""
Aug 27 18:39:09.109: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 27 18:39:09.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 label pods pause testing-label- --namespace=kubectl-3415'
Aug 27 18:39:09.254: INFO: stderr: ""
Aug 27 18:39:09.254: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 27 18:39:09.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pod pause -L testing-label --namespace=kubectl-3415'
Aug 27 18:39:09.376: INFO: stderr: ""
Aug 27 18:39:09.376: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1181
STEP: using delete to clean up resources
Aug 27 18:39:09.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 delete --grace-period=0 --force -f - --namespace=kubectl-3415'
Aug 27 18:39:09.507: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 27 18:39:09.507: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 27 18:39:09.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get rc,svc -l name=pause --no-headers --namespace=kubectl-3415'
Aug 27 18:39:09.642: INFO: stderr: "No resources found.\n"
Aug 27 18:39:09.642: INFO: stdout: ""
Aug 27 18:39:09.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods -l name=pause --namespace=kubectl-3415 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 27 18:39:09.754: INFO: stderr: ""
Aug 27 18:39:09.754: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:39:09.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3415" for this suite.
Aug 27 18:39:15.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:39:16.132: INFO: namespace kubectl-3415 deletion completed in 6.367892306s

• [SLOW TEST:11.906 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:39:16.133: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8197
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-8197/configmap-test-f91999ac-c8f9-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume configMaps
Aug 27 18:39:16.340: INFO: Waiting up to 5m0s for pod "pod-configmaps-f91b4ce2-c8f9-11e9-a9da-1ec1181dd093" in namespace "configmap-8197" to be "success or failure"
Aug 27 18:39:16.349: INFO: Pod "pod-configmaps-f91b4ce2-c8f9-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 8.845393ms
Aug 27 18:39:18.363: INFO: Pod "pod-configmaps-f91b4ce2-c8f9-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022958049s
STEP: Saw pod success
Aug 27 18:39:18.364: INFO: Pod "pod-configmaps-f91b4ce2-c8f9-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:39:18.376: INFO: Trying to get logs from node 10.134.235.118 pod pod-configmaps-f91b4ce2-c8f9-11e9-a9da-1ec1181dd093 container env-test: <nil>
STEP: delete the pod
Aug 27 18:39:18.422: INFO: Waiting for pod pod-configmaps-f91b4ce2-c8f9-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:39:18.436: INFO: Pod pod-configmaps-f91b4ce2-c8f9-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:39:18.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8197" for this suite.
Aug 27 18:39:24.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:39:24.787: INFO: namespace configmap-8197 deletion completed in 6.327610405s

• [SLOW TEST:8.654 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:39:24.787: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9093
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-fe436912-c8f9-11e9-a9da-1ec1181dd093
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-fe436912-c8f9-11e9-a9da-1ec1181dd093
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:39:29.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9093" for this suite.
Aug 27 18:39:51.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:39:51.421: INFO: namespace configmap-9093 deletion completed in 22.307939908s

• [SLOW TEST:26.634 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:39:51.421: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9244
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-7674
STEP: Creating secret with name secret-test-0e230d63-c8fa-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume secrets
Aug 27 18:39:51.823: INFO: Waiting up to 5m0s for pod "pod-secrets-0e41192c-c8fa-11e9-a9da-1ec1181dd093" in namespace "secrets-9244" to be "success or failure"
Aug 27 18:39:51.836: INFO: Pod "pod-secrets-0e41192c-c8fa-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 12.321398ms
Aug 27 18:39:53.845: INFO: Pod "pod-secrets-0e41192c-c8fa-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02183784s
Aug 27 18:39:55.855: INFO: Pod "pod-secrets-0e41192c-c8fa-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03110749s
STEP: Saw pod success
Aug 27 18:39:55.855: INFO: Pod "pod-secrets-0e41192c-c8fa-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:39:55.864: INFO: Trying to get logs from node 10.135.206.197 pod pod-secrets-0e41192c-c8fa-11e9-a9da-1ec1181dd093 container secret-volume-test: <nil>
STEP: delete the pod
Aug 27 18:39:55.920: INFO: Waiting for pod pod-secrets-0e41192c-c8fa-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:39:55.928: INFO: Pod pod-secrets-0e41192c-c8fa-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:39:55.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9244" for this suite.
Aug 27 18:40:01.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:40:02.306: INFO: namespace secrets-9244 deletion completed in 6.367961938s
STEP: Destroying namespace "secret-namespace-7674" for this suite.
Aug 27 18:40:08.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:40:08.653: INFO: namespace secret-namespace-7674 deletion completed in 6.347503882s

• [SLOW TEST:17.232 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:40:08.654: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2460
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-18692fcf-c8fa-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume secrets
Aug 27 18:40:08.870: INFO: Waiting up to 5m0s for pod "pod-secrets-186a71bb-c8fa-11e9-a9da-1ec1181dd093" in namespace "secrets-2460" to be "success or failure"
Aug 27 18:40:08.883: INFO: Pod "pod-secrets-186a71bb-c8fa-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 12.573744ms
Aug 27 18:40:10.893: INFO: Pod "pod-secrets-186a71bb-c8fa-11e9-a9da-1ec1181dd093": Phase="Running", Reason="", readiness=true. Elapsed: 2.022136238s
Aug 27 18:40:12.904: INFO: Pod "pod-secrets-186a71bb-c8fa-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033084911s
STEP: Saw pod success
Aug 27 18:40:12.904: INFO: Pod "pod-secrets-186a71bb-c8fa-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:40:12.913: INFO: Trying to get logs from node 10.135.206.222 pod pod-secrets-186a71bb-c8fa-11e9-a9da-1ec1181dd093 container secret-volume-test: <nil>
STEP: delete the pod
Aug 27 18:40:12.971: INFO: Waiting for pod pod-secrets-186a71bb-c8fa-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:40:12.982: INFO: Pod pod-secrets-186a71bb-c8fa-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:40:12.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2460" for this suite.
Aug 27 18:40:19.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:40:19.366: INFO: namespace secrets-2460 deletion completed in 6.375570235s

• [SLOW TEST:10.712 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:40:19.366: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1390
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Aug 27 18:40:22.141: INFO: Successfully updated pod "labelsupdate1eca330b-c8fa-11e9-a9da-1ec1181dd093"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:40:24.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1390" for this suite.
Aug 27 18:40:46.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:40:46.543: INFO: namespace projected-1390 deletion completed in 22.352062534s

• [SLOW TEST:27.177 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:40:46.545: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1308
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 18:40:46.749: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 27 18:40:51.762: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 27 18:40:51.762: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug 27 18:40:55.832: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-1308,SelfLink:/apis/apps/v1/namespaces/deployment-1308/deployments/test-cleanup-deployment,UID:31fceba6-c8fa-11e9-9af8-1e15abefec17,ResourceVersion:25791,Generation:1,CreationTimestamp:2019-08-27 18:40:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-27 18:40:51 +0000 UTC 2019-08-27 18:40:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-27 18:40:55 +0000 UTC 2019-08-27 18:40:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-6865c98b76" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 27 18:40:55.839: INFO: New ReplicaSet "test-cleanup-deployment-6865c98b76" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-6865c98b76,GenerateName:,Namespace:deployment-1308,SelfLink:/apis/apps/v1/namespaces/deployment-1308/replicasets/test-cleanup-deployment-6865c98b76,UID:3200929f-c8fa-11e9-9af8-1e15abefec17,ResourceVersion:25781,Generation:1,CreationTimestamp:2019-08-27 18:40:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 31fceba6-c8fa-11e9-9af8-1e15abefec17 0xc00210b287 0xc00210b288}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 27 18:40:55.848: INFO: Pod "test-cleanup-deployment-6865c98b76-hz9ll" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-6865c98b76-hz9ll,GenerateName:test-cleanup-deployment-6865c98b76-,Namespace:deployment-1308,SelfLink:/api/v1/namespaces/deployment-1308/pods/test-cleanup-deployment-6865c98b76-hz9ll,UID:32018f6f-c8fa-11e9-9af8-1e15abefec17,ResourceVersion:25780,Generation:0,CreationTimestamp:2019-08-27 18:40:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-6865c98b76 3200929f-c8fa-11e9-9af8-1e15abefec17 0xc002b4a787 0xc002b4a788}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7plms {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7plms,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-7plms true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.235.118,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b4a800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b4a820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:40:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:40:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:40:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:40:51 +0000 UTC  }],Message:,Reason:,HostIP:10.134.235.118,PodIP:172.30.79.89,StartTime:2019-08-27 18:40:51 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-27 18:40:54 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://7d82d0d463f155ec8d2c71d409566e5c428e7bf5ebcfd39834ff5faa92ec15d6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:40:55.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1308" for this suite.
Aug 27 18:41:01.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:41:02.207: INFO: namespace deployment-1308 deletion completed in 6.350468903s

• [SLOW TEST:15.663 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:41:02.209: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5562
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 27 18:41:02.419: INFO: Waiting up to 5m0s for pod "pod-3854aac7-c8fa-11e9-a9da-1ec1181dd093" in namespace "emptydir-5562" to be "success or failure"
Aug 27 18:41:02.432: INFO: Pod "pod-3854aac7-c8fa-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 13.531039ms
Aug 27 18:41:04.447: INFO: Pod "pod-3854aac7-c8fa-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027897419s
Aug 27 18:41:06.456: INFO: Pod "pod-3854aac7-c8fa-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037388821s
STEP: Saw pod success
Aug 27 18:41:06.456: INFO: Pod "pod-3854aac7-c8fa-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:41:06.465: INFO: Trying to get logs from node 10.135.206.222 pod pod-3854aac7-c8fa-11e9-a9da-1ec1181dd093 container test-container: <nil>
STEP: delete the pod
Aug 27 18:41:06.526: INFO: Waiting for pod pod-3854aac7-c8fa-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:41:06.534: INFO: Pod pod-3854aac7-c8fa-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:41:06.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5562" for this suite.
Aug 27 18:41:12.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:41:12.968: INFO: namespace emptydir-5562 deletion completed in 6.41757732s

• [SLOW TEST:10.760 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:41:12.968: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1251
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 18:41:13.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 create -f - --namespace=kubectl-1251'
Aug 27 18:41:13.378: INFO: stderr: ""
Aug 27 18:41:13.378: INFO: stdout: "replicationcontroller/redis-master created\n"
Aug 27 18:41:13.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 create -f - --namespace=kubectl-1251'
Aug 27 18:41:13.618: INFO: stderr: ""
Aug 27 18:41:13.618: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 27 18:41:14.629: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 18:41:14.629: INFO: Found 0 / 1
Aug 27 18:41:15.630: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 18:41:15.630: INFO: Found 0 / 1
Aug 27 18:41:16.628: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 18:41:16.628: INFO: Found 0 / 1
Aug 27 18:41:17.629: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 18:41:17.629: INFO: Found 1 / 1
Aug 27 18:41:17.629: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 27 18:41:17.638: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 18:41:17.638: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 27 18:41:17.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 describe pod redis-master-npqwh --namespace=kubectl-1251'
Aug 27 18:41:17.787: INFO: stderr: ""
Aug 27 18:41:17.787: INFO: stdout: "Name:               redis-master-npqwh\nNamespace:          kubectl-1251\nPriority:           0\nPriorityClassName:  <none>\nNode:               10.135.206.197/10.135.206.197\nStart Time:         Tue, 27 Aug 2019 18:41:13 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 172.30.183.85\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   containerd://c480cd8cb9e12a35e1275cb267f5962f6625589fc00888dce2368a82dd6ceecb\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 27 Aug 2019 18:41:16 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-hfrm9 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-hfrm9:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-hfrm9\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 600s\n                 node.kubernetes.io/unreachable:NoExecute for 600s\nEvents:\n  Type    Reason     Age   From                     Message\n  ----    ------     ----  ----                     -------\n  Normal  Scheduled  4s    default-scheduler        Successfully assigned kubectl-1251/redis-master-npqwh to 10.135.206.197\n  Normal  Pulling    3s    kubelet, 10.135.206.197  Pulling image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Pulled     1s    kubelet, 10.135.206.197  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Created    1s    kubelet, 10.135.206.197  Created container redis-master\n  Normal  Started    1s    kubelet, 10.135.206.197  Started container redis-master\n"
Aug 27 18:41:17.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 describe rc redis-master --namespace=kubectl-1251'
Aug 27 18:41:17.940: INFO: stderr: ""
Aug 27 18:41:17.940: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-1251\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-npqwh\n"
Aug 27 18:41:17.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 describe service redis-master --namespace=kubectl-1251'
Aug 27 18:41:18.263: INFO: stderr: ""
Aug 27 18:41:18.263: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-1251\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.21.201.2\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.30.183.85:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 27 18:41:18.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 describe node 10.134.235.118'
Aug 27 18:41:18.466: INFO: stderr: ""
Aug 27 18:41:18.466: INFO: stdout: "Name:               10.134.235.118\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-de\n                    failure-domain.beta.kubernetes.io/zone=fra02\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=169.50.53.189\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.134.235.118\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_18_64\n                    ibm-cloud.kubernetes.io/region=eu-de\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-bliljj8f06leh3daoe40-kubee2epvgr-default-0000033c\n                    ibm-cloud.kubernetes.io/worker-pool-id=bliljj8f06leh3daoe40-2dbe217\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.14.5_1530\n                    ibm-cloud.kubernetes.io/zone=fra02\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.134.235.118\n                    kubernetes.io/os=linux\n                    privateVLAN=1739935\n                    publicVLAN=1739933\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 27 Aug 2019 16:49:07 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 27 Aug 2019 18:41:14 +0000   Tue, 27 Aug 2019 16:49:07 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 27 Aug 2019 18:41:14 +0000   Tue, 27 Aug 2019 16:49:07 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 27 Aug 2019 18:41:14 +0000   Tue, 27 Aug 2019 16:49:07 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 27 Aug 2019 18:41:14 +0000   Tue, 27 Aug 2019 16:49:37 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.134.235.118\n  ExternalIP:  169.50.53.189\n  Hostname:    10.134.235.118\nCapacity:\n cpu:                4\n ephemeral-storage:  102685624Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16419940Ki\n pods:               110\nAllocatable:\n cpu:                3910m\n ephemeral-storage:  99892574949\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             13627492Ki\n pods:               110\nSystem Info:\n Machine ID:                 cec0414a02aa43399c27f65c899b1e98\n System UUID:                2E9BFACA-2ECC-66AC-A247-12667353F866\n Boot ID:                    5643159e-cf1f-4655-91d6-b5a0e47b91b3\n Kernel Version:             4.15.0-58-generic\n OS Image:                   Ubuntu 18.04.3 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  containerd://1.2.7\n Kubelet Version:            v1.14.5+IKS\n Kube-Proxy Version:         v1.14.5+IKS\nProviderID:                  ibm://cc7530878c499d74ad77f31c918c626e///bliljj8f06leh3daoe40/kube-bliljj8f06leh3daoe40-kubee2epvgr-default-0000033c\nNon-terminated Pods:         (9 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  default                    test-k8s-e2e-pvg-master-verification                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         109m\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  heptio-sonobuoy            sonobuoy-e2e-job-94e2d131d27f4c56                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-c96a900eeb9a46c8-qhnrg    0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  kube-system                calico-node-fbflt                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         112m\n  kube-system                ibm-keepalived-watcher-29grz                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         112m\n  kube-system                ibm-kube-fluentd-ddtlg                                     25m (0%)      300m (7%)   150Mi (1%)       800M (5%)      112m\n  kube-system                ibm-master-proxy-static-10.134.235.118                     25m (0%)      300m (7%)   32M (0%)         512M (3%)      112m\n  kube-system                vpn-7754bb6d4-zks4g                                        5m (0%)       0 (0%)      5Mi (0%)         0 (0%)         102m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                310m (7%)      600m (15%)\n  memory             282130Ki (2%)  1312M (9%)\n  ephemeral-storage  0 (0%)         0 (0%)\nEvents:              <none>\n"
Aug 27 18:41:18.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 describe namespace kubectl-1251'
Aug 27 18:41:18.609: INFO: stderr: ""
Aug 27 18:41:18.609: INFO: stdout: "Name:         kubectl-1251\nLabels:       e2e-framework=kubectl\n              e2e-run=49a7a9d1-c8f6-11e9-a9da-1ec1181dd093\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:41:18.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1251" for this suite.
Aug 27 18:41:40.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:41:40.971: INFO: namespace kubectl-1251 deletion completed in 22.352203962s

• [SLOW TEST:28.003 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:41:40.971: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5818
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-6mz7
STEP: Creating a pod to test atomic-volume-subpath
Aug 27 18:41:41.185: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-6mz7" in namespace "subpath-5818" to be "success or failure"
Aug 27 18:41:41.195: INFO: Pod "pod-subpath-test-downwardapi-6mz7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.718638ms
Aug 27 18:41:43.206: INFO: Pod "pod-subpath-test-downwardapi-6mz7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020297727s
Aug 27 18:41:45.215: INFO: Pod "pod-subpath-test-downwardapi-6mz7": Phase="Running", Reason="", readiness=true. Elapsed: 4.029972944s
Aug 27 18:41:47.226: INFO: Pod "pod-subpath-test-downwardapi-6mz7": Phase="Running", Reason="", readiness=true. Elapsed: 6.040396166s
Aug 27 18:41:49.235: INFO: Pod "pod-subpath-test-downwardapi-6mz7": Phase="Running", Reason="", readiness=true. Elapsed: 8.049618738s
Aug 27 18:41:51.268: INFO: Pod "pod-subpath-test-downwardapi-6mz7": Phase="Running", Reason="", readiness=true. Elapsed: 10.082723805s
Aug 27 18:41:53.278: INFO: Pod "pod-subpath-test-downwardapi-6mz7": Phase="Running", Reason="", readiness=true. Elapsed: 12.092346156s
Aug 27 18:41:55.296: INFO: Pod "pod-subpath-test-downwardapi-6mz7": Phase="Running", Reason="", readiness=true. Elapsed: 14.110970366s
Aug 27 18:41:57.311: INFO: Pod "pod-subpath-test-downwardapi-6mz7": Phase="Running", Reason="", readiness=true. Elapsed: 16.125279182s
Aug 27 18:41:59.320: INFO: Pod "pod-subpath-test-downwardapi-6mz7": Phase="Running", Reason="", readiness=true. Elapsed: 18.134353538s
Aug 27 18:42:01.336: INFO: Pod "pod-subpath-test-downwardapi-6mz7": Phase="Running", Reason="", readiness=true. Elapsed: 20.150827692s
Aug 27 18:42:03.363: INFO: Pod "pod-subpath-test-downwardapi-6mz7": Phase="Running", Reason="", readiness=true. Elapsed: 22.178014465s
Aug 27 18:42:05.373: INFO: Pod "pod-subpath-test-downwardapi-6mz7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.188067494s
STEP: Saw pod success
Aug 27 18:42:05.374: INFO: Pod "pod-subpath-test-downwardapi-6mz7" satisfied condition "success or failure"
Aug 27 18:42:05.382: INFO: Trying to get logs from node 10.134.235.118 pod pod-subpath-test-downwardapi-6mz7 container test-container-subpath-downwardapi-6mz7: <nil>
STEP: delete the pod
Aug 27 18:42:05.439: INFO: Waiting for pod pod-subpath-test-downwardapi-6mz7 to disappear
Aug 27 18:42:05.450: INFO: Pod pod-subpath-test-downwardapi-6mz7 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-6mz7
Aug 27 18:42:05.450: INFO: Deleting pod "pod-subpath-test-downwardapi-6mz7" in namespace "subpath-5818"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:42:05.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5818" for this suite.
Aug 27 18:42:11.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:42:12.056: INFO: namespace subpath-5818 deletion completed in 6.582803127s

• [SLOW TEST:31.084 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:42:12.056: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6666
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 18:42:12.247: INFO: Creating ReplicaSet my-hostname-basic-61f730a9-c8fa-11e9-a9da-1ec1181dd093
Aug 27 18:42:12.268: INFO: Pod name my-hostname-basic-61f730a9-c8fa-11e9-a9da-1ec1181dd093: Found 0 pods out of 1
Aug 27 18:42:17.277: INFO: Pod name my-hostname-basic-61f730a9-c8fa-11e9-a9da-1ec1181dd093: Found 1 pods out of 1
Aug 27 18:42:17.278: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-61f730a9-c8fa-11e9-a9da-1ec1181dd093" is running
Aug 27 18:42:17.286: INFO: Pod "my-hostname-basic-61f730a9-c8fa-11e9-a9da-1ec1181dd093-29cqd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-27 18:42:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-27 18:42:14 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-27 18:42:14 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-27 18:42:12 +0000 UTC Reason: Message:}])
Aug 27 18:42:17.286: INFO: Trying to dial the pod
Aug 27 18:42:22.321: INFO: Controller my-hostname-basic-61f730a9-c8fa-11e9-a9da-1ec1181dd093: Got expected result from replica 1 [my-hostname-basic-61f730a9-c8fa-11e9-a9da-1ec1181dd093-29cqd]: "my-hostname-basic-61f730a9-c8fa-11e9-a9da-1ec1181dd093-29cqd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:42:22.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6666" for this suite.
Aug 27 18:42:28.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:42:28.676: INFO: namespace replicaset-6666 deletion completed in 6.34563587s

• [SLOW TEST:16.620 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:42:28.677: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6596
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Aug 27 18:42:28.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 api-versions'
Aug 27 18:42:28.958: INFO: stderr: ""
Aug 27 18:42:28.958: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:42:28.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6596" for this suite.
Aug 27 18:42:34.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:42:35.302: INFO: namespace kubectl-6596 deletion completed in 6.335897221s

• [SLOW TEST:6.625 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:42:35.303: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2471
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-6fd2e1d1-c8fa-11e9-a9da-1ec1181dd093
STEP: Creating configMap with name cm-test-opt-upd-6fd2e22e-c8fa-11e9-a9da-1ec1181dd093
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-6fd2e1d1-c8fa-11e9-a9da-1ec1181dd093
STEP: Updating configmap cm-test-opt-upd-6fd2e22e-c8fa-11e9-a9da-1ec1181dd093
STEP: Creating configMap with name cm-test-opt-create-6fd2e25d-c8fa-11e9-a9da-1ec1181dd093
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:43:48.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2471" for this suite.
Aug 27 18:44:12.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:44:12.911: INFO: namespace configmap-2471 deletion completed in 24.373936289s

• [SLOW TEST:97.608 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:44:12.911: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8301
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-a9fe3c42-c8fa-11e9-a9da-1ec1181dd093
Aug 27 18:44:13.110: INFO: Pod name my-hostname-basic-a9fe3c42-c8fa-11e9-a9da-1ec1181dd093: Found 0 pods out of 1
Aug 27 18:44:18.120: INFO: Pod name my-hostname-basic-a9fe3c42-c8fa-11e9-a9da-1ec1181dd093: Found 1 pods out of 1
Aug 27 18:44:18.120: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a9fe3c42-c8fa-11e9-a9da-1ec1181dd093" are running
Aug 27 18:44:18.131: INFO: Pod "my-hostname-basic-a9fe3c42-c8fa-11e9-a9da-1ec1181dd093-6v4c9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-27 18:44:13 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-27 18:44:16 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-27 18:44:16 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-27 18:44:13 +0000 UTC Reason: Message:}])
Aug 27 18:44:18.131: INFO: Trying to dial the pod
Aug 27 18:44:23.168: INFO: Controller my-hostname-basic-a9fe3c42-c8fa-11e9-a9da-1ec1181dd093: Got expected result from replica 1 [my-hostname-basic-a9fe3c42-c8fa-11e9-a9da-1ec1181dd093-6v4c9]: "my-hostname-basic-a9fe3c42-c8fa-11e9-a9da-1ec1181dd093-6v4c9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:44:23.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8301" for this suite.
Aug 27 18:44:29.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:44:29.504: INFO: namespace replication-controller-8301 deletion completed in 6.327162529s

• [SLOW TEST:16.593 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:44:29.504: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-6805
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Aug 27 18:44:29.698: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-6805" to be "success or failure"
Aug 27 18:44:29.721: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 23.085917ms
Aug 27 18:44:31.731: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032603469s
Aug 27 18:44:33.742: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043564432s
STEP: Saw pod success
Aug 27 18:44:33.742: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Aug 27 18:44:33.751: INFO: Trying to get logs from node 10.135.206.197 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Aug 27 18:44:33.806: INFO: Waiting for pod pod-host-path-test to disappear
Aug 27 18:44:33.815: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:44:33.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-6805" for this suite.
Aug 27 18:44:39.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:44:40.136: INFO: namespace hostpath-6805 deletion completed in 6.311948595s

• [SLOW TEST:10.632 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:44:40.138: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9324
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-9324
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 27 18:44:40.313: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 27 18:44:58.533: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.79.92:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9324 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 18:44:58.533: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 18:44:58.861: INFO: Found all expected endpoints: [netserver-0]
Aug 27 18:44:58.873: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.183.89:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9324 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 18:44:58.873: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 18:44:59.190: INFO: Found all expected endpoints: [netserver-1]
Aug 27 18:44:59.201: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.172.16:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9324 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 18:44:59.201: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 18:44:59.537: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:44:59.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9324" for this suite.
Aug 27 18:45:23.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:45:23.898: INFO: namespace pod-network-test-9324 deletion completed in 24.349456849s

• [SLOW TEST:43.760 seconds]
[sig-network] Networking
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:45:23.903: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9191
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Aug 27 18:45:24.083: INFO: namespace kubectl-9191
Aug 27 18:45:24.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 create -f - --namespace=kubectl-9191'
Aug 27 18:45:24.318: INFO: stderr: ""
Aug 27 18:45:24.318: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 27 18:45:25.330: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 18:45:25.330: INFO: Found 0 / 1
Aug 27 18:45:26.327: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 18:45:26.327: INFO: Found 1 / 1
Aug 27 18:45:26.327: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 27 18:45:26.338: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 18:45:26.338: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 27 18:45:26.338: INFO: wait on redis-master startup in kubectl-9191 
Aug 27 18:45:26.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 logs redis-master-swfbh redis-master --namespace=kubectl-9191'
Aug 27 18:45:26.526: INFO: stderr: ""
Aug 27 18:45:26.526: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Aug 18:45:25.714 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Aug 18:45:25.714 # Server started, Redis version 3.2.12\n1:M 27 Aug 18:45:25.714 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Aug 18:45:25.714 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Aug 27 18:45:26.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9191'
Aug 27 18:45:26.692: INFO: stderr: ""
Aug 27 18:45:26.692: INFO: stdout: "service/rm2 exposed\n"
Aug 27 18:45:26.701: INFO: Service rm2 in namespace kubectl-9191 found.
STEP: exposing service
Aug 27 18:45:28.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9191'
Aug 27 18:45:28.860: INFO: stderr: ""
Aug 27 18:45:28.860: INFO: stdout: "service/rm3 exposed\n"
Aug 27 18:45:28.870: INFO: Service rm3 in namespace kubectl-9191 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:45:30.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9191" for this suite.
Aug 27 18:45:54.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:45:55.227: INFO: namespace kubectl-9191 deletion completed in 24.330501178s

• [SLOW TEST:31.325 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:45:55.233: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6259
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Aug 27 18:45:57.468: INFO: Pod pod-hostip-e6fb3046-c8fa-11e9-a9da-1ec1181dd093 has hostIP: 10.135.206.197
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:45:57.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6259" for this suite.
Aug 27 18:46:21.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:46:21.799: INFO: namespace pods-6259 deletion completed in 24.30985672s

• [SLOW TEST:26.566 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:46:21.799: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1736
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-f6d1d048-c8fa-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume configMaps
Aug 27 18:46:22.015: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f6d3530b-c8fa-11e9-a9da-1ec1181dd093" in namespace "projected-1736" to be "success or failure"
Aug 27 18:46:22.027: INFO: Pod "pod-projected-configmaps-f6d3530b-c8fa-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 12.016397ms
Aug 27 18:46:24.039: INFO: Pod "pod-projected-configmaps-f6d3530b-c8fa-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02394369s
Aug 27 18:46:26.049: INFO: Pod "pod-projected-configmaps-f6d3530b-c8fa-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033705166s
STEP: Saw pod success
Aug 27 18:46:26.049: INFO: Pod "pod-projected-configmaps-f6d3530b-c8fa-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:46:26.059: INFO: Trying to get logs from node 10.134.235.118 pod pod-projected-configmaps-f6d3530b-c8fa-11e9-a9da-1ec1181dd093 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 18:46:26.110: INFO: Waiting for pod pod-projected-configmaps-f6d3530b-c8fa-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:46:26.122: INFO: Pod pod-projected-configmaps-f6d3530b-c8fa-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:46:26.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1736" for this suite.
Aug 27 18:46:32.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:46:32.530: INFO: namespace projected-1736 deletion completed in 6.397068144s

• [SLOW TEST:10.731 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:46:32.539: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9303
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Aug 27 18:46:32.721: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-561368167 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:46:32.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9303" for this suite.
Aug 27 18:46:38.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:46:39.160: INFO: namespace kubectl-9303 deletion completed in 6.327951228s

• [SLOW TEST:6.621 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:46:39.161: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4128
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-hx9t
STEP: Creating a pod to test atomic-volume-subpath
Aug 27 18:46:39.382: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hx9t" in namespace "subpath-4128" to be "success or failure"
Aug 27 18:46:39.396: INFO: Pod "pod-subpath-test-configmap-hx9t": Phase="Pending", Reason="", readiness=false. Elapsed: 13.256981ms
Aug 27 18:46:41.405: INFO: Pod "pod-subpath-test-configmap-hx9t": Phase="Running", Reason="", readiness=true. Elapsed: 2.022344121s
Aug 27 18:46:43.414: INFO: Pod "pod-subpath-test-configmap-hx9t": Phase="Running", Reason="", readiness=true. Elapsed: 4.031171962s
Aug 27 18:46:45.424: INFO: Pod "pod-subpath-test-configmap-hx9t": Phase="Running", Reason="", readiness=true. Elapsed: 6.041516856s
Aug 27 18:46:47.435: INFO: Pod "pod-subpath-test-configmap-hx9t": Phase="Running", Reason="", readiness=true. Elapsed: 8.052660655s
Aug 27 18:46:49.445: INFO: Pod "pod-subpath-test-configmap-hx9t": Phase="Running", Reason="", readiness=true. Elapsed: 10.062676391s
Aug 27 18:46:51.454: INFO: Pod "pod-subpath-test-configmap-hx9t": Phase="Running", Reason="", readiness=true. Elapsed: 12.071977196s
Aug 27 18:46:53.464: INFO: Pod "pod-subpath-test-configmap-hx9t": Phase="Running", Reason="", readiness=true. Elapsed: 14.082000034s
Aug 27 18:46:55.474: INFO: Pod "pod-subpath-test-configmap-hx9t": Phase="Running", Reason="", readiness=true. Elapsed: 16.091219083s
Aug 27 18:46:57.484: INFO: Pod "pod-subpath-test-configmap-hx9t": Phase="Running", Reason="", readiness=true. Elapsed: 18.101185711s
Aug 27 18:46:59.497: INFO: Pod "pod-subpath-test-configmap-hx9t": Phase="Running", Reason="", readiness=true. Elapsed: 20.114305803s
Aug 27 18:47:01.507: INFO: Pod "pod-subpath-test-configmap-hx9t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.124167953s
STEP: Saw pod success
Aug 27 18:47:01.507: INFO: Pod "pod-subpath-test-configmap-hx9t" satisfied condition "success or failure"
Aug 27 18:47:01.515: INFO: Trying to get logs from node 10.135.206.197 pod pod-subpath-test-configmap-hx9t container test-container-subpath-configmap-hx9t: <nil>
STEP: delete the pod
Aug 27 18:47:01.575: INFO: Waiting for pod pod-subpath-test-configmap-hx9t to disappear
Aug 27 18:47:01.588: INFO: Pod pod-subpath-test-configmap-hx9t no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hx9t
Aug 27 18:47:01.588: INFO: Deleting pod "pod-subpath-test-configmap-hx9t" in namespace "subpath-4128"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:47:01.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4128" for this suite.
Aug 27 18:47:07.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:47:07.997: INFO: namespace subpath-4128 deletion completed in 6.387898484s

• [SLOW TEST:28.836 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:47:07.997: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2971
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-2971
Aug 27 18:47:10.237: INFO: Started pod liveness-http in namespace container-probe-2971
STEP: checking the pod's current state and verifying that restartCount is present
Aug 27 18:47:10.246: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:51:11.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2971" for this suite.
Aug 27 18:51:17.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:51:17.894: INFO: namespace container-probe-2971 deletion completed in 6.326464366s

• [SLOW TEST:249.898 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:51:17.895: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7441
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 18:51:18.097: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 27 18:51:23.107: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 27 18:51:23.107: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 27 18:51:25.114: INFO: Creating deployment "test-rollover-deployment"
Aug 27 18:51:25.134: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 27 18:51:27.151: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 27 18:51:27.168: INFO: Ensure that both replica sets have 1 created replica
Aug 27 18:51:27.180: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 27 18:51:27.198: INFO: Updating deployment test-rollover-deployment
Aug 27 18:51:27.198: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 27 18:51:29.216: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 27 18:51:29.237: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 27 18:51:29.254: INFO: all replica sets need to contain the pod-template-hash label
Aug 27 18:51:29.254: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528685, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528685, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528688, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528685, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 18:51:31.270: INFO: all replica sets need to contain the pod-template-hash label
Aug 27 18:51:31.270: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528685, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528685, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528688, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528685, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 18:51:33.271: INFO: all replica sets need to contain the pod-template-hash label
Aug 27 18:51:33.271: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528685, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528685, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528688, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528685, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 18:51:35.270: INFO: all replica sets need to contain the pod-template-hash label
Aug 27 18:51:35.270: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528685, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528685, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528688, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528685, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 18:51:37.271: INFO: all replica sets need to contain the pod-template-hash label
Aug 27 18:51:37.271: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528685, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528685, loc:(*time.Location)(0x8825120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528688, loc:(*time.Location)(0x8825120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702528685, loc:(*time.Location)(0x8825120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 18:51:39.272: INFO: 
Aug 27 18:51:39.272: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug 27 18:51:39.293: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-7441,SelfLink:/apis/apps/v1/namespaces/deployment-7441/deployments/test-rollover-deployment,UID:ab7c3dc0-c8fb-11e9-9af8-1e15abefec17,ResourceVersion:27675,Generation:2,CreationTimestamp:2019-08-27 18:51:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-27 18:51:25 +0000 UTC 2019-08-27 18:51:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-27 18:51:38 +0000 UTC 2019-08-27 18:51:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-659c699649" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 27 18:51:39.300: INFO: New ReplicaSet "test-rollover-deployment-659c699649" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-659c699649,GenerateName:,Namespace:deployment-7441,SelfLink:/apis/apps/v1/namespaces/deployment-7441/replicasets/test-rollover-deployment-659c699649,UID:acb9f33f-c8fb-11e9-9af8-1e15abefec17,ResourceVersion:27664,Generation:2,CreationTimestamp:2019-08-27 18:51:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment ab7c3dc0-c8fb-11e9-9af8-1e15abefec17 0xc0025304f7 0xc0025304f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 27 18:51:39.300: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 27 18:51:39.300: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-7441,SelfLink:/apis/apps/v1/namespaces/deployment-7441/replicasets/test-rollover-controller,UID:a74a7aae-c8fb-11e9-9af8-1e15abefec17,ResourceVersion:27673,Generation:2,CreationTimestamp:2019-08-27 18:51:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment ab7c3dc0-c8fb-11e9-9af8-1e15abefec17 0xc002530427 0xc002530428}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 27 18:51:39.300: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-7b45b6464,GenerateName:,Namespace:deployment-7441,SelfLink:/apis/apps/v1/namespaces/deployment-7441/replicasets/test-rollover-deployment-7b45b6464,UID:ab7fb5c0-c8fb-11e9-9af8-1e15abefec17,ResourceVersion:27624,Generation:2,CreationTimestamp:2019-08-27 18:51:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment ab7c3dc0-c8fb-11e9-9af8-1e15abefec17 0xc0025305c0 0xc0025305c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 27 18:51:39.310: INFO: Pod "test-rollover-deployment-659c699649-ftn84" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-659c699649-ftn84,GenerateName:test-rollover-deployment-659c699649-,Namespace:deployment-7441,SelfLink:/api/v1/namespaces/deployment-7441/pods/test-rollover-deployment-659c699649-ftn84,UID:acbf2963-c8fb-11e9-9af8-1e15abefec17,ResourceVersion:27641,Generation:0,CreationTimestamp:2019-08-27 18:51:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-659c699649 acb9f33f-c8fb-11e9-9af8-1e15abefec17 0xc000a7a7e7 0xc000a7a7e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gxv88 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gxv88,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-gxv88 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.197,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a7a8e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a7a910}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:51:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:51:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:51:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 18:51:27 +0000 UTC  }],Message:,Reason:,HostIP:10.135.206.197,PodIP:172.30.183.95,StartTime:2019-08-27 18:51:27 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-27 18:51:28 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://f22c2d7c32bbf166b4600f1d7c0616b6c9b79bbbca8c5a32a6dff186015dc78c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:51:39.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7441" for this suite.
Aug 27 18:51:47.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:51:47.710: INFO: namespace deployment-7441 deletion completed in 8.390097221s

• [SLOW TEST:29.815 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:51:47.710: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8701
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 18:51:47.925: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b915ea05-c8fb-11e9-a9da-1ec1181dd093" in namespace "projected-8701" to be "success or failure"
Aug 27 18:51:47.936: INFO: Pod "downwardapi-volume-b915ea05-c8fb-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 11.869146ms
Aug 27 18:51:49.947: INFO: Pod "downwardapi-volume-b915ea05-c8fb-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022069731s
STEP: Saw pod success
Aug 27 18:51:49.947: INFO: Pod "downwardapi-volume-b915ea05-c8fb-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:51:49.955: INFO: Trying to get logs from node 10.134.235.118 pod downwardapi-volume-b915ea05-c8fb-11e9-a9da-1ec1181dd093 container client-container: <nil>
STEP: delete the pod
Aug 27 18:51:50.004: INFO: Waiting for pod downwardapi-volume-b915ea05-c8fb-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:51:50.013: INFO: Pod downwardapi-volume-b915ea05-c8fb-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:51:50.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8701" for this suite.
Aug 27 18:51:56.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:51:56.339: INFO: namespace projected-8701 deletion completed in 6.316565443s

• [SLOW TEST:8.629 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:51:56.340: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6965
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:52:19.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6965" for this suite.
Aug 27 18:52:25.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:52:25.504: INFO: namespace container-runtime-6965 deletion completed in 6.307303644s

• [SLOW TEST:29.164 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:52:25.505: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1267
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Aug 27 18:52:25.710: INFO: Waiting up to 5m0s for pod "downward-api-cf9b153d-c8fb-11e9-a9da-1ec1181dd093" in namespace "downward-api-1267" to be "success or failure"
Aug 27 18:52:25.722: INFO: Pod "downward-api-cf9b153d-c8fb-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 12.275154ms
Aug 27 18:52:27.739: INFO: Pod "downward-api-cf9b153d-c8fb-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029055689s
Aug 27 18:52:29.751: INFO: Pod "downward-api-cf9b153d-c8fb-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041831643s
STEP: Saw pod success
Aug 27 18:52:29.752: INFO: Pod "downward-api-cf9b153d-c8fb-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:52:29.763: INFO: Trying to get logs from node 10.134.235.118 pod downward-api-cf9b153d-c8fb-11e9-a9da-1ec1181dd093 container dapi-container: <nil>
STEP: delete the pod
Aug 27 18:52:29.814: INFO: Waiting for pod downward-api-cf9b153d-c8fb-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:52:29.822: INFO: Pod downward-api-cf9b153d-c8fb-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:52:29.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1267" for this suite.
Aug 27 18:52:35.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:52:36.176: INFO: namespace downward-api-1267 deletion completed in 6.345031382s

• [SLOW TEST:10.670 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:52:36.176: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1926
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 18:52:36.407: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 27 18:52:36.440: INFO: Number of nodes with available pods: 0
Aug 27 18:52:36.440: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 18:52:37.460: INFO: Number of nodes with available pods: 0
Aug 27 18:52:37.460: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 18:52:38.460: INFO: Number of nodes with available pods: 1
Aug 27 18:52:38.460: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 18:52:39.461: INFO: Number of nodes with available pods: 3
Aug 27 18:52:39.461: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 27 18:52:39.535: INFO: Wrong image for pod: daemon-set-9nxt8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:39.535: INFO: Wrong image for pod: daemon-set-n4qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:39.535: INFO: Wrong image for pod: daemon-set-psnqt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:40.552: INFO: Wrong image for pod: daemon-set-9nxt8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:40.552: INFO: Wrong image for pod: daemon-set-n4qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:40.552: INFO: Wrong image for pod: daemon-set-psnqt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:41.552: INFO: Wrong image for pod: daemon-set-9nxt8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:41.552: INFO: Wrong image for pod: daemon-set-n4qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:41.552: INFO: Wrong image for pod: daemon-set-psnqt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:42.552: INFO: Wrong image for pod: daemon-set-9nxt8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:42.552: INFO: Pod daemon-set-9nxt8 is not available
Aug 27 18:52:42.552: INFO: Wrong image for pod: daemon-set-n4qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:42.552: INFO: Wrong image for pod: daemon-set-psnqt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:43.554: INFO: Pod daemon-set-4vkqh is not available
Aug 27 18:52:43.554: INFO: Wrong image for pod: daemon-set-n4qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:43.554: INFO: Wrong image for pod: daemon-set-psnqt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:44.551: INFO: Pod daemon-set-4vkqh is not available
Aug 27 18:52:44.551: INFO: Wrong image for pod: daemon-set-n4qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:44.551: INFO: Wrong image for pod: daemon-set-psnqt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:45.552: INFO: Wrong image for pod: daemon-set-n4qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:45.552: INFO: Wrong image for pod: daemon-set-psnqt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:46.553: INFO: Wrong image for pod: daemon-set-n4qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:46.553: INFO: Pod daemon-set-n4qzq is not available
Aug 27 18:52:46.553: INFO: Wrong image for pod: daemon-set-psnqt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:47.552: INFO: Pod daemon-set-bldh6 is not available
Aug 27 18:52:47.552: INFO: Wrong image for pod: daemon-set-psnqt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:48.552: INFO: Pod daemon-set-bldh6 is not available
Aug 27 18:52:48.552: INFO: Wrong image for pod: daemon-set-psnqt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:49.552: INFO: Wrong image for pod: daemon-set-psnqt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:50.551: INFO: Wrong image for pod: daemon-set-psnqt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 18:52:50.551: INFO: Pod daemon-set-psnqt is not available
Aug 27 18:52:51.551: INFO: Pod daemon-set-lksxn is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 27 18:52:51.581: INFO: Number of nodes with available pods: 2
Aug 27 18:52:51.581: INFO: Node 10.135.206.197 is running more than one daemon pod
Aug 27 18:52:52.599: INFO: Number of nodes with available pods: 2
Aug 27 18:52:52.599: INFO: Node 10.135.206.197 is running more than one daemon pod
Aug 27 18:52:53.600: INFO: Number of nodes with available pods: 3
Aug 27 18:52:53.600: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1926, will wait for the garbage collector to delete the pods
Aug 27 18:52:53.732: INFO: Deleting DaemonSet.extensions daemon-set took: 26.521171ms
Aug 27 18:52:53.832: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.471292ms
Aug 27 18:53:06.242: INFO: Number of nodes with available pods: 0
Aug 27 18:53:06.242: INFO: Number of running nodes: 0, number of available pods: 0
Aug 27 18:53:06.250: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1926/daemonsets","resourceVersion":"28188"},"items":null}

Aug 27 18:53:06.263: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1926/pods","resourceVersion":"28188"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:53:06.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1926" for this suite.
Aug 27 18:53:12.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:53:12.622: INFO: namespace daemonsets-1926 deletion completed in 6.316961252s

• [SLOW TEST:36.446 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:53:12.622: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5387
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0827 18:53:13.900501      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 27 18:53:13.900: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:53:13.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5387" for this suite.
Aug 27 18:53:19.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:53:20.225: INFO: namespace gc-5387 deletion completed in 6.315651457s

• [SLOW TEST:7.602 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:53:20.225: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1772
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-f038c435-c8fb-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume secrets
Aug 27 18:53:20.436: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f039db29-c8fb-11e9-a9da-1ec1181dd093" in namespace "projected-1772" to be "success or failure"
Aug 27 18:53:20.449: INFO: Pod "pod-projected-secrets-f039db29-c8fb-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 13.128358ms
Aug 27 18:53:22.458: INFO: Pod "pod-projected-secrets-f039db29-c8fb-11e9-a9da-1ec1181dd093": Phase="Running", Reason="", readiness=true. Elapsed: 2.022106453s
Aug 27 18:53:24.472: INFO: Pod "pod-projected-secrets-f039db29-c8fb-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036262762s
STEP: Saw pod success
Aug 27 18:53:24.473: INFO: Pod "pod-projected-secrets-f039db29-c8fb-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:53:24.482: INFO: Trying to get logs from node 10.135.206.222 pod pod-projected-secrets-f039db29-c8fb-11e9-a9da-1ec1181dd093 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 27 18:53:24.535: INFO: Waiting for pod pod-projected-secrets-f039db29-c8fb-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:53:24.544: INFO: Pod pod-projected-secrets-f039db29-c8fb-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:53:24.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1772" for this suite.
Aug 27 18:53:30.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:53:30.853: INFO: namespace projected-1772 deletion completed in 6.300910785s

• [SLOW TEST:10.628 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:53:30.853: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1459
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-1459
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 27 18:53:31.033: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 27 18:53:55.276: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.172.25:8080/dial?request=hostName&protocol=udp&host=172.30.79.103&port=8081&tries=1'] Namespace:pod-network-test-1459 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 18:53:55.276: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 18:53:55.730: INFO: Waiting for endpoints: map[]
Aug 27 18:53:55.741: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.172.25:8080/dial?request=hostName&protocol=udp&host=172.30.183.99&port=8081&tries=1'] Namespace:pod-network-test-1459 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 18:53:55.741: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 18:53:56.057: INFO: Waiting for endpoints: map[]
Aug 27 18:53:56.066: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.172.25:8080/dial?request=hostName&protocol=udp&host=172.30.172.23&port=8081&tries=1'] Namespace:pod-network-test-1459 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 18:53:56.066: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 18:53:56.381: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:53:56.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1459" for this suite.
Aug 27 18:54:20.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:54:20.739: INFO: namespace pod-network-test-1459 deletion completed in 24.34585818s

• [SLOW TEST:49.886 seconds]
[sig-network] Networking
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:54:20.740: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 18:54:42.979: INFO: Container started at 2019-08-27 18:54:23 +0000 UTC, pod became ready at 2019-08-27 18:54:42 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:54:42.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3109" for this suite.
Aug 27 18:55:05.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:55:05.333: INFO: namespace container-probe-3109 deletion completed in 22.344504381s

• [SLOW TEST:44.593 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:55:05.333: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Aug 27 18:55:05.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 create -f - --namespace=kubectl-7917'
Aug 27 18:55:05.879: INFO: stderr: ""
Aug 27 18:55:05.879: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 27 18:55:06.889: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 18:55:06.889: INFO: Found 0 / 1
Aug 27 18:55:07.889: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 18:55:07.889: INFO: Found 0 / 1
Aug 27 18:55:08.889: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 18:55:08.889: INFO: Found 1 / 1
Aug 27 18:55:08.889: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 27 18:55:08.898: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 18:55:08.898: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 27 18:55:08.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 patch pod redis-master-bqmtc --namespace=kubectl-7917 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 27 18:55:09.027: INFO: stderr: ""
Aug 27 18:55:09.027: INFO: stdout: "pod/redis-master-bqmtc patched\n"
STEP: checking annotations
Aug 27 18:55:09.041: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 18:55:09.041: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:55:09.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7917" for this suite.
Aug 27 18:55:33.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:55:33.361: INFO: namespace kubectl-7917 deletion completed in 24.310744093s

• [SLOW TEST:28.028 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:55:33.361: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8816
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8816.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8816.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 27 18:55:49.681: INFO: DNS probes using dns-8816/dns-test-3f92e44e-c8fc-11e9-a9da-1ec1181dd093 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:55:49.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8816" for this suite.
Aug 27 18:55:55.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:55:56.037: INFO: namespace dns-8816 deletion completed in 6.311095769s

• [SLOW TEST:22.676 seconds]
[sig-network] DNS
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:55:56.038: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-278
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Aug 27 18:55:56.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 create -f - --namespace=kubectl-278'
Aug 27 18:55:56.542: INFO: stderr: ""
Aug 27 18:55:56.542: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 27 18:55:56.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-278'
Aug 27 18:55:56.665: INFO: stderr: ""
Aug 27 18:55:56.665: INFO: stdout: "update-demo-nautilus-9l98g update-demo-nautilus-mzztw "
Aug 27 18:55:56.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-9l98g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-278'
Aug 27 18:55:56.791: INFO: stderr: ""
Aug 27 18:55:56.791: INFO: stdout: ""
Aug 27 18:55:56.791: INFO: update-demo-nautilus-9l98g is created but not running
Aug 27 18:56:01.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-278'
Aug 27 18:56:01.907: INFO: stderr: ""
Aug 27 18:56:01.907: INFO: stdout: "update-demo-nautilus-9l98g update-demo-nautilus-mzztw "
Aug 27 18:56:01.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-9l98g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-278'
Aug 27 18:56:02.019: INFO: stderr: ""
Aug 27 18:56:02.019: INFO: stdout: "true"
Aug 27 18:56:02.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-9l98g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-278'
Aug 27 18:56:02.142: INFO: stderr: ""
Aug 27 18:56:02.142: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 27 18:56:02.142: INFO: validating pod update-demo-nautilus-9l98g
Aug 27 18:56:02.159: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 27 18:56:02.159: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 27 18:56:02.159: INFO: update-demo-nautilus-9l98g is verified up and running
Aug 27 18:56:02.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-mzztw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-278'
Aug 27 18:56:02.274: INFO: stderr: ""
Aug 27 18:56:02.274: INFO: stdout: "true"
Aug 27 18:56:02.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-mzztw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-278'
Aug 27 18:56:02.396: INFO: stderr: ""
Aug 27 18:56:02.396: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 27 18:56:02.396: INFO: validating pod update-demo-nautilus-mzztw
Aug 27 18:56:02.413: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 27 18:56:02.413: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 27 18:56:02.413: INFO: update-demo-nautilus-mzztw is verified up and running
STEP: rolling-update to new replication controller
Aug 27 18:56:02.415: INFO: scanned /root for discovery docs: <nil>
Aug 27 18:56:02.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-278'
Aug 27 18:56:25.191: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 27 18:56:25.191: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 27 18:56:25.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-278'
Aug 27 18:56:25.326: INFO: stderr: ""
Aug 27 18:56:25.326: INFO: stdout: "update-demo-kitten-kt2nb update-demo-kitten-sg9xh "
Aug 27 18:56:25.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-kitten-kt2nb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-278'
Aug 27 18:56:25.440: INFO: stderr: ""
Aug 27 18:56:25.440: INFO: stdout: "true"
Aug 27 18:56:25.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-kitten-kt2nb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-278'
Aug 27 18:56:25.556: INFO: stderr: ""
Aug 27 18:56:25.556: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 27 18:56:25.556: INFO: validating pod update-demo-kitten-kt2nb
Aug 27 18:56:25.570: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 27 18:56:25.570: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 27 18:56:25.570: INFO: update-demo-kitten-kt2nb is verified up and running
Aug 27 18:56:25.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-kitten-sg9xh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-278'
Aug 27 18:56:25.688: INFO: stderr: ""
Aug 27 18:56:25.688: INFO: stdout: "true"
Aug 27 18:56:25.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-kitten-sg9xh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-278'
Aug 27 18:56:25.813: INFO: stderr: ""
Aug 27 18:56:25.813: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 27 18:56:25.813: INFO: validating pod update-demo-kitten-sg9xh
Aug 27 18:56:25.837: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 27 18:56:25.837: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 27 18:56:25.837: INFO: update-demo-kitten-sg9xh is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:56:25.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-278" for this suite.
Aug 27 18:56:49.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:56:50.171: INFO: namespace kubectl-278 deletion completed in 24.324004341s

• [SLOW TEST:54.133 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:56:50.172: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7146
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Aug 27 18:56:50.353: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 27 18:56:50.370: INFO: Waiting for terminating namespaces to be deleted...
Aug 27 18:56:50.379: INFO: 
Logging pods the kubelet thinks is on node 10.134.235.118 before test
Aug 27 18:56:50.405: INFO: ibm-master-proxy-static-10.134.235.118 from kube-system started at <nil> (0 container statuses recorded)
Aug 27 18:56:50.405: INFO: calico-node-fbflt from kube-system started at 2019-08-27 16:49:08 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.405: INFO: 	Container calico-node ready: true, restart count 0
Aug 27 18:56:50.405: INFO: sonobuoy-systemd-logs-daemon-set-c96a900eeb9a46c8-qhnrg from heptio-sonobuoy started at 2019-08-27 18:12:22 +0000 UTC (2 container statuses recorded)
Aug 27 18:56:50.405: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 27 18:56:50.405: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 27 18:56:50.405: INFO: ibm-kube-fluentd-ddtlg from kube-system started at 2019-08-27 16:49:08 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.405: INFO: 	Container fluentd ready: true, restart count 0
Aug 27 18:56:50.405: INFO: sonobuoy from heptio-sonobuoy started at 2019-08-27 18:12:16 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.405: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 27 18:56:50.405: INFO: ibm-keepalived-watcher-29grz from kube-system started at 2019-08-27 16:49:08 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.405: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 27 18:56:50.405: INFO: test-k8s-e2e-pvg-master-verification from default started at 2019-08-27 16:51:22 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.405: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Aug 27 18:56:50.405: INFO: vpn-7754bb6d4-zks4g from kube-system started at 2019-08-27 16:59:00 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.405: INFO: 	Container vpn ready: true, restart count 0
Aug 27 18:56:50.405: INFO: sonobuoy-e2e-job-94e2d131d27f4c56 from heptio-sonobuoy started at 2019-08-27 18:12:22 +0000 UTC (2 container statuses recorded)
Aug 27 18:56:50.405: INFO: 	Container e2e ready: true, restart count 0
Aug 27 18:56:50.405: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 27 18:56:50.405: INFO: 
Logging pods the kubelet thinks is on node 10.135.206.197 before test
Aug 27 18:56:50.437: INFO: coredns-autoscaler-5d4db8dd68-whxxk from kube-system started at 2019-08-27 16:48:50 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.437: INFO: 	Container autoscaler ready: true, restart count 0
Aug 27 18:56:50.437: INFO: ibm-cloud-provider-ip-158-177-215-165-79887d87b5-svwm9 from ibm-system started at 2019-08-27 17:00:47 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.437: INFO: 	Container ibm-cloud-provider-ip-158-177-215-165 ready: true, restart count 0
Aug 27 18:56:50.437: INFO: public-crbliljj8f06leh3daoe40-alb1-545ccf9dcb-4sskw from kube-system started at 2019-08-27 17:02:44 +0000 UTC (4 container statuses recorded)
Aug 27 18:56:50.437: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Aug 27 18:56:50.438: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Aug 27 18:56:50.438: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Aug 27 18:56:50.438: INFO: 	Container nginx-ingress ready: true, restart count 0
Aug 27 18:56:50.438: INFO: ibm-keepalived-watcher-972mx from kube-system started at 2019-08-27 16:48:32 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.438: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 27 18:56:50.438: INFO: calico-node-2s8wd from kube-system started at 2019-08-27 16:48:32 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.438: INFO: 	Container calico-node ready: true, restart count 0
Aug 27 18:56:50.438: INFO: coredns-9dd7747c7-lxd4t from kube-system started at 2019-08-27 16:59:22 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.438: INFO: 	Container coredns ready: true, restart count 0
Aug 27 18:56:50.438: INFO: ibm-storage-watcher-55867f764f-vwpv2 from kube-system started at 2019-08-27 16:48:50 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.438: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 27 18:56:50.438: INFO: ibm-kube-fluentd-hzmm2 from kube-system started at 2019-08-27 16:49:00 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.438: INFO: 	Container fluentd ready: true, restart count 0
Aug 27 18:56:50.438: INFO: ibm-master-proxy-static-10.135.206.197 from kube-system started at <nil> (0 container statuses recorded)
Aug 27 18:56:50.438: INFO: calico-kube-controllers-64fcffdcf4-8nnzm from kube-system started at 2019-08-27 16:48:50 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.438: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 27 18:56:50.438: INFO: ibm-file-plugin-554c9f89b6-pmh82 from kube-system started at 2019-08-27 16:48:50 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.438: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 27 18:56:50.438: INFO: kubernetes-dashboard-5c8c9b7546-sl4zb from kube-system started at 2019-08-27 16:48:50 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.438: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Aug 27 18:56:50.438: INFO: metrics-server-5d78bd76f6-ktqhz from kube-system started at 2019-08-27 16:49:29 +0000 UTC (2 container statuses recorded)
Aug 27 18:56:50.438: INFO: 	Container metrics-server ready: true, restart count 0
Aug 27 18:56:50.438: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Aug 27 18:56:50.438: INFO: sonobuoy-systemd-logs-daemon-set-c96a900eeb9a46c8-zk44g from heptio-sonobuoy started at 2019-08-27 18:12:22 +0000 UTC (2 container statuses recorded)
Aug 27 18:56:50.438: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 27 18:56:50.438: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 27 18:56:50.438: INFO: 
Logging pods the kubelet thinks is on node 10.135.206.222 before test
Aug 27 18:56:50.467: INFO: coredns-9dd7747c7-tp5vn from kube-system started at 2019-08-27 16:59:22 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.467: INFO: 	Container coredns ready: true, restart count 0
Aug 27 18:56:50.467: INFO: sonobuoy-systemd-logs-daemon-set-c96a900eeb9a46c8-qq5nz from heptio-sonobuoy started at 2019-08-27 18:12:22 +0000 UTC (2 container statuses recorded)
Aug 27 18:56:50.467: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 27 18:56:50.467: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 27 18:56:50.467: INFO: ibm-keepalived-watcher-kg5cs from kube-system started at 2019-08-27 16:49:24 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.467: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 27 18:56:50.467: INFO: ibm-kube-fluentd-9rjpc from kube-system started at 2019-08-27 16:49:24 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.467: INFO: 	Container fluentd ready: true, restart count 0
Aug 27 18:56:50.467: INFO: ibm-cloud-provider-ip-158-177-215-165-79887d87b5-f4cv5 from ibm-system started at 2019-08-27 17:00:47 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.467: INFO: 	Container ibm-cloud-provider-ip-158-177-215-165 ready: true, restart count 0
Aug 27 18:56:50.467: INFO: ibm-master-proxy-static-10.135.206.222 from kube-system started at <nil> (0 container statuses recorded)
Aug 27 18:56:50.467: INFO: calico-node-k2dkh from kube-system started at 2019-08-27 16:49:24 +0000 UTC (1 container statuses recorded)
Aug 27 18:56:50.467: INFO: 	Container calico-node ready: true, restart count 0
Aug 27 18:56:50.467: INFO: public-crbliljj8f06leh3daoe40-alb1-545ccf9dcb-pbqzv from kube-system started at 2019-08-27 17:02:44 +0000 UTC (4 container statuses recorded)
Aug 27 18:56:50.467: INFO: 	Container ingress-auth-1 ready: true, restart count 0
Aug 27 18:56:50.467: INFO: 	Container ingress-auth-2 ready: true, restart count 0
Aug 27 18:56:50.467: INFO: 	Container ingress-auth-3 ready: true, restart count 0
Aug 27 18:56:50.467: INFO: 	Container nginx-ingress ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node 10.134.235.118
STEP: verifying the node has the label node 10.135.206.197
STEP: verifying the node has the label node 10.135.206.222
Aug 27 18:56:50.563: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.134.235.118
Aug 27 18:56:50.563: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.134.235.118
Aug 27 18:56:50.563: INFO: Pod sonobuoy-e2e-job-94e2d131d27f4c56 requesting resource cpu=0m on Node 10.134.235.118
Aug 27 18:56:50.563: INFO: Pod sonobuoy-systemd-logs-daemon-set-c96a900eeb9a46c8-qhnrg requesting resource cpu=0m on Node 10.134.235.118
Aug 27 18:56:50.563: INFO: Pod sonobuoy-systemd-logs-daemon-set-c96a900eeb9a46c8-qq5nz requesting resource cpu=0m on Node 10.135.206.222
Aug 27 18:56:50.563: INFO: Pod sonobuoy-systemd-logs-daemon-set-c96a900eeb9a46c8-zk44g requesting resource cpu=0m on Node 10.135.206.197
Aug 27 18:56:50.564: INFO: Pod ibm-cloud-provider-ip-158-177-215-165-79887d87b5-f4cv5 requesting resource cpu=5m on Node 10.135.206.222
Aug 27 18:56:50.564: INFO: Pod ibm-cloud-provider-ip-158-177-215-165-79887d87b5-svwm9 requesting resource cpu=5m on Node 10.135.206.197
Aug 27 18:56:50.564: INFO: Pod calico-kube-controllers-64fcffdcf4-8nnzm requesting resource cpu=10m on Node 10.135.206.197
Aug 27 18:56:50.564: INFO: Pod calico-node-2s8wd requesting resource cpu=250m on Node 10.135.206.197
Aug 27 18:56:50.564: INFO: Pod calico-node-fbflt requesting resource cpu=250m on Node 10.134.235.118
Aug 27 18:56:50.564: INFO: Pod calico-node-k2dkh requesting resource cpu=250m on Node 10.135.206.222
Aug 27 18:56:50.564: INFO: Pod coredns-9dd7747c7-lxd4t requesting resource cpu=100m on Node 10.135.206.197
Aug 27 18:56:50.564: INFO: Pod coredns-9dd7747c7-tp5vn requesting resource cpu=100m on Node 10.135.206.222
Aug 27 18:56:50.564: INFO: Pod coredns-autoscaler-5d4db8dd68-whxxk requesting resource cpu=20m on Node 10.135.206.197
Aug 27 18:56:50.564: INFO: Pod ibm-file-plugin-554c9f89b6-pmh82 requesting resource cpu=50m on Node 10.135.206.197
Aug 27 18:56:50.564: INFO: Pod ibm-keepalived-watcher-29grz requesting resource cpu=5m on Node 10.134.235.118
Aug 27 18:56:50.564: INFO: Pod ibm-keepalived-watcher-972mx requesting resource cpu=5m on Node 10.135.206.197
Aug 27 18:56:50.564: INFO: Pod ibm-keepalived-watcher-kg5cs requesting resource cpu=5m on Node 10.135.206.222
Aug 27 18:56:50.564: INFO: Pod ibm-kube-fluentd-9rjpc requesting resource cpu=25m on Node 10.135.206.222
Aug 27 18:56:50.564: INFO: Pod ibm-kube-fluentd-ddtlg requesting resource cpu=25m on Node 10.134.235.118
Aug 27 18:56:50.564: INFO: Pod ibm-kube-fluentd-hzmm2 requesting resource cpu=25m on Node 10.135.206.197
Aug 27 18:56:50.564: INFO: Pod ibm-master-proxy-static-10.134.235.118 requesting resource cpu=25m on Node 10.134.235.118
Aug 27 18:56:50.564: INFO: Pod ibm-master-proxy-static-10.135.206.197 requesting resource cpu=25m on Node 10.135.206.197
Aug 27 18:56:50.564: INFO: Pod ibm-master-proxy-static-10.135.206.222 requesting resource cpu=25m on Node 10.135.206.222
Aug 27 18:56:50.564: INFO: Pod ibm-storage-watcher-55867f764f-vwpv2 requesting resource cpu=50m on Node 10.135.206.197
Aug 27 18:56:50.564: INFO: Pod kubernetes-dashboard-5c8c9b7546-sl4zb requesting resource cpu=50m on Node 10.135.206.197
Aug 27 18:56:50.564: INFO: Pod metrics-server-5d78bd76f6-ktqhz requesting resource cpu=53m on Node 10.135.206.197
Aug 27 18:56:50.564: INFO: Pod public-crbliljj8f06leh3daoe40-alb1-545ccf9dcb-4sskw requesting resource cpu=0m on Node 10.135.206.197
Aug 27 18:56:50.564: INFO: Pod public-crbliljj8f06leh3daoe40-alb1-545ccf9dcb-pbqzv requesting resource cpu=0m on Node 10.135.206.222
Aug 27 18:56:50.564: INFO: Pod vpn-7754bb6d4-zks4g requesting resource cpu=5m on Node 10.134.235.118
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d7b9c27-c8fc-11e9-a9da-1ec1181dd093.15bedc8b25a81ea0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7146/filler-pod-6d7b9c27-c8fc-11e9-a9da-1ec1181dd093 to 10.134.235.118]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d7b9c27-c8fc-11e9-a9da-1ec1181dd093.15bedc8b65f598ff], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d7b9c27-c8fc-11e9-a9da-1ec1181dd093.15bedc8b6a9be26e], Reason = [Created], Message = [Created container filler-pod-6d7b9c27-c8fc-11e9-a9da-1ec1181dd093]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d7b9c27-c8fc-11e9-a9da-1ec1181dd093.15bedc8b7865fbea], Reason = [Started], Message = [Started container filler-pod-6d7b9c27-c8fc-11e9-a9da-1ec1181dd093]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d7f2050-c8fc-11e9-a9da-1ec1181dd093.15bedc8b26875cbc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7146/filler-pod-6d7f2050-c8fc-11e9-a9da-1ec1181dd093 to 10.135.206.197]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d7f2050-c8fc-11e9-a9da-1ec1181dd093.15bedc8b663ed1f2], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d7f2050-c8fc-11e9-a9da-1ec1181dd093.15bedc8b6b0281dc], Reason = [Created], Message = [Created container filler-pod-6d7f2050-c8fc-11e9-a9da-1ec1181dd093]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d7f2050-c8fc-11e9-a9da-1ec1181dd093.15bedc8b79cf80de], Reason = [Started], Message = [Started container filler-pod-6d7f2050-c8fc-11e9-a9da-1ec1181dd093]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d8134e7-c8fc-11e9-a9da-1ec1181dd093.15bedc8b2745c63f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7146/filler-pod-6d8134e7-c8fc-11e9-a9da-1ec1181dd093 to 10.135.206.222]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d8134e7-c8fc-11e9-a9da-1ec1181dd093.15bedc8b67748241], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d8134e7-c8fc-11e9-a9da-1ec1181dd093.15bedc8b6b577bd4], Reason = [Created], Message = [Created container filler-pod-6d8134e7-c8fc-11e9-a9da-1ec1181dd093]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6d8134e7-c8fc-11e9-a9da-1ec1181dd093.15bedc8b772d540f], Reason = [Started], Message = [Started container filler-pod-6d8134e7-c8fc-11e9-a9da-1ec1181dd093]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15bedc8ba1fadd6e], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.134.235.118
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.135.206.197
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.135.206.222
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:56:53.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7146" for this suite.
Aug 27 18:56:59.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:57:00.147: INFO: namespace sched-pred-7146 deletion completed in 6.312984441s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:9.975 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:57:00.147: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3264
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Aug 27 18:57:00.326: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Aug 27 18:57:00.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 create -f - --namespace=kubectl-3264'
Aug 27 18:57:00.570: INFO: stderr: ""
Aug 27 18:57:00.570: INFO: stdout: "service/redis-slave created\n"
Aug 27 18:57:00.570: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Aug 27 18:57:00.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 create -f - --namespace=kubectl-3264'
Aug 27 18:57:00.888: INFO: stderr: ""
Aug 27 18:57:00.888: INFO: stdout: "service/redis-master created\n"
Aug 27 18:57:00.888: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 27 18:57:00.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 create -f - --namespace=kubectl-3264'
Aug 27 18:57:01.141: INFO: stderr: ""
Aug 27 18:57:01.141: INFO: stdout: "service/frontend created\n"
Aug 27 18:57:01.141: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Aug 27 18:57:01.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 create -f - --namespace=kubectl-3264'
Aug 27 18:57:01.363: INFO: stderr: ""
Aug 27 18:57:01.363: INFO: stdout: "deployment.apps/frontend created\n"
Aug 27 18:57:01.363: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 27 18:57:01.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 create -f - --namespace=kubectl-3264'
Aug 27 18:57:01.662: INFO: stderr: ""
Aug 27 18:57:01.662: INFO: stdout: "deployment.apps/redis-master created\n"
Aug 27 18:57:01.662: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Aug 27 18:57:01.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 create -f - --namespace=kubectl-3264'
Aug 27 18:57:01.972: INFO: stderr: ""
Aug 27 18:57:01.972: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Aug 27 18:57:01.972: INFO: Waiting for all frontend pods to be Running.
Aug 27 18:57:22.024: INFO: Waiting for frontend to serve content.
Aug 27 18:57:22.056: INFO: Trying to add a new entry to the guestbook.
Aug 27 18:57:22.083: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug 27 18:57:22.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 delete --grace-period=0 --force -f - --namespace=kubectl-3264'
Aug 27 18:57:22.279: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 27 18:57:22.279: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Aug 27 18:57:22.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 delete --grace-period=0 --force -f - --namespace=kubectl-3264'
Aug 27 18:57:22.448: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 27 18:57:22.448: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 27 18:57:22.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 delete --grace-period=0 --force -f - --namespace=kubectl-3264'
Aug 27 18:57:22.654: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 27 18:57:22.654: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 27 18:57:22.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 delete --grace-period=0 --force -f - --namespace=kubectl-3264'
Aug 27 18:57:22.793: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 27 18:57:22.793: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 27 18:57:22.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 delete --grace-period=0 --force -f - --namespace=kubectl-3264'
Aug 27 18:57:22.935: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 27 18:57:22.935: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 27 18:57:22.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 delete --grace-period=0 --force -f - --namespace=kubectl-3264'
Aug 27 18:57:23.108: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 27 18:57:23.108: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:57:23.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3264" for this suite.
Aug 27 18:58:07.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:58:07.515: INFO: namespace kubectl-3264 deletion completed in 44.39245232s

• [SLOW TEST:67.368 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:58:07.515: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3228
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 27 18:58:07.791: INFO: Number of nodes with available pods: 0
Aug 27 18:58:07.791: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 18:58:08.813: INFO: Number of nodes with available pods: 0
Aug 27 18:58:08.813: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 18:58:09.808: INFO: Number of nodes with available pods: 3
Aug 27 18:58:09.809: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 27 18:58:09.858: INFO: Number of nodes with available pods: 3
Aug 27 18:58:09.858: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3228, will wait for the garbage collector to delete the pods
Aug 27 18:58:10.968: INFO: Deleting DaemonSet.extensions daemon-set took: 22.526586ms
Aug 27 18:58:11.169: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.351263ms
Aug 27 18:58:22.579: INFO: Number of nodes with available pods: 0
Aug 27 18:58:22.579: INFO: Number of running nodes: 0, number of available pods: 0
Aug 27 18:58:22.587: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3228/daemonsets","resourceVersion":"29718"},"items":null}

Aug 27 18:58:22.597: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3228/pods","resourceVersion":"29718"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:58:22.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3228" for this suite.
Aug 27 18:58:28.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:58:29.072: INFO: namespace daemonsets-3228 deletion completed in 6.438889761s

• [SLOW TEST:21.558 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:58:29.075: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3810
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 27 18:58:29.278: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3810,SelfLink:/api/v1/namespaces/watch-3810/configmaps/e2e-watch-test-configmap-a,UID:a84cacbe-c8fc-11e9-9af8-1e15abefec17,ResourceVersion:29773,Generation:0,CreationTimestamp:2019-08-27 18:58:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 27 18:58:29.278: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3810,SelfLink:/api/v1/namespaces/watch-3810/configmaps/e2e-watch-test-configmap-a,UID:a84cacbe-c8fc-11e9-9af8-1e15abefec17,ResourceVersion:29773,Generation:0,CreationTimestamp:2019-08-27 18:58:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 27 18:58:39.301: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3810,SelfLink:/api/v1/namespaces/watch-3810/configmaps/e2e-watch-test-configmap-a,UID:a84cacbe-c8fc-11e9-9af8-1e15abefec17,ResourceVersion:29792,Generation:0,CreationTimestamp:2019-08-27 18:58:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 27 18:58:39.301: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3810,SelfLink:/api/v1/namespaces/watch-3810/configmaps/e2e-watch-test-configmap-a,UID:a84cacbe-c8fc-11e9-9af8-1e15abefec17,ResourceVersion:29792,Generation:0,CreationTimestamp:2019-08-27 18:58:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 27 18:58:49.326: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3810,SelfLink:/api/v1/namespaces/watch-3810/configmaps/e2e-watch-test-configmap-a,UID:a84cacbe-c8fc-11e9-9af8-1e15abefec17,ResourceVersion:29810,Generation:0,CreationTimestamp:2019-08-27 18:58:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 27 18:58:49.326: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3810,SelfLink:/api/v1/namespaces/watch-3810/configmaps/e2e-watch-test-configmap-a,UID:a84cacbe-c8fc-11e9-9af8-1e15abefec17,ResourceVersion:29810,Generation:0,CreationTimestamp:2019-08-27 18:58:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 27 18:58:59.350: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3810,SelfLink:/api/v1/namespaces/watch-3810/configmaps/e2e-watch-test-configmap-a,UID:a84cacbe-c8fc-11e9-9af8-1e15abefec17,ResourceVersion:29827,Generation:0,CreationTimestamp:2019-08-27 18:58:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 27 18:58:59.350: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3810,SelfLink:/api/v1/namespaces/watch-3810/configmaps/e2e-watch-test-configmap-a,UID:a84cacbe-c8fc-11e9-9af8-1e15abefec17,ResourceVersion:29827,Generation:0,CreationTimestamp:2019-08-27 18:58:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 27 18:59:09.365: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3810,SelfLink:/api/v1/namespaces/watch-3810/configmaps/e2e-watch-test-configmap-b,UID:c030ec10-c8fc-11e9-9af8-1e15abefec17,ResourceVersion:29844,Generation:0,CreationTimestamp:2019-08-27 18:59:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 27 18:59:09.365: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3810,SelfLink:/api/v1/namespaces/watch-3810/configmaps/e2e-watch-test-configmap-b,UID:c030ec10-c8fc-11e9-9af8-1e15abefec17,ResourceVersion:29844,Generation:0,CreationTimestamp:2019-08-27 18:59:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 27 18:59:19.394: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3810,SelfLink:/api/v1/namespaces/watch-3810/configmaps/e2e-watch-test-configmap-b,UID:c030ec10-c8fc-11e9-9af8-1e15abefec17,ResourceVersion:29861,Generation:0,CreationTimestamp:2019-08-27 18:59:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 27 18:59:19.394: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3810,SelfLink:/api/v1/namespaces/watch-3810/configmaps/e2e-watch-test-configmap-b,UID:c030ec10-c8fc-11e9-9af8-1e15abefec17,ResourceVersion:29861,Generation:0,CreationTimestamp:2019-08-27 18:59:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:59:29.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3810" for this suite.
Aug 27 18:59:35.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:59:35.717: INFO: namespace watch-3810 deletion completed in 6.311849445s

• [SLOW TEST:66.642 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:59:35.719: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9435
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-d0093a15-c8fc-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume configMaps
Aug 27 18:59:35.937: INFO: Waiting up to 5m0s for pod "pod-configmaps-d00ac248-c8fc-11e9-a9da-1ec1181dd093" in namespace "configmap-9435" to be "success or failure"
Aug 27 18:59:35.952: INFO: Pod "pod-configmaps-d00ac248-c8fc-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 15.03642ms
Aug 27 18:59:37.961: INFO: Pod "pod-configmaps-d00ac248-c8fc-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023968653s
STEP: Saw pod success
Aug 27 18:59:37.961: INFO: Pod "pod-configmaps-d00ac248-c8fc-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:59:37.971: INFO: Trying to get logs from node 10.135.206.197 pod pod-configmaps-d00ac248-c8fc-11e9-a9da-1ec1181dd093 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 18:59:38.016: INFO: Waiting for pod pod-configmaps-d00ac248-c8fc-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:59:38.024: INFO: Pod pod-configmaps-d00ac248-c8fc-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:59:38.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9435" for this suite.
Aug 27 18:59:44.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:59:44.447: INFO: namespace configmap-9435 deletion completed in 6.4137663s

• [SLOW TEST:8.728 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:59:44.447: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8006
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Aug 27 18:59:44.641: INFO: Waiting up to 5m0s for pod "client-containers-d53a9620-c8fc-11e9-a9da-1ec1181dd093" in namespace "containers-8006" to be "success or failure"
Aug 27 18:59:44.654: INFO: Pod "client-containers-d53a9620-c8fc-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 13.402337ms
Aug 27 18:59:46.668: INFO: Pod "client-containers-d53a9620-c8fc-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027163397s
Aug 27 18:59:48.677: INFO: Pod "client-containers-d53a9620-c8fc-11e9-a9da-1ec1181dd093": Phase="Running", Reason="", readiness=true. Elapsed: 4.036268898s
Aug 27 18:59:50.687: INFO: Pod "client-containers-d53a9620-c8fc-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046039406s
STEP: Saw pod success
Aug 27 18:59:50.687: INFO: Pod "client-containers-d53a9620-c8fc-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 18:59:50.695: INFO: Trying to get logs from node 10.134.235.118 pod client-containers-d53a9620-c8fc-11e9-a9da-1ec1181dd093 container test-container: <nil>
STEP: delete the pod
Aug 27 18:59:50.739: INFO: Waiting for pod client-containers-d53a9620-c8fc-11e9-a9da-1ec1181dd093 to disappear
Aug 27 18:59:50.750: INFO: Pod client-containers-d53a9620-c8fc-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 18:59:50.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8006" for this suite.
Aug 27 18:59:56.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 18:59:57.099: INFO: namespace containers-8006 deletion completed in 6.340335816s

• [SLOW TEST:12.652 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 18:59:57.099: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3766
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-dcc9e7ff-c8fc-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume configMaps
Aug 27 18:59:57.332: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dccb68cb-c8fc-11e9-a9da-1ec1181dd093" in namespace "projected-3766" to be "success or failure"
Aug 27 18:59:57.346: INFO: Pod "pod-projected-configmaps-dccb68cb-c8fc-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 13.416835ms
Aug 27 18:59:59.358: INFO: Pod "pod-projected-configmaps-dccb68cb-c8fc-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025480527s
Aug 27 19:00:01.367: INFO: Pod "pod-projected-configmaps-dccb68cb-c8fc-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035247545s
STEP: Saw pod success
Aug 27 19:00:01.367: INFO: Pod "pod-projected-configmaps-dccb68cb-c8fc-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:00:01.376: INFO: Trying to get logs from node 10.135.206.222 pod pod-projected-configmaps-dccb68cb-c8fc-11e9-a9da-1ec1181dd093 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 19:00:01.428: INFO: Waiting for pod pod-projected-configmaps-dccb68cb-c8fc-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:00:01.436: INFO: Pod pod-projected-configmaps-dccb68cb-c8fc-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:00:01.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3766" for this suite.
Aug 27 19:00:07.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:00:07.887: INFO: namespace projected-3766 deletion completed in 6.439113017s

• [SLOW TEST:10.788 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:00:07.888: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7251
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Aug 27 19:00:08.146: INFO: Waiting up to 5m0s for pod "downward-api-e33cf779-c8fc-11e9-a9da-1ec1181dd093" in namespace "downward-api-7251" to be "success or failure"
Aug 27 19:00:08.161: INFO: Pod "downward-api-e33cf779-c8fc-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 15.26179ms
Aug 27 19:00:10.171: INFO: Pod "downward-api-e33cf779-c8fc-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024842325s
STEP: Saw pod success
Aug 27 19:00:10.171: INFO: Pod "downward-api-e33cf779-c8fc-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:00:10.198: INFO: Trying to get logs from node 10.134.235.118 pod downward-api-e33cf779-c8fc-11e9-a9da-1ec1181dd093 container dapi-container: <nil>
STEP: delete the pod
Aug 27 19:00:10.251: INFO: Waiting for pod downward-api-e33cf779-c8fc-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:00:10.259: INFO: Pod downward-api-e33cf779-c8fc-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:00:10.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7251" for this suite.
Aug 27 19:00:16.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:00:16.583: INFO: namespace downward-api-7251 deletion completed in 6.314980018s

• [SLOW TEST:8.695 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:00:16.587: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1204
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-e8651a75-c8fc-11e9-a9da-1ec1181dd093
STEP: Creating configMap with name cm-test-opt-upd-e8651ad7-c8fc-11e9-a9da-1ec1181dd093
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e8651a75-c8fc-11e9-a9da-1ec1181dd093
STEP: Updating configmap cm-test-opt-upd-e8651ad7-c8fc-11e9-a9da-1ec1181dd093
STEP: Creating configMap with name cm-test-opt-create-e8651b05-c8fc-11e9-a9da-1ec1181dd093
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:01:38.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1204" for this suite.
Aug 27 19:02:02.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:02:02.372: INFO: namespace projected-1204 deletion completed in 24.346694407s

• [SLOW TEST:105.785 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:02:02.373: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-7706
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7706
I0827 19:02:02.557656      18 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7706, replica count: 1
I0827 19:02:03.608140      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0827 19:02:04.608354      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0827 19:02:05.608648      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 27 19:02:05.729: INFO: Created: latency-svc-zmzqv
Aug 27 19:02:05.739: INFO: Got endpoints: latency-svc-zmzqv [30.167369ms]
Aug 27 19:02:05.764: INFO: Created: latency-svc-frt4c
Aug 27 19:02:05.773: INFO: Got endpoints: latency-svc-frt4c [33.52862ms]
Aug 27 19:02:05.782: INFO: Created: latency-svc-kxwhl
Aug 27 19:02:05.792: INFO: Got endpoints: latency-svc-kxwhl [53.651322ms]
Aug 27 19:02:05.798: INFO: Created: latency-svc-n52x8
Aug 27 19:02:05.810: INFO: Got endpoints: latency-svc-n52x8 [70.529868ms]
Aug 27 19:02:05.817: INFO: Created: latency-svc-54qv8
Aug 27 19:02:05.828: INFO: Got endpoints: latency-svc-54qv8 [88.509007ms]
Aug 27 19:02:05.830: INFO: Created: latency-svc-dr25m
Aug 27 19:02:05.839: INFO: Got endpoints: latency-svc-dr25m [99.472828ms]
Aug 27 19:02:05.844: INFO: Created: latency-svc-rk72s
Aug 27 19:02:05.853: INFO: Got endpoints: latency-svc-rk72s [113.745879ms]
Aug 27 19:02:05.857: INFO: Created: latency-svc-xhb27
Aug 27 19:02:05.864: INFO: Got endpoints: latency-svc-xhb27 [125.301892ms]
Aug 27 19:02:05.869: INFO: Created: latency-svc-stv8n
Aug 27 19:02:05.878: INFO: Got endpoints: latency-svc-stv8n [139.055595ms]
Aug 27 19:02:05.882: INFO: Created: latency-svc-rk9m6
Aug 27 19:02:05.890: INFO: Got endpoints: latency-svc-rk9m6 [150.874012ms]
Aug 27 19:02:05.896: INFO: Created: latency-svc-pmb6v
Aug 27 19:02:05.908: INFO: Got endpoints: latency-svc-pmb6v [168.327432ms]
Aug 27 19:02:05.910: INFO: Created: latency-svc-jj4qh
Aug 27 19:02:05.919: INFO: Got endpoints: latency-svc-jj4qh [179.564713ms]
Aug 27 19:02:05.925: INFO: Created: latency-svc-vtss4
Aug 27 19:02:05.935: INFO: Got endpoints: latency-svc-vtss4 [195.458102ms]
Aug 27 19:02:05.938: INFO: Created: latency-svc-bwt5p
Aug 27 19:02:05.947: INFO: Got endpoints: latency-svc-bwt5p [207.731334ms]
Aug 27 19:02:05.953: INFO: Created: latency-svc-qmr6n
Aug 27 19:02:05.965: INFO: Got endpoints: latency-svc-qmr6n [226.32835ms]
Aug 27 19:02:05.969: INFO: Created: latency-svc-fn7kq
Aug 27 19:02:05.977: INFO: Got endpoints: latency-svc-fn7kq [237.680508ms]
Aug 27 19:02:05.988: INFO: Created: latency-svc-tc5qr
Aug 27 19:02:05.997: INFO: Got endpoints: latency-svc-tc5qr [224.6ms]
Aug 27 19:02:06.001: INFO: Created: latency-svc-qhkk6
Aug 27 19:02:06.013: INFO: Got endpoints: latency-svc-qhkk6 [220.222494ms]
Aug 27 19:02:06.020: INFO: Created: latency-svc-f7bxc
Aug 27 19:02:06.028: INFO: Got endpoints: latency-svc-f7bxc [218.591047ms]
Aug 27 19:02:06.031: INFO: Created: latency-svc-45g99
Aug 27 19:02:06.041: INFO: Got endpoints: latency-svc-45g99 [213.865138ms]
Aug 27 19:02:06.047: INFO: Created: latency-svc-c4cl4
Aug 27 19:02:06.054: INFO: Got endpoints: latency-svc-c4cl4 [215.802065ms]
Aug 27 19:02:06.058: INFO: Created: latency-svc-t8tp6
Aug 27 19:02:06.067: INFO: Got endpoints: latency-svc-t8tp6 [213.747029ms]
Aug 27 19:02:06.072: INFO: Created: latency-svc-ghkwt
Aug 27 19:02:06.083: INFO: Got endpoints: latency-svc-ghkwt [219.043686ms]
Aug 27 19:02:06.086: INFO: Created: latency-svc-nf9xc
Aug 27 19:02:06.094: INFO: Got endpoints: latency-svc-nf9xc [215.771462ms]
Aug 27 19:02:06.100: INFO: Created: latency-svc-nr9b2
Aug 27 19:02:06.119: INFO: Got endpoints: latency-svc-nr9b2 [229.355166ms]
Aug 27 19:02:06.120: INFO: Created: latency-svc-nr7v9
Aug 27 19:02:06.121: INFO: Got endpoints: latency-svc-nr7v9 [212.90716ms]
Aug 27 19:02:06.125: INFO: Created: latency-svc-b8nz6
Aug 27 19:02:06.133: INFO: Got endpoints: latency-svc-b8nz6 [214.526584ms]
Aug 27 19:02:06.138: INFO: Created: latency-svc-mxkq2
Aug 27 19:02:06.146: INFO: Got endpoints: latency-svc-mxkq2 [211.034265ms]
Aug 27 19:02:06.153: INFO: Created: latency-svc-6mgpl
Aug 27 19:02:06.166: INFO: Got endpoints: latency-svc-6mgpl [219.403135ms]
Aug 27 19:02:06.169: INFO: Created: latency-svc-qlfkv
Aug 27 19:02:06.177: INFO: Got endpoints: latency-svc-qlfkv [211.657773ms]
Aug 27 19:02:06.181: INFO: Created: latency-svc-r6cqv
Aug 27 19:02:06.191: INFO: Got endpoints: latency-svc-r6cqv [214.112519ms]
Aug 27 19:02:06.195: INFO: Created: latency-svc-dd4nj
Aug 27 19:02:06.205: INFO: Got endpoints: latency-svc-dd4nj [208.197468ms]
Aug 27 19:02:06.208: INFO: Created: latency-svc-dcnc9
Aug 27 19:02:06.218: INFO: Got endpoints: latency-svc-dcnc9 [204.904211ms]
Aug 27 19:02:06.222: INFO: Created: latency-svc-tf4pc
Aug 27 19:02:06.231: INFO: Got endpoints: latency-svc-tf4pc [202.624845ms]
Aug 27 19:02:06.236: INFO: Created: latency-svc-ltpxj
Aug 27 19:02:06.245: INFO: Got endpoints: latency-svc-ltpxj [203.818102ms]
Aug 27 19:02:06.250: INFO: Created: latency-svc-xzlc2
Aug 27 19:02:06.260: INFO: Got endpoints: latency-svc-xzlc2 [205.185943ms]
Aug 27 19:02:06.262: INFO: Created: latency-svc-wgsn9
Aug 27 19:02:06.272: INFO: Got endpoints: latency-svc-wgsn9 [205.066024ms]
Aug 27 19:02:06.277: INFO: Created: latency-svc-mlcp9
Aug 27 19:02:06.285: INFO: Got endpoints: latency-svc-mlcp9 [201.911341ms]
Aug 27 19:02:06.289: INFO: Created: latency-svc-zh5cp
Aug 27 19:02:06.297: INFO: Got endpoints: latency-svc-zh5cp [203.136723ms]
Aug 27 19:02:06.301: INFO: Created: latency-svc-s2sv6
Aug 27 19:02:06.309: INFO: Got endpoints: latency-svc-s2sv6 [189.92438ms]
Aug 27 19:02:06.314: INFO: Created: latency-svc-jcf95
Aug 27 19:02:06.322: INFO: Got endpoints: latency-svc-jcf95 [201.290127ms]
Aug 27 19:02:06.327: INFO: Created: latency-svc-tjgdk
Aug 27 19:02:06.337: INFO: Got endpoints: latency-svc-tjgdk [203.381202ms]
Aug 27 19:02:06.338: INFO: Created: latency-svc-s47sk
Aug 27 19:02:06.346: INFO: Got endpoints: latency-svc-s47sk [199.714773ms]
Aug 27 19:02:06.353: INFO: Created: latency-svc-4m6xc
Aug 27 19:02:06.361: INFO: Got endpoints: latency-svc-4m6xc [194.648911ms]
Aug 27 19:02:06.371: INFO: Created: latency-svc-tqg8h
Aug 27 19:02:06.378: INFO: Got endpoints: latency-svc-tqg8h [200.729539ms]
Aug 27 19:02:06.388: INFO: Created: latency-svc-lxgwb
Aug 27 19:02:06.398: INFO: Got endpoints: latency-svc-lxgwb [206.732215ms]
Aug 27 19:02:06.400: INFO: Created: latency-svc-wmpxr
Aug 27 19:02:06.409: INFO: Got endpoints: latency-svc-wmpxr [203.605781ms]
Aug 27 19:02:06.414: INFO: Created: latency-svc-kdgd9
Aug 27 19:02:06.423: INFO: Got endpoints: latency-svc-kdgd9 [204.806228ms]
Aug 27 19:02:06.427: INFO: Created: latency-svc-hcvrr
Aug 27 19:02:06.438: INFO: Got endpoints: latency-svc-hcvrr [206.86639ms]
Aug 27 19:02:06.439: INFO: Created: latency-svc-7g7kg
Aug 27 19:02:06.450: INFO: Got endpoints: latency-svc-7g7kg [204.043075ms]
Aug 27 19:02:06.468: INFO: Created: latency-svc-gg54z
Aug 27 19:02:06.468: INFO: Got endpoints: latency-svc-gg54z [208.817433ms]
Aug 27 19:02:06.469: INFO: Created: latency-svc-xcr5r
Aug 27 19:02:06.479: INFO: Got endpoints: latency-svc-xcr5r [207.117522ms]
Aug 27 19:02:06.483: INFO: Created: latency-svc-tc5rz
Aug 27 19:02:06.494: INFO: Got endpoints: latency-svc-tc5rz [208.748667ms]
Aug 27 19:02:06.502: INFO: Created: latency-svc-hhnsx
Aug 27 19:02:06.508: INFO: Got endpoints: latency-svc-hhnsx [211.19714ms]
Aug 27 19:02:06.513: INFO: Created: latency-svc-dtfkr
Aug 27 19:02:06.521: INFO: Got endpoints: latency-svc-dtfkr [212.03174ms]
Aug 27 19:02:06.525: INFO: Created: latency-svc-swvzc
Aug 27 19:02:06.534: INFO: Got endpoints: latency-svc-swvzc [212.078545ms]
Aug 27 19:02:06.543: INFO: Created: latency-svc-7s2mq
Aug 27 19:02:06.551: INFO: Got endpoints: latency-svc-7s2mq [214.347476ms]
Aug 27 19:02:06.556: INFO: Created: latency-svc-s6mtr
Aug 27 19:02:06.564: INFO: Got endpoints: latency-svc-s6mtr [218.4688ms]
Aug 27 19:02:06.569: INFO: Created: latency-svc-88h5j
Aug 27 19:02:06.577: INFO: Got endpoints: latency-svc-88h5j [216.018588ms]
Aug 27 19:02:06.581: INFO: Created: latency-svc-njzq9
Aug 27 19:02:06.590: INFO: Got endpoints: latency-svc-njzq9 [211.42999ms]
Aug 27 19:02:06.594: INFO: Created: latency-svc-zsqrt
Aug 27 19:02:06.605: INFO: Got endpoints: latency-svc-zsqrt [206.42474ms]
Aug 27 19:02:06.609: INFO: Created: latency-svc-ttc77
Aug 27 19:02:06.618: INFO: Got endpoints: latency-svc-ttc77 [208.511219ms]
Aug 27 19:02:06.623: INFO: Created: latency-svc-tsnfr
Aug 27 19:02:06.633: INFO: Got endpoints: latency-svc-tsnfr [210.06835ms]
Aug 27 19:02:06.635: INFO: Created: latency-svc-2kh6c
Aug 27 19:02:06.647: INFO: Got endpoints: latency-svc-2kh6c [208.438977ms]
Aug 27 19:02:06.648: INFO: Created: latency-svc-swbcv
Aug 27 19:02:06.656: INFO: Got endpoints: latency-svc-swbcv [206.207133ms]
Aug 27 19:02:06.661: INFO: Created: latency-svc-4qc2v
Aug 27 19:02:06.670: INFO: Got endpoints: latency-svc-4qc2v [201.770259ms]
Aug 27 19:02:06.677: INFO: Created: latency-svc-gfbpx
Aug 27 19:02:06.685: INFO: Got endpoints: latency-svc-gfbpx [206.454206ms]
Aug 27 19:02:06.690: INFO: Created: latency-svc-z924d
Aug 27 19:02:06.699: INFO: Got endpoints: latency-svc-z924d [204.473369ms]
Aug 27 19:02:06.704: INFO: Created: latency-svc-46vzq
Aug 27 19:02:06.712: INFO: Got endpoints: latency-svc-46vzq [203.841814ms]
Aug 27 19:02:06.716: INFO: Created: latency-svc-26sxq
Aug 27 19:02:06.725: INFO: Got endpoints: latency-svc-26sxq [203.746822ms]
Aug 27 19:02:06.728: INFO: Created: latency-svc-64km2
Aug 27 19:02:06.739: INFO: Got endpoints: latency-svc-64km2 [204.51474ms]
Aug 27 19:02:06.742: INFO: Created: latency-svc-6t55c
Aug 27 19:02:06.750: INFO: Got endpoints: latency-svc-6t55c [198.540756ms]
Aug 27 19:02:06.755: INFO: Created: latency-svc-b2qf2
Aug 27 19:02:06.766: INFO: Got endpoints: latency-svc-b2qf2 [201.155814ms]
Aug 27 19:02:06.767: INFO: Created: latency-svc-6v2cl
Aug 27 19:02:06.776: INFO: Got endpoints: latency-svc-6v2cl [198.522737ms]
Aug 27 19:02:06.779: INFO: Created: latency-svc-2btqw
Aug 27 19:02:06.788: INFO: Got endpoints: latency-svc-2btqw [197.911139ms]
Aug 27 19:02:06.794: INFO: Created: latency-svc-wmd6m
Aug 27 19:02:06.803: INFO: Got endpoints: latency-svc-wmd6m [198.137864ms]
Aug 27 19:02:06.808: INFO: Created: latency-svc-hh95v
Aug 27 19:02:06.819: INFO: Created: latency-svc-p88vp
Aug 27 19:02:06.820: INFO: Got endpoints: latency-svc-hh95v [202.460489ms]
Aug 27 19:02:06.830: INFO: Got endpoints: latency-svc-p88vp [196.937588ms]
Aug 27 19:02:06.833: INFO: Created: latency-svc-62fxf
Aug 27 19:02:06.842: INFO: Got endpoints: latency-svc-62fxf [194.844515ms]
Aug 27 19:02:06.846: INFO: Created: latency-svc-w4r74
Aug 27 19:02:06.855: INFO: Got endpoints: latency-svc-w4r74 [198.591027ms]
Aug 27 19:02:06.859: INFO: Created: latency-svc-prxhv
Aug 27 19:02:06.867: INFO: Got endpoints: latency-svc-prxhv [196.066965ms]
Aug 27 19:02:06.876: INFO: Created: latency-svc-7mndv
Aug 27 19:02:06.884: INFO: Got endpoints: latency-svc-7mndv [198.638462ms]
Aug 27 19:02:06.888: INFO: Created: latency-svc-glkn7
Aug 27 19:02:06.897: INFO: Got endpoints: latency-svc-glkn7 [198.258563ms]
Aug 27 19:02:06.901: INFO: Created: latency-svc-l4vw2
Aug 27 19:02:06.910: INFO: Got endpoints: latency-svc-l4vw2 [198.256375ms]
Aug 27 19:02:06.914: INFO: Created: latency-svc-k4pfn
Aug 27 19:02:06.922: INFO: Got endpoints: latency-svc-k4pfn [197.073983ms]
Aug 27 19:02:06.926: INFO: Created: latency-svc-fxhnq
Aug 27 19:02:06.935: INFO: Got endpoints: latency-svc-fxhnq [196.271782ms]
Aug 27 19:02:06.938: INFO: Created: latency-svc-fzh42
Aug 27 19:02:06.947: INFO: Got endpoints: latency-svc-fzh42 [197.399211ms]
Aug 27 19:02:06.955: INFO: Created: latency-svc-p87b2
Aug 27 19:02:06.965: INFO: Got endpoints: latency-svc-p87b2 [199.633905ms]
Aug 27 19:02:06.969: INFO: Created: latency-svc-qxdzq
Aug 27 19:02:06.979: INFO: Got endpoints: latency-svc-qxdzq [202.878932ms]
Aug 27 19:02:06.982: INFO: Created: latency-svc-7tzvh
Aug 27 19:02:06.993: INFO: Got endpoints: latency-svc-7tzvh [204.973851ms]
Aug 27 19:02:06.999: INFO: Created: latency-svc-lswpv
Aug 27 19:02:07.009: INFO: Got endpoints: latency-svc-lswpv [206.352375ms]
Aug 27 19:02:07.012: INFO: Created: latency-svc-tj8z9
Aug 27 19:02:07.025: INFO: Created: latency-svc-znk9r
Aug 27 19:02:07.025: INFO: Got endpoints: latency-svc-tj8z9 [204.885331ms]
Aug 27 19:02:07.034: INFO: Got endpoints: latency-svc-znk9r [203.959316ms]
Aug 27 19:02:07.037: INFO: Created: latency-svc-hk2lr
Aug 27 19:02:07.052: INFO: Got endpoints: latency-svc-hk2lr [210.561906ms]
Aug 27 19:02:07.057: INFO: Created: latency-svc-f9t9p
Aug 27 19:02:07.065: INFO: Got endpoints: latency-svc-f9t9p [210.159789ms]
Aug 27 19:02:07.068: INFO: Created: latency-svc-xwwjg
Aug 27 19:02:07.077: INFO: Got endpoints: latency-svc-xwwjg [210.724289ms]
Aug 27 19:02:07.081: INFO: Created: latency-svc-dmzqs
Aug 27 19:02:07.089: INFO: Got endpoints: latency-svc-dmzqs [204.816092ms]
Aug 27 19:02:07.093: INFO: Created: latency-svc-t84nq
Aug 27 19:02:07.101: INFO: Got endpoints: latency-svc-t84nq [203.558177ms]
Aug 27 19:02:07.106: INFO: Created: latency-svc-2lccx
Aug 27 19:02:07.115: INFO: Got endpoints: latency-svc-2lccx [204.73359ms]
Aug 27 19:02:07.121: INFO: Created: latency-svc-2fhhh
Aug 27 19:02:07.130: INFO: Got endpoints: latency-svc-2fhhh [207.203732ms]
Aug 27 19:02:07.134: INFO: Created: latency-svc-fwzzx
Aug 27 19:02:07.142: INFO: Got endpoints: latency-svc-fwzzx [206.811732ms]
Aug 27 19:02:07.145: INFO: Created: latency-svc-kfcj9
Aug 27 19:02:07.152: INFO: Got endpoints: latency-svc-kfcj9 [204.888077ms]
Aug 27 19:02:07.157: INFO: Created: latency-svc-n28z7
Aug 27 19:02:07.169: INFO: Got endpoints: latency-svc-n28z7 [203.151551ms]
Aug 27 19:02:07.175: INFO: Created: latency-svc-dv2vl
Aug 27 19:02:07.184: INFO: Got endpoints: latency-svc-dv2vl [205.01402ms]
Aug 27 19:02:07.187: INFO: Created: latency-svc-vgl2c
Aug 27 19:02:07.196: INFO: Got endpoints: latency-svc-vgl2c [203.272667ms]
Aug 27 19:02:07.200: INFO: Created: latency-svc-xvf42
Aug 27 19:02:07.208: INFO: Got endpoints: latency-svc-xvf42 [199.140552ms]
Aug 27 19:02:07.214: INFO: Created: latency-svc-2pcfh
Aug 27 19:02:07.222: INFO: Got endpoints: latency-svc-2pcfh [196.296994ms]
Aug 27 19:02:07.227: INFO: Created: latency-svc-4wp2m
Aug 27 19:02:07.235: INFO: Got endpoints: latency-svc-4wp2m [201.480968ms]
Aug 27 19:02:07.241: INFO: Created: latency-svc-qzhns
Aug 27 19:02:07.250: INFO: Got endpoints: latency-svc-qzhns [197.194608ms]
Aug 27 19:02:07.253: INFO: Created: latency-svc-rnrzs
Aug 27 19:02:07.263: INFO: Got endpoints: latency-svc-rnrzs [197.541675ms]
Aug 27 19:02:07.266: INFO: Created: latency-svc-v8ghn
Aug 27 19:02:07.274: INFO: Got endpoints: latency-svc-v8ghn [197.015649ms]
Aug 27 19:02:07.278: INFO: Created: latency-svc-lmrql
Aug 27 19:02:07.290: INFO: Created: latency-svc-vn2vg
Aug 27 19:02:07.292: INFO: Got endpoints: latency-svc-lmrql [202.586411ms]
Aug 27 19:02:07.299: INFO: Got endpoints: latency-svc-vn2vg [198.492646ms]
Aug 27 19:02:07.305: INFO: Created: latency-svc-fkf87
Aug 27 19:02:07.318: INFO: Got endpoints: latency-svc-fkf87 [202.489466ms]
Aug 27 19:02:07.319: INFO: Created: latency-svc-f58sb
Aug 27 19:02:07.330: INFO: Created: latency-svc-5jcd6
Aug 27 19:02:07.330: INFO: Got endpoints: latency-svc-f58sb [200.268049ms]
Aug 27 19:02:07.339: INFO: Got endpoints: latency-svc-5jcd6 [196.806353ms]
Aug 27 19:02:07.343: INFO: Created: latency-svc-5wppj
Aug 27 19:02:07.350: INFO: Got endpoints: latency-svc-5wppj [198.242885ms]
Aug 27 19:02:07.355: INFO: Created: latency-svc-xg6lh
Aug 27 19:02:07.363: INFO: Got endpoints: latency-svc-xg6lh [194.390326ms]
Aug 27 19:02:07.368: INFO: Created: latency-svc-rwtqn
Aug 27 19:02:07.376: INFO: Got endpoints: latency-svc-rwtqn [192.468458ms]
Aug 27 19:02:07.380: INFO: Created: latency-svc-bcjwp
Aug 27 19:02:07.389: INFO: Got endpoints: latency-svc-bcjwp [193.182364ms]
Aug 27 19:02:07.394: INFO: Created: latency-svc-bvrwq
Aug 27 19:02:07.405: INFO: Got endpoints: latency-svc-bvrwq [196.276817ms]
Aug 27 19:02:07.409: INFO: Created: latency-svc-sgpns
Aug 27 19:02:07.417: INFO: Got endpoints: latency-svc-sgpns [195.456383ms]
Aug 27 19:02:07.422: INFO: Created: latency-svc-plkdk
Aug 27 19:02:07.432: INFO: Got endpoints: latency-svc-plkdk [196.599396ms]
Aug 27 19:02:07.434: INFO: Created: latency-svc-z4bz6
Aug 27 19:02:07.442: INFO: Got endpoints: latency-svc-z4bz6 [192.872834ms]
Aug 27 19:02:07.447: INFO: Created: latency-svc-jmkcj
Aug 27 19:02:07.456: INFO: Got endpoints: latency-svc-jmkcj [192.99434ms]
Aug 27 19:02:07.461: INFO: Created: latency-svc-mffbk
Aug 27 19:02:07.476: INFO: Created: latency-svc-t75dv
Aug 27 19:02:07.476: INFO: Got endpoints: latency-svc-mffbk [201.677206ms]
Aug 27 19:02:07.482: INFO: Got endpoints: latency-svc-t75dv [190.498041ms]
Aug 27 19:02:07.487: INFO: Created: latency-svc-xcj9v
Aug 27 19:02:07.501: INFO: Got endpoints: latency-svc-xcj9v [201.757737ms]
Aug 27 19:02:07.509: INFO: Created: latency-svc-g6pth
Aug 27 19:02:07.513: INFO: Got endpoints: latency-svc-g6pth [194.805225ms]
Aug 27 19:02:07.514: INFO: Created: latency-svc-56rt8
Aug 27 19:02:07.522: INFO: Got endpoints: latency-svc-56rt8 [191.576559ms]
Aug 27 19:02:07.527: INFO: Created: latency-svc-rgm9r
Aug 27 19:02:07.537: INFO: Got endpoints: latency-svc-rgm9r [197.61007ms]
Aug 27 19:02:07.543: INFO: Created: latency-svc-pd8lr
Aug 27 19:02:07.549: INFO: Got endpoints: latency-svc-pd8lr [198.655781ms]
Aug 27 19:02:07.552: INFO: Created: latency-svc-dl4h6
Aug 27 19:02:07.561: INFO: Got endpoints: latency-svc-dl4h6 [197.551372ms]
Aug 27 19:02:07.565: INFO: Created: latency-svc-xqh75
Aug 27 19:02:07.575: INFO: Got endpoints: latency-svc-xqh75 [198.39522ms]
Aug 27 19:02:07.578: INFO: Created: latency-svc-nxn2s
Aug 27 19:02:07.586: INFO: Got endpoints: latency-svc-nxn2s [196.12128ms]
Aug 27 19:02:07.592: INFO: Created: latency-svc-74kwm
Aug 27 19:02:07.601: INFO: Got endpoints: latency-svc-74kwm [196.242986ms]
Aug 27 19:02:07.603: INFO: Created: latency-svc-qrdl8
Aug 27 19:02:07.614: INFO: Got endpoints: latency-svc-qrdl8 [196.918687ms]
Aug 27 19:02:07.618: INFO: Created: latency-svc-dj4lr
Aug 27 19:02:07.626: INFO: Got endpoints: latency-svc-dj4lr [193.622908ms]
Aug 27 19:02:07.630: INFO: Created: latency-svc-5nvhj
Aug 27 19:02:07.639: INFO: Got endpoints: latency-svc-5nvhj [196.595419ms]
Aug 27 19:02:07.650: INFO: Created: latency-svc-2fvl7
Aug 27 19:02:07.658: INFO: Got endpoints: latency-svc-2fvl7 [202.519236ms]
Aug 27 19:02:07.659: INFO: Created: latency-svc-wsflk
Aug 27 19:02:07.668: INFO: Got endpoints: latency-svc-wsflk [191.774932ms]
Aug 27 19:02:07.674: INFO: Created: latency-svc-rb9xt
Aug 27 19:02:07.682: INFO: Got endpoints: latency-svc-rb9xt [199.98013ms]
Aug 27 19:02:07.687: INFO: Created: latency-svc-6dq2f
Aug 27 19:02:07.695: INFO: Got endpoints: latency-svc-6dq2f [194.078363ms]
Aug 27 19:02:07.699: INFO: Created: latency-svc-pcc7g
Aug 27 19:02:07.708: INFO: Got endpoints: latency-svc-pcc7g [195.405529ms]
Aug 27 19:02:07.717: INFO: Created: latency-svc-kncdb
Aug 27 19:02:07.726: INFO: Got endpoints: latency-svc-kncdb [204.3141ms]
Aug 27 19:02:07.729: INFO: Created: latency-svc-cnqdl
Aug 27 19:02:07.738: INFO: Got endpoints: latency-svc-cnqdl [201.585265ms]
Aug 27 19:02:07.744: INFO: Created: latency-svc-vp6dx
Aug 27 19:02:07.751: INFO: Got endpoints: latency-svc-vp6dx [201.912392ms]
Aug 27 19:02:07.755: INFO: Created: latency-svc-nf6ft
Aug 27 19:02:07.762: INFO: Got endpoints: latency-svc-nf6ft [201.450573ms]
Aug 27 19:02:07.767: INFO: Created: latency-svc-tj2fr
Aug 27 19:02:07.776: INFO: Got endpoints: latency-svc-tj2fr [201.007772ms]
Aug 27 19:02:07.782: INFO: Created: latency-svc-kj8tg
Aug 27 19:02:07.790: INFO: Got endpoints: latency-svc-kj8tg [204.366557ms]
Aug 27 19:02:07.793: INFO: Created: latency-svc-fkhl6
Aug 27 19:02:07.802: INFO: Got endpoints: latency-svc-fkhl6 [201.086844ms]
Aug 27 19:02:07.807: INFO: Created: latency-svc-n4vlk
Aug 27 19:02:07.815: INFO: Got endpoints: latency-svc-n4vlk [200.698768ms]
Aug 27 19:02:07.822: INFO: Created: latency-svc-wtz59
Aug 27 19:02:07.830: INFO: Got endpoints: latency-svc-wtz59 [204.624982ms]
Aug 27 19:02:07.837: INFO: Created: latency-svc-5wmgs
Aug 27 19:02:07.849: INFO: Got endpoints: latency-svc-5wmgs [209.403627ms]
Aug 27 19:02:07.852: INFO: Created: latency-svc-xjjxb
Aug 27 19:02:07.861: INFO: Got endpoints: latency-svc-xjjxb [203.047691ms]
Aug 27 19:02:07.867: INFO: Created: latency-svc-6w9fw
Aug 27 19:02:07.875: INFO: Got endpoints: latency-svc-6w9fw [207.076642ms]
Aug 27 19:02:07.880: INFO: Created: latency-svc-t9jl8
Aug 27 19:02:07.888: INFO: Got endpoints: latency-svc-t9jl8 [205.883745ms]
Aug 27 19:02:07.893: INFO: Created: latency-svc-5ghzf
Aug 27 19:02:07.903: INFO: Got endpoints: latency-svc-5ghzf [207.312376ms]
Aug 27 19:02:07.906: INFO: Created: latency-svc-rhn7s
Aug 27 19:02:07.915: INFO: Got endpoints: latency-svc-rhn7s [206.475993ms]
Aug 27 19:02:07.922: INFO: Created: latency-svc-8jh5t
Aug 27 19:02:07.930: INFO: Got endpoints: latency-svc-8jh5t [204.171096ms]
Aug 27 19:02:07.935: INFO: Created: latency-svc-t95nv
Aug 27 19:02:07.943: INFO: Got endpoints: latency-svc-t95nv [205.016113ms]
Aug 27 19:02:07.948: INFO: Created: latency-svc-xdrnd
Aug 27 19:02:07.957: INFO: Got endpoints: latency-svc-xdrnd [205.67516ms]
Aug 27 19:02:07.960: INFO: Created: latency-svc-gqclw
Aug 27 19:02:07.969: INFO: Got endpoints: latency-svc-gqclw [206.815264ms]
Aug 27 19:02:07.974: INFO: Created: latency-svc-4fg2x
Aug 27 19:02:07.982: INFO: Got endpoints: latency-svc-4fg2x [205.714579ms]
Aug 27 19:02:07.987: INFO: Created: latency-svc-9c74z
Aug 27 19:02:07.995: INFO: Got endpoints: latency-svc-9c74z [204.763146ms]
Aug 27 19:02:07.999: INFO: Created: latency-svc-cns2s
Aug 27 19:02:08.008: INFO: Got endpoints: latency-svc-cns2s [205.79542ms]
Aug 27 19:02:08.013: INFO: Created: latency-svc-jlw75
Aug 27 19:02:08.023: INFO: Got endpoints: latency-svc-jlw75 [208.376586ms]
Aug 27 19:02:08.025: INFO: Created: latency-svc-cqkcw
Aug 27 19:02:08.039: INFO: Created: latency-svc-c74g7
Aug 27 19:02:08.039: INFO: Got endpoints: latency-svc-cqkcw [208.796701ms]
Aug 27 19:02:08.050: INFO: Got endpoints: latency-svc-c74g7 [201.865478ms]
Aug 27 19:02:08.051: INFO: Created: latency-svc-hrbzf
Aug 27 19:02:08.060: INFO: Got endpoints: latency-svc-hrbzf [198.38524ms]
Aug 27 19:02:08.066: INFO: Created: latency-svc-bvbc9
Aug 27 19:02:08.074: INFO: Got endpoints: latency-svc-bvbc9 [198.903369ms]
Aug 27 19:02:08.081: INFO: Created: latency-svc-ds26z
Aug 27 19:02:08.093: INFO: Got endpoints: latency-svc-ds26z [204.609043ms]
Aug 27 19:02:08.093: INFO: Created: latency-svc-lbbq8
Aug 27 19:02:08.101: INFO: Got endpoints: latency-svc-lbbq8 [198.143752ms]
Aug 27 19:02:08.105: INFO: Created: latency-svc-625gz
Aug 27 19:02:08.114: INFO: Got endpoints: latency-svc-625gz [198.915791ms]
Aug 27 19:02:08.119: INFO: Created: latency-svc-2f9nm
Aug 27 19:02:08.127: INFO: Got endpoints: latency-svc-2f9nm [196.846172ms]
Aug 27 19:02:08.132: INFO: Created: latency-svc-mq76k
Aug 27 19:02:08.139: INFO: Got endpoints: latency-svc-mq76k [195.54115ms]
Aug 27 19:02:08.145: INFO: Created: latency-svc-8vt2z
Aug 27 19:02:08.153: INFO: Got endpoints: latency-svc-8vt2z [196.15752ms]
Aug 27 19:02:08.157: INFO: Created: latency-svc-zmhzr
Aug 27 19:02:08.166: INFO: Got endpoints: latency-svc-zmhzr [197.024662ms]
Aug 27 19:02:08.178: INFO: Created: latency-svc-b5fd6
Aug 27 19:02:08.180: INFO: Got endpoints: latency-svc-b5fd6 [198.031886ms]
Aug 27 19:02:08.186: INFO: Created: latency-svc-ct7sd
Aug 27 19:02:08.194: INFO: Got endpoints: latency-svc-ct7sd [198.739248ms]
Aug 27 19:02:08.199: INFO: Created: latency-svc-95kfr
Aug 27 19:02:08.208: INFO: Got endpoints: latency-svc-95kfr [199.995682ms]
Aug 27 19:02:08.212: INFO: Created: latency-svc-q9c4s
Aug 27 19:02:08.222: INFO: Got endpoints: latency-svc-q9c4s [198.731402ms]
Aug 27 19:02:08.225: INFO: Created: latency-svc-rfdcv
Aug 27 19:02:08.234: INFO: Got endpoints: latency-svc-rfdcv [194.423843ms]
Aug 27 19:02:08.240: INFO: Created: latency-svc-8rvcf
Aug 27 19:02:08.249: INFO: Got endpoints: latency-svc-8rvcf [198.205227ms]
Aug 27 19:02:08.252: INFO: Created: latency-svc-wtsd4
Aug 27 19:02:08.260: INFO: Got endpoints: latency-svc-wtsd4 [200.460559ms]
Aug 27 19:02:08.265: INFO: Created: latency-svc-xgj24
Aug 27 19:02:08.274: INFO: Got endpoints: latency-svc-xgj24 [199.473794ms]
Aug 27 19:02:08.279: INFO: Created: latency-svc-69lwm
Aug 27 19:02:08.286: INFO: Got endpoints: latency-svc-69lwm [193.417827ms]
Aug 27 19:02:08.291: INFO: Created: latency-svc-ttx6w
Aug 27 19:02:08.298: INFO: Got endpoints: latency-svc-ttx6w [197.624738ms]
Aug 27 19:02:08.304: INFO: Created: latency-svc-sj7ts
Aug 27 19:02:08.312: INFO: Got endpoints: latency-svc-sj7ts [198.011084ms]
Aug 27 19:02:08.316: INFO: Created: latency-svc-w5lwg
Aug 27 19:02:08.325: INFO: Got endpoints: latency-svc-w5lwg [197.738692ms]
Aug 27 19:02:08.329: INFO: Created: latency-svc-wsmpj
Aug 27 19:02:08.337: INFO: Got endpoints: latency-svc-wsmpj [197.495024ms]
Aug 27 19:02:08.341: INFO: Created: latency-svc-xd7dh
Aug 27 19:02:08.350: INFO: Got endpoints: latency-svc-xd7dh [196.684545ms]
Aug 27 19:02:08.354: INFO: Created: latency-svc-zhkn7
Aug 27 19:02:08.361: INFO: Got endpoints: latency-svc-zhkn7 [195.025413ms]
Aug 27 19:02:08.366: INFO: Created: latency-svc-cdmnb
Aug 27 19:02:08.375: INFO: Got endpoints: latency-svc-cdmnb [194.831807ms]
Aug 27 19:02:08.379: INFO: Created: latency-svc-gnm6k
Aug 27 19:02:08.390: INFO: Got endpoints: latency-svc-gnm6k [196.515503ms]
Aug 27 19:02:08.391: INFO: Created: latency-svc-684j6
Aug 27 19:02:08.399: INFO: Got endpoints: latency-svc-684j6 [190.915431ms]
Aug 27 19:02:08.403: INFO: Created: latency-svc-z29n4
Aug 27 19:02:08.414: INFO: Got endpoints: latency-svc-z29n4 [191.99671ms]
Aug 27 19:02:08.416: INFO: Created: latency-svc-wvjvb
Aug 27 19:02:08.425: INFO: Got endpoints: latency-svc-wvjvb [191.59912ms]
Aug 27 19:02:08.429: INFO: Created: latency-svc-zrskh
Aug 27 19:02:08.437: INFO: Got endpoints: latency-svc-zrskh [188.042817ms]
Aug 27 19:02:08.442: INFO: Created: latency-svc-fj25h
Aug 27 19:02:08.452: INFO: Got endpoints: latency-svc-fj25h [191.796299ms]
Aug 27 19:02:08.454: INFO: Created: latency-svc-b89d8
Aug 27 19:02:08.462: INFO: Got endpoints: latency-svc-b89d8 [188.701141ms]
Aug 27 19:02:08.463: INFO: Latencies: [33.52862ms 53.651322ms 70.529868ms 88.509007ms 99.472828ms 113.745879ms 125.301892ms 139.055595ms 150.874012ms 168.327432ms 179.564713ms 188.042817ms 188.701141ms 189.92438ms 190.498041ms 190.915431ms 191.576559ms 191.59912ms 191.774932ms 191.796299ms 191.99671ms 192.468458ms 192.872834ms 192.99434ms 193.182364ms 193.417827ms 193.622908ms 194.078363ms 194.390326ms 194.423843ms 194.648911ms 194.805225ms 194.831807ms 194.844515ms 195.025413ms 195.405529ms 195.456383ms 195.458102ms 195.54115ms 196.066965ms 196.12128ms 196.15752ms 196.242986ms 196.271782ms 196.276817ms 196.296994ms 196.515503ms 196.595419ms 196.599396ms 196.684545ms 196.806353ms 196.846172ms 196.918687ms 196.937588ms 197.015649ms 197.024662ms 197.073983ms 197.194608ms 197.399211ms 197.495024ms 197.541675ms 197.551372ms 197.61007ms 197.624738ms 197.738692ms 197.911139ms 198.011084ms 198.031886ms 198.137864ms 198.143752ms 198.205227ms 198.242885ms 198.256375ms 198.258563ms 198.38524ms 198.39522ms 198.492646ms 198.522737ms 198.540756ms 198.591027ms 198.638462ms 198.655781ms 198.731402ms 198.739248ms 198.903369ms 198.915791ms 199.140552ms 199.473794ms 199.633905ms 199.714773ms 199.98013ms 199.995682ms 200.268049ms 200.460559ms 200.698768ms 200.729539ms 201.007772ms 201.086844ms 201.155814ms 201.290127ms 201.450573ms 201.480968ms 201.585265ms 201.677206ms 201.757737ms 201.770259ms 201.865478ms 201.911341ms 201.912392ms 202.460489ms 202.489466ms 202.519236ms 202.586411ms 202.624845ms 202.878932ms 203.047691ms 203.136723ms 203.151551ms 203.272667ms 203.381202ms 203.558177ms 203.605781ms 203.746822ms 203.818102ms 203.841814ms 203.959316ms 204.043075ms 204.171096ms 204.3141ms 204.366557ms 204.473369ms 204.51474ms 204.609043ms 204.624982ms 204.73359ms 204.763146ms 204.806228ms 204.816092ms 204.885331ms 204.888077ms 204.904211ms 204.973851ms 205.01402ms 205.016113ms 205.066024ms 205.185943ms 205.67516ms 205.714579ms 205.79542ms 205.883745ms 206.207133ms 206.352375ms 206.42474ms 206.454206ms 206.475993ms 206.732215ms 206.811732ms 206.815264ms 206.86639ms 207.076642ms 207.117522ms 207.203732ms 207.312376ms 207.731334ms 208.197468ms 208.376586ms 208.438977ms 208.511219ms 208.748667ms 208.796701ms 208.817433ms 209.403627ms 210.06835ms 210.159789ms 210.561906ms 210.724289ms 211.034265ms 211.19714ms 211.42999ms 211.657773ms 212.03174ms 212.078545ms 212.90716ms 213.747029ms 213.865138ms 214.112519ms 214.347476ms 214.526584ms 215.771462ms 215.802065ms 216.018588ms 218.4688ms 218.591047ms 219.043686ms 219.403135ms 220.222494ms 224.6ms 226.32835ms 229.355166ms 237.680508ms]
Aug 27 19:02:08.463: INFO: 50 %ile: 201.450573ms
Aug 27 19:02:08.463: INFO: 90 %ile: 212.03174ms
Aug 27 19:02:08.463: INFO: 99 %ile: 229.355166ms
Aug 27 19:02:08.463: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:02:08.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7706" for this suite.
Aug 27 19:02:24.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:02:24.789: INFO: namespace svc-latency-7706 deletion completed in 16.315847506s

• [SLOW TEST:22.417 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:02:24.790: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5851
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-34cfa98a-c8fd-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume configMaps
Aug 27 19:02:25.009: INFO: Waiting up to 5m0s for pod "pod-configmaps-34d13654-c8fd-11e9-a9da-1ec1181dd093" in namespace "configmap-5851" to be "success or failure"
Aug 27 19:02:25.018: INFO: Pod "pod-configmaps-34d13654-c8fd-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 8.983579ms
Aug 27 19:02:27.027: INFO: Pod "pod-configmaps-34d13654-c8fd-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018231271s
STEP: Saw pod success
Aug 27 19:02:27.027: INFO: Pod "pod-configmaps-34d13654-c8fd-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:02:27.035: INFO: Trying to get logs from node 10.135.206.222 pod pod-configmaps-34d13654-c8fd-11e9-a9da-1ec1181dd093 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 19:02:27.093: INFO: Waiting for pod pod-configmaps-34d13654-c8fd-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:02:27.104: INFO: Pod pod-configmaps-34d13654-c8fd-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:02:27.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5851" for this suite.
Aug 27 19:02:33.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:02:33.456: INFO: namespace configmap-5851 deletion completed in 6.338145231s

• [SLOW TEST:8.665 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:02:33.456: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2428
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:02:37.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2428" for this suite.
Aug 27 19:02:43.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:02:44.120: INFO: namespace kubelet-test-2428 deletion completed in 6.411636747s

• [SLOW TEST:10.665 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:02:44.126: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7149
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 19:02:44.302: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:02:45.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7149" for this suite.
Aug 27 19:02:51.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:02:51.715: INFO: namespace custom-resource-definition-7149 deletion completed in 6.319603788s

• [SLOW TEST:7.589 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:32
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:02:51.715: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8994
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 19:02:51.907: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44d98073-c8fd-11e9-a9da-1ec1181dd093" in namespace "projected-8994" to be "success or failure"
Aug 27 19:02:51.920: INFO: Pod "downwardapi-volume-44d98073-c8fd-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 12.821799ms
Aug 27 19:02:53.929: INFO: Pod "downwardapi-volume-44d98073-c8fd-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022181325s
Aug 27 19:02:55.939: INFO: Pod "downwardapi-volume-44d98073-c8fd-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031757952s
STEP: Saw pod success
Aug 27 19:02:55.939: INFO: Pod "downwardapi-volume-44d98073-c8fd-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:02:55.952: INFO: Trying to get logs from node 10.135.206.222 pod downwardapi-volume-44d98073-c8fd-11e9-a9da-1ec1181dd093 container client-container: <nil>
STEP: delete the pod
Aug 27 19:02:56.001: INFO: Waiting for pod downwardapi-volume-44d98073-c8fd-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:02:56.010: INFO: Pod downwardapi-volume-44d98073-c8fd-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:02:56.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8994" for this suite.
Aug 27 19:03:02.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:03:02.353: INFO: namespace projected-8994 deletion completed in 6.333912461s

• [SLOW TEST:10.638 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:03:02.354: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-879
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-4b31834a-c8fd-11e9-a9da-1ec1181dd093
STEP: Creating secret with name secret-projected-all-test-volume-4b318324-c8fd-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 27 19:03:02.568: INFO: Waiting up to 5m0s for pod "projected-volume-4b3182de-c8fd-11e9-a9da-1ec1181dd093" in namespace "projected-879" to be "success or failure"
Aug 27 19:03:02.581: INFO: Pod "projected-volume-4b3182de-c8fd-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 12.156479ms
Aug 27 19:03:04.590: INFO: Pod "projected-volume-4b3182de-c8fd-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021097981s
STEP: Saw pod success
Aug 27 19:03:04.590: INFO: Pod "projected-volume-4b3182de-c8fd-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:03:04.601: INFO: Trying to get logs from node 10.135.206.222 pod projected-volume-4b3182de-c8fd-11e9-a9da-1ec1181dd093 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 27 19:03:04.650: INFO: Waiting for pod projected-volume-4b3182de-c8fd-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:03:04.660: INFO: Pod projected-volume-4b3182de-c8fd-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:03:04.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-879" for this suite.
Aug 27 19:03:10.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:03:11.064: INFO: namespace projected-879 deletion completed in 6.395419221s

• [SLOW TEST:8.709 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:03:11.064: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1083
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 27 19:03:11.293: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1083,SelfLink:/api/v1/namespaces/watch-1083/configmaps/e2e-watch-test-label-changed,UID:505f5001-c8fd-11e9-9af8-1e15abefec17,ResourceVersion:32091,Generation:0,CreationTimestamp:2019-08-27 19:03:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 27 19:03:11.294: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1083,SelfLink:/api/v1/namespaces/watch-1083/configmaps/e2e-watch-test-label-changed,UID:505f5001-c8fd-11e9-9af8-1e15abefec17,ResourceVersion:32092,Generation:0,CreationTimestamp:2019-08-27 19:03:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 27 19:03:11.294: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1083,SelfLink:/api/v1/namespaces/watch-1083/configmaps/e2e-watch-test-label-changed,UID:505f5001-c8fd-11e9-9af8-1e15abefec17,ResourceVersion:32093,Generation:0,CreationTimestamp:2019-08-27 19:03:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 27 19:03:21.369: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1083,SelfLink:/api/v1/namespaces/watch-1083/configmaps/e2e-watch-test-label-changed,UID:505f5001-c8fd-11e9-9af8-1e15abefec17,ResourceVersion:32112,Generation:0,CreationTimestamp:2019-08-27 19:03:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 27 19:03:21.369: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1083,SelfLink:/api/v1/namespaces/watch-1083/configmaps/e2e-watch-test-label-changed,UID:505f5001-c8fd-11e9-9af8-1e15abefec17,ResourceVersion:32113,Generation:0,CreationTimestamp:2019-08-27 19:03:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Aug 27 19:03:21.369: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1083,SelfLink:/api/v1/namespaces/watch-1083/configmaps/e2e-watch-test-label-changed,UID:505f5001-c8fd-11e9-9af8-1e15abefec17,ResourceVersion:32114,Generation:0,CreationTimestamp:2019-08-27 19:03:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:03:21.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1083" for this suite.
Aug 27 19:03:27.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:03:27.720: INFO: namespace watch-1083 deletion completed in 6.341362808s

• [SLOW TEST:16.656 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:03:27.720: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7727
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 19:03:27.897: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:03:32.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7727" for this suite.
Aug 27 19:04:12.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:04:12.682: INFO: namespace pods-7727 deletion completed in 40.422301816s

• [SLOW TEST:44.962 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:04:12.683: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3875
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Aug 27 19:04:17.468: INFO: Successfully updated pod "annotationupdate751ce79c-c8fd-11e9-a9da-1ec1181dd093"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:04:19.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3875" for this suite.
Aug 27 19:04:41.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:04:41.859: INFO: namespace downward-api-3875 deletion completed in 22.326694129s

• [SLOW TEST:29.176 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:04:41.860: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-993
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 27 19:04:42.052: INFO: Waiting up to 5m0s for pod "pod-8680705e-c8fd-11e9-a9da-1ec1181dd093" in namespace "emptydir-993" to be "success or failure"
Aug 27 19:04:42.061: INFO: Pod "pod-8680705e-c8fd-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 8.954217ms
Aug 27 19:04:44.073: INFO: Pod "pod-8680705e-c8fd-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021138332s
Aug 27 19:04:46.084: INFO: Pod "pod-8680705e-c8fd-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031892473s
STEP: Saw pod success
Aug 27 19:04:46.084: INFO: Pod "pod-8680705e-c8fd-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:04:46.098: INFO: Trying to get logs from node 10.135.206.197 pod pod-8680705e-c8fd-11e9-a9da-1ec1181dd093 container test-container: <nil>
STEP: delete the pod
Aug 27 19:04:46.152: INFO: Waiting for pod pod-8680705e-c8fd-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:04:46.166: INFO: Pod pod-8680705e-c8fd-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:04:46.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-993" for this suite.
Aug 27 19:04:52.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:04:52.524: INFO: namespace emptydir-993 deletion completed in 6.348037158s

• [SLOW TEST:10.664 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:04:52.526: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1820
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Aug 27 19:04:52.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 create -f - --namespace=kubectl-1820'
Aug 27 19:04:53.017: INFO: stderr: ""
Aug 27 19:04:53.017: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 27 19:04:53.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1820'
Aug 27 19:04:53.142: INFO: stderr: ""
Aug 27 19:04:53.142: INFO: stdout: "update-demo-nautilus-k74zz update-demo-nautilus-tfvvb "
Aug 27 19:04:53.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-k74zz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1820'
Aug 27 19:04:53.261: INFO: stderr: ""
Aug 27 19:04:53.261: INFO: stdout: ""
Aug 27 19:04:53.261: INFO: update-demo-nautilus-k74zz is created but not running
Aug 27 19:04:58.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1820'
Aug 27 19:04:58.378: INFO: stderr: ""
Aug 27 19:04:58.378: INFO: stdout: "update-demo-nautilus-k74zz update-demo-nautilus-tfvvb "
Aug 27 19:04:58.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-k74zz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1820'
Aug 27 19:04:58.491: INFO: stderr: ""
Aug 27 19:04:58.491: INFO: stdout: "true"
Aug 27 19:04:58.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-k74zz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1820'
Aug 27 19:04:58.611: INFO: stderr: ""
Aug 27 19:04:58.611: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 27 19:04:58.611: INFO: validating pod update-demo-nautilus-k74zz
Aug 27 19:04:58.633: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 27 19:04:58.633: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 27 19:04:58.633: INFO: update-demo-nautilus-k74zz is verified up and running
Aug 27 19:04:58.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-tfvvb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1820'
Aug 27 19:04:58.746: INFO: stderr: ""
Aug 27 19:04:58.746: INFO: stdout: "true"
Aug 27 19:04:58.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-tfvvb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1820'
Aug 27 19:04:58.871: INFO: stderr: ""
Aug 27 19:04:58.871: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 27 19:04:58.871: INFO: validating pod update-demo-nautilus-tfvvb
Aug 27 19:04:58.888: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 27 19:04:58.888: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 27 19:04:58.888: INFO: update-demo-nautilus-tfvvb is verified up and running
STEP: scaling down the replication controller
Aug 27 19:04:58.890: INFO: scanned /root for discovery docs: <nil>
Aug 27 19:04:58.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1820'
Aug 27 19:05:00.057: INFO: stderr: ""
Aug 27 19:05:00.057: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 27 19:05:00.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1820'
Aug 27 19:05:00.180: INFO: stderr: ""
Aug 27 19:05:00.180: INFO: stdout: "update-demo-nautilus-k74zz update-demo-nautilus-tfvvb "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 27 19:05:05.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1820'
Aug 27 19:05:05.310: INFO: stderr: ""
Aug 27 19:05:05.310: INFO: stdout: "update-demo-nautilus-tfvvb "
Aug 27 19:05:05.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-tfvvb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1820'
Aug 27 19:05:05.423: INFO: stderr: ""
Aug 27 19:05:05.423: INFO: stdout: "true"
Aug 27 19:05:05.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-tfvvb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1820'
Aug 27 19:05:05.546: INFO: stderr: ""
Aug 27 19:05:05.546: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 27 19:05:05.546: INFO: validating pod update-demo-nautilus-tfvvb
Aug 27 19:05:05.559: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 27 19:05:05.559: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 27 19:05:05.559: INFO: update-demo-nautilus-tfvvb is verified up and running
STEP: scaling up the replication controller
Aug 27 19:05:05.560: INFO: scanned /root for discovery docs: <nil>
Aug 27 19:05:05.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1820'
Aug 27 19:05:06.945: INFO: stderr: ""
Aug 27 19:05:06.945: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 27 19:05:06.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1820'
Aug 27 19:05:07.076: INFO: stderr: ""
Aug 27 19:05:07.076: INFO: stdout: "update-demo-nautilus-rg7xn update-demo-nautilus-tfvvb "
Aug 27 19:05:07.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-rg7xn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1820'
Aug 27 19:05:07.191: INFO: stderr: ""
Aug 27 19:05:07.191: INFO: stdout: ""
Aug 27 19:05:07.191: INFO: update-demo-nautilus-rg7xn is created but not running
Aug 27 19:05:12.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1820'
Aug 27 19:05:12.312: INFO: stderr: ""
Aug 27 19:05:12.312: INFO: stdout: "update-demo-nautilus-rg7xn update-demo-nautilus-tfvvb "
Aug 27 19:05:12.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-rg7xn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1820'
Aug 27 19:05:12.452: INFO: stderr: ""
Aug 27 19:05:12.452: INFO: stdout: "true"
Aug 27 19:05:12.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-rg7xn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1820'
Aug 27 19:05:12.575: INFO: stderr: ""
Aug 27 19:05:12.575: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 27 19:05:12.575: INFO: validating pod update-demo-nautilus-rg7xn
Aug 27 19:05:12.593: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 27 19:05:12.593: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 27 19:05:12.593: INFO: update-demo-nautilus-rg7xn is verified up and running
Aug 27 19:05:12.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-tfvvb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1820'
Aug 27 19:05:12.718: INFO: stderr: ""
Aug 27 19:05:12.718: INFO: stdout: "true"
Aug 27 19:05:12.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-tfvvb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1820'
Aug 27 19:05:12.841: INFO: stderr: ""
Aug 27 19:05:12.841: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 27 19:05:12.841: INFO: validating pod update-demo-nautilus-tfvvb
Aug 27 19:05:12.854: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 27 19:05:12.854: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 27 19:05:12.854: INFO: update-demo-nautilus-tfvvb is verified up and running
STEP: using delete to clean up resources
Aug 27 19:05:12.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 delete --grace-period=0 --force -f - --namespace=kubectl-1820'
Aug 27 19:05:12.994: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 27 19:05:12.994: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 27 19:05:12.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1820'
Aug 27 19:05:13.129: INFO: stderr: "No resources found.\n"
Aug 27 19:05:13.129: INFO: stdout: ""
Aug 27 19:05:13.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods -l name=update-demo --namespace=kubectl-1820 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 27 19:05:13.274: INFO: stderr: ""
Aug 27 19:05:13.274: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:05:13.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1820" for this suite.
Aug 27 19:05:37.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:05:37.646: INFO: namespace kubectl-1820 deletion completed in 24.359747786s

• [SLOW TEST:45.121 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:05:37.647: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-626
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-626
Aug 27 19:05:39.865: INFO: Started pod liveness-exec in namespace container-probe-626
STEP: checking the pod's current state and verifying that restartCount is present
Aug 27 19:05:39.874: INFO: Initial restart count of pod liveness-exec is 0
Aug 27 19:06:34.171: INFO: Restart count of pod container-probe-626/liveness-exec is now 1 (54.296795727s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:06:34.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-626" for this suite.
Aug 27 19:06:40.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:06:40.576: INFO: namespace container-probe-626 deletion completed in 6.357848281s

• [SLOW TEST:62.929 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:06:40.576: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6849
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 19:06:40.781: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd44e34b-c8fd-11e9-a9da-1ec1181dd093" in namespace "projected-6849" to be "success or failure"
Aug 27 19:06:40.794: INFO: Pod "downwardapi-volume-cd44e34b-c8fd-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 13.187624ms
Aug 27 19:06:42.804: INFO: Pod "downwardapi-volume-cd44e34b-c8fd-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023030276s
STEP: Saw pod success
Aug 27 19:06:42.804: INFO: Pod "downwardapi-volume-cd44e34b-c8fd-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:06:42.816: INFO: Trying to get logs from node 10.134.235.118 pod downwardapi-volume-cd44e34b-c8fd-11e9-a9da-1ec1181dd093 container client-container: <nil>
STEP: delete the pod
Aug 27 19:06:42.868: INFO: Waiting for pod downwardapi-volume-cd44e34b-c8fd-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:06:42.881: INFO: Pod downwardapi-volume-cd44e34b-c8fd-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:06:42.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6849" for this suite.
Aug 27 19:06:48.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:06:49.213: INFO: namespace projected-6849 deletion completed in 6.322528653s

• [SLOW TEST:8.637 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:06:49.213: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2301
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Aug 27 19:06:49.409: INFO: Waiting up to 5m0s for pod "downward-api-d269203b-c8fd-11e9-a9da-1ec1181dd093" in namespace "downward-api-2301" to be "success or failure"
Aug 27 19:06:49.420: INFO: Pod "downward-api-d269203b-c8fd-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 11.052974ms
Aug 27 19:06:51.429: INFO: Pod "downward-api-d269203b-c8fd-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020416919s
Aug 27 19:06:53.438: INFO: Pod "downward-api-d269203b-c8fd-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029454077s
STEP: Saw pod success
Aug 27 19:06:53.438: INFO: Pod "downward-api-d269203b-c8fd-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:06:53.449: INFO: Trying to get logs from node 10.134.235.118 pod downward-api-d269203b-c8fd-11e9-a9da-1ec1181dd093 container dapi-container: <nil>
STEP: delete the pod
Aug 27 19:06:53.494: INFO: Waiting for pod downward-api-d269203b-c8fd-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:06:53.539: INFO: Pod downward-api-d269203b-c8fd-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:06:53.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2301" for this suite.
Aug 27 19:06:59.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:06:59.864: INFO: namespace downward-api-2301 deletion completed in 6.315163149s

• [SLOW TEST:10.652 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:06:59.865: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4932
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-4932
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4932 to expose endpoints map[]
Aug 27 19:07:00.070: INFO: successfully validated that service endpoint-test2 in namespace services-4932 exposes endpoints map[] (10.340995ms elapsed)
STEP: Creating pod pod1 in namespace services-4932
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4932 to expose endpoints map[pod1:[80]]
Aug 27 19:07:02.153: INFO: successfully validated that service endpoint-test2 in namespace services-4932 exposes endpoints map[pod1:[80]] (2.061453342s elapsed)
STEP: Creating pod pod2 in namespace services-4932
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4932 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 27 19:07:05.272: INFO: successfully validated that service endpoint-test2 in namespace services-4932 exposes endpoints map[pod1:[80] pod2:[80]] (3.106353473s elapsed)
STEP: Deleting pod pod1 in namespace services-4932
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4932 to expose endpoints map[pod2:[80]]
Aug 27 19:07:05.310: INFO: successfully validated that service endpoint-test2 in namespace services-4932 exposes endpoints map[pod2:[80]] (20.364555ms elapsed)
STEP: Deleting pod pod2 in namespace services-4932
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4932 to expose endpoints map[]
Aug 27 19:07:05.337: INFO: successfully validated that service endpoint-test2 in namespace services-4932 exposes endpoints map[] (11.862045ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:07:05.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4932" for this suite.
Aug 27 19:07:29.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:07:29.743: INFO: namespace services-4932 deletion completed in 24.334501823s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:29.878 seconds]
[sig-network] Services
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:07:29.743: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6595
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6595.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6595.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6595.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6595.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6595.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6595.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6595.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6595.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6595.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6595.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6595.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 27.212.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.212.27_udp@PTR;check="$$(dig +tcp +noall +answer +search 27.212.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.212.27_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6595.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6595.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6595.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6595.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6595.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6595.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6595.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6595.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6595.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6595.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6595.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 27.212.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.212.27_udp@PTR;check="$$(dig +tcp +noall +answer +search 27.212.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.212.27_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 27 19:07:34.016: INFO: Unable to read wheezy_udp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:34.032: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:34.044: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:34.056: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:34.142: INFO: Unable to read jessie_udp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:34.154: INFO: Unable to read jessie_tcp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:34.169: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:34.181: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:34.261: INFO: Lookups using dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093 failed for: [wheezy_udp@dns-test-service.dns-6595.svc.cluster.local wheezy_tcp@dns-test-service.dns-6595.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local jessie_udp@dns-test-service.dns-6595.svc.cluster.local jessie_tcp@dns-test-service.dns-6595.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local]

Aug 27 19:07:39.274: INFO: Unable to read wheezy_udp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:39.287: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:39.300: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:39.394: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:39.481: INFO: Unable to read jessie_udp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:39.494: INFO: Unable to read jessie_tcp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:39.507: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:39.519: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:39.599: INFO: Lookups using dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093 failed for: [wheezy_udp@dns-test-service.dns-6595.svc.cluster.local wheezy_tcp@dns-test-service.dns-6595.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local jessie_udp@dns-test-service.dns-6595.svc.cluster.local jessie_tcp@dns-test-service.dns-6595.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local]

Aug 27 19:07:44.273: INFO: Unable to read wheezy_udp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:44.285: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:44.296: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:44.308: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:44.390: INFO: Unable to read jessie_udp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:44.405: INFO: Unable to read jessie_tcp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:44.421: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:44.433: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:44.521: INFO: Lookups using dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093 failed for: [wheezy_udp@dns-test-service.dns-6595.svc.cluster.local wheezy_tcp@dns-test-service.dns-6595.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local jessie_udp@dns-test-service.dns-6595.svc.cluster.local jessie_tcp@dns-test-service.dns-6595.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local]

Aug 27 19:07:49.273: INFO: Unable to read wheezy_udp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:49.284: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:49.296: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:49.307: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:49.394: INFO: Unable to read jessie_udp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:49.406: INFO: Unable to read jessie_tcp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:49.423: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:49.435: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:49.514: INFO: Lookups using dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093 failed for: [wheezy_udp@dns-test-service.dns-6595.svc.cluster.local wheezy_tcp@dns-test-service.dns-6595.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local jessie_udp@dns-test-service.dns-6595.svc.cluster.local jessie_tcp@dns-test-service.dns-6595.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local]

Aug 27 19:07:54.274: INFO: Unable to read wheezy_udp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:54.292: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:54.307: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:54.320: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:54.412: INFO: Unable to read jessie_udp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:54.424: INFO: Unable to read jessie_tcp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:54.435: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:54.447: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:54.521: INFO: Lookups using dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093 failed for: [wheezy_udp@dns-test-service.dns-6595.svc.cluster.local wheezy_tcp@dns-test-service.dns-6595.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local jessie_udp@dns-test-service.dns-6595.svc.cluster.local jessie_tcp@dns-test-service.dns-6595.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local]

Aug 27 19:07:59.273: INFO: Unable to read wheezy_udp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:59.286: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:59.298: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:59.310: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:59.403: INFO: Unable to read jessie_udp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:59.415: INFO: Unable to read jessie_tcp@dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:59.427: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:59.440: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local from pod dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093: the server could not find the requested resource (get pods dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093)
Aug 27 19:07:59.522: INFO: Lookups using dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093 failed for: [wheezy_udp@dns-test-service.dns-6595.svc.cluster.local wheezy_tcp@dns-test-service.dns-6595.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local jessie_udp@dns-test-service.dns-6595.svc.cluster.local jessie_tcp@dns-test-service.dns-6595.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6595.svc.cluster.local]

Aug 27 19:08:04.544: INFO: DNS probes using dns-6595/dns-test-ea968cef-c8fd-11e9-a9da-1ec1181dd093 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:08:04.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6595" for this suite.
Aug 27 19:08:10.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:08:11.015: INFO: namespace dns-6595 deletion completed in 6.350114037s

• [SLOW TEST:41.272 seconds]
[sig-network] DNS
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:08:11.015: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6662
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6662
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6662
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6662
Aug 27 19:08:11.236: INFO: Found 0 stateful pods, waiting for 1
Aug 27 19:08:21.246: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 27 19:08:21.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-6662 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 27 19:08:21.724: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 27 19:08:21.724: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 27 19:08:21.724: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 27 19:08:21.733: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 27 19:08:31.744: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 27 19:08:31.744: INFO: Waiting for statefulset status.replicas updated to 0
Aug 27 19:08:31.787: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998106s
Aug 27 19:08:32.797: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99130417s
Aug 27 19:08:33.807: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.981952561s
Aug 27 19:08:34.816: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.972079036s
Aug 27 19:08:35.828: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.962072723s
Aug 27 19:08:36.837: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.950860071s
Aug 27 19:08:37.847: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.941012243s
Aug 27 19:08:38.856: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.931337621s
Aug 27 19:08:39.866: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.922664284s
Aug 27 19:08:40.875: INFO: Verifying statefulset ss doesn't scale past 1 for another 913.025877ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6662
Aug 27 19:08:41.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-6662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 19:08:42.289: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 27 19:08:42.289: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 27 19:08:42.289: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 27 19:08:42.298: INFO: Found 1 stateful pods, waiting for 3
Aug 27 19:08:52.309: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 19:08:52.309: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 19:08:52.309: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 27 19:08:52.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-6662 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 27 19:08:52.764: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 27 19:08:52.764: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 27 19:08:52.764: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 27 19:08:52.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-6662 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 27 19:08:53.196: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 27 19:08:53.196: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 27 19:08:53.196: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 27 19:08:53.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-6662 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 27 19:08:53.778: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 27 19:08:53.778: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 27 19:08:53.778: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 27 19:08:53.778: INFO: Waiting for statefulset status.replicas updated to 0
Aug 27 19:08:53.786: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug 27 19:09:03.809: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 27 19:09:03.809: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 27 19:09:03.809: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 27 19:09:03.842: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998096s
Aug 27 19:09:04.852: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990187781s
Aug 27 19:09:05.863: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980165588s
Aug 27 19:09:06.898: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.969269717s
Aug 27 19:09:07.911: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.933944259s
Aug 27 19:09:08.920: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.921614558s
Aug 27 19:09:09.934: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.911962659s
Aug 27 19:09:10.945: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.897875492s
Aug 27 19:09:11.954: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.887561554s
Aug 27 19:09:12.965: INFO: Verifying statefulset ss doesn't scale past 3 for another 877.995363ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6662
Aug 27 19:09:13.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-6662 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 19:09:14.399: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 27 19:09:14.399: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 27 19:09:14.399: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 27 19:09:14.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-6662 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 19:09:14.838: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 27 19:09:14.838: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 27 19:09:14.838: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 27 19:09:14.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 exec --namespace=statefulset-6662 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 19:09:15.292: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 27 19:09:15.293: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 27 19:09:15.293: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 27 19:09:15.293: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug 27 19:09:35.332: INFO: Deleting all statefulset in ns statefulset-6662
Aug 27 19:09:35.341: INFO: Scaling statefulset ss to 0
Aug 27 19:09:35.366: INFO: Waiting for statefulset status.replicas updated to 0
Aug 27 19:09:35.375: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:09:35.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6662" for this suite.
Aug 27 19:09:41.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:09:41.782: INFO: namespace statefulset-6662 deletion completed in 6.36321952s

• [SLOW TEST:90.767 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:09:41.783: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-380
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-394666ae-c8fe-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume configMaps
Aug 27 19:09:41.995: INFO: Waiting up to 5m0s for pod "pod-configmaps-3947de30-c8fe-11e9-a9da-1ec1181dd093" in namespace "configmap-380" to be "success or failure"
Aug 27 19:09:42.004: INFO: Pod "pod-configmaps-3947de30-c8fe-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 9.069744ms
Aug 27 19:09:44.013: INFO: Pod "pod-configmaps-3947de30-c8fe-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01856343s
STEP: Saw pod success
Aug 27 19:09:44.013: INFO: Pod "pod-configmaps-3947de30-c8fe-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:09:44.024: INFO: Trying to get logs from node 10.134.235.118 pod pod-configmaps-3947de30-c8fe-11e9-a9da-1ec1181dd093 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 19:09:44.081: INFO: Waiting for pod pod-configmaps-3947de30-c8fe-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:09:44.089: INFO: Pod pod-configmaps-3947de30-c8fe-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:09:44.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-380" for this suite.
Aug 27 19:09:50.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:09:50.390: INFO: namespace configmap-380 deletion completed in 6.292249999s

• [SLOW TEST:8.607 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:09:50.390: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3297
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:09:54.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3297" for this suite.
Aug 27 19:10:48.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:10:48.981: INFO: namespace kubelet-test-3297 deletion completed in 54.323787086s

• [SLOW TEST:58.591 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:10:48.981: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7392
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-61530310-c8fe-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume secrets
Aug 27 19:10:49.184: INFO: Waiting up to 5m0s for pod "pod-secrets-61542c15-c8fe-11e9-a9da-1ec1181dd093" in namespace "secrets-7392" to be "success or failure"
Aug 27 19:10:49.201: INFO: Pod "pod-secrets-61542c15-c8fe-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 17.454025ms
Aug 27 19:10:51.211: INFO: Pod "pod-secrets-61542c15-c8fe-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027010821s
Aug 27 19:10:53.220: INFO: Pod "pod-secrets-61542c15-c8fe-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036209975s
STEP: Saw pod success
Aug 27 19:10:53.220: INFO: Pod "pod-secrets-61542c15-c8fe-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:10:53.229: INFO: Trying to get logs from node 10.135.206.197 pod pod-secrets-61542c15-c8fe-11e9-a9da-1ec1181dd093 container secret-volume-test: <nil>
STEP: delete the pod
Aug 27 19:10:53.276: INFO: Waiting for pod pod-secrets-61542c15-c8fe-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:10:53.284: INFO: Pod pod-secrets-61542c15-c8fe-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:10:53.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7392" for this suite.
Aug 27 19:10:59.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:10:59.585: INFO: namespace secrets-7392 deletion completed in 6.292289122s

• [SLOW TEST:10.604 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:10:59.586: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1972
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 27 19:10:59.778: INFO: Waiting up to 5m0s for pod "pod-67a4d73f-c8fe-11e9-a9da-1ec1181dd093" in namespace "emptydir-1972" to be "success or failure"
Aug 27 19:10:59.789: INFO: Pod "pod-67a4d73f-c8fe-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 11.23664ms
Aug 27 19:11:01.801: INFO: Pod "pod-67a4d73f-c8fe-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022374857s
Aug 27 19:11:03.810: INFO: Pod "pod-67a4d73f-c8fe-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031736849s
STEP: Saw pod success
Aug 27 19:11:03.810: INFO: Pod "pod-67a4d73f-c8fe-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:11:03.819: INFO: Trying to get logs from node 10.134.235.118 pod pod-67a4d73f-c8fe-11e9-a9da-1ec1181dd093 container test-container: <nil>
STEP: delete the pod
Aug 27 19:11:03.868: INFO: Waiting for pod pod-67a4d73f-c8fe-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:11:03.876: INFO: Pod pod-67a4d73f-c8fe-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:11:03.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1972" for this suite.
Aug 27 19:11:09.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:11:10.192: INFO: namespace emptydir-1972 deletion completed in 6.304128689s

• [SLOW TEST:10.606 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:11:10.192: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6705
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-6705
Aug 27 19:11:14.412: INFO: Started pod liveness-http in namespace container-probe-6705
STEP: checking the pod's current state and verifying that restartCount is present
Aug 27 19:11:14.421: INFO: Initial restart count of pod liveness-http is 0
Aug 27 19:11:34.527: INFO: Restart count of pod container-probe-6705/liveness-http is now 1 (20.105802179s elapsed)
Aug 27 19:11:54.624: INFO: Restart count of pod container-probe-6705/liveness-http is now 2 (40.203550261s elapsed)
Aug 27 19:12:14.817: INFO: Restart count of pod container-probe-6705/liveness-http is now 3 (1m0.396276028s elapsed)
Aug 27 19:12:34.914: INFO: Restart count of pod container-probe-6705/liveness-http is now 4 (1m20.493409137s elapsed)
Aug 27 19:13:35.288: INFO: Restart count of pod container-probe-6705/liveness-http is now 5 (2m20.866776633s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:13:35.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6705" for this suite.
Aug 27 19:13:41.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:13:41.641: INFO: namespace container-probe-6705 deletion completed in 6.315177405s

• [SLOW TEST:151.449 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:13:41.642: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5330
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-c83dc8ab-c8fe-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume secrets
Aug 27 19:13:41.858: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c83f694d-c8fe-11e9-a9da-1ec1181dd093" in namespace "projected-5330" to be "success or failure"
Aug 27 19:13:41.873: INFO: Pod "pod-projected-secrets-c83f694d-c8fe-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 14.688063ms
Aug 27 19:13:43.884: INFO: Pod "pod-projected-secrets-c83f694d-c8fe-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025700337s
STEP: Saw pod success
Aug 27 19:13:43.884: INFO: Pod "pod-projected-secrets-c83f694d-c8fe-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:13:43.895: INFO: Trying to get logs from node 10.135.206.197 pod pod-projected-secrets-c83f694d-c8fe-11e9-a9da-1ec1181dd093 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 27 19:13:43.955: INFO: Waiting for pod pod-projected-secrets-c83f694d-c8fe-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:13:43.966: INFO: Pod pod-projected-secrets-c83f694d-c8fe-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:13:43.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5330" for this suite.
Aug 27 19:13:50.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:13:50.304: INFO: namespace projected-5330 deletion completed in 6.328344341s

• [SLOW TEST:8.663 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:13:50.305: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9650
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Aug 27 19:13:50.491: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:13:54.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9650" for this suite.
Aug 27 19:14:00.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:14:00.698: INFO: namespace init-container-9650 deletion completed in 6.324921674s

• [SLOW TEST:10.394 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:14:00.703: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2835
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 27 19:14:00.900: INFO: Waiting up to 5m0s for pod "pod-d3998b2d-c8fe-11e9-a9da-1ec1181dd093" in namespace "emptydir-2835" to be "success or failure"
Aug 27 19:14:00.909: INFO: Pod "pod-d3998b2d-c8fe-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 8.369798ms
Aug 27 19:14:02.919: INFO: Pod "pod-d3998b2d-c8fe-11e9-a9da-1ec1181dd093": Phase="Running", Reason="", readiness=true. Elapsed: 2.018869154s
Aug 27 19:14:04.929: INFO: Pod "pod-d3998b2d-c8fe-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02858085s
STEP: Saw pod success
Aug 27 19:14:04.929: INFO: Pod "pod-d3998b2d-c8fe-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:14:04.943: INFO: Trying to get logs from node 10.135.206.222 pod pod-d3998b2d-c8fe-11e9-a9da-1ec1181dd093 container test-container: <nil>
STEP: delete the pod
Aug 27 19:14:05.003: INFO: Waiting for pod pod-d3998b2d-c8fe-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:14:05.011: INFO: Pod pod-d3998b2d-c8fe-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:14:05.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2835" for this suite.
Aug 27 19:14:11.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:14:11.397: INFO: namespace emptydir-2835 deletion completed in 6.376678461s

• [SLOW TEST:10.694 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:14:11.397: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3909
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-d9fb0965-c8fe-11e9-a9da-1ec1181dd093
STEP: Creating secret with name s-test-opt-upd-d9fb09f2-c8fe-11e9-a9da-1ec1181dd093
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d9fb0965-c8fe-11e9-a9da-1ec1181dd093
STEP: Updating secret s-test-opt-upd-d9fb09f2-c8fe-11e9-a9da-1ec1181dd093
STEP: Creating secret with name s-test-opt-create-d9fb0a23-c8fe-11e9-a9da-1ec1181dd093
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:15:48.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3909" for this suite.
Aug 27 19:16:12.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:16:13.109: INFO: namespace projected-3909 deletion completed in 24.322381865s

• [SLOW TEST:121.712 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:16:13.111: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5102
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 19:16:13.344: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 27 19:16:13.369: INFO: Number of nodes with available pods: 0
Aug 27 19:16:13.369: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug 27 19:16:13.403: INFO: Number of nodes with available pods: 0
Aug 27 19:16:13.403: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 19:16:14.414: INFO: Number of nodes with available pods: 0
Aug 27 19:16:14.414: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 19:16:15.413: INFO: Number of nodes with available pods: 0
Aug 27 19:16:15.413: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 19:16:16.422: INFO: Number of nodes with available pods: 1
Aug 27 19:16:16.422: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 27 19:16:16.461: INFO: Number of nodes with available pods: 1
Aug 27 19:16:16.461: INFO: Number of running nodes: 0, number of available pods: 1
Aug 27 19:16:17.473: INFO: Number of nodes with available pods: 0
Aug 27 19:16:17.473: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 27 19:16:17.494: INFO: Number of nodes with available pods: 0
Aug 27 19:16:17.495: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 19:16:18.509: INFO: Number of nodes with available pods: 0
Aug 27 19:16:18.509: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 19:16:19.507: INFO: Number of nodes with available pods: 0
Aug 27 19:16:19.507: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 19:16:20.504: INFO: Number of nodes with available pods: 0
Aug 27 19:16:20.504: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 19:16:21.504: INFO: Number of nodes with available pods: 0
Aug 27 19:16:21.504: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 19:16:22.537: INFO: Number of nodes with available pods: 0
Aug 27 19:16:22.537: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 19:16:23.506: INFO: Number of nodes with available pods: 0
Aug 27 19:16:23.506: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 19:16:24.504: INFO: Number of nodes with available pods: 0
Aug 27 19:16:24.504: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 19:16:25.503: INFO: Number of nodes with available pods: 0
Aug 27 19:16:25.504: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 19:16:26.507: INFO: Number of nodes with available pods: 0
Aug 27 19:16:26.507: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 19:16:27.509: INFO: Number of nodes with available pods: 0
Aug 27 19:16:27.509: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 19:16:28.506: INFO: Number of nodes with available pods: 1
Aug 27 19:16:28.506: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5102, will wait for the garbage collector to delete the pods
Aug 27 19:16:28.604: INFO: Deleting DaemonSet.extensions daemon-set took: 21.143069ms
Aug 27 19:16:28.704: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.349975ms
Aug 27 19:16:36.317: INFO: Number of nodes with available pods: 0
Aug 27 19:16:36.317: INFO: Number of running nodes: 0, number of available pods: 0
Aug 27 19:16:36.325: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5102/daemonsets","resourceVersion":"34652"},"items":null}

Aug 27 19:16:36.333: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5102/pods","resourceVersion":"34652"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:16:36.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5102" for this suite.
Aug 27 19:16:42.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:16:42.730: INFO: namespace daemonsets-5102 deletion completed in 6.344056705s

• [SLOW TEST:29.620 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:16:42.732: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-358
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 19:16:42.969: INFO: Create a RollingUpdate DaemonSet
Aug 27 19:16:42.982: INFO: Check that daemon pods launch on every node of the cluster
Aug 27 19:16:42.997: INFO: Number of nodes with available pods: 0
Aug 27 19:16:42.997: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 19:16:44.018: INFO: Number of nodes with available pods: 0
Aug 27 19:16:44.018: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 19:16:45.015: INFO: Number of nodes with available pods: 2
Aug 27 19:16:45.015: INFO: Node 10.135.206.197 is running more than one daemon pod
Aug 27 19:16:46.016: INFO: Number of nodes with available pods: 3
Aug 27 19:16:46.016: INFO: Number of running nodes: 3, number of available pods: 3
Aug 27 19:16:46.017: INFO: Update the DaemonSet to trigger a rollout
Aug 27 19:16:46.037: INFO: Updating DaemonSet daemon-set
Aug 27 19:16:57.066: INFO: Roll back the DaemonSet before rollout is complete
Aug 27 19:16:57.086: INFO: Updating DaemonSet daemon-set
Aug 27 19:16:57.086: INFO: Make sure DaemonSet rollback is complete
Aug 27 19:16:57.098: INFO: Wrong image for pod: daemon-set-56nmt. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 27 19:16:57.098: INFO: Pod daemon-set-56nmt is not available
Aug 27 19:16:58.116: INFO: Wrong image for pod: daemon-set-56nmt. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 27 19:16:58.116: INFO: Pod daemon-set-56nmt is not available
Aug 27 19:16:59.117: INFO: Wrong image for pod: daemon-set-56nmt. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 27 19:16:59.117: INFO: Pod daemon-set-56nmt is not available
Aug 27 19:17:00.118: INFO: Pod daemon-set-8bnfz is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-358, will wait for the garbage collector to delete the pods
Aug 27 19:17:00.232: INFO: Deleting DaemonSet.extensions daemon-set took: 25.919671ms
Aug 27 19:17:00.432: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.329491ms
Aug 27 19:17:12.542: INFO: Number of nodes with available pods: 0
Aug 27 19:17:12.542: INFO: Number of running nodes: 0, number of available pods: 0
Aug 27 19:17:12.550: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-358/daemonsets","resourceVersion":"34855"},"items":null}

Aug 27 19:17:12.558: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-358/pods","resourceVersion":"34855"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:17:12.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-358" for this suite.
Aug 27 19:17:18.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:17:18.958: INFO: namespace daemonsets-358 deletion completed in 6.363905726s

• [SLOW TEST:36.226 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:17:18.958: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1236
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 19:17:19.137: INFO: Creating deployment "test-recreate-deployment"
Aug 27 19:17:19.147: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 27 19:17:19.163: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 27 19:17:21.179: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 27 19:17:21.187: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 27 19:17:21.208: INFO: Updating deployment test-recreate-deployment
Aug 27 19:17:21.208: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug 27 19:17:21.297: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-1236,SelfLink:/apis/apps/v1/namespaces/deployment-1236/deployments/test-recreate-deployment,UID:49c0a5ae-c8ff-11e9-9af8-1e15abefec17,ResourceVersion:34956,Generation:2,CreationTimestamp:2019-08-27 19:17:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-08-27 19:17:21 +0000 UTC 2019-08-27 19:17:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-27 19:17:21 +0000 UTC 2019-08-27 19:17:19 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-745fb9c84c" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Aug 27 19:17:21.304: INFO: New ReplicaSet "test-recreate-deployment-745fb9c84c" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-745fb9c84c,GenerateName:,Namespace:deployment-1236,SelfLink:/apis/apps/v1/namespaces/deployment-1236/replicasets/test-recreate-deployment-745fb9c84c,UID:4b030b3f-c8ff-11e9-9af8-1e15abefec17,ResourceVersion:34953,Generation:1,CreationTimestamp:2019-08-27 19:17:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 49c0a5ae-c8ff-11e9-9af8-1e15abefec17 0xc001f81167 0xc001f81168}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 27 19:17:21.304: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 27 19:17:21.304: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6566d46b4b,GenerateName:,Namespace:deployment-1236,SelfLink:/apis/apps/v1/namespaces/deployment-1236/replicasets/test-recreate-deployment-6566d46b4b,UID:49c1e7ab-c8ff-11e9-9af8-1e15abefec17,ResourceVersion:34945,Generation:2,CreationTimestamp:2019-08-27 19:17:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 49c0a5ae-c8ff-11e9-9af8-1e15abefec17 0xc001f81097 0xc001f81098}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 27 19:17:21.316: INFO: Pod "test-recreate-deployment-745fb9c84c-n8kj4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-745fb9c84c-n8kj4,GenerateName:test-recreate-deployment-745fb9c84c-,Namespace:deployment-1236,SelfLink:/api/v1/namespaces/deployment-1236/pods/test-recreate-deployment-745fb9c84c-n8kj4,UID:4b042389-c8ff-11e9-9af8-1e15abefec17,ResourceVersion:34957,Generation:0,CreationTimestamp:2019-08-27 19:17:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-745fb9c84c 4b030b3f-c8ff-11e9-9af8-1e15abefec17 0xc001f81a17 0xc001f81a18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-p8fh2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p8fh2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p8fh2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.197,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f81a90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f81ab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:17:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:17:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:17:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:17:21 +0000 UTC  }],Message:,Reason:,HostIP:10.135.206.197,PodIP:,StartTime:2019-08-27 19:17:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:17:21.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1236" for this suite.
Aug 27 19:17:27.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:17:27.635: INFO: namespace deployment-1236 deletion completed in 6.309748675s

• [SLOW TEST:8.677 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:17:27.635: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9703
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0827 19:18:07.902050      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 27 19:18:07.902: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:18:07.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9703" for this suite.
Aug 27 19:18:15.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:18:16.264: INFO: namespace gc-9703 deletion completed in 8.353837693s

• [SLOW TEST:48.629 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:18:16.265: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8178
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-6bef323d-c8ff-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume configMaps
Aug 27 19:18:16.484: INFO: Waiting up to 5m0s for pod "pod-configmaps-6bf0ae60-c8ff-11e9-a9da-1ec1181dd093" in namespace "configmap-8178" to be "success or failure"
Aug 27 19:18:16.497: INFO: Pod "pod-configmaps-6bf0ae60-c8ff-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 12.638067ms
Aug 27 19:18:18.507: INFO: Pod "pod-configmaps-6bf0ae60-c8ff-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022885534s
Aug 27 19:18:20.517: INFO: Pod "pod-configmaps-6bf0ae60-c8ff-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032844362s
STEP: Saw pod success
Aug 27 19:18:20.517: INFO: Pod "pod-configmaps-6bf0ae60-c8ff-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:18:20.525: INFO: Trying to get logs from node 10.134.235.118 pod pod-configmaps-6bf0ae60-c8ff-11e9-a9da-1ec1181dd093 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 19:18:20.571: INFO: Waiting for pod pod-configmaps-6bf0ae60-c8ff-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:18:20.579: INFO: Pod pod-configmaps-6bf0ae60-c8ff-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:18:20.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8178" for this suite.
Aug 27 19:18:26.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:18:26.897: INFO: namespace configmap-8178 deletion completed in 6.307872268s

• [SLOW TEST:10.631 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:18:26.897: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2101
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0827 19:18:37.134381      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 27 19:18:37.134: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:18:37.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2101" for this suite.
Aug 27 19:18:43.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:18:43.448: INFO: namespace gc-2101 deletion completed in 6.306239502s

• [SLOW TEST:16.551 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:18:43.449: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2027
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 27 19:18:48.216: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7c23474b-c8ff-11e9-a9da-1ec1181dd093"
Aug 27 19:18:48.216: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7c23474b-c8ff-11e9-a9da-1ec1181dd093" in namespace "pods-2027" to be "terminated due to deadline exceeded"
Aug 27 19:18:48.229: INFO: Pod "pod-update-activedeadlineseconds-7c23474b-c8ff-11e9-a9da-1ec1181dd093": Phase="Running", Reason="", readiness=true. Elapsed: 13.147443ms
Aug 27 19:18:50.239: INFO: Pod "pod-update-activedeadlineseconds-7c23474b-c8ff-11e9-a9da-1ec1181dd093": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.023072202s
Aug 27 19:18:50.239: INFO: Pod "pod-update-activedeadlineseconds-7c23474b-c8ff-11e9-a9da-1ec1181dd093" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:18:50.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2027" for this suite.
Aug 27 19:18:56.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:18:56.550: INFO: namespace pods-2027 deletion completed in 6.302018219s

• [SLOW TEST:13.101 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:18:56.553: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7690
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 27 19:18:56.750: INFO: Waiting up to 5m0s for pod "pod-83f08f64-c8ff-11e9-a9da-1ec1181dd093" in namespace "emptydir-7690" to be "success or failure"
Aug 27 19:18:56.761: INFO: Pod "pod-83f08f64-c8ff-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 11.428116ms
Aug 27 19:18:58.771: INFO: Pod "pod-83f08f64-c8ff-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020866067s
Aug 27 19:19:00.781: INFO: Pod "pod-83f08f64-c8ff-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030818629s
STEP: Saw pod success
Aug 27 19:19:00.781: INFO: Pod "pod-83f08f64-c8ff-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:19:00.791: INFO: Trying to get logs from node 10.135.206.222 pod pod-83f08f64-c8ff-11e9-a9da-1ec1181dd093 container test-container: <nil>
STEP: delete the pod
Aug 27 19:19:00.850: INFO: Waiting for pod pod-83f08f64-c8ff-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:19:00.858: INFO: Pod pod-83f08f64-c8ff-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:19:00.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7690" for this suite.
Aug 27 19:19:06.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:19:07.213: INFO: namespace emptydir-7690 deletion completed in 6.345759891s

• [SLOW TEST:10.661 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:19:07.213: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7287
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 27 19:19:07.486: INFO: Number of nodes with available pods: 0
Aug 27 19:19:07.486: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 19:19:08.508: INFO: Number of nodes with available pods: 0
Aug 27 19:19:08.508: INFO: Node 10.134.235.118 is running more than one daemon pod
Aug 27 19:19:09.508: INFO: Number of nodes with available pods: 3
Aug 27 19:19:09.508: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 27 19:19:09.558: INFO: Number of nodes with available pods: 2
Aug 27 19:19:09.558: INFO: Node 10.135.206.197 is running more than one daemon pod
Aug 27 19:19:10.576: INFO: Number of nodes with available pods: 2
Aug 27 19:19:10.576: INFO: Node 10.135.206.197 is running more than one daemon pod
Aug 27 19:19:11.576: INFO: Number of nodes with available pods: 2
Aug 27 19:19:11.576: INFO: Node 10.135.206.197 is running more than one daemon pod
Aug 27 19:19:12.592: INFO: Number of nodes with available pods: 2
Aug 27 19:19:12.592: INFO: Node 10.135.206.197 is running more than one daemon pod
Aug 27 19:19:13.577: INFO: Number of nodes with available pods: 2
Aug 27 19:19:13.577: INFO: Node 10.135.206.197 is running more than one daemon pod
Aug 27 19:19:14.577: INFO: Number of nodes with available pods: 2
Aug 27 19:19:14.577: INFO: Node 10.135.206.197 is running more than one daemon pod
Aug 27 19:19:15.579: INFO: Number of nodes with available pods: 2
Aug 27 19:19:15.579: INFO: Node 10.135.206.197 is running more than one daemon pod
Aug 27 19:19:16.576: INFO: Number of nodes with available pods: 2
Aug 27 19:19:16.577: INFO: Node 10.135.206.197 is running more than one daemon pod
Aug 27 19:19:17.578: INFO: Number of nodes with available pods: 2
Aug 27 19:19:17.578: INFO: Node 10.135.206.197 is running more than one daemon pod
Aug 27 19:19:18.581: INFO: Number of nodes with available pods: 3
Aug 27 19:19:18.581: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7287, will wait for the garbage collector to delete the pods
Aug 27 19:19:18.675: INFO: Deleting DaemonSet.extensions daemon-set took: 23.645373ms
Aug 27 19:19:18.775: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.302334ms
Aug 27 19:19:26.285: INFO: Number of nodes with available pods: 0
Aug 27 19:19:26.285: INFO: Number of running nodes: 0, number of available pods: 0
Aug 27 19:19:26.294: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7287/daemonsets","resourceVersion":"35754"},"items":null}

Aug 27 19:19:26.303: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7287/pods","resourceVersion":"35754"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:19:26.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7287" for this suite.
Aug 27 19:19:34.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:19:34.696: INFO: namespace daemonsets-7287 deletion completed in 8.357306705s

• [SLOW TEST:27.483 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:19:34.698: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2218
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:19:37.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2218" for this suite.
Aug 27 19:20:00.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:20:00.327: INFO: namespace replication-controller-2218 deletion completed in 22.354137841s

• [SLOW TEST:25.629 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:20:00.329: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4638
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Aug 27 19:20:00.523: INFO: Waiting up to 5m0s for pod "downward-api-a9f3e593-c8ff-11e9-a9da-1ec1181dd093" in namespace "downward-api-4638" to be "success or failure"
Aug 27 19:20:00.533: INFO: Pod "downward-api-a9f3e593-c8ff-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 9.334332ms
Aug 27 19:20:02.543: INFO: Pod "downward-api-a9f3e593-c8ff-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019148931s
STEP: Saw pod success
Aug 27 19:20:02.543: INFO: Pod "downward-api-a9f3e593-c8ff-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:20:02.555: INFO: Trying to get logs from node 10.134.235.118 pod downward-api-a9f3e593-c8ff-11e9-a9da-1ec1181dd093 container dapi-container: <nil>
STEP: delete the pod
Aug 27 19:20:02.606: INFO: Waiting for pod downward-api-a9f3e593-c8ff-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:20:02.614: INFO: Pod downward-api-a9f3e593-c8ff-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:20:02.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4638" for this suite.
Aug 27 19:20:08.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:20:08.970: INFO: namespace downward-api-4638 deletion completed in 6.34708873s

• [SLOW TEST:8.641 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:20:08.970: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9746
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-pf6p
STEP: Creating a pod to test atomic-volume-subpath
Aug 27 19:20:09.188: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-pf6p" in namespace "subpath-9746" to be "success or failure"
Aug 27 19:20:09.197: INFO: Pod "pod-subpath-test-secret-pf6p": Phase="Pending", Reason="", readiness=false. Elapsed: 8.940986ms
Aug 27 19:20:11.207: INFO: Pod "pod-subpath-test-secret-pf6p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019096867s
Aug 27 19:20:13.216: INFO: Pod "pod-subpath-test-secret-pf6p": Phase="Running", Reason="", readiness=true. Elapsed: 4.027513454s
Aug 27 19:20:15.225: INFO: Pod "pod-subpath-test-secret-pf6p": Phase="Running", Reason="", readiness=true. Elapsed: 6.037187868s
Aug 27 19:20:17.235: INFO: Pod "pod-subpath-test-secret-pf6p": Phase="Running", Reason="", readiness=true. Elapsed: 8.046601156s
Aug 27 19:20:19.244: INFO: Pod "pod-subpath-test-secret-pf6p": Phase="Running", Reason="", readiness=true. Elapsed: 10.056167635s
Aug 27 19:20:21.254: INFO: Pod "pod-subpath-test-secret-pf6p": Phase="Running", Reason="", readiness=true. Elapsed: 12.065862314s
Aug 27 19:20:23.263: INFO: Pod "pod-subpath-test-secret-pf6p": Phase="Running", Reason="", readiness=true. Elapsed: 14.074835303s
Aug 27 19:20:25.272: INFO: Pod "pod-subpath-test-secret-pf6p": Phase="Running", Reason="", readiness=true. Elapsed: 16.084012448s
Aug 27 19:20:27.282: INFO: Pod "pod-subpath-test-secret-pf6p": Phase="Running", Reason="", readiness=true. Elapsed: 18.093545214s
Aug 27 19:20:29.293: INFO: Pod "pod-subpath-test-secret-pf6p": Phase="Running", Reason="", readiness=true. Elapsed: 20.104725601s
Aug 27 19:20:31.304: INFO: Pod "pod-subpath-test-secret-pf6p": Phase="Running", Reason="", readiness=true. Elapsed: 22.115507098s
Aug 27 19:20:33.313: INFO: Pod "pod-subpath-test-secret-pf6p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.124998968s
STEP: Saw pod success
Aug 27 19:20:33.313: INFO: Pod "pod-subpath-test-secret-pf6p" satisfied condition "success or failure"
Aug 27 19:20:33.323: INFO: Trying to get logs from node 10.135.206.197 pod pod-subpath-test-secret-pf6p container test-container-subpath-secret-pf6p: <nil>
STEP: delete the pod
Aug 27 19:20:33.383: INFO: Waiting for pod pod-subpath-test-secret-pf6p to disappear
Aug 27 19:20:33.391: INFO: Pod pod-subpath-test-secret-pf6p no longer exists
STEP: Deleting pod pod-subpath-test-secret-pf6p
Aug 27 19:20:33.391: INFO: Deleting pod "pod-subpath-test-secret-pf6p" in namespace "subpath-9746"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:20:33.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9746" for this suite.
Aug 27 19:20:39.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:20:39.737: INFO: namespace subpath-9746 deletion completed in 6.327679684s

• [SLOW TEST:30.767 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:20:39.738: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2210
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:20:39.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2210" for this suite.
Aug 27 19:21:01.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:21:02.282: INFO: namespace kubelet-test-2210 deletion completed in 22.319365327s

• [SLOW TEST:22.544 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:21:02.282: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-89
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Aug 27 19:21:05.057: INFO: Successfully updated pod "labelsupdatecee1e033-c8ff-11e9-a9da-1ec1181dd093"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:21:07.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-89" for this suite.
Aug 27 19:21:23.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:21:23.431: INFO: namespace downward-api-89 deletion completed in 16.327816201s

• [SLOW TEST:21.149 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:21:23.432: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 19:21:23.626: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db7c78c3-c8ff-11e9-a9da-1ec1181dd093" in namespace "downward-api-7621" to be "success or failure"
Aug 27 19:21:23.637: INFO: Pod "downwardapi-volume-db7c78c3-c8ff-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 11.068763ms
Aug 27 19:21:25.648: INFO: Pod "downwardapi-volume-db7c78c3-c8ff-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021540206s
STEP: Saw pod success
Aug 27 19:21:25.648: INFO: Pod "downwardapi-volume-db7c78c3-c8ff-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:21:25.658: INFO: Trying to get logs from node 10.135.206.222 pod downwardapi-volume-db7c78c3-c8ff-11e9-a9da-1ec1181dd093 container client-container: <nil>
STEP: delete the pod
Aug 27 19:21:25.702: INFO: Waiting for pod downwardapi-volume-db7c78c3-c8ff-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:21:25.710: INFO: Pod downwardapi-volume-db7c78c3-c8ff-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:21:25.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7621" for this suite.
Aug 27 19:21:31.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:21:32.027: INFO: namespace downward-api-7621 deletion completed in 6.308278821s

• [SLOW TEST:8.596 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:21:32.033: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-134
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 27 19:21:32.232: INFO: Waiting up to 5m0s for pod "pod-e09d66f6-c8ff-11e9-a9da-1ec1181dd093" in namespace "emptydir-134" to be "success or failure"
Aug 27 19:21:32.240: INFO: Pod "pod-e09d66f6-c8ff-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 8.71865ms
Aug 27 19:21:34.250: INFO: Pod "pod-e09d66f6-c8ff-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018710998s
STEP: Saw pod success
Aug 27 19:21:34.250: INFO: Pod "pod-e09d66f6-c8ff-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:21:34.262: INFO: Trying to get logs from node 10.135.206.197 pod pod-e09d66f6-c8ff-11e9-a9da-1ec1181dd093 container test-container: <nil>
STEP: delete the pod
Aug 27 19:21:34.306: INFO: Waiting for pod pod-e09d66f6-c8ff-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:21:34.314: INFO: Pod pod-e09d66f6-c8ff-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:21:34.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-134" for this suite.
Aug 27 19:21:40.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:21:40.659: INFO: namespace emptydir-134 deletion completed in 6.33601519s

• [SLOW TEST:8.626 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:21:40.659: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1028
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-e5c1a9c0-c8ff-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume configMaps
Aug 27 19:21:40.940: INFO: Waiting up to 5m0s for pod "pod-configmaps-e5c332a2-c8ff-11e9-a9da-1ec1181dd093" in namespace "configmap-1028" to be "success or failure"
Aug 27 19:21:40.950: INFO: Pod "pod-configmaps-e5c332a2-c8ff-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 10.202206ms
Aug 27 19:21:42.960: INFO: Pod "pod-configmaps-e5c332a2-c8ff-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019937521s
Aug 27 19:21:44.970: INFO: Pod "pod-configmaps-e5c332a2-c8ff-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030185633s
STEP: Saw pod success
Aug 27 19:21:44.970: INFO: Pod "pod-configmaps-e5c332a2-c8ff-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:21:44.980: INFO: Trying to get logs from node 10.134.235.118 pod pod-configmaps-e5c332a2-c8ff-11e9-a9da-1ec1181dd093 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 19:21:45.025: INFO: Waiting for pod pod-configmaps-e5c332a2-c8ff-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:21:45.094: INFO: Pod pod-configmaps-e5c332a2-c8ff-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:21:45.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1028" for this suite.
Aug 27 19:21:51.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:21:51.451: INFO: namespace configmap-1028 deletion completed in 6.345835159s

• [SLOW TEST:10.792 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:21:51.452: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4663
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug 27 19:21:55.681: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-ec2fcf93-c8ff-11e9-a9da-1ec1181dd093,GenerateName:,Namespace:events-4663,SelfLink:/api/v1/namespaces/events-4663/pods/send-events-ec2fcf93-c8ff-11e9-a9da-1ec1181dd093,UID:ec2c8569-c8ff-11e9-9af8-1e15abefec17,ResourceVersion:36334,Generation:0,CreationTimestamp:2019-08-27 19:21:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 628283877,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5clbp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5clbp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-5clbp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.197,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000db9290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000db92b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:21:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:21:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:21:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:21:51 +0000 UTC  }],Message:,Reason:,HostIP:10.135.206.197,PodIP:172.30.183.125,StartTime:2019-08-27 19:21:51 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-08-27 19:21:52 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 containerd://cfaf8dec8651332abdbcae82d8c4d6db1639de926d4ccc021558ed1bf579ad45}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Aug 27 19:21:57.689: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug 27 19:21:59.696: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:21:59.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4663" for this suite.
Aug 27 19:22:39.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:22:40.083: INFO: namespace events-4663 deletion completed in 40.363693295s

• [SLOW TEST:48.632 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:22:40.084: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3162
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-8xqdm in namespace proxy-3162
I0827 19:22:40.295139      18 runners.go:184] Created replication controller with name: proxy-service-8xqdm, namespace: proxy-3162, replica count: 1
I0827 19:22:41.345630      18 runners.go:184] proxy-service-8xqdm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0827 19:22:42.345836      18 runners.go:184] proxy-service-8xqdm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0827 19:22:43.346122      18 runners.go:184] proxy-service-8xqdm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0827 19:22:44.346333      18 runners.go:184] proxy-service-8xqdm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0827 19:22:45.346564      18 runners.go:184] proxy-service-8xqdm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0827 19:22:46.346798      18 runners.go:184] proxy-service-8xqdm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0827 19:22:47.347020      18 runners.go:184] proxy-service-8xqdm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0827 19:22:48.347216      18 runners.go:184] proxy-service-8xqdm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0827 19:22:49.347423      18 runners.go:184] proxy-service-8xqdm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0827 19:22:50.347680      18 runners.go:184] proxy-service-8xqdm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0827 19:22:51.348053      18 runners.go:184] proxy-service-8xqdm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0827 19:22:52.348362      18 runners.go:184] proxy-service-8xqdm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0827 19:22:53.348602      18 runners.go:184] proxy-service-8xqdm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0827 19:22:54.348924      18 runners.go:184] proxy-service-8xqdm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0827 19:22:55.349220      18 runners.go:184] proxy-service-8xqdm Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 27 19:22:55.357: INFO: setup took 15.096451139s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 27 19:22:55.375: INFO: (0) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 17.762619ms)
Aug 27 19:22:55.379: INFO: (0) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 21.090462ms)
Aug 27 19:22:55.379: INFO: (0) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 20.818159ms)
Aug 27 19:22:55.379: INFO: (0) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 21.039156ms)
Aug 27 19:22:55.379: INFO: (0) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 21.551342ms)
Aug 27 19:22:55.390: INFO: (0) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 32.325314ms)
Aug 27 19:22:55.391: INFO: (0) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 33.302136ms)
Aug 27 19:22:55.391: INFO: (0) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 33.445758ms)
Aug 27 19:22:55.391: INFO: (0) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 33.532947ms)
Aug 27 19:22:55.391: INFO: (0) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 33.445095ms)
Aug 27 19:22:55.391: INFO: (0) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 33.221472ms)
Aug 27 19:22:55.402: INFO: (0) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 44.539835ms)
Aug 27 19:22:55.403: INFO: (0) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 44.680571ms)
Aug 27 19:22:55.403: INFO: (0) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 45.121843ms)
Aug 27 19:22:55.405: INFO: (0) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 47.097153ms)
Aug 27 19:22:55.420: INFO: (0) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 62.024034ms)
Aug 27 19:22:55.433: INFO: (1) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 13.488349ms)
Aug 27 19:22:55.433: INFO: (1) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 13.127622ms)
Aug 27 19:22:55.436: INFO: (1) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 15.911413ms)
Aug 27 19:22:55.436: INFO: (1) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 16.15793ms)
Aug 27 19:22:55.436: INFO: (1) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 16.214316ms)
Aug 27 19:22:55.437: INFO: (1) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 16.364073ms)
Aug 27 19:22:55.437: INFO: (1) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 16.689409ms)
Aug 27 19:22:55.437: INFO: (1) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 16.454008ms)
Aug 27 19:22:55.437: INFO: (1) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 16.594191ms)
Aug 27 19:22:55.437: INFO: (1) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 16.497481ms)
Aug 27 19:22:55.441: INFO: (1) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 20.867804ms)
Aug 27 19:22:55.441: INFO: (1) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 20.969766ms)
Aug 27 19:22:55.441: INFO: (1) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 21.130206ms)
Aug 27 19:22:55.441: INFO: (1) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 21.322728ms)
Aug 27 19:22:55.443: INFO: (1) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 22.411267ms)
Aug 27 19:22:55.444: INFO: (1) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 23.407385ms)
Aug 27 19:22:55.459: INFO: (2) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 14.696044ms)
Aug 27 19:22:55.460: INFO: (2) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 15.378898ms)
Aug 27 19:22:55.460: INFO: (2) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 15.420452ms)
Aug 27 19:22:55.460: INFO: (2) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 15.771531ms)
Aug 27 19:22:55.460: INFO: (2) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 15.71022ms)
Aug 27 19:22:55.460: INFO: (2) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 16.280214ms)
Aug 27 19:22:55.461: INFO: (2) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 16.510636ms)
Aug 27 19:22:55.461: INFO: (2) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 16.508152ms)
Aug 27 19:22:55.461: INFO: (2) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 16.39089ms)
Aug 27 19:22:55.461: INFO: (2) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 17.249401ms)
Aug 27 19:22:55.462: INFO: (2) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 18.22558ms)
Aug 27 19:22:55.464: INFO: (2) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 20.437598ms)
Aug 27 19:22:55.467: INFO: (2) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 23.30844ms)
Aug 27 19:22:55.467: INFO: (2) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 23.293828ms)
Aug 27 19:22:55.468: INFO: (2) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 23.737522ms)
Aug 27 19:22:55.468: INFO: (2) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 23.520647ms)
Aug 27 19:22:55.483: INFO: (3) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 14.687025ms)
Aug 27 19:22:55.483: INFO: (3) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 14.640921ms)
Aug 27 19:22:55.483: INFO: (3) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 14.790312ms)
Aug 27 19:22:55.483: INFO: (3) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 14.890082ms)
Aug 27 19:22:55.483: INFO: (3) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 14.768662ms)
Aug 27 19:22:55.483: INFO: (3) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 15.039786ms)
Aug 27 19:22:55.483: INFO: (3) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 15.016024ms)
Aug 27 19:22:55.483: INFO: (3) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 14.967743ms)
Aug 27 19:22:55.483: INFO: (3) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 15.087453ms)
Aug 27 19:22:55.483: INFO: (3) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 15.233752ms)
Aug 27 19:22:55.485: INFO: (3) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 16.587217ms)
Aug 27 19:22:55.487: INFO: (3) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 19.205288ms)
Aug 27 19:22:55.487: INFO: (3) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 19.31886ms)
Aug 27 19:22:55.488: INFO: (3) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 19.306446ms)
Aug 27 19:22:55.488: INFO: (3) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 19.596057ms)
Aug 27 19:22:55.488: INFO: (3) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 19.753162ms)
Aug 27 19:22:55.499: INFO: (4) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 11.187465ms)
Aug 27 19:22:55.503: INFO: (4) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 14.846684ms)
Aug 27 19:22:55.503: INFO: (4) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 14.508512ms)
Aug 27 19:22:55.503: INFO: (4) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 14.518866ms)
Aug 27 19:22:55.503: INFO: (4) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 15.083408ms)
Aug 27 19:22:55.504: INFO: (4) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 15.543647ms)
Aug 27 19:22:55.504: INFO: (4) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 15.624497ms)
Aug 27 19:22:55.504: INFO: (4) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 15.549083ms)
Aug 27 19:22:55.504: INFO: (4) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 15.788299ms)
Aug 27 19:22:55.504: INFO: (4) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 15.842137ms)
Aug 27 19:22:55.508: INFO: (4) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 19.556057ms)
Aug 27 19:22:55.511: INFO: (4) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 23.124014ms)
Aug 27 19:22:55.511: INFO: (4) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 23.115714ms)
Aug 27 19:22:55.512: INFO: (4) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 23.585173ms)
Aug 27 19:22:55.512: INFO: (4) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 23.578527ms)
Aug 27 19:22:55.512: INFO: (4) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 23.558382ms)
Aug 27 19:22:55.527: INFO: (5) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 15.201843ms)
Aug 27 19:22:55.527: INFO: (5) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 14.549417ms)
Aug 27 19:22:55.528: INFO: (5) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 15.371663ms)
Aug 27 19:22:55.528: INFO: (5) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 15.19365ms)
Aug 27 19:22:55.528: INFO: (5) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 15.543501ms)
Aug 27 19:22:55.528: INFO: (5) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 15.372073ms)
Aug 27 19:22:55.528: INFO: (5) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 15.283833ms)
Aug 27 19:22:55.528: INFO: (5) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 15.52271ms)
Aug 27 19:22:55.528: INFO: (5) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 15.692806ms)
Aug 27 19:22:55.528: INFO: (5) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 15.462995ms)
Aug 27 19:22:55.530: INFO: (5) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 18.144318ms)
Aug 27 19:22:55.531: INFO: (5) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 19.259688ms)
Aug 27 19:22:55.534: INFO: (5) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 22.225611ms)
Aug 27 19:22:55.534: INFO: (5) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 21.964742ms)
Aug 27 19:22:55.537: INFO: (5) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 25.123146ms)
Aug 27 19:22:55.537: INFO: (5) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 25.331239ms)
Aug 27 19:22:55.548: INFO: (6) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 10.873611ms)
Aug 27 19:22:55.554: INFO: (6) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 16.015581ms)
Aug 27 19:22:55.554: INFO: (6) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 15.710179ms)
Aug 27 19:22:55.554: INFO: (6) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 15.42928ms)
Aug 27 19:22:55.554: INFO: (6) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 15.773504ms)
Aug 27 19:22:55.554: INFO: (6) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 15.964779ms)
Aug 27 19:22:55.554: INFO: (6) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 16.069762ms)
Aug 27 19:22:55.554: INFO: (6) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 15.956917ms)
Aug 27 19:22:55.554: INFO: (6) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 15.93145ms)
Aug 27 19:22:55.554: INFO: (6) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 16.273887ms)
Aug 27 19:22:55.558: INFO: (6) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 20.515442ms)
Aug 27 19:22:55.561: INFO: (6) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 22.997343ms)
Aug 27 19:22:55.561: INFO: (6) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 23.720666ms)
Aug 27 19:22:55.562: INFO: (6) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 23.873824ms)
Aug 27 19:22:55.562: INFO: (6) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 23.95704ms)
Aug 27 19:22:55.562: INFO: (6) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 23.95252ms)
Aug 27 19:22:55.580: INFO: (7) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 18.012419ms)
Aug 27 19:22:55.580: INFO: (7) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 17.95794ms)
Aug 27 19:22:55.580: INFO: (7) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 18.045743ms)
Aug 27 19:22:55.580: INFO: (7) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 18.140496ms)
Aug 27 19:22:55.580: INFO: (7) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 18.114585ms)
Aug 27 19:22:55.580: INFO: (7) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 18.314585ms)
Aug 27 19:22:55.581: INFO: (7) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 18.827945ms)
Aug 27 19:22:55.581: INFO: (7) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 19.045592ms)
Aug 27 19:22:55.582: INFO: (7) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 19.144838ms)
Aug 27 19:22:55.582: INFO: (7) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 19.361585ms)
Aug 27 19:22:55.584: INFO: (7) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 21.491308ms)
Aug 27 19:22:55.585: INFO: (7) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 22.688926ms)
Aug 27 19:22:55.587: INFO: (7) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 25.231669ms)
Aug 27 19:22:55.588: INFO: (7) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 25.118872ms)
Aug 27 19:22:55.588: INFO: (7) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 25.135906ms)
Aug 27 19:22:55.588: INFO: (7) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 25.610454ms)
Aug 27 19:22:55.602: INFO: (8) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 13.392542ms)
Aug 27 19:22:55.602: INFO: (8) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 13.794909ms)
Aug 27 19:22:55.602: INFO: (8) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 13.313883ms)
Aug 27 19:22:55.602: INFO: (8) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 13.578303ms)
Aug 27 19:22:55.602: INFO: (8) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 13.796664ms)
Aug 27 19:22:55.604: INFO: (8) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 16.184428ms)
Aug 27 19:22:55.605: INFO: (8) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 16.079493ms)
Aug 27 19:22:55.605: INFO: (8) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 15.993658ms)
Aug 27 19:22:55.605: INFO: (8) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 16.223461ms)
Aug 27 19:22:55.605: INFO: (8) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 16.127024ms)
Aug 27 19:22:55.607: INFO: (8) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 18.058929ms)
Aug 27 19:22:55.609: INFO: (8) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 20.36672ms)
Aug 27 19:22:55.610: INFO: (8) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 21.14395ms)
Aug 27 19:22:55.610: INFO: (8) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 21.186756ms)
Aug 27 19:22:55.610: INFO: (8) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 21.623732ms)
Aug 27 19:22:55.611: INFO: (8) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 22.706536ms)
Aug 27 19:22:55.629: INFO: (9) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 17.306329ms)
Aug 27 19:22:55.629: INFO: (9) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 17.168004ms)
Aug 27 19:22:55.629: INFO: (9) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 17.438241ms)
Aug 27 19:22:55.629: INFO: (9) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 17.782445ms)
Aug 27 19:22:55.629: INFO: (9) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 17.515766ms)
Aug 27 19:22:55.629: INFO: (9) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 17.68696ms)
Aug 27 19:22:55.629: INFO: (9) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 17.580984ms)
Aug 27 19:22:55.629: INFO: (9) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 17.389131ms)
Aug 27 19:22:55.629: INFO: (9) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 17.356356ms)
Aug 27 19:22:55.629: INFO: (9) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 17.790749ms)
Aug 27 19:22:55.632: INFO: (9) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 19.786987ms)
Aug 27 19:22:55.634: INFO: (9) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 21.461322ms)
Aug 27 19:22:55.634: INFO: (9) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 21.718966ms)
Aug 27 19:22:55.634: INFO: (9) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 21.73758ms)
Aug 27 19:22:55.634: INFO: (9) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 22.642604ms)
Aug 27 19:22:55.637: INFO: (9) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 25.47012ms)
Aug 27 19:22:55.654: INFO: (10) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 16.719295ms)
Aug 27 19:22:55.654: INFO: (10) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 17.32113ms)
Aug 27 19:22:55.655: INFO: (10) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 17.319813ms)
Aug 27 19:22:55.655: INFO: (10) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 17.290729ms)
Aug 27 19:22:55.655: INFO: (10) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 17.469523ms)
Aug 27 19:22:55.655: INFO: (10) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 17.325342ms)
Aug 27 19:22:55.655: INFO: (10) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 17.327077ms)
Aug 27 19:22:55.655: INFO: (10) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 17.239906ms)
Aug 27 19:22:55.655: INFO: (10) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 17.487376ms)
Aug 27 19:22:55.655: INFO: (10) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 17.622798ms)
Aug 27 19:22:55.656: INFO: (10) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 18.757543ms)
Aug 27 19:22:55.659: INFO: (10) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 22.213672ms)
Aug 27 19:22:55.659: INFO: (10) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 22.349693ms)
Aug 27 19:22:55.660: INFO: (10) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 22.651723ms)
Aug 27 19:22:55.660: INFO: (10) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 22.451623ms)
Aug 27 19:22:55.660: INFO: (10) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 22.364817ms)
Aug 27 19:22:55.677: INFO: (11) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 16.997414ms)
Aug 27 19:22:55.680: INFO: (11) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 19.848897ms)
Aug 27 19:22:55.680: INFO: (11) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 19.866428ms)
Aug 27 19:22:55.680: INFO: (11) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 19.956789ms)
Aug 27 19:22:55.684: INFO: (11) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 24.385614ms)
Aug 27 19:22:55.685: INFO: (11) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 24.554833ms)
Aug 27 19:22:55.689: INFO: (11) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 29.138603ms)
Aug 27 19:22:55.691: INFO: (11) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 31.208987ms)
Aug 27 19:22:55.692: INFO: (11) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 31.832252ms)
Aug 27 19:22:55.692: INFO: (11) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 32.210943ms)
Aug 27 19:22:55.693: INFO: (11) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 32.412618ms)
Aug 27 19:22:55.693: INFO: (11) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 32.972039ms)
Aug 27 19:22:55.693: INFO: (11) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 33.275063ms)
Aug 27 19:22:55.694: INFO: (11) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 33.899184ms)
Aug 27 19:22:55.698: INFO: (11) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 37.989733ms)
Aug 27 19:22:55.698: INFO: (11) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 38.20462ms)
Aug 27 19:22:55.714: INFO: (12) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 14.762039ms)
Aug 27 19:22:55.714: INFO: (12) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 15.090422ms)
Aug 27 19:22:55.714: INFO: (12) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 15.381579ms)
Aug 27 19:22:55.714: INFO: (12) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 15.396658ms)
Aug 27 19:22:55.714: INFO: (12) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 15.577205ms)
Aug 27 19:22:55.715: INFO: (12) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 15.62902ms)
Aug 27 19:22:55.715: INFO: (12) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 15.552976ms)
Aug 27 19:22:55.715: INFO: (12) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 15.91111ms)
Aug 27 19:22:55.715: INFO: (12) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 15.928556ms)
Aug 27 19:22:55.715: INFO: (12) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 15.703015ms)
Aug 27 19:22:55.717: INFO: (12) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 18.334518ms)
Aug 27 19:22:55.719: INFO: (12) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 20.222843ms)
Aug 27 19:22:55.720: INFO: (12) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 21.144232ms)
Aug 27 19:22:55.721: INFO: (12) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 21.896836ms)
Aug 27 19:22:55.723: INFO: (12) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 24.196747ms)
Aug 27 19:22:55.723: INFO: (12) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 24.451537ms)
Aug 27 19:22:55.739: INFO: (13) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 15.813092ms)
Aug 27 19:22:55.739: INFO: (13) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 15.892072ms)
Aug 27 19:22:55.739: INFO: (13) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 16.040214ms)
Aug 27 19:22:55.739: INFO: (13) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 15.920206ms)
Aug 27 19:22:55.739: INFO: (13) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 16.2372ms)
Aug 27 19:22:55.740: INFO: (13) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 16.021293ms)
Aug 27 19:22:55.740: INFO: (13) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 16.041722ms)
Aug 27 19:22:55.740: INFO: (13) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 16.134422ms)
Aug 27 19:22:55.740: INFO: (13) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 16.082059ms)
Aug 27 19:22:55.740: INFO: (13) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 16.415824ms)
Aug 27 19:22:55.741: INFO: (13) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 18.123325ms)
Aug 27 19:22:55.745: INFO: (13) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 21.462728ms)
Aug 27 19:22:55.745: INFO: (13) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 21.598703ms)
Aug 27 19:22:55.745: INFO: (13) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 21.638916ms)
Aug 27 19:22:55.745: INFO: (13) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 21.625864ms)
Aug 27 19:22:55.745: INFO: (13) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 22.0382ms)
Aug 27 19:22:55.756: INFO: (14) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 10.725626ms)
Aug 27 19:22:55.762: INFO: (14) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 15.970444ms)
Aug 27 19:22:55.762: INFO: (14) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 16.370577ms)
Aug 27 19:22:55.762: INFO: (14) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 16.207756ms)
Aug 27 19:22:55.762: INFO: (14) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 16.532697ms)
Aug 27 19:22:55.762: INFO: (14) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 16.524183ms)
Aug 27 19:22:55.762: INFO: (14) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 16.579766ms)
Aug 27 19:22:55.762: INFO: (14) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 16.822442ms)
Aug 27 19:22:55.762: INFO: (14) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 16.684474ms)
Aug 27 19:22:55.765: INFO: (14) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 18.798564ms)
Aug 27 19:22:55.768: INFO: (14) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 22.00661ms)
Aug 27 19:22:55.768: INFO: (14) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 21.979605ms)
Aug 27 19:22:55.768: INFO: (14) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 22.417657ms)
Aug 27 19:22:55.768: INFO: (14) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 22.421529ms)
Aug 27 19:22:55.768: INFO: (14) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 22.671867ms)
Aug 27 19:22:55.768: INFO: (14) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 22.639368ms)
Aug 27 19:22:55.789: INFO: (15) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 19.939523ms)
Aug 27 19:22:55.790: INFO: (15) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 20.878634ms)
Aug 27 19:22:55.790: INFO: (15) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 21.101669ms)
Aug 27 19:22:55.790: INFO: (15) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 21.320362ms)
Aug 27 19:22:55.790: INFO: (15) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 21.420433ms)
Aug 27 19:22:55.790: INFO: (15) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 21.394726ms)
Aug 27 19:22:55.791: INFO: (15) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 22.823223ms)
Aug 27 19:22:55.792: INFO: (15) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 22.72596ms)
Aug 27 19:22:55.792: INFO: (15) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 23.17652ms)
Aug 27 19:22:55.792: INFO: (15) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 23.033059ms)
Aug 27 19:22:55.792: INFO: (15) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 23.28046ms)
Aug 27 19:22:55.793: INFO: (15) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 24.624291ms)
Aug 27 19:22:55.793: INFO: (15) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 24.409729ms)
Aug 27 19:22:55.793: INFO: (15) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 24.561835ms)
Aug 27 19:22:55.794: INFO: (15) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 25.554313ms)
Aug 27 19:22:55.795: INFO: (15) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 26.089705ms)
Aug 27 19:22:55.807: INFO: (16) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 12.264188ms)
Aug 27 19:22:55.814: INFO: (16) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 18.032372ms)
Aug 27 19:22:55.815: INFO: (16) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 18.970378ms)
Aug 27 19:22:55.815: INFO: (16) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 18.916434ms)
Aug 27 19:22:55.815: INFO: (16) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 19.818735ms)
Aug 27 19:22:55.815: INFO: (16) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 18.656718ms)
Aug 27 19:22:55.815: INFO: (16) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 19.44015ms)
Aug 27 19:22:55.815: INFO: (16) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 19.107497ms)
Aug 27 19:22:55.816: INFO: (16) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 19.297935ms)
Aug 27 19:22:55.816: INFO: (16) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 20.087952ms)
Aug 27 19:22:55.818: INFO: (16) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 22.703835ms)
Aug 27 19:22:55.818: INFO: (16) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 22.756879ms)
Aug 27 19:22:55.818: INFO: (16) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 22.345107ms)
Aug 27 19:22:55.818: INFO: (16) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 22.328646ms)
Aug 27 19:22:55.819: INFO: (16) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 24.126476ms)
Aug 27 19:22:55.820: INFO: (16) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 24.12202ms)
Aug 27 19:22:55.834: INFO: (17) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 14.200879ms)
Aug 27 19:22:55.839: INFO: (17) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 18.207092ms)
Aug 27 19:22:55.839: INFO: (17) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 18.458769ms)
Aug 27 19:22:55.839: INFO: (17) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 18.604991ms)
Aug 27 19:22:55.839: INFO: (17) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 18.996997ms)
Aug 27 19:22:55.839: INFO: (17) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 18.901855ms)
Aug 27 19:22:55.839: INFO: (17) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 18.497154ms)
Aug 27 19:22:55.839: INFO: (17) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 18.849374ms)
Aug 27 19:22:55.839: INFO: (17) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 18.648511ms)
Aug 27 19:22:55.839: INFO: (17) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 18.922301ms)
Aug 27 19:22:55.848: INFO: (17) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 27.524936ms)
Aug 27 19:22:55.848: INFO: (17) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 27.367084ms)
Aug 27 19:22:55.848: INFO: (17) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 27.425811ms)
Aug 27 19:22:55.848: INFO: (17) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 27.781721ms)
Aug 27 19:22:55.848: INFO: (17) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 27.611248ms)
Aug 27 19:22:55.848: INFO: (17) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 27.844947ms)
Aug 27 19:22:55.871: INFO: (18) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 23.078827ms)
Aug 27 19:22:55.871: INFO: (18) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 23.167093ms)
Aug 27 19:22:55.871: INFO: (18) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 23.236508ms)
Aug 27 19:22:55.871: INFO: (18) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 23.17493ms)
Aug 27 19:22:55.871: INFO: (18) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 23.270965ms)
Aug 27 19:22:55.872: INFO: (18) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 23.567763ms)
Aug 27 19:22:55.872: INFO: (18) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 23.522102ms)
Aug 27 19:22:55.872: INFO: (18) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 23.488359ms)
Aug 27 19:22:55.872: INFO: (18) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 23.274557ms)
Aug 27 19:22:55.872: INFO: (18) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 23.837949ms)
Aug 27 19:22:55.873: INFO: (18) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 24.901605ms)
Aug 27 19:22:55.881: INFO: (18) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 32.318798ms)
Aug 27 19:22:55.883: INFO: (18) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 34.933207ms)
Aug 27 19:22:55.883: INFO: (18) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 35.083409ms)
Aug 27 19:22:55.883: INFO: (18) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 34.881542ms)
Aug 27 19:22:55.883: INFO: (18) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 35.139591ms)
Aug 27 19:22:55.911: INFO: (19) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:462/proxy/: tls qux (200; 27.301356ms)
Aug 27 19:22:55.911: INFO: (19) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 27.186997ms)
Aug 27 19:22:55.911: INFO: (19) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">test<... (200; 27.182408ms)
Aug 27 19:22:55.911: INFO: (19) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:460/proxy/: tls baz (200; 26.997597ms)
Aug 27 19:22:55.911: INFO: (19) /api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/https:proxy-service-8xqdm-gd4tb:443/proxy/tlsrewritem... (200; 27.171887ms)
Aug 27 19:22:55.911: INFO: (19) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:160/proxy/: foo (200; 27.100317ms)
Aug 27 19:22:55.911: INFO: (19) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 27.319923ms)
Aug 27 19:22:55.911: INFO: (19) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb/proxy/rewriteme">test</a> (200; 27.156233ms)
Aug 27 19:22:55.911: INFO: (19) /api/v1/namespaces/proxy-3162/pods/proxy-service-8xqdm-gd4tb:162/proxy/: bar (200; 27.093361ms)
Aug 27 19:22:55.911: INFO: (19) /api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3162/pods/http:proxy-service-8xqdm-gd4tb:1080/proxy/rewriteme">... (200; 27.385854ms)
Aug 27 19:22:55.915: INFO: (19) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname2/proxy/: bar (200; 30.931354ms)
Aug 27 19:22:55.915: INFO: (19) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname1/proxy/: tls baz (200; 31.783761ms)
Aug 27 19:22:55.922: INFO: (19) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname1/proxy/: foo (200; 38.732717ms)
Aug 27 19:22:55.922: INFO: (19) /api/v1/namespaces/proxy-3162/services/proxy-service-8xqdm:portname1/proxy/: foo (200; 38.877451ms)
Aug 27 19:22:55.922: INFO: (19) /api/v1/namespaces/proxy-3162/services/http:proxy-service-8xqdm:portname2/proxy/: bar (200; 38.611757ms)
Aug 27 19:22:55.922: INFO: (19) /api/v1/namespaces/proxy-3162/services/https:proxy-service-8xqdm:tlsportname2/proxy/: tls qux (200; 38.558377ms)
STEP: deleting ReplicationController proxy-service-8xqdm in namespace proxy-3162, will wait for the garbage collector to delete the pods
Aug 27 19:22:55.994: INFO: Deleting ReplicationController proxy-service-8xqdm took: 14.142964ms
Aug 27 19:22:56.094: INFO: Terminating ReplicationController proxy-service-8xqdm pods took: 100.242338ms
[AfterEach] version v1
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:23:06.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3162" for this suite.
Aug 27 19:23:12.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:23:12.554: INFO: namespace proxy-3162 deletion completed in 6.346771518s

• [SLOW TEST:32.470 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:23:12.554: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8918
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0827 19:23:42.822150      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 27 19:23:42.822: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:23:42.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8918" for this suite.
Aug 27 19:23:48.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:23:49.226: INFO: namespace gc-8918 deletion completed in 6.396962275s

• [SLOW TEST:36.672 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:23:49.226: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4850
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 27 19:23:49.422: INFO: Waiting up to 5m0s for pod "pod-3262d9a5-c900-11e9-a9da-1ec1181dd093" in namespace "emptydir-4850" to be "success or failure"
Aug 27 19:23:49.436: INFO: Pod "pod-3262d9a5-c900-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 13.118502ms
Aug 27 19:23:51.445: INFO: Pod "pod-3262d9a5-c900-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022215765s
Aug 27 19:23:53.454: INFO: Pod "pod-3262d9a5-c900-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031743949s
STEP: Saw pod success
Aug 27 19:23:53.454: INFO: Pod "pod-3262d9a5-c900-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:23:53.463: INFO: Trying to get logs from node 10.135.206.222 pod pod-3262d9a5-c900-11e9-a9da-1ec1181dd093 container test-container: <nil>
STEP: delete the pod
Aug 27 19:23:53.508: INFO: Waiting for pod pod-3262d9a5-c900-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:23:53.517: INFO: Pod pod-3262d9a5-c900-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:23:53.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4850" for this suite.
Aug 27 19:23:59.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:23:59.845: INFO: namespace emptydir-4850 deletion completed in 6.319127196s

• [SLOW TEST:10.619 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:23:59.846: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6210
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Aug 27 19:24:00.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 cluster-info'
Aug 27 19:24:00.278: INFO: stderr: ""
Aug 27 19:24:00.278: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:24:00.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6210" for this suite.
Aug 27 19:24:06.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:24:06.604: INFO: namespace kubectl-6210 deletion completed in 6.31729287s

• [SLOW TEST:6.758 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:24:06.606: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6287
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Aug 27 19:24:06.805: INFO: Waiting up to 5m0s for pod "client-containers-3cbf7819-c900-11e9-a9da-1ec1181dd093" in namespace "containers-6287" to be "success or failure"
Aug 27 19:24:06.821: INFO: Pod "client-containers-3cbf7819-c900-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 16.093753ms
Aug 27 19:24:08.831: INFO: Pod "client-containers-3cbf7819-c900-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026080983s
STEP: Saw pod success
Aug 27 19:24:08.831: INFO: Pod "client-containers-3cbf7819-c900-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:24:08.840: INFO: Trying to get logs from node 10.134.235.118 pod client-containers-3cbf7819-c900-11e9-a9da-1ec1181dd093 container test-container: <nil>
STEP: delete the pod
Aug 27 19:24:08.896: INFO: Waiting for pod client-containers-3cbf7819-c900-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:24:08.904: INFO: Pod client-containers-3cbf7819-c900-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:24:08.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6287" for this suite.
Aug 27 19:24:14.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:24:15.265: INFO: namespace containers-6287 deletion completed in 6.351549654s

• [SLOW TEST:8.660 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:24:15.266: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2506
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-41e8e10c-c900-11e9-a9da-1ec1181dd093
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:24:19.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2506" for this suite.
Aug 27 19:24:43.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:24:43.916: INFO: namespace configmap-2506 deletion completed in 24.346582851s

• [SLOW TEST:28.650 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:24:43.919: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-153
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-153
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-153
STEP: Deleting pre-stop pod
Aug 27 19:24:57.215: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:24:57.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-153" for this suite.
Aug 27 19:25:37.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:25:37.611: INFO: namespace prestop-153 deletion completed in 40.372623925s

• [SLOW TEST:53.693 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:25:37.613: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3023
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Aug 27 19:25:37.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 create -f - --namespace=kubectl-3023'
Aug 27 19:25:38.124: INFO: stderr: ""
Aug 27 19:25:38.124: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 27 19:25:38.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3023'
Aug 27 19:25:38.249: INFO: stderr: ""
Aug 27 19:25:38.249: INFO: stdout: "update-demo-nautilus-ck5fz update-demo-nautilus-v6vhx "
Aug 27 19:25:38.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-ck5fz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3023'
Aug 27 19:25:38.377: INFO: stderr: ""
Aug 27 19:25:38.377: INFO: stdout: ""
Aug 27 19:25:38.377: INFO: update-demo-nautilus-ck5fz is created but not running
Aug 27 19:25:43.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3023'
Aug 27 19:25:43.489: INFO: stderr: ""
Aug 27 19:25:43.489: INFO: stdout: "update-demo-nautilus-ck5fz update-demo-nautilus-v6vhx "
Aug 27 19:25:43.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-ck5fz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3023'
Aug 27 19:25:43.606: INFO: stderr: ""
Aug 27 19:25:43.606: INFO: stdout: "true"
Aug 27 19:25:43.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-ck5fz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3023'
Aug 27 19:25:43.720: INFO: stderr: ""
Aug 27 19:25:43.720: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 27 19:25:43.720: INFO: validating pod update-demo-nautilus-ck5fz
Aug 27 19:25:43.741: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 27 19:25:43.741: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 27 19:25:43.741: INFO: update-demo-nautilus-ck5fz is verified up and running
Aug 27 19:25:43.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-v6vhx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3023'
Aug 27 19:25:43.855: INFO: stderr: ""
Aug 27 19:25:43.855: INFO: stdout: "true"
Aug 27 19:25:43.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods update-demo-nautilus-v6vhx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3023'
Aug 27 19:25:43.973: INFO: stderr: ""
Aug 27 19:25:43.973: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 27 19:25:43.973: INFO: validating pod update-demo-nautilus-v6vhx
Aug 27 19:25:43.993: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 27 19:25:43.993: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 27 19:25:43.993: INFO: update-demo-nautilus-v6vhx is verified up and running
STEP: using delete to clean up resources
Aug 27 19:25:43.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 delete --grace-period=0 --force -f - --namespace=kubectl-3023'
Aug 27 19:25:44.154: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 27 19:25:44.154: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 27 19:25:44.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3023'
Aug 27 19:25:44.303: INFO: stderr: "No resources found.\n"
Aug 27 19:25:44.303: INFO: stdout: ""
Aug 27 19:25:44.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods -l name=update-demo --namespace=kubectl-3023 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 27 19:25:44.451: INFO: stderr: ""
Aug 27 19:25:44.451: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:25:44.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3023" for this suite.
Aug 27 19:25:50.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:25:50.777: INFO: namespace kubectl-3023 deletion completed in 6.315794645s

• [SLOW TEST:13.165 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:25:50.779: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9624
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 27 19:25:50.978: INFO: Waiting up to 5m0s for pod "pod-7ad70bc0-c900-11e9-a9da-1ec1181dd093" in namespace "emptydir-9624" to be "success or failure"
Aug 27 19:25:50.992: INFO: Pod "pod-7ad70bc0-c900-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 13.969344ms
Aug 27 19:25:53.011: INFO: Pod "pod-7ad70bc0-c900-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033361016s
STEP: Saw pod success
Aug 27 19:25:53.011: INFO: Pod "pod-7ad70bc0-c900-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:25:53.020: INFO: Trying to get logs from node 10.134.235.118 pod pod-7ad70bc0-c900-11e9-a9da-1ec1181dd093 container test-container: <nil>
STEP: delete the pod
Aug 27 19:25:53.069: INFO: Waiting for pod pod-7ad70bc0-c900-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:25:53.078: INFO: Pod pod-7ad70bc0-c900-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:25:53.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9624" for this suite.
Aug 27 19:25:59.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:25:59.388: INFO: namespace emptydir-9624 deletion completed in 6.301032933s

• [SLOW TEST:8.609 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:25:59.389: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8369
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-7ff7e205-c900-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume configMaps
Aug 27 19:25:59.592: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7ff95150-c900-11e9-a9da-1ec1181dd093" in namespace "projected-8369" to be "success or failure"
Aug 27 19:25:59.600: INFO: Pod "pod-projected-configmaps-7ff95150-c900-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 8.169853ms
Aug 27 19:26:01.609: INFO: Pod "pod-projected-configmaps-7ff95150-c900-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017281705s
Aug 27 19:26:03.618: INFO: Pod "pod-projected-configmaps-7ff95150-c900-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026760385s
STEP: Saw pod success
Aug 27 19:26:03.619: INFO: Pod "pod-projected-configmaps-7ff95150-c900-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:26:03.631: INFO: Trying to get logs from node 10.135.206.222 pod pod-projected-configmaps-7ff95150-c900-11e9-a9da-1ec1181dd093 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 19:26:03.676: INFO: Waiting for pod pod-projected-configmaps-7ff95150-c900-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:26:03.685: INFO: Pod pod-projected-configmaps-7ff95150-c900-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:26:03.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8369" for this suite.
Aug 27 19:26:09.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:26:10.033: INFO: namespace projected-8369 deletion completed in 6.338960988s

• [SLOW TEST:10.645 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:26:10.034: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4154
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:26:10.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4154" for this suite.
Aug 27 19:26:16.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:26:16.540: INFO: namespace services-4154 deletion completed in 6.310312418s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.507 seconds]
[sig-network] Services
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:26:16.543: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3926
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 27 19:26:16.738: INFO: Waiting up to 5m0s for pod "pod-8a317b0e-c900-11e9-a9da-1ec1181dd093" in namespace "emptydir-3926" to be "success or failure"
Aug 27 19:26:16.747: INFO: Pod "pod-8a317b0e-c900-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 8.432233ms
Aug 27 19:26:18.756: INFO: Pod "pod-8a317b0e-c900-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01803264s
Aug 27 19:26:20.767: INFO: Pod "pod-8a317b0e-c900-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029231994s
STEP: Saw pod success
Aug 27 19:26:20.767: INFO: Pod "pod-8a317b0e-c900-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:26:20.779: INFO: Trying to get logs from node 10.135.206.222 pod pod-8a317b0e-c900-11e9-a9da-1ec1181dd093 container test-container: <nil>
STEP: delete the pod
Aug 27 19:26:20.825: INFO: Waiting for pod pod-8a317b0e-c900-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:26:20.833: INFO: Pod pod-8a317b0e-c900-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:26:20.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3926" for this suite.
Aug 27 19:26:26.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:26:27.192: INFO: namespace emptydir-3926 deletion completed in 6.350030169s

• [SLOW TEST:10.649 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:26:27.192: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9609
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-908aaf98-c900-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume secrets
Aug 27 19:26:27.405: INFO: Waiting up to 5m0s for pod "pod-secrets-908bd883-c900-11e9-a9da-1ec1181dd093" in namespace "secrets-9609" to be "success or failure"
Aug 27 19:26:27.420: INFO: Pod "pod-secrets-908bd883-c900-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 15.348781ms
Aug 27 19:26:29.432: INFO: Pod "pod-secrets-908bd883-c900-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026761625s
STEP: Saw pod success
Aug 27 19:26:29.432: INFO: Pod "pod-secrets-908bd883-c900-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:26:29.440: INFO: Trying to get logs from node 10.134.235.118 pod pod-secrets-908bd883-c900-11e9-a9da-1ec1181dd093 container secret-volume-test: <nil>
STEP: delete the pod
Aug 27 19:26:29.485: INFO: Waiting for pod pod-secrets-908bd883-c900-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:26:29.493: INFO: Pod pod-secrets-908bd883-c900-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:26:29.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9609" for this suite.
Aug 27 19:26:35.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:26:35.822: INFO: namespace secrets-9609 deletion completed in 6.31998988s

• [SLOW TEST:8.630 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:26:35.823: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9856
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-9856
Aug 27 19:26:38.043: INFO: Started pod liveness-exec in namespace container-probe-9856
STEP: checking the pod's current state and verifying that restartCount is present
Aug 27 19:26:38.052: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:30:39.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9856" for this suite.
Aug 27 19:30:45.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:30:45.766: INFO: namespace container-probe-9856 deletion completed in 6.444287212s

• [SLOW TEST:249.944 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:30:45.767: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9283
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 27 19:30:45.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9283'
Aug 27 19:30:46.090: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 27 19:30:46.090: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Aug 27 19:30:46.107: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-nhhgl]
Aug 27 19:30:46.107: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-nhhgl" in namespace "kubectl-9283" to be "running and ready"
Aug 27 19:30:46.115: INFO: Pod "e2e-test-nginx-rc-nhhgl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.12889ms
Aug 27 19:30:48.126: INFO: Pod "e2e-test-nginx-rc-nhhgl": Phase="Running", Reason="", readiness=true. Elapsed: 2.019355479s
Aug 27 19:30:48.126: INFO: Pod "e2e-test-nginx-rc-nhhgl" satisfied condition "running and ready"
Aug 27 19:30:48.126: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-nhhgl]
Aug 27 19:30:48.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 logs rc/e2e-test-nginx-rc --namespace=kubectl-9283'
Aug 27 19:30:48.329: INFO: stderr: ""
Aug 27 19:30:48.329: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1425
Aug 27 19:30:48.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 delete rc e2e-test-nginx-rc --namespace=kubectl-9283'
Aug 27 19:30:48.456: INFO: stderr: ""
Aug 27 19:30:48.456: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:30:48.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9283" for this suite.
Aug 27 19:31:12.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:31:12.861: INFO: namespace kubectl-9283 deletion completed in 24.395391788s

• [SLOW TEST:27.094 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:31:12.862: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9055
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Aug 27 19:31:15.648: INFO: Successfully updated pod "annotationupdate3ad19ac0-c901-11e9-a9da-1ec1181dd093"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:31:19.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9055" for this suite.
Aug 27 19:31:43.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:31:44.039: INFO: namespace projected-9055 deletion completed in 24.322269457s

• [SLOW TEST:31.178 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:31:44.040: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1387
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 27 19:31:44.759: INFO: Pod name wrapped-volume-race-4db4163d-c901-11e9-a9da-1ec1181dd093: Found 0 pods out of 5
Aug 27 19:31:49.774: INFO: Pod name wrapped-volume-race-4db4163d-c901-11e9-a9da-1ec1181dd093: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-4db4163d-c901-11e9-a9da-1ec1181dd093 in namespace emptydir-wrapper-1387, will wait for the garbage collector to delete the pods
Aug 27 19:31:49.897: INFO: Deleting ReplicationController wrapped-volume-race-4db4163d-c901-11e9-a9da-1ec1181dd093 took: 13.512758ms
Aug 27 19:31:50.097: INFO: Terminating ReplicationController wrapped-volume-race-4db4163d-c901-11e9-a9da-1ec1181dd093 pods took: 200.271994ms
STEP: Creating RC which spawns configmap-volume pods
Aug 27 19:32:26.435: INFO: Pod name wrapped-volume-race-6689ec1f-c901-11e9-a9da-1ec1181dd093: Found 0 pods out of 5
Aug 27 19:32:31.449: INFO: Pod name wrapped-volume-race-6689ec1f-c901-11e9-a9da-1ec1181dd093: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6689ec1f-c901-11e9-a9da-1ec1181dd093 in namespace emptydir-wrapper-1387, will wait for the garbage collector to delete the pods
Aug 27 19:32:31.676: INFO: Deleting ReplicationController wrapped-volume-race-6689ec1f-c901-11e9-a9da-1ec1181dd093 took: 111.262981ms
Aug 27 19:32:31.676: INFO: Terminating ReplicationController wrapped-volume-race-6689ec1f-c901-11e9-a9da-1ec1181dd093 pods took: 42.901µs
STEP: Creating RC which spawns configmap-volume pods
Aug 27 19:33:16.516: INFO: Pod name wrapped-volume-race-846366ee-c901-11e9-a9da-1ec1181dd093: Found 0 pods out of 5
Aug 27 19:33:21.530: INFO: Pod name wrapped-volume-race-846366ee-c901-11e9-a9da-1ec1181dd093: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-846366ee-c901-11e9-a9da-1ec1181dd093 in namespace emptydir-wrapper-1387, will wait for the garbage collector to delete the pods
Aug 27 19:33:21.652: INFO: Deleting ReplicationController wrapped-volume-race-846366ee-c901-11e9-a9da-1ec1181dd093 took: 14.947276ms
Aug 27 19:33:21.852: INFO: Terminating ReplicationController wrapped-volume-race-846366ee-c901-11e9-a9da-1ec1181dd093 pods took: 200.246774ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:34:07.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1387" for this suite.
Aug 27 19:34:15.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:34:15.935: INFO: namespace emptydir-wrapper-1387 deletion completed in 8.366338134s

• [SLOW TEST:151.895 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:34:15.936: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-152
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Aug 27 19:34:16.661: INFO: created pod pod-service-account-defaultsa
Aug 27 19:34:16.661: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 27 19:34:16.673: INFO: created pod pod-service-account-mountsa
Aug 27 19:34:16.673: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 27 19:34:16.684: INFO: created pod pod-service-account-nomountsa
Aug 27 19:34:16.684: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 27 19:34:16.695: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 27 19:34:16.695: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 27 19:34:16.707: INFO: created pod pod-service-account-mountsa-mountspec
Aug 27 19:34:16.708: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 27 19:34:16.719: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 27 19:34:16.719: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 27 19:34:16.731: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 27 19:34:16.731: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 27 19:34:16.743: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 27 19:34:16.743: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 27 19:34:16.755: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 27 19:34:16.755: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:34:16.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-152" for this suite.
Aug 27 19:34:22.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:34:23.050: INFO: namespace svcaccounts-152 deletion completed in 6.28738229s

• [SLOW TEST:7.114 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:34:23.052: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7035
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7035
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Aug 27 19:34:23.257: INFO: Found 0 stateful pods, waiting for 3
Aug 27 19:34:33.269: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 19:34:33.269: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 19:34:33.269: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug 27 19:34:33.329: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 27 19:34:43.406: INFO: Updating stateful set ss2
Aug 27 19:34:43.430: INFO: Waiting for Pod statefulset-7035/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Aug 27 19:34:53.525: INFO: Found 2 stateful pods, waiting for 3
Aug 27 19:35:03.539: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 19:35:03.539: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 19:35:03.539: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 27 19:35:03.593: INFO: Updating stateful set ss2
Aug 27 19:35:03.620: INFO: Waiting for Pod statefulset-7035/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 27 19:35:13.639: INFO: Waiting for Pod statefulset-7035/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 27 19:35:23.667: INFO: Updating stateful set ss2
Aug 27 19:35:23.687: INFO: Waiting for StatefulSet statefulset-7035/ss2 to complete update
Aug 27 19:35:23.687: INFO: Waiting for Pod statefulset-7035/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 27 19:35:33.706: INFO: Waiting for StatefulSet statefulset-7035/ss2 to complete update
Aug 27 19:35:33.706: INFO: Waiting for Pod statefulset-7035/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug 27 19:35:43.708: INFO: Deleting all statefulset in ns statefulset-7035
Aug 27 19:35:43.716: INFO: Scaling statefulset ss2 to 0
Aug 27 19:36:03.760: INFO: Waiting for statefulset status.replicas updated to 0
Aug 27 19:36:03.770: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:36:03.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7035" for this suite.
Aug 27 19:36:11.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:36:12.190: INFO: namespace statefulset-7035 deletion completed in 8.365619269s

• [SLOW TEST:109.139 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:36:12.192: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6596
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 19:36:12.408: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ed3d65ec-c901-11e9-a9da-1ec1181dd093" in namespace "projected-6596" to be "success or failure"
Aug 27 19:36:12.424: INFO: Pod "downwardapi-volume-ed3d65ec-c901-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 15.552218ms
Aug 27 19:36:14.434: INFO: Pod "downwardapi-volume-ed3d65ec-c901-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02595124s
STEP: Saw pod success
Aug 27 19:36:14.434: INFO: Pod "downwardapi-volume-ed3d65ec-c901-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:36:14.445: INFO: Trying to get logs from node 10.134.235.118 pod downwardapi-volume-ed3d65ec-c901-11e9-a9da-1ec1181dd093 container client-container: <nil>
STEP: delete the pod
Aug 27 19:36:14.495: INFO: Waiting for pod downwardapi-volume-ed3d65ec-c901-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:36:14.505: INFO: Pod downwardapi-volume-ed3d65ec-c901-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:36:14.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6596" for this suite.
Aug 27 19:36:20.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:36:20.865: INFO: namespace projected-6596 deletion completed in 6.34669132s

• [SLOW TEST:8.673 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:36:20.865: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-2984
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 27 19:36:27.151: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2984 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 19:36:27.152: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 19:36:27.554: INFO: Exec stderr: ""
Aug 27 19:36:27.554: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2984 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 19:36:27.554: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 19:36:27.865: INFO: Exec stderr: ""
Aug 27 19:36:27.865: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2984 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 19:36:27.865: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 19:36:28.176: INFO: Exec stderr: ""
Aug 27 19:36:28.176: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2984 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 19:36:28.176: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 19:36:28.466: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 27 19:36:28.466: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2984 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 19:36:28.466: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 19:36:28.768: INFO: Exec stderr: ""
Aug 27 19:36:28.768: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2984 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 19:36:28.768: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 19:36:29.058: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 27 19:36:29.058: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2984 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 19:36:29.058: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 19:36:29.374: INFO: Exec stderr: ""
Aug 27 19:36:29.375: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2984 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 19:36:29.375: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 19:36:29.707: INFO: Exec stderr: ""
Aug 27 19:36:29.707: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2984 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 19:36:29.707: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 19:36:30.027: INFO: Exec stderr: ""
Aug 27 19:36:30.027: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2984 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 19:36:30.027: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 19:36:30.330: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:36:30.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2984" for this suite.
Aug 27 19:37:20.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:37:20.675: INFO: namespace e2e-kubelet-etc-hosts-2984 deletion completed in 50.333955368s

• [SLOW TEST:59.809 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:37:20.675: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8891
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-160c50ac-c902-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume configMaps
Aug 27 19:37:20.881: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-160dd4b5-c902-11e9-a9da-1ec1181dd093" in namespace "projected-8891" to be "success or failure"
Aug 27 19:37:20.893: INFO: Pod "pod-projected-configmaps-160dd4b5-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 11.370411ms
Aug 27 19:37:22.903: INFO: Pod "pod-projected-configmaps-160dd4b5-c902-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021733644s
STEP: Saw pod success
Aug 27 19:37:22.903: INFO: Pod "pod-projected-configmaps-160dd4b5-c902-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:37:22.912: INFO: Trying to get logs from node 10.134.235.118 pod pod-projected-configmaps-160dd4b5-c902-11e9-a9da-1ec1181dd093 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 19:37:22.963: INFO: Waiting for pod pod-projected-configmaps-160dd4b5-c902-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:37:22.977: INFO: Pod pod-projected-configmaps-160dd4b5-c902-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:37:22.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8891" for this suite.
Aug 27 19:37:29.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:37:29.295: INFO: namespace projected-8891 deletion completed in 6.309193137s

• [SLOW TEST:8.621 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:37:29.296: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-44
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 19:37:29.490: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b2f577a-c902-11e9-a9da-1ec1181dd093" in namespace "downward-api-44" to be "success or failure"
Aug 27 19:37:29.500: INFO: Pod "downwardapi-volume-1b2f577a-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 10.1908ms
Aug 27 19:37:31.511: INFO: Pod "downwardapi-volume-1b2f577a-c902-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021226211s
STEP: Saw pod success
Aug 27 19:37:31.511: INFO: Pod "downwardapi-volume-1b2f577a-c902-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:37:31.520: INFO: Trying to get logs from node 10.134.235.118 pod downwardapi-volume-1b2f577a-c902-11e9-a9da-1ec1181dd093 container client-container: <nil>
STEP: delete the pod
Aug 27 19:37:31.569: INFO: Waiting for pod downwardapi-volume-1b2f577a-c902-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:37:31.579: INFO: Pod downwardapi-volume-1b2f577a-c902-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:37:31.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-44" for this suite.
Aug 27 19:37:37.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:37:37.937: INFO: namespace downward-api-44 deletion completed in 6.349095439s

• [SLOW TEST:8.641 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:37:37.937: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9188
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2882
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4448
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:38:03.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9188" for this suite.
Aug 27 19:38:09.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:38:09.915: INFO: namespace namespaces-9188 deletion completed in 6.344230154s
STEP: Destroying namespace "nsdeletetest-2882" for this suite.
Aug 27 19:38:09.927: INFO: Namespace nsdeletetest-2882 was already deleted
STEP: Destroying namespace "nsdeletetest-4448" for this suite.
Aug 27 19:38:15.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:38:16.243: INFO: namespace nsdeletetest-4448 deletion completed in 6.316506564s

• [SLOW TEST:38.307 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:38:16.244: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6535
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 19:38:16.449: INFO: Waiting up to 5m0s for pod "downwardapi-volume-372c0c29-c902-11e9-a9da-1ec1181dd093" in namespace "downward-api-6535" to be "success or failure"
Aug 27 19:38:16.471: INFO: Pod "downwardapi-volume-372c0c29-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 22.770804ms
Aug 27 19:38:18.481: INFO: Pod "downwardapi-volume-372c0c29-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032079769s
Aug 27 19:38:20.493: INFO: Pod "downwardapi-volume-372c0c29-c902-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044467134s
STEP: Saw pod success
Aug 27 19:38:20.493: INFO: Pod "downwardapi-volume-372c0c29-c902-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:38:20.503: INFO: Trying to get logs from node 10.134.235.118 pod downwardapi-volume-372c0c29-c902-11e9-a9da-1ec1181dd093 container client-container: <nil>
STEP: delete the pod
Aug 27 19:38:20.566: INFO: Waiting for pod downwardapi-volume-372c0c29-c902-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:38:20.577: INFO: Pod downwardapi-volume-372c0c29-c902-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:38:20.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6535" for this suite.
Aug 27 19:38:26.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:38:27.000: INFO: namespace downward-api-6535 deletion completed in 6.410070953s

• [SLOW TEST:10.757 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:38:27.001: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9925
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug 27 19:38:29.737: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9925 pod-service-account-3de3b660-c902-11e9-a9da-1ec1181dd093 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 27 19:38:30.200: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9925 pod-service-account-3de3b660-c902-11e9-a9da-1ec1181dd093 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 27 19:38:30.629: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9925 pod-service-account-3de3b660-c902-11e9-a9da-1ec1181dd093 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:38:31.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9925" for this suite.
Aug 27 19:38:37.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:38:37.472: INFO: namespace svcaccounts-9925 deletion completed in 6.36156397s

• [SLOW TEST:10.471 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:38:37.474: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7779
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1384
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 27 19:38:37.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7779'
Aug 27 19:38:37.929: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 27 19:38:37.929: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1390
Aug 27 19:38:39.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 delete deployment e2e-test-nginx-deployment --namespace=kubectl-7779'
Aug 27 19:38:40.092: INFO: stderr: ""
Aug 27 19:38:40.092: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:38:40.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7779" for this suite.
Aug 27 19:39:04.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:39:04.467: INFO: namespace kubectl-7779 deletion completed in 24.366222205s

• [SLOW TEST:26.993 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:39:04.469: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5000
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1521
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 27 19:39:04.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-5000'
Aug 27 19:39:04.776: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 27 19:39:04.776: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1526
Aug 27 19:39:06.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 delete deployment e2e-test-nginx-deployment --namespace=kubectl-5000'
Aug 27 19:39:06.942: INFO: stderr: ""
Aug 27 19:39:06.942: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:39:06.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5000" for this suite.
Aug 27 19:39:12.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:39:13.246: INFO: namespace kubectl-5000 deletion completed in 6.29494615s

• [SLOW TEST:8.777 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:39:13.246: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7437
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Aug 27 19:39:13.429: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-561368167 proxy --unix-socket=/tmp/kubectl-proxy-unix755990638/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:39:13.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7437" for this suite.
Aug 27 19:39:19.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:39:19.838: INFO: namespace kubectl-7437 deletion completed in 6.327612153s

• [SLOW TEST:6.592 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:39:19.847: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3015
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 27 19:39:26.142: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 19:39:26.150: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 19:39:28.150: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 19:39:28.160: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 19:39:30.150: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 19:39:30.163: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 19:39:32.150: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 19:39:32.160: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 19:39:34.150: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 19:39:34.159: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 19:39:36.150: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 19:39:36.159: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 19:39:38.150: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 19:39:38.159: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 19:39:40.150: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 19:39:40.162: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 19:39:42.150: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 19:39:42.159: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 19:39:44.150: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 19:39:44.160: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 19:39:46.150: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 19:39:46.159: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:39:46.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3015" for this suite.
Aug 27 19:40:10.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:40:10.575: INFO: namespace container-lifecycle-hook-3015 deletion completed in 24.390016545s

• [SLOW TEST:50.729 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:40:10.576: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 19:40:10.777: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b517583-c902-11e9-a9da-1ec1181dd093" in namespace "downward-api-3773" to be "success or failure"
Aug 27 19:40:10.786: INFO: Pod "downwardapi-volume-7b517583-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 9.30866ms
Aug 27 19:40:12.797: INFO: Pod "downwardapi-volume-7b517583-c902-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019953328s
STEP: Saw pod success
Aug 27 19:40:12.797: INFO: Pod "downwardapi-volume-7b517583-c902-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:40:12.808: INFO: Trying to get logs from node 10.134.235.118 pod downwardapi-volume-7b517583-c902-11e9-a9da-1ec1181dd093 container client-container: <nil>
STEP: delete the pod
Aug 27 19:40:12.870: INFO: Waiting for pod downwardapi-volume-7b517583-c902-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:40:12.882: INFO: Pod downwardapi-volume-7b517583-c902-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:40:12.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3773" for this suite.
Aug 27 19:40:18.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:40:19.187: INFO: namespace downward-api-3773 deletion completed in 6.295758669s

• [SLOW TEST:8.611 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:40:19.187: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2105
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Aug 27 19:40:19.391: INFO: Waiting up to 5m0s for pod "var-expansion-8073ab85-c902-11e9-a9da-1ec1181dd093" in namespace "var-expansion-2105" to be "success or failure"
Aug 27 19:40:19.400: INFO: Pod "var-expansion-8073ab85-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 9.514004ms
Aug 27 19:40:21.410: INFO: Pod "var-expansion-8073ab85-c902-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018854203s
STEP: Saw pod success
Aug 27 19:40:21.410: INFO: Pod "var-expansion-8073ab85-c902-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:40:21.421: INFO: Trying to get logs from node 10.135.206.197 pod var-expansion-8073ab85-c902-11e9-a9da-1ec1181dd093 container dapi-container: <nil>
STEP: delete the pod
Aug 27 19:40:21.472: INFO: Waiting for pod var-expansion-8073ab85-c902-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:40:21.480: INFO: Pod var-expansion-8073ab85-c902-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:40:21.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2105" for this suite.
Aug 27 19:40:27.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:40:27.826: INFO: namespace var-expansion-2105 deletion completed in 6.336889125s

• [SLOW TEST:8.639 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:40:27.827: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7512
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 27 19:40:28.015: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Aug 27 19:40:37.121: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:40:37.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7512" for this suite.
Aug 27 19:40:43.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:40:43.485: INFO: namespace pods-7512 deletion completed in 6.346542678s

• [SLOW TEST:15.659 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:40:43.485: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-736
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-8eef4494-c902-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume secrets
Aug 27 19:40:43.695: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8ef0bfbb-c902-11e9-a9da-1ec1181dd093" in namespace "projected-736" to be "success or failure"
Aug 27 19:40:43.708: INFO: Pod "pod-projected-secrets-8ef0bfbb-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 13.198258ms
Aug 27 19:40:45.721: INFO: Pod "pod-projected-secrets-8ef0bfbb-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02577318s
Aug 27 19:40:47.730: INFO: Pod "pod-projected-secrets-8ef0bfbb-c902-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035139503s
STEP: Saw pod success
Aug 27 19:40:47.730: INFO: Pod "pod-projected-secrets-8ef0bfbb-c902-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:40:47.745: INFO: Trying to get logs from node 10.134.235.118 pod pod-projected-secrets-8ef0bfbb-c902-11e9-a9da-1ec1181dd093 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 27 19:40:47.792: INFO: Waiting for pod pod-projected-secrets-8ef0bfbb-c902-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:40:47.802: INFO: Pod pod-projected-secrets-8ef0bfbb-c902-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:40:47.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-736" for this suite.
Aug 27 19:40:53.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:40:54.133: INFO: namespace projected-736 deletion completed in 6.319650977s

• [SLOW TEST:10.647 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:40:54.134: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1237
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 27 19:40:54.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-1237'
Aug 27 19:40:54.442: INFO: stderr: ""
Aug 27 19:40:54.442: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Aug 27 19:40:59.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pod e2e-test-nginx-pod --namespace=kubectl-1237 -o json'
Aug 27 19:40:59.614: INFO: stderr: ""
Aug 27 19:40:59.614: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-08-27T19:40:54Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-1237\",\n        \"resourceVersion\": \"40616\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-1237/pods/e2e-test-nginx-pod\",\n        \"uid\": \"9552eb81-c902-11e9-9af8-1e15abefec17\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-7wk9b\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.135.206.197\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-7wk9b\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-7wk9b\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-27T19:40:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-27T19:40:56Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-27T19:40:56Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-27T19:40:54Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://470e70a556ab740f7219fafe1cc7aaa25847d4076d491a0466c2bbc33fdd5076\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-08-27T19:40:55Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.135.206.197\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.183.81\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-08-27T19:40:54Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 27 19:40:59.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 replace -f - --namespace=kubectl-1237'
Aug 27 19:40:59.843: INFO: stderr: ""
Aug 27 19:40:59.843: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Aug 27 19:40:59.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 delete pods e2e-test-nginx-pod --namespace=kubectl-1237'
Aug 27 19:41:06.105: INFO: stderr: ""
Aug 27 19:41:06.105: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:41:06.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1237" for this suite.
Aug 27 19:41:12.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:41:12.481: INFO: namespace kubectl-1237 deletion completed in 6.365354579s

• [SLOW TEST:18.347 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:41:12.483: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5478
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 19:41:12.685: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a037b4a2-c902-11e9-a9da-1ec1181dd093" in namespace "projected-5478" to be "success or failure"
Aug 27 19:41:12.700: INFO: Pod "downwardapi-volume-a037b4a2-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 14.067841ms
Aug 27 19:41:14.709: INFO: Pod "downwardapi-volume-a037b4a2-c902-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023895398s
STEP: Saw pod success
Aug 27 19:41:14.710: INFO: Pod "downwardapi-volume-a037b4a2-c902-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:41:14.720: INFO: Trying to get logs from node 10.135.206.197 pod downwardapi-volume-a037b4a2-c902-11e9-a9da-1ec1181dd093 container client-container: <nil>
STEP: delete the pod
Aug 27 19:41:14.769: INFO: Waiting for pod downwardapi-volume-a037b4a2-c902-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:41:14.779: INFO: Pod downwardapi-volume-a037b4a2-c902-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:41:14.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5478" for this suite.
Aug 27 19:41:20.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:41:21.092: INFO: namespace projected-5478 deletion completed in 6.301086854s

• [SLOW TEST:8.609 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:41:21.092: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5497
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 19:41:21.335: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"a558700f-c902-11e9-9af8-1e15abefec17", Controller:(*bool)(0xc00268a98a), BlockOwnerDeletion:(*bool)(0xc00268a98b)}}
Aug 27 19:41:21.347: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a554d934-c902-11e9-9af8-1e15abefec17", Controller:(*bool)(0xc002f1c456), BlockOwnerDeletion:(*bool)(0xc002f1c457)}}
Aug 27 19:41:21.357: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a5566e17-c902-11e9-9af8-1e15abefec17", Controller:(*bool)(0xc00305f4e6), BlockOwnerDeletion:(*bool)(0xc00305f4e7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:41:26.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5497" for this suite.
Aug 27 19:41:32.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:41:32.732: INFO: namespace gc-5497 deletion completed in 6.336699645s

• [SLOW TEST:11.640 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:41:32.733: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3389
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 19:41:32.927: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ac49288c-c902-11e9-a9da-1ec1181dd093" in namespace "projected-3389" to be "success or failure"
Aug 27 19:41:32.941: INFO: Pod "downwardapi-volume-ac49288c-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 13.254134ms
Aug 27 19:41:34.954: INFO: Pod "downwardapi-volume-ac49288c-c902-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026087572s
STEP: Saw pod success
Aug 27 19:41:34.954: INFO: Pod "downwardapi-volume-ac49288c-c902-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:41:34.966: INFO: Trying to get logs from node 10.134.235.118 pod downwardapi-volume-ac49288c-c902-11e9-a9da-1ec1181dd093 container client-container: <nil>
STEP: delete the pod
Aug 27 19:41:35.019: INFO: Waiting for pod downwardapi-volume-ac49288c-c902-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:41:35.030: INFO: Pod downwardapi-volume-ac49288c-c902-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:41:35.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3389" for this suite.
Aug 27 19:41:41.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:41:41.363: INFO: namespace projected-3389 deletion completed in 6.323701127s

• [SLOW TEST:8.631 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:41:41.364: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3369
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-3369/secret-test-b1703aad-c902-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume secrets
Aug 27 19:41:41.585: INFO: Waiting up to 5m0s for pod "pod-configmaps-b171f38e-c902-11e9-a9da-1ec1181dd093" in namespace "secrets-3369" to be "success or failure"
Aug 27 19:41:41.596: INFO: Pod "pod-configmaps-b171f38e-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 11.248018ms
Aug 27 19:41:43.605: INFO: Pod "pod-configmaps-b171f38e-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020080641s
Aug 27 19:41:45.614: INFO: Pod "pod-configmaps-b171f38e-c902-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029771542s
STEP: Saw pod success
Aug 27 19:41:45.614: INFO: Pod "pod-configmaps-b171f38e-c902-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:41:45.626: INFO: Trying to get logs from node 10.135.206.222 pod pod-configmaps-b171f38e-c902-11e9-a9da-1ec1181dd093 container env-test: <nil>
STEP: delete the pod
Aug 27 19:41:45.684: INFO: Waiting for pod pod-configmaps-b171f38e-c902-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:41:45.692: INFO: Pod pod-configmaps-b171f38e-c902-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:41:45.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3369" for this suite.
Aug 27 19:41:51.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:41:52.064: INFO: namespace secrets-3369 deletion completed in 6.362698489s

• [SLOW TEST:10.700 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:41:52.065: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6399
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 19:41:52.268: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b7cff464-c902-11e9-a9da-1ec1181dd093" in namespace "downward-api-6399" to be "success or failure"
Aug 27 19:41:52.280: INFO: Pod "downwardapi-volume-b7cff464-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 12.12456ms
Aug 27 19:41:54.289: INFO: Pod "downwardapi-volume-b7cff464-c902-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021511019s
STEP: Saw pod success
Aug 27 19:41:54.289: INFO: Pod "downwardapi-volume-b7cff464-c902-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:41:54.299: INFO: Trying to get logs from node 10.135.206.222 pod downwardapi-volume-b7cff464-c902-11e9-a9da-1ec1181dd093 container client-container: <nil>
STEP: delete the pod
Aug 27 19:41:54.358: INFO: Waiting for pod downwardapi-volume-b7cff464-c902-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:41:54.373: INFO: Pod downwardapi-volume-b7cff464-c902-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:41:54.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6399" for this suite.
Aug 27 19:42:00.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:42:00.685: INFO: namespace downward-api-6399 deletion completed in 6.302676299s

• [SLOW TEST:8.620 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:42:00.685: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6017
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 19:42:00.884: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bcf2ca40-c902-11e9-a9da-1ec1181dd093" in namespace "projected-6017" to be "success or failure"
Aug 27 19:42:00.892: INFO: Pod "downwardapi-volume-bcf2ca40-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 8.638945ms
Aug 27 19:42:02.902: INFO: Pod "downwardapi-volume-bcf2ca40-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018185486s
Aug 27 19:42:04.911: INFO: Pod "downwardapi-volume-bcf2ca40-c902-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027442741s
STEP: Saw pod success
Aug 27 19:42:04.911: INFO: Pod "downwardapi-volume-bcf2ca40-c902-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:42:04.920: INFO: Trying to get logs from node 10.134.235.118 pod downwardapi-volume-bcf2ca40-c902-11e9-a9da-1ec1181dd093 container client-container: <nil>
STEP: delete the pod
Aug 27 19:42:04.968: INFO: Waiting for pod downwardapi-volume-bcf2ca40-c902-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:42:04.978: INFO: Pod downwardapi-volume-bcf2ca40-c902-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:42:04.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6017" for this suite.
Aug 27 19:42:11.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:42:11.506: INFO: namespace projected-6017 deletion completed in 6.519146141s

• [SLOW TEST:10.821 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:42:11.507: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7370
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7370.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7370.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7370.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7370.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7370.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7370.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 27 19:42:25.865: INFO: DNS probes using dns-7370/dns-test-c3670783-c902-11e9-a9da-1ec1181dd093 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:42:25.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7370" for this suite.
Aug 27 19:42:31.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:42:32.230: INFO: namespace dns-7370 deletion completed in 6.320447752s

• [SLOW TEST:20.723 seconds]
[sig-network] DNS
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:42:32.230: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7478
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 19:42:32.418: INFO: Creating deployment "nginx-deployment"
Aug 27 19:42:32.429: INFO: Waiting for observed generation 1
Aug 27 19:42:34.447: INFO: Waiting for all required pods to come up
Aug 27 19:42:34.459: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 27 19:42:36.494: INFO: Waiting for deployment "nginx-deployment" to complete
Aug 27 19:42:36.508: INFO: Updating deployment "nginx-deployment" with a non-existent image
Aug 27 19:42:36.529: INFO: Updating deployment nginx-deployment
Aug 27 19:42:36.529: INFO: Waiting for observed generation 2
Aug 27 19:42:38.551: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 27 19:42:38.557: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 27 19:42:38.562: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug 27 19:42:38.580: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 27 19:42:38.580: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 27 19:42:38.586: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug 27 19:42:38.596: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Aug 27 19:42:38.596: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Aug 27 19:42:38.616: INFO: Updating deployment nginx-deployment
Aug 27 19:42:38.616: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Aug 27 19:42:38.632: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 27 19:42:38.643: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug 27 19:42:38.670: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-7478,SelfLink:/apis/apps/v1/namespaces/deployment-7478/deployments/nginx-deployment,UID:cfbc193e-c902-11e9-9af8-1e15abefec17,ResourceVersion:41354,Generation:3,CreationTimestamp:2019-08-27 19:42:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-08-27 19:42:36 +0000 UTC 2019-08-27 19:42:32 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-b79c9d74d" is progressing.} {Available False 2019-08-27 19:42:38 +0000 UTC 2019-08-27 19:42:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Aug 27 19:42:38.678: INFO: New ReplicaSet "nginx-deployment-b79c9d74d" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d,GenerateName:,Namespace:deployment-7478,SelfLink:/apis/apps/v1/namespaces/deployment-7478/replicasets/nginx-deployment-b79c9d74d,UID:d22f997e-c902-11e9-9af8-1e15abefec17,ResourceVersion:41340,Generation:3,CreationTimestamp:2019-08-27 19:42:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment cfbc193e-c902-11e9-9af8-1e15abefec17 0xc0032e5e37 0xc0032e5e38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 27 19:42:38.678: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Aug 27 19:42:38.678: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5,GenerateName:,Namespace:deployment-7478,SelfLink:/apis/apps/v1/namespaces/deployment-7478/replicasets/nginx-deployment-85db8c99c5,UID:cfbd83c3-c902-11e9-9af8-1e15abefec17,ResourceVersion:41393,Generation:3,CreationTimestamp:2019-08-27 19:42:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment cfbc193e-c902-11e9-9af8-1e15abefec17 0xc0032e5d57 0xc0032e5d58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Aug 27 19:42:38.693: INFO: Pod "nginx-deployment-85db8c99c5-5wmk4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-5wmk4,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-5wmk4,UID:cfc2d129-c902-11e9-9af8-1e15abefec17,ResourceVersion:41236,Generation:0,CreationTimestamp:2019-08-27 19:42:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002d5ee77 0xc002d5ee78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d5eef0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d5ef10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:32 +0000 UTC  }],Message:,Reason:,HostIP:10.135.206.222,PodIP:172.30.172.10,StartTime:2019-08-27 19:42:32 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-27 19:42:34 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://52217dce713c56969de442c7a8873590f29f9efc5fe45605f67913ba470ed104}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.693: INFO: Pod "nginx-deployment-85db8c99c5-b8f95" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-b8f95,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-b8f95,UID:d371d54f-c902-11e9-9af8-1e15abefec17,ResourceVersion:41373,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002d5efe7 0xc002d5efe8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d5f060} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d5f080}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.693: INFO: Pod "nginx-deployment-85db8c99c5-dpksq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-dpksq,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-dpksq,UID:d3702ea3-c902-11e9-9af8-1e15abefec17,ResourceVersion:41356,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002d5f100 0xc002d5f101}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.197,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d5f170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d5f190}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.693: INFO: Pod "nginx-deployment-85db8c99c5-dq7fl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-dq7fl,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-dq7fl,UID:d3735ca6-c902-11e9-9af8-1e15abefec17,ResourceVersion:41388,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002d5f210 0xc002d5f211}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.235.118,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d5f280} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d5f2a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.693: INFO: Pod "nginx-deployment-85db8c99c5-fx5sx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-fx5sx,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-fx5sx,UID:cfc0fa74-c902-11e9-9af8-1e15abefec17,ResourceVersion:41210,Generation:0,CreationTimestamp:2019-08-27 19:42:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002d5f320 0xc002d5f321}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.235.118,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d5f390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d5f3b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:32 +0000 UTC  }],Message:,Reason:,HostIP:10.134.235.118,PodIP:172.30.79.118,StartTime:2019-08-27 19:42:32 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-27 19:42:33 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://141fb21763a763cd32cb76ed15c078affc5297a9e1209c186a56f9cd9085649e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.694: INFO: Pod "nginx-deployment-85db8c99c5-hmdt7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-hmdt7,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-hmdt7,UID:d36eaa2d-c902-11e9-9af8-1e15abefec17,ResourceVersion:41382,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002d5f487 0xc002d5f488}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.235.118,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d5f500} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d5f520}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:10.134.235.118,PodIP:,StartTime:2019-08-27 19:42:38 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.694: INFO: Pod "nginx-deployment-85db8c99c5-hmzvt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-hmzvt,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-hmzvt,UID:d3736f22-c902-11e9-9af8-1e15abefec17,ResourceVersion:41390,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002d5f5e7 0xc002d5f5e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d5f660} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d5f680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.694: INFO: Pod "nginx-deployment-85db8c99c5-hv6wn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-hv6wn,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-hv6wn,UID:cfc2c74a-c902-11e9-9af8-1e15abefec17,ResourceVersion:41242,Generation:0,CreationTimestamp:2019-08-27 19:42:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002d5f700 0xc002d5f701}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d5f770} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d5f790}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:32 +0000 UTC  }],Message:,Reason:,HostIP:10.135.206.222,PodIP:172.30.172.8,StartTime:2019-08-27 19:42:32 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-27 19:42:34 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://7e2cf4cac91a52ede5e629e1fa952961c0b1a2ce1343ed98a4b993b589765185}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.694: INFO: Pod "nginx-deployment-85db8c99c5-j296v" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-j296v,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-j296v,UID:cfc110f0-c902-11e9-9af8-1e15abefec17,ResourceVersion:41233,Generation:0,CreationTimestamp:2019-08-27 19:42:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002d5f867 0xc002d5f868}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d5f8e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d5f910}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:32 +0000 UTC  }],Message:,Reason:,HostIP:10.135.206.222,PodIP:172.30.172.13,StartTime:2019-08-27 19:42:32 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-27 19:42:33 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://727cbde9fb9319b84b1e997be5fe994ffa4c2d0c01fbbcad310fb0251ed5c767}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.694: INFO: Pod "nginx-deployment-85db8c99c5-qrr4d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-qrr4d,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-qrr4d,UID:d3702bf2-c902-11e9-9af8-1e15abefec17,ResourceVersion:41355,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002d5f9e7 0xc002d5f9e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.235.118,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d5fa60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d5fa80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.694: INFO: Pod "nginx-deployment-85db8c99c5-qs9d5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-qs9d5,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-qs9d5,UID:d371d77f-c902-11e9-9af8-1e15abefec17,ResourceVersion:41368,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002d5fb00 0xc002d5fb01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.235.118,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d5fb70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d5fb90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.694: INFO: Pod "nginx-deployment-85db8c99c5-r2fqj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-r2fqj,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-r2fqj,UID:cfc46723-c902-11e9-9af8-1e15abefec17,ResourceVersion:41252,Generation:0,CreationTimestamp:2019-08-27 19:42:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002d5fc10 0xc002d5fc11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.235.118,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d5fc80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d5fca0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:32 +0000 UTC  }],Message:,Reason:,HostIP:10.134.235.118,PodIP:172.30.79.122,StartTime:2019-08-27 19:42:32 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-27 19:42:34 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://15e357634c3692a1b16459a2383b91756b4a8aff2f5186b8086fef8004e36365}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.695: INFO: Pod "nginx-deployment-85db8c99c5-sqs8h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-sqs8h,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-sqs8h,UID:d371c64c-c902-11e9-9af8-1e15abefec17,ResourceVersion:41371,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002d5fd77 0xc002d5fd78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.235.118,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d5fe00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d5fe20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.695: INFO: Pod "nginx-deployment-85db8c99c5-t84lg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-t84lg,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-t84lg,UID:d3737b7f-c902-11e9-9af8-1e15abefec17,ResourceVersion:41391,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002d5fea0 0xc002d5fea1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.197,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d5ff10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d5ff30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.695: INFO: Pod "nginx-deployment-85db8c99c5-t96xs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-t96xs,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-t96xs,UID:d3735d98-c902-11e9-9af8-1e15abefec17,ResourceVersion:41387,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002d5ffb0 0xc002d5ffb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.197,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002530030} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002530060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.695: INFO: Pod "nginx-deployment-85db8c99c5-w592c" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-w592c,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-w592c,UID:cfc2c3a2-c902-11e9-9af8-1e15abefec17,ResourceVersion:41226,Generation:0,CreationTimestamp:2019-08-27 19:42:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc0025300e0 0xc0025300e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.197,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002530150} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002530170}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:32 +0000 UTC  }],Message:,Reason:,HostIP:10.135.206.197,PodIP:172.30.183.83,StartTime:2019-08-27 19:42:32 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-27 19:42:34 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://10f25c28628850d095e76338918aef10971e4dafe30700b0f0b8472c1c063bba}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.695: INFO: Pod "nginx-deployment-85db8c99c5-xjpgn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-xjpgn,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-xjpgn,UID:cfbf8f4b-c902-11e9-9af8-1e15abefec17,ResourceVersion:41229,Generation:0,CreationTimestamp:2019-08-27 19:42:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002530287 0xc002530288}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.197,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002530300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002530320}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:32 +0000 UTC  }],Message:,Reason:,HostIP:10.135.206.197,PodIP:172.30.183.86,StartTime:2019-08-27 19:42:32 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-27 19:42:33 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://0c4e90eda1c9e3836e94418eecfa5e33605ef7eb7deb404af920b42fb5bf1c5a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.695: INFO: Pod "nginx-deployment-85db8c99c5-xwqxs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-xwqxs,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-xwqxs,UID:d371d94f-c902-11e9-9af8-1e15abefec17,ResourceVersion:41372,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002530417 0xc002530418}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002530490} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025304c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.695: INFO: Pod "nginx-deployment-85db8c99c5-z9fw8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-z9fw8,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-z9fw8,UID:d37357d4-c902-11e9-9af8-1e15abefec17,ResourceVersion:41386,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002530540 0xc002530541}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.197,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025305b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025305d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.695: INFO: Pod "nginx-deployment-85db8c99c5-zr7fr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-zr7fr,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-85db8c99c5-zr7fr,UID:cfc46205-c902-11e9-9af8-1e15abefec17,ResourceVersion:41239,Generation:0,CreationTimestamp:2019-08-27 19:42:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 cfbd83c3-c902-11e9-9af8-1e15abefec17 0xc002530650 0xc002530651}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025306d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002530710}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:32 +0000 UTC  }],Message:,Reason:,HostIP:10.135.206.222,PodIP:172.30.172.11,StartTime:2019-08-27 19:42:32 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-27 19:42:34 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://f838a74a8b7b9bbb44cdf06601f7caaed49bbf1289086ecd4145dad1c51d6651}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.696: INFO: Pod "nginx-deployment-b79c9d74d-4g4h8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-4g4h8,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-b79c9d74d-4g4h8,UID:d372c643-c902-11e9-9af8-1e15abefec17,ResourceVersion:41380,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d d22f997e-c902-11e9-9af8-1e15abefec17 0xc002530817 0xc002530818}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002530890} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025308b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.696: INFO: Pod "nginx-deployment-b79c9d74d-5sj9n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-5sj9n,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-b79c9d74d-5sj9n,UID:d374d854-c902-11e9-9af8-1e15abefec17,ResourceVersion:41398,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d d22f997e-c902-11e9-9af8-1e15abefec17 0xc002530930 0xc002530931}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025309b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025309d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.696: INFO: Pod "nginx-deployment-b79c9d74d-9vxhg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-9vxhg,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-b79c9d74d-9vxhg,UID:d370d695-c902-11e9-9af8-1e15abefec17,ResourceVersion:41362,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d d22f997e-c902-11e9-9af8-1e15abefec17 0xc002530a50 0xc002530a51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.235.118,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002530ad0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002530b00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.696: INFO: Pod "nginx-deployment-b79c9d74d-bb484" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-bb484,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-b79c9d74d-bb484,UID:d232b456-c902-11e9-9af8-1e15abefec17,ResourceVersion:41286,Generation:0,CreationTimestamp:2019-08-27 19:42:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d d22f997e-c902-11e9-9af8-1e15abefec17 0xc002530b80 0xc002530b81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.197,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002530c00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002530c20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC  }],Message:,Reason:,HostIP:10.135.206.197,PodIP:,StartTime:2019-08-27 19:42:36 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.696: INFO: Pod "nginx-deployment-b79c9d74d-ggwcx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-ggwcx,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-b79c9d74d-ggwcx,UID:d2310f84-c902-11e9-9af8-1e15abefec17,ResourceVersion:41280,Generation:0,CreationTimestamp:2019-08-27 19:42:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d d22f997e-c902-11e9-9af8-1e15abefec17 0xc002530cf0 0xc002530cf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.235.118,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002530d80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002530da0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC  }],Message:,Reason:,HostIP:10.134.235.118,PodIP:,StartTime:2019-08-27 19:42:36 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.697: INFO: Pod "nginx-deployment-b79c9d74d-j8rwf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-j8rwf,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-b79c9d74d-j8rwf,UID:d23982d7-c902-11e9-9af8-1e15abefec17,ResourceVersion:41308,Generation:0,CreationTimestamp:2019-08-27 19:42:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d d22f997e-c902-11e9-9af8-1e15abefec17 0xc002530e70 0xc002530e71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002530ef0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002530f10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC  }],Message:,Reason:,HostIP:10.135.206.222,PodIP:,StartTime:2019-08-27 19:42:36 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.697: INFO: Pod "nginx-deployment-b79c9d74d-kxk4d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-kxk4d,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-b79c9d74d-kxk4d,UID:d36f8a95-c902-11e9-9af8-1e15abefec17,ResourceVersion:41395,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d d22f997e-c902-11e9-9af8-1e15abefec17 0xc002530fe0 0xc002530fe1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.197,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002531060} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002531080}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:10.135.206.197,PodIP:,StartTime:2019-08-27 19:42:38 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.698: INFO: Pod "nginx-deployment-b79c9d74d-mqbmk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-mqbmk,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-b79c9d74d-mqbmk,UID:d237d87e-c902-11e9-9af8-1e15abefec17,ResourceVersion:41306,Generation:0,CreationTimestamp:2019-08-27 19:42:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d d22f997e-c902-11e9-9af8-1e15abefec17 0xc002531150 0xc002531151}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.235.118,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025311f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002531210}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC  }],Message:,Reason:,HostIP:10.134.235.118,PodIP:,StartTime:2019-08-27 19:42:36 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.698: INFO: Pod "nginx-deployment-b79c9d74d-prmhz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-prmhz,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-b79c9d74d-prmhz,UID:d372e2ba-c902-11e9-9af8-1e15abefec17,ResourceVersion:41384,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d d22f997e-c902-11e9-9af8-1e15abefec17 0xc0025312e0 0xc0025312e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.235.118,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025313d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002531400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.698: INFO: Pod "nginx-deployment-b79c9d74d-qjsgj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-qjsgj,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-b79c9d74d-qjsgj,UID:d232dabf-c902-11e9-9af8-1e15abefec17,ResourceVersion:41285,Generation:0,CreationTimestamp:2019-08-27 19:42:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d d22f997e-c902-11e9-9af8-1e15abefec17 0xc0025314c0 0xc0025314c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025315b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025315d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:36 +0000 UTC  }],Message:,Reason:,HostIP:10.135.206.222,PodIP:,StartTime:2019-08-27 19:42:36 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.698: INFO: Pod "nginx-deployment-b79c9d74d-qq4lz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-qq4lz,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-b79c9d74d-qq4lz,UID:d372d123-c902-11e9-9af8-1e15abefec17,ResourceVersion:41383,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d d22f997e-c902-11e9-9af8-1e15abefec17 0xc002531810 0xc002531811}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.197,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002531890} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025318b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.698: INFO: Pod "nginx-deployment-b79c9d74d-tjw5z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-tjw5z,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-b79c9d74d-tjw5z,UID:d370d99d-c902-11e9-9af8-1e15abefec17,ResourceVersion:41363,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d d22f997e-c902-11e9-9af8-1e15abefec17 0xc002531940 0xc002531941}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.197,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002531a10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002531a40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 19:42:38.698: INFO: Pod "nginx-deployment-b79c9d74d-zrkfh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-zrkfh,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7478,SelfLink:/api/v1/namespaces/deployment-7478/pods/nginx-deployment-b79c9d74d-zrkfh,UID:d372ccbc-c902-11e9-9af8-1e15abefec17,ResourceVersion:41381,Generation:0,CreationTimestamp:2019-08-27 19:42:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d d22f997e-c902-11e9-9af8-1e15abefec17 0xc002531b00 0xc002531b01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-b2qln {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-b2qln,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-b2qln true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.206.222,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002531ba0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002531bc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 19:42:38 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:42:38.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7478" for this suite.
Aug 27 19:42:46.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:42:47.041: INFO: namespace deployment-7478 deletion completed in 8.334657652s

• [SLOW TEST:14.810 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:42:47.041: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3020
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-3020/configmap-test-d893f95c-c902-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume configMaps
Aug 27 19:42:47.256: INFO: Waiting up to 5m0s for pod "pod-configmaps-d89588de-c902-11e9-a9da-1ec1181dd093" in namespace "configmap-3020" to be "success or failure"
Aug 27 19:42:47.284: INFO: Pod "pod-configmaps-d89588de-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 27.982549ms
Aug 27 19:42:49.293: INFO: Pod "pod-configmaps-d89588de-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037278853s
Aug 27 19:42:51.303: INFO: Pod "pod-configmaps-d89588de-c902-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046865914s
STEP: Saw pod success
Aug 27 19:42:51.303: INFO: Pod "pod-configmaps-d89588de-c902-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:42:51.311: INFO: Trying to get logs from node 10.135.206.197 pod pod-configmaps-d89588de-c902-11e9-a9da-1ec1181dd093 container env-test: <nil>
STEP: delete the pod
Aug 27 19:42:51.357: INFO: Waiting for pod pod-configmaps-d89588de-c902-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:42:51.365: INFO: Pod pod-configmaps-d89588de-c902-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:42:51.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3020" for this suite.
Aug 27 19:42:57.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:42:57.760: INFO: namespace configmap-3020 deletion completed in 6.386986292s

• [SLOW TEST:10.719 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:42:57.762: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4884
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Aug 27 19:42:57.964: INFO: Waiting up to 5m0s for pod "client-containers-def7bd70-c902-11e9-a9da-1ec1181dd093" in namespace "containers-4884" to be "success or failure"
Aug 27 19:42:57.975: INFO: Pod "client-containers-def7bd70-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 11.652661ms
Aug 27 19:42:59.984: INFO: Pod "client-containers-def7bd70-c902-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020696439s
Aug 27 19:43:01.994: INFO: Pod "client-containers-def7bd70-c902-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030167131s
STEP: Saw pod success
Aug 27 19:43:01.994: INFO: Pod "client-containers-def7bd70-c902-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:43:02.003: INFO: Trying to get logs from node 10.135.206.222 pod client-containers-def7bd70-c902-11e9-a9da-1ec1181dd093 container test-container: <nil>
STEP: delete the pod
Aug 27 19:43:02.052: INFO: Waiting for pod client-containers-def7bd70-c902-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:43:02.064: INFO: Pod client-containers-def7bd70-c902-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:43:02.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4884" for this suite.
Aug 27 19:43:08.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:43:08.477: INFO: namespace containers-4884 deletion completed in 6.403059047s

• [SLOW TEST:10.716 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:43:08.478: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6972
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 27 19:43:08.689: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6972,SelfLink:/api/v1/namespaces/watch-6972/configmaps/e2e-watch-test-watch-closed,UID:e55681de-c902-11e9-9af8-1e15abefec17,ResourceVersion:41948,Generation:0,CreationTimestamp:2019-08-27 19:43:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 27 19:43:08.689: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6972,SelfLink:/api/v1/namespaces/watch-6972/configmaps/e2e-watch-test-watch-closed,UID:e55681de-c902-11e9-9af8-1e15abefec17,ResourceVersion:41949,Generation:0,CreationTimestamp:2019-08-27 19:43:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 27 19:43:08.733: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6972,SelfLink:/api/v1/namespaces/watch-6972/configmaps/e2e-watch-test-watch-closed,UID:e55681de-c902-11e9-9af8-1e15abefec17,ResourceVersion:41950,Generation:0,CreationTimestamp:2019-08-27 19:43:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 27 19:43:08.733: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6972,SelfLink:/api/v1/namespaces/watch-6972/configmaps/e2e-watch-test-watch-closed,UID:e55681de-c902-11e9-9af8-1e15abefec17,ResourceVersion:41951,Generation:0,CreationTimestamp:2019-08-27 19:43:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:43:08.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6972" for this suite.
Aug 27 19:43:14.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:43:15.067: INFO: namespace watch-6972 deletion completed in 6.325510321s

• [SLOW TEST:6.589 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:43:15.067: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-393
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:43:17.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-393" for this suite.
Aug 27 19:44:11.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:44:11.757: INFO: namespace kubelet-test-393 deletion completed in 54.341396901s

• [SLOW TEST:56.690 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:44:11.758: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-971
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 27 19:44:11.954: INFO: Waiting up to 5m0s for pod "pod-0b123ca5-c903-11e9-a9da-1ec1181dd093" in namespace "emptydir-971" to be "success or failure"
Aug 27 19:44:11.964: INFO: Pod "pod-0b123ca5-c903-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 9.788016ms
Aug 27 19:44:13.974: INFO: Pod "pod-0b123ca5-c903-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020658774s
Aug 27 19:44:15.987: INFO: Pod "pod-0b123ca5-c903-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033319301s
STEP: Saw pod success
Aug 27 19:44:15.987: INFO: Pod "pod-0b123ca5-c903-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:44:15.998: INFO: Trying to get logs from node 10.135.206.222 pod pod-0b123ca5-c903-11e9-a9da-1ec1181dd093 container test-container: <nil>
STEP: delete the pod
Aug 27 19:44:16.053: INFO: Waiting for pod pod-0b123ca5-c903-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:44:16.061: INFO: Pod pod-0b123ca5-c903-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:44:16.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-971" for this suite.
Aug 27 19:44:22.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:44:22.485: INFO: namespace emptydir-971 deletion completed in 6.410224434s

• [SLOW TEST:10.727 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:44:22.485: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2496
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 19:44:22.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 version'
Aug 27 19:44:22.774: INFO: stderr: ""
Aug 27 19:44:22.774: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.6\", GitCommit:\"96fac5cd13a5dc064f7d9f4f23030a6aeface6cc\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:13:49Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.6+IKS\", GitCommit:\"44b769243cf9b3fe09c1105a4a8749e8ff5f4ba8\", GitTreeState:\"clean\", BuildDate:\"2019-08-21T12:48:49Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:44:22.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2496" for this suite.
Aug 27 19:44:28.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:44:29.077: INFO: namespace kubectl-2496 deletion completed in 6.295010004s

• [SLOW TEST:6.592 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:44:29.078: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5539
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 19:44:29.284: INFO: (0) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 18.186825ms)
Aug 27 19:44:29.296: INFO: (1) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 11.785183ms)
Aug 27 19:44:29.306: INFO: (2) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.698162ms)
Aug 27 19:44:29.317: INFO: (3) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.405161ms)
Aug 27 19:44:29.326: INFO: (4) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.390271ms)
Aug 27 19:44:29.336: INFO: (5) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.932797ms)
Aug 27 19:44:29.345: INFO: (6) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.300454ms)
Aug 27 19:44:29.358: INFO: (7) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 12.447089ms)
Aug 27 19:44:29.369: INFO: (8) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 11.185613ms)
Aug 27 19:44:29.379: INFO: (9) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.886154ms)
Aug 27 19:44:29.394: INFO: (10) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 15.293772ms)
Aug 27 19:44:29.404: INFO: (11) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.095395ms)
Aug 27 19:44:29.416: INFO: (12) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 11.77679ms)
Aug 27 19:44:29.429: INFO: (13) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 12.505792ms)
Aug 27 19:44:29.439: INFO: (14) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.022903ms)
Aug 27 19:44:29.449: INFO: (15) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.270832ms)
Aug 27 19:44:29.459: INFO: (16) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.374771ms)
Aug 27 19:44:29.468: INFO: (17) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.260421ms)
Aug 27 19:44:29.478: INFO: (18) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.61105ms)
Aug 27 19:44:29.488: INFO: (19) /api/v1/nodes/10.134.235.118/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.649004ms)
[AfterEach] version v1
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:44:29.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5539" for this suite.
Aug 27 19:44:35.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:44:35.840: INFO: namespace proxy-5539 deletion completed in 6.345541208s

• [SLOW TEST:6.763 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:44:35.843: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6447
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1256
STEP: creating an rc
Aug 27 19:44:36.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 create -f - --namespace=kubectl-6447'
Aug 27 19:44:36.327: INFO: stderr: ""
Aug 27 19:44:36.327: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Aug 27 19:44:37.337: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 19:44:37.337: INFO: Found 0 / 1
Aug 27 19:44:38.338: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 19:44:38.338: INFO: Found 1 / 1
Aug 27 19:44:38.338: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 27 19:44:38.349: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 19:44:38.349: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Aug 27 19:44:38.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 logs redis-master-dd2rc redis-master --namespace=kubectl-6447'
Aug 27 19:44:38.537: INFO: stderr: ""
Aug 27 19:44:38.537: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Aug 19:44:37.692 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Aug 19:44:37.692 # Server started, Redis version 3.2.12\n1:M 27 Aug 19:44:37.692 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Aug 19:44:37.692 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Aug 27 19:44:38.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 log redis-master-dd2rc redis-master --namespace=kubectl-6447 --tail=1'
Aug 27 19:44:38.709: INFO: stderr: ""
Aug 27 19:44:38.710: INFO: stdout: "1:M 27 Aug 19:44:37.692 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Aug 27 19:44:38.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 log redis-master-dd2rc redis-master --namespace=kubectl-6447 --limit-bytes=1'
Aug 27 19:44:38.850: INFO: stderr: ""
Aug 27 19:44:38.850: INFO: stdout: " "
STEP: exposing timestamps
Aug 27 19:44:38.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 log redis-master-dd2rc redis-master --namespace=kubectl-6447 --tail=1 --timestamps'
Aug 27 19:44:38.993: INFO: stderr: ""
Aug 27 19:44:38.993: INFO: stdout: "2019-08-27T19:44:37.692580325Z 1:M 27 Aug 19:44:37.692 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Aug 27 19:44:41.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 log redis-master-dd2rc redis-master --namespace=kubectl-6447 --since=1s'
Aug 27 19:44:41.633: INFO: stderr: ""
Aug 27 19:44:41.633: INFO: stdout: ""
Aug 27 19:44:41.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 log redis-master-dd2rc redis-master --namespace=kubectl-6447 --since=24h'
Aug 27 19:44:41.829: INFO: stderr: ""
Aug 27 19:44:41.829: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Aug 19:44:37.692 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Aug 19:44:37.692 # Server started, Redis version 3.2.12\n1:M 27 Aug 19:44:37.692 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Aug 19:44:37.692 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
STEP: using delete to clean up resources
Aug 27 19:44:41.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 delete --grace-period=0 --force -f - --namespace=kubectl-6447'
Aug 27 19:44:41.944: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 27 19:44:41.944: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Aug 27 19:44:41.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get rc,svc -l name=nginx --no-headers --namespace=kubectl-6447'
Aug 27 19:44:42.075: INFO: stderr: "No resources found.\n"
Aug 27 19:44:42.075: INFO: stdout: ""
Aug 27 19:44:42.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-561368167 get pods -l name=nginx --namespace=kubectl-6447 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 27 19:44:42.196: INFO: stderr: ""
Aug 27 19:44:42.196: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:44:42.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6447" for this suite.
Aug 27 19:44:48.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:44:48.603: INFO: namespace kubectl-6447 deletion completed in 6.397664908s

• [SLOW TEST:12.761 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:44:48.605: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-451
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 27 19:44:54.908: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 19:44:54.917: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 19:44:56.917: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 19:44:56.928: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 19:44:58.917: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 19:44:58.928: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 19:45:00.917: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 19:45:00.927: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 19:45:02.917: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 19:45:02.927: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 19:45:04.917: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 19:45:04.934: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 19:45:06.918: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 19:45:06.927: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 19:45:08.917: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 19:45:08.931: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 19:45:10.917: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 19:45:10.926: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 19:45:12.918: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 19:45:12.927: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 19:45:14.917: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 19:45:14.926: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 19:45:16.918: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 19:45:16.927: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 19:45:18.917: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 19:45:18.927: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 19:45:20.917: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 19:45:20.927: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 19:45:22.917: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 19:45:22.928: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:45:22.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-451" for this suite.
Aug 27 19:45:46.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:45:47.282: INFO: namespace container-lifecycle-hook-451 deletion completed in 24.345043051s

• [SLOW TEST:58.678 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:45:47.282: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2847
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 19:45:47.471: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:45:49.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2847" for this suite.
Aug 27 19:46:37.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:46:38.119: INFO: namespace pods-2847 deletion completed in 48.349254118s

• [SLOW TEST:50.837 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:46:38.120: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7793
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-7793
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7793 to expose endpoints map[]
Aug 27 19:46:38.327: INFO: Get endpoints failed (8.894877ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Aug 27 19:46:39.336: INFO: successfully validated that service multi-endpoint-test in namespace services-7793 exposes endpoints map[] (1.018245313s elapsed)
STEP: Creating pod pod1 in namespace services-7793
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7793 to expose endpoints map[pod1:[100]]
Aug 27 19:46:42.435: INFO: successfully validated that service multi-endpoint-test in namespace services-7793 exposes endpoints map[pod1:[100]] (3.081947328s elapsed)
STEP: Creating pod pod2 in namespace services-7793
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7793 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 27 19:46:44.551: INFO: successfully validated that service multi-endpoint-test in namespace services-7793 exposes endpoints map[pod1:[100] pod2:[101]] (2.09986187s elapsed)
STEP: Deleting pod pod1 in namespace services-7793
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7793 to expose endpoints map[pod2:[101]]
Aug 27 19:46:44.590: INFO: successfully validated that service multi-endpoint-test in namespace services-7793 exposes endpoints map[pod2:[101]] (22.896908ms elapsed)
STEP: Deleting pod pod2 in namespace services-7793
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7793 to expose endpoints map[]
Aug 27 19:46:44.615: INFO: successfully validated that service multi-endpoint-test in namespace services-7793 exposes endpoints map[] (9.108652ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:46:44.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7793" for this suite.
Aug 27 19:47:08.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:47:09.015: INFO: namespace services-7793 deletion completed in 24.343355639s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:30.896 seconds]
[sig-network] Services
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:47:09.016: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8885
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-8885
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 27 19:47:09.204: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 27 19:47:31.425: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.172.26 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8885 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 19:47:31.425: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 19:47:32.719: INFO: Found all expected endpoints: [netserver-0]
Aug 27 19:47:32.729: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.79.73 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8885 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 19:47:32.729: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 19:47:34.019: INFO: Found all expected endpoints: [netserver-1]
Aug 27 19:47:34.029: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.183.97 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8885 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 19:47:34.029: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
Aug 27 19:47:35.318: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:47:35.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8885" for this suite.
Aug 27 19:47:59.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:47:59.625: INFO: namespace pod-network-test-8885 deletion completed in 24.29354153s

• [SLOW TEST:50.609 seconds]
[sig-network] Networking
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:47:59.626: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5569
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:48:01.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5569" for this suite.
Aug 27 19:48:07.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:48:08.239: INFO: namespace emptydir-wrapper-5569 deletion completed in 6.294561091s

• [SLOW TEST:8.613 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:48:08.239: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5682
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 19:48:08.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98066eb4-c903-11e9-a9da-1ec1181dd093" in namespace "downward-api-5682" to be "success or failure"
Aug 27 19:48:08.443: INFO: Pod "downwardapi-volume-98066eb4-c903-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 8.220122ms
Aug 27 19:48:10.452: INFO: Pod "downwardapi-volume-98066eb4-c903-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017690455s
Aug 27 19:48:12.465: INFO: Pod "downwardapi-volume-98066eb4-c903-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030372234s
STEP: Saw pod success
Aug 27 19:48:12.465: INFO: Pod "downwardapi-volume-98066eb4-c903-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:48:12.474: INFO: Trying to get logs from node 10.135.206.197 pod downwardapi-volume-98066eb4-c903-11e9-a9da-1ec1181dd093 container client-container: <nil>
STEP: delete the pod
Aug 27 19:48:12.525: INFO: Waiting for pod downwardapi-volume-98066eb4-c903-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:48:12.533: INFO: Pod downwardapi-volume-98066eb4-c903-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:48:12.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5682" for this suite.
Aug 27 19:48:18.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:48:18.884: INFO: namespace downward-api-5682 deletion completed in 6.342517557s

• [SLOW TEST:10.645 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:48:18.884: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5119
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-9e60938d-c903-11e9-a9da-1ec1181dd093
STEP: Creating secret with name s-test-opt-upd-9e6093ed-c903-11e9-a9da-1ec1181dd093
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9e60938d-c903-11e9-a9da-1ec1181dd093
STEP: Updating secret s-test-opt-upd-9e6093ed-c903-11e9-a9da-1ec1181dd093
STEP: Creating secret with name s-test-opt-create-9e60941a-c903-11e9-a9da-1ec1181dd093
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:49:36.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5119" for this suite.
Aug 27 19:50:00.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:50:00.376: INFO: namespace secrets-5119 deletion completed in 24.347259972s

• [SLOW TEST:101.492 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:50:00.377: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9167
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 19:50:00.572: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dadcdab8-c903-11e9-a9da-1ec1181dd093" in namespace "downward-api-9167" to be "success or failure"
Aug 27 19:50:00.585: INFO: Pod "downwardapi-volume-dadcdab8-c903-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 13.370711ms
Aug 27 19:50:02.595: INFO: Pod "downwardapi-volume-dadcdab8-c903-11e9-a9da-1ec1181dd093": Phase="Running", Reason="", readiness=true. Elapsed: 2.022756449s
Aug 27 19:50:04.605: INFO: Pod "downwardapi-volume-dadcdab8-c903-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032650944s
STEP: Saw pod success
Aug 27 19:50:04.605: INFO: Pod "downwardapi-volume-dadcdab8-c903-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:50:04.615: INFO: Trying to get logs from node 10.134.235.118 pod downwardapi-volume-dadcdab8-c903-11e9-a9da-1ec1181dd093 container client-container: <nil>
STEP: delete the pod
Aug 27 19:50:04.666: INFO: Waiting for pod downwardapi-volume-dadcdab8-c903-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:50:04.677: INFO: Pod downwardapi-volume-dadcdab8-c903-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:50:04.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9167" for this suite.
Aug 27 19:50:10.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:50:11.046: INFO: namespace downward-api-9167 deletion completed in 6.35970595s

• [SLOW TEST:10.670 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:50:11.049: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1585
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 19:50:11.258: INFO: (0) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 14.977578ms)
Aug 27 19:50:11.268: INFO: (1) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.929044ms)
Aug 27 19:50:11.278: INFO: (2) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.948271ms)
Aug 27 19:50:11.289: INFO: (3) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.02175ms)
Aug 27 19:50:11.298: INFO: (4) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.758342ms)
Aug 27 19:50:11.309: INFO: (5) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.162126ms)
Aug 27 19:50:11.319: INFO: (6) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.91216ms)
Aug 27 19:50:11.329: INFO: (7) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.475133ms)
Aug 27 19:50:11.338: INFO: (8) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.35004ms)
Aug 27 19:50:11.348: INFO: (9) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.706769ms)
Aug 27 19:50:11.358: INFO: (10) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.083751ms)
Aug 27 19:50:11.368: INFO: (11) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.856235ms)
Aug 27 19:50:11.380: INFO: (12) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 11.380931ms)
Aug 27 19:50:11.390: INFO: (13) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.134981ms)
Aug 27 19:50:11.405: INFO: (14) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 15.271158ms)
Aug 27 19:50:11.415: INFO: (15) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.600429ms)
Aug 27 19:50:11.425: INFO: (16) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.371206ms)
Aug 27 19:50:11.436: INFO: (17) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 11.080749ms)
Aug 27 19:50:11.446: INFO: (18) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.769734ms)
Aug 27 19:50:11.457: INFO: (19) /api/v1/nodes/10.134.235.118:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.360832ms)
[AfterEach] version v1
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:50:11.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1585" for this suite.
Aug 27 19:50:17.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:50:17.790: INFO: namespace proxy-1585 deletion completed in 6.32517755s

• [SLOW TEST:6.741 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:50:17.791: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4016
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 19:50:17.991: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e53f429b-c903-11e9-a9da-1ec1181dd093" in namespace "projected-4016" to be "success or failure"
Aug 27 19:50:18.005: INFO: Pod "downwardapi-volume-e53f429b-c903-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 14.110467ms
Aug 27 19:50:20.016: INFO: Pod "downwardapi-volume-e53f429b-c903-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024803132s
Aug 27 19:50:22.027: INFO: Pod "downwardapi-volume-e53f429b-c903-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035268872s
STEP: Saw pod success
Aug 27 19:50:22.027: INFO: Pod "downwardapi-volume-e53f429b-c903-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:50:22.038: INFO: Trying to get logs from node 10.134.235.118 pod downwardapi-volume-e53f429b-c903-11e9-a9da-1ec1181dd093 container client-container: <nil>
STEP: delete the pod
Aug 27 19:50:22.085: INFO: Waiting for pod downwardapi-volume-e53f429b-c903-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:50:22.093: INFO: Pod downwardapi-volume-e53f429b-c903-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:50:22.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4016" for this suite.
Aug 27 19:50:28.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:50:28.417: INFO: namespace projected-4016 deletion completed in 6.313540629s

• [SLOW TEST:10.626 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:50:28.417: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3362
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 27 19:50:34.718: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 27 19:50:34.727: INFO: Pod pod-with-poststart-http-hook still exists
Aug 27 19:50:36.727: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 27 19:50:36.737: INFO: Pod pod-with-poststart-http-hook still exists
Aug 27 19:50:38.727: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 27 19:50:38.736: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:50:38.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3362" for this suite.
Aug 27 19:51:02.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:51:03.072: INFO: namespace container-lifecycle-hook-3362 deletion completed in 24.326613732s

• [SLOW TEST:34.655 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:51:03.073: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8452
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-003cc24b-c904-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume secrets
Aug 27 19:51:03.280: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-003de7af-c904-11e9-a9da-1ec1181dd093" in namespace "projected-8452" to be "success or failure"
Aug 27 19:51:03.297: INFO: Pod "pod-projected-secrets-003de7af-c904-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 16.081646ms
Aug 27 19:51:05.306: INFO: Pod "pod-projected-secrets-003de7af-c904-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025951438s
STEP: Saw pod success
Aug 27 19:51:05.306: INFO: Pod "pod-projected-secrets-003de7af-c904-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:51:05.315: INFO: Trying to get logs from node 10.135.206.197 pod pod-projected-secrets-003de7af-c904-11e9-a9da-1ec1181dd093 container secret-volume-test: <nil>
STEP: delete the pod
Aug 27 19:51:05.362: INFO: Waiting for pod pod-projected-secrets-003de7af-c904-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:51:05.370: INFO: Pod pod-projected-secrets-003de7af-c904-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:51:05.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8452" for this suite.
Aug 27 19:51:11.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:51:11.684: INFO: namespace projected-8452 deletion completed in 6.30470958s

• [SLOW TEST:8.611 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:51:11.684: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3738
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Aug 27 19:51:11.886: INFO: Waiting up to 5m0s for pod "var-expansion-055f13fe-c904-11e9-a9da-1ec1181dd093" in namespace "var-expansion-3738" to be "success or failure"
Aug 27 19:51:11.911: INFO: Pod "var-expansion-055f13fe-c904-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 24.654568ms
Aug 27 19:51:13.923: INFO: Pod "var-expansion-055f13fe-c904-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03745941s
Aug 27 19:51:15.933: INFO: Pod "var-expansion-055f13fe-c904-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046678363s
STEP: Saw pod success
Aug 27 19:51:15.933: INFO: Pod "var-expansion-055f13fe-c904-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:51:15.942: INFO: Trying to get logs from node 10.134.235.118 pod var-expansion-055f13fe-c904-11e9-a9da-1ec1181dd093 container dapi-container: <nil>
STEP: delete the pod
Aug 27 19:51:15.987: INFO: Waiting for pod var-expansion-055f13fe-c904-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:51:15.995: INFO: Pod var-expansion-055f13fe-c904-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:51:15.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3738" for this suite.
Aug 27 19:51:22.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:51:22.316: INFO: namespace var-expansion-3738 deletion completed in 6.311244702s

• [SLOW TEST:10.631 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:51:22.317: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9973
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 27 19:51:22.568: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-9973,SelfLink:/api/v1/namespaces/watch-9973/configmaps/e2e-watch-test-resource-version,UID:0bafa8c8-c904-11e9-9af8-1e15abefec17,ResourceVersion:43577,Generation:0,CreationTimestamp:2019-08-27 19:51:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 27 19:51:22.568: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-9973,SelfLink:/api/v1/namespaces/watch-9973/configmaps/e2e-watch-test-resource-version,UID:0bafa8c8-c904-11e9-9af8-1e15abefec17,ResourceVersion:43578,Generation:0,CreationTimestamp:2019-08-27 19:51:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:51:22.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9973" for this suite.
Aug 27 19:51:28.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:51:28.868: INFO: namespace watch-9973 deletion completed in 6.291736121s

• [SLOW TEST:6.552 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:51:28.868: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2099
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 27 19:51:32.128: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:51:32.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2099" for this suite.
Aug 27 19:51:56.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:51:56.532: INFO: namespace replicaset-2099 deletion completed in 24.360906847s

• [SLOW TEST:27.664 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:51:56.536: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1597
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 19:51:56.741: INFO: Waiting up to 5m0s for pod "downwardapi-volume-201b12cd-c904-11e9-a9da-1ec1181dd093" in namespace "downward-api-1597" to be "success or failure"
Aug 27 19:51:56.750: INFO: Pod "downwardapi-volume-201b12cd-c904-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 8.913014ms
Aug 27 19:51:58.760: INFO: Pod "downwardapi-volume-201b12cd-c904-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018924493s
STEP: Saw pod success
Aug 27 19:51:58.760: INFO: Pod "downwardapi-volume-201b12cd-c904-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:51:58.770: INFO: Trying to get logs from node 10.134.235.118 pod downwardapi-volume-201b12cd-c904-11e9-a9da-1ec1181dd093 container client-container: <nil>
STEP: delete the pod
Aug 27 19:51:58.814: INFO: Waiting for pod downwardapi-volume-201b12cd-c904-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:51:58.934: INFO: Pod downwardapi-volume-201b12cd-c904-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:51:58.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1597" for this suite.
Aug 27 19:52:04.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:52:05.264: INFO: namespace downward-api-1597 deletion completed in 6.31819678s

• [SLOW TEST:8.727 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 19:52:05.264: INFO: >>> kubeConfig: /tmp/kubeconfig-561368167
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2272
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-254e4db6-c904-11e9-a9da-1ec1181dd093
STEP: Creating a pod to test consume secrets
Aug 27 19:52:05.476: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-254fb902-c904-11e9-a9da-1ec1181dd093" in namespace "projected-2272" to be "success or failure"
Aug 27 19:52:05.487: INFO: Pod "pod-projected-secrets-254fb902-c904-11e9-a9da-1ec1181dd093": Phase="Pending", Reason="", readiness=false. Elapsed: 11.298299ms
Aug 27 19:52:07.496: INFO: Pod "pod-projected-secrets-254fb902-c904-11e9-a9da-1ec1181dd093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020735851s
STEP: Saw pod success
Aug 27 19:52:07.497: INFO: Pod "pod-projected-secrets-254fb902-c904-11e9-a9da-1ec1181dd093" satisfied condition "success or failure"
Aug 27 19:52:07.506: INFO: Trying to get logs from node 10.135.206.197 pod pod-projected-secrets-254fb902-c904-11e9-a9da-1ec1181dd093 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 27 19:52:07.557: INFO: Waiting for pod pod-projected-secrets-254fb902-c904-11e9-a9da-1ec1181dd093 to disappear
Aug 27 19:52:07.565: INFO: Pod pod-projected-secrets-254fb902-c904-11e9-a9da-1ec1181dd093 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 19:52:07.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2272" for this suite.
Aug 27 19:52:13.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 19:52:13.895: INFO: namespace projected-2272 deletion completed in 6.320343291s

• [SLOW TEST:8.631 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.6-beta.0.47+96fac5cd13a5dc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSAug 27 19:52:13.895: INFO: Running AfterSuite actions on all nodes
Aug 27 19:52:13.896: INFO: Running AfterSuite actions on node 1
Aug 27 19:52:13.896: INFO: Skipping dumping logs from cluster

Ran 204 of 3586 Specs in 5959.004 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3382 Skipped PASS

Ginkgo ran 1 suite in 1h39m20.530554736s
Test Suite Passed
